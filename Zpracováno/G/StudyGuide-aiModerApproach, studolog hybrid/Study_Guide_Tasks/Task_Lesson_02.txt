
START_INSTRUCTION
You are an expert AI tutor creating a comprehensive study guide.
Your task is to rewrite and expand the "Lesson Notes" provided below into a cohesive, readable study text.

1. **Structure:** Follow the exact structure and order of the topics in the "Lesson Notes". Do not reorder them.
2. **Enrichment:** Use the "Textbook Context" provided to expand on every bullet point in the notes.
    - If the notes mention a concept (e.g., "Rationality"), define it precisely using the textbook.
    - If the notes list algorithms (e.g., "A*", "Simulated Annealing"), explain how they work, their complexity, and pros/cons using the textbook details.
    - Add examples from the textbook where appropriate.
3. **Tone:** Academic but accessible study material. Not just bullet points—write paragraphs where necessary to explain complex ideas.
4. **Language:** The output must be in **Czech** (since the lesson notes are in Czech), but you can keep standard English AI terminology (like "Overfitting") in parentheses.

--- LESSON NOTES (Skeleton) ---
Vyhodnocování inteligence umělých systémů

Umí AI myslet a jak to poznat?
Slabá AI - pro poznání lidské mysli, umožnuje simulovat mentální schopnosti
Specifická AI - AI jako tvorba programu pro řešení specifických úloh
Silná AI - má rozumění a další kognitivní stavy, snaha vytvořit stroj srovnatelný s člověkem
Obecná AI - tvorba programů pro obecné řešení úloh a obecné inteligentní jednání
Rané Descartes
Metodologický skepticizmus
○ Skrze pochybování k pevným principům vědění
•
Racionalizmus x Empirizmus
○ rozum jako rozhodující zdroj vědění
○ smysly klamou
•
Dualizmus
○ nemateriální mysl
○ materiální tělo a svět
•
Publikace "Rozprava o metodě"
○ Rozporuje schopnost strojů schopnost rozumné řeči
Strojům chybí univerzálnost myšlení
▪ Určité věci umí dobře, možná i lépe než člověk. Na jiných však pohoří.
○
•
Turing (článek + přednáška)
• Mohou stroje myslet?
• Test "imitační hra" (také "turingův test")
• Otázka strojového učení by měla nahradit otázka turingova testu
Imitační hra
○ Mohou stroje myslet?
•
Stroje zapojené do imitační hry
○ Co můžeme považovat za stroj?
Turing se nakonec omezuje na digitální počítače.
Paměť - ukládání informací
□ Paměť by teoreticky měla být nekonečná
▪
▪ Výkonná jednotka - provádění jednotlivých operací
Řídící jednotka
□ Zajišťuje správné provedení instrukcí
▪
○
Instrukce
Typické instrukce
□ Sečti X a Y a ulož na pozici Z
▪
Speciální instrukce
□ Cykly
□ Podmínky
□ Odkazování na instrukce v paměti
▪
▪ Instrukce je stroji předávána v číselné podobě
○
Stroje s diskrétním stavem
▪ Turing uznává, že takové stroje v realitě neexistují
○
•
Námitky
Teologická námitka
▪ "myšlení je funkce lidské neumírající duše"
○
"strkání hlavy do písku"
▪ Myslící stroje? To by bylo strašné! Stroje tedy nemyslí.
○
°argument z extrasenzorického vnímání
Možné důsledky úspěchu ve hře
○ Stroj je srovnatelný s člověkem
○ Stroj je lepší než skutečný hráč
•
Myšlenkový experiment s čínským pokojem
• Jaké by bylo, kdyby mysl fungovala jako program?
Člověk neumí čínsky, je zavřený v pokoji s knihou s instrukcemi
○ Zvenku vypadá jako instance programu pro čínštinu
Zevnitřsearl čínštině nerozumí, jen manipuluje se znaky
•
• => Searl se nenaučil čínsky -> ani stroj čínsky neumí
Agent -> interakce => prostředí
Interakce agenta s prostředím - Pozorování -> akce -> odměna
Agent (pi) -
○ fce. Dostane interakční sekvenci -> namapuje na následující akci
○ Podmíněná pravděpodobnostní míra nad prostorem akcí
Prostředí (mí)
○ Fce. Přiřazující historii interakcí nějakou odměnu a pozorování
○ Vyčíslitelná podmíněná pravděpodobnostní míra nad prostorem odměn a pozorování

--- TEXTBOOK CONTEXT (Source Material) ---


--- BOOK CHAPTER: 2_Intelligent_Agents ---

2 INTELLIGENT AGENTS
In which we discuss the nature of agents, perfect or otherwise, the diversity of
environments, and the resulting menagerie of agent types.
Chapter 1 identiﬁed the concept of rational agents as central to our approach to artiﬁcial
intelligence. In this chapter, we make this notion more concrete. We will see that the concept
of rationality can be applied to a wide variety of agents operating in any imaginable environ-
ment. Our plan in this book is to use this concept to develop a small set of design principles
for building successful agents—systems that can reasonably be called intelligent.
We begin by examining agents, environments, and the coupling between them. The
observation that some agents behave better than others leads naturally to the idea of a rational
agent—one that behaves as well as possible. How well an agent can behave depends on
the nature of the environment; some environments are more difﬁcult than others. We give a
crude categorization of environments and show how properties of an environment inﬂuence
the design of suitable agents for that environment. We describe a number of basic “skeleton”
agent designs, which we ﬂesh out in the rest of the book.
2.1 A GENTS AND ENVIRONMENTS
An agent is anything that can be viewed as perceiving its environment through sensors andENVIRONMENT
SENSOR acting upon that environment through actuators. This simple idea is illustrated in Figure 2.1.
ACTUATOR A human agent has eyes, ears, and other organs for sensors and hands, legs, vocal tract, and so
on for actuators. A robotic agent might have cameras and infrared range ﬁnders for sensors
and various motors for actuators. A software agent receives keystrokes, ﬁle contents, and
network packets as sensory inputs and acts on the environment by displaying on the screen,
writing ﬁles, and sending network packets.
We use the termpercept to refer to the agent’s perceptual inputs at any given instant. An
PERCEPT
agent’s percept sequence is the complete history of everything the agent has ever perceived.PERCEPT SEQUENCE
In general, an agent’s choice of action at any given instant can depend on the entire percept
sequence observed to date, but not on anything it hasn’t perceived. By specifying the agent’s
choice of action for every possible percept sequence, we have said more or less everything
34
Section 2.1. Agents and Environments 35
Agent Sensors
Actuators
Environment
Percepts
Actions
?
Figure 2.1 Agents interact with environments through sensors and actuators.
there is to say about the agent. Mathematically speaking, we say that an agent’s behavior is
described by the agent function that maps any given percept sequence to an action.AGENT FUNCTION
We can imagine tabulating the agent function that describes any given agent; for most
agents, this would be a very large table—inﬁnite, in fact, unless we place a bound on the
length of percept sequences we want to consider. Given an agent to experiment with, we can,
in principle, construct this table by trying out all possible percept sequences and recording
which actions the agent does in response.
1 The table is, of course, anexternal characterization
of the agent. Internally, the agent function for an artiﬁcial agent will be implemented by an
agent program. It is important to keep these two ideas distinct. The agent function is anAGENT PROGRAM
abstract mathematical description; the agent program is a concrete implementation, running
within some physical system.
To illustrate these ideas, we use a very simple example—the vacuum-cleaner world
shown in Figure 2.2. This world is so simple that we can describe everything that happens;
it’s also a made-up world, so we can invent many variations. This particular world has just two
locations: squares A and B. The vacuum agent perceives which square it is in and whether
there is dirt in the square. It can choose to move left, move right, suck up the dirt, or do
nothing. One very simple agent function is the following: if the current square is dirty, then
suck; otherwise, move to the other square. A partial tabulation of this agent function is shown
in Figure 2.3 and an agent program that implements it appears in Figure 2.8 on page 48.
Looking at Figure 2.3, we see that various vacuum-world agents can be deﬁned simply
by ﬁlling in the right-hand column in various ways. The obvious question, then, is this: What
is the right way to ﬁll out the table? In other words, what makes an agent good or bad,
intelligent or stupid? We answer these questions in the next section.
1 If the agent uses some randomization to choose its actions, then we would have to try each sequence many
times to identify the probability of each action. One might imagine that acting randomly is rather silly, but we
show later in this chapter that it can be very intelligent.
36 Chapter 2. Intelligent Agents
AB
Figure 2.2 A vacuum-cleaner world with just two locations.
Percept sequence
 Action
[A,Clean]
 Right
[A,Dirty]
 Suck
[B, Clean]
 Left
[B, Dirty]
 Suck
[A,Clean], [A,Clean]
 Right
[A,Clean], [A,Dirty]
 Suck
...
 ...
[A,Clean], [A,Clean], [A,Clean]
 Right
[A,Clean], [A,Clean], [A,Dirty]
 Suck
...
 ...
Figure 2.3 Partial tabulation of a simple agent function for the vacuum-cleaner world
shown in Figure 2.2.
Before closing this section, we should emphasize that the notion of an agent is meant to
be a tool for analyzing systems, not an absolute characterization that divides the world into
agents and non-agents. One could view a hand-held calculator as an agent that chooses the
action of displaying “4” when given the percept sequence “2 + 2 =,” but such an analysis
would hardly aid our understanding of the calculator. In a sense, all areas of engineering can
be seen as designing artifacts that interact with the world; AI operates at (what the authors
consider to be) the most interesting end of the spectrum, where the artifacts have signiﬁcant
computational resources and the task environment requires nontrivial decision making.
2.2 G OOD BEHA VIOR:T HE CONCEPT OF RATIONALITY
A rational agent is one that does the right thing—conceptually speaking, every entry in theRA TIONAL AGENT
table for the agent function is ﬁlled out correctly. Obviously, doing the right thing is better
than doing the wrong thing, but what does it mean to do the right thing?
Section 2.2. Good Behavior: The Concept of Rationality 37
We answer this age-old question in an age-old way: by considering the consequences
of the agent’s behavior. When an agent is plunked down in an environment, it generates a
sequence of actions according to the percepts it receives. This sequence of actions causes the
environment to go through a sequence of states. If the sequence is desirable, then the agent
has performed well. This notion of desirability is captured by a performance measure that
PERFORMANCE
MEASURE
evaluates any given sequence of environment states.
Notice that we said environment states, not agent states. If we deﬁne success in terms
of agent’s opinion of its own performance, an agent could achieve perfect rationality simply
by deluding itself that its performance was perfect. Human agents in particular are notorious
for “sour grapes”—believing they did not really want something (e.g., a Nobel Prize) after
not getting it.
Obviously, there is not one ﬁxed performance measure for all tasks and agents; typically,
a designer will devise one appropriate to the circumstances. This is not as easy as it sounds.
Consider, for example, the vacuum-cleaner agent from the preceding section. We might
propose to measure performance by the amount of dirt cleaned up in a single eight-hour shift.
With a rational agent, of course, what you ask for is what you get. A rational agent can
maximize this performance measure by cleaning up the dirt, then dumping it all on the ﬂoor,
then cleaning it up again, and so on. A more suitable performance measure would reward the
agent for having a clean ﬂoor. For example, one point could be awarded for each clean square
at each time step (perhaps with a penalty for electricity consumed and noise generated). As
a general rule, it is better to design performance measures according to what one actually
wants in the environment, rather than according to how one thinks the agent should behave.
Even when the obvious pitfalls are avoided, there remain some knotty issues to untangle.
For example, the notion of “clean ﬂoor” in the preceding paragraph is based on average
cleanliness over time. Yet the same average cleanliness can be achieved by two different
agents, one of which does a mediocre job all the time while the other cleans energetically but
takes long breaks. Which is preferable might seem to be a ﬁne point of janitorial science, but
in fact it is a deep philosophical question with far-reaching implications. Which is better—
a reckless life of highs and lows, or a safe but humdrum existence? Which is better—an
economy where everyone lives in moderate poverty, or one in which some live in plenty
while others are very poor? We leave these questions as an exercise for the diligent reader.
2.2.1 Rationality
What is rational at any given time depends on four things:
•The performance measure that deﬁnes the criterion of success.
•The agent’s prior knowledge of the environment.
•The actions that the agent can perform.
•The agent’s percept sequence to date.
This leads to a deﬁnition of a rational agent :DEFINITION OF A
RA TIONAL AGENT
F or each possible percept sequence, a rational agent should select an action that is ex-
pected to maximize its performance measure, given the evidence provided by the percept
sequence and whatever built-in knowledge the agent has.
38 Chapter 2. Intelligent Agents
Consider the simple vacuum-cleaner agent that cleans a square if it is dirty and moves to the
other square if not; this is the agent function tabulated in Figure 2.3. Is this a rational agent?
That depends! First, we need to say what the performance measure is, what is known about
the environment, and what sensors and actuators the agent has. Let us assume the following:
•The performance measure awards one point for each clean square at each time step,
over a “lifetime” of 1000 time steps.
•The “geography” of the environment is known ap r i o r i(Figure 2.2) but the dirt distri-
bution and the initial location of the agent are not. Clean squares stay clean and sucking
cleans the current square. The Left and Right actions move the agent left and right
except when this would take the agent outside the environment, in which case the agent
remains where it is.
•The only available actions are Left, Right,a n dSuck.
•The agent correctly perceives its location and whether that location contains dirt.
We claim that under these circumstances the agent is indeed rational; its expected perfor-
mance is at least as high as any other agent’s. Exercise 2.2 asks you to prove this.
One can see easily that the same agent would be irrational under different circum-
stances. For example, once all the dirt is cleaned up, the agent will oscillate needlessly back
and forth; if the performance measure includes a penalty of one point for each movement left
or right, the agent will fare poorly. A better agent for this case would do nothing once it is
sure that all the squares are clean. If clean squares can become dirty again, the agent should
occasionally check and re-clean them if needed. If the geography of the environment is un-
known, the agent will need to explore it rather than stick to squares A and B.E x e r c i s e 2 . 2
asks you to design agents for these cases.
2.2.2 Omniscience, learning, and autonomy
We need to be careful to distinguish between rationality and omniscience. An omniscientOMNISCIENCE
agent knows the actual outcome of its actions and can act accordingly; but omniscience is
impossible in reality. Consider the following example: I am walking along the Champs
Elys´ees one day and I see an old friend across the street. There is no trafﬁc nearby and I’m
not otherwise engaged, so, being rational, I start to cross the street. Meanwhile, at 33,000
feet, a cargo door falls off a passing airliner, 2 and before I make it to the other side of the
street I am ﬂattened. Was I irrational to cross the street? It is unlikely that my obituary would
read “Idiot attempts to cross street.”
This example shows that rationality is not the same as perfection. Rationality max-
imizes expected performance, while perfection maximizes actual performance. Retreating
from a requirement of perfection is not just a question of being fair to agents. The point is
that if we expect an agent to do what turns out to be the best action after the fact, it will be
impossible to design an agent to fulﬁll this speciﬁcation—unless we improve the performance
of crystal balls or time machines.
2 See N. Henderson, “New door latches urged for Boeing 747 jumbo jets,” Washington Post, August 24, 1989.
Section 2.2. Good Behavior: The Concept of Rationality 39
Our deﬁnition of rationality does not require omniscience, then, because the rational
choice depends only on the percept sequence to date. We must also ensure that we haven’t
inadvertently allowed the agent to engage in decidedly underintelligent activities. For exam-
ple, if an agent does not look both ways before crossing a busy road, then its percept sequence
will not tell it that there is a large truck approaching at high speed. Does our deﬁnition of
rationality say that it’s now OK to cross the road? Far from it! First, it would not be rational
to cross the road given this uninformative percept sequence: the risk of accident from cross-
ing without looking is too great. Second, a rational agent should choose the “looking” action
before stepping into the street, because looking helps maximize the expected performance.
Doing actions in order to modify future percepts —sometimes called information gather-
ing—is an important part of rationality and is covered in depth in Chapter 16. A second
INFORMA TION
GA THERING
example of information gathering is provided by the exploration that must be undertaken byEXPLORA TION
a vacuum-cleaning agent in an initially unknown environment.
Our deﬁnition requires a rational agent not only to gather information but also to learnLEARNING
as much as possible from what it perceives. The agent’s initial conﬁguration could reﬂect
some prior knowledge of the environment, but as the agent gains experience this may be
modiﬁed and augmented. There are extreme cases in which the environment is completely
known ap r i o r i. In such cases, the agent need not perceive or learn; it simply acts correctly.
Of course, such agents are fragile. Consider the lowly dung beetle. After digging its nest and
laying its eggs, it fetches a ball of dung from a nearby heap to plug the entrance. If the ball of
dung is removed from its grasp en route, the beetle continues its task and pantomimes plug-
ging the nest with the nonexistent dung ball, never noticing that it is missing. Evolution has
built an assumption into the beetle’s behavior, and when it is violated, unsuccessful behavior
results. Slightly more intelligent is the sphex wasp. The female sphex will dig a burrow, go
out and sting a caterpillar and drag it to the burrow, enter the burrow again to check all is
well, drag the caterpillar inside, and lay its eggs. The caterpillar serves as a food source when
the eggs hatch. So far so good, but if an entomologist moves the caterpillar a few inches
away while the sphex is doing the check, it will revert to the “drag” step of its plan and will
continue the plan without modiﬁcation, even after dozens of caterpillar-moving interventions.
The sphex is unable to learn that its innate plan is failing, and thus will not change it.
To the extent that an agent relies on the prior knowledge of its designer rather than
on its own percepts, we say that the agent lacks autonomy. A rational agent should be
AUTONOMY
autonomous—it should learn what it can to compensate for partial or incorrect prior knowl-
edge. For example, a vacuum-cleaning agent that learns to foresee where and when additional
dirt will appear will do better than one that does not. As a practical matter, one seldom re-
quires complete autonomy from the start: when the agent has had little or no experience, it
would have to act randomly unless the designer gave some assistance. So, just as evolution
provides animals with enough built-in reﬂexes to survive long enough to learn for themselves,
it would be reasonable to provide an artiﬁcial intelligent agent with some initial knowledge
as well as an ability to learn. After sufﬁcient experience of its environment, the behavior
of a rational agent can become effectively independent of its prior knowledge. Hence, the
incorporation of learning allows one to design a single rational agent that will succeed in a
vast variety of environments.
40 Chapter 2. Intelligent Agents
2.3 T HE NATUR E OF ENVIRONMENTS
Now that we have a deﬁnition of rationality, we are almost ready to think about building
rational agents. First, however, we must think about task environments, which are essen-TASK ENVIRONMENT
tially the “problems” to which rational agents are the “solutions.” We begin by showing how
to specify a task environment, illustrating the process with a number of examples. We then
show that task environments come in a variety of ﬂavors. The ﬂavor of the task environment
directly affects the appropriate design for the agent program.
2.3.1 Specifying the task environment
In our discussion of the rationality of the simple vacuum-cleaner agent, we had to specify
the performance measure, the environment, and the agent’s actuators and sensors. We group
all these under the heading of the task environment. For the acronymically minded, we call
this the PEAS (Performance, Environment, Actuators, Sensors) description. In designing an
PEAS
agent, the ﬁrst step must always be to specify the task environment as fully as possible.
The vacuum world was a simple example; let us consider a more complex problem: an
automated taxi driver. We should point out, before the reader becomes alarmed, that a fully
automated taxi is currently somewhat beyond the capabilities of existing technology. (page 28
describes an existing driving robot.) The full driving task is extremely open-ended.T h e r e i s
no limit to the novel combinations of circumstances that can arise—another reason we chose
it as a focus for discussion. Figure 2.4 summarizes the PEAS description for the taxi’s task
environment. We discuss each element in more detail in the following paragraphs.
Agent Type
 Performance
Measure
Environment
 Actuators
 Sensors
Taxi driver
 Safe, fast, legal,
comfortable trip,
maximize proﬁts
Roads, other
trafﬁc,
pedestrians,
customers
Steering,
accelerator,
brake, signal,
horn, display
Cameras, sonar,
speedometer,
GPS, odometer,
accelerometer,
engine sensors,
keyboard
Figure 2.4 PEAS description of the task environment for an automated taxi.
First, what is the performance measure to which we would like our automated driver
to aspire? Desirable qualities include getting to the correct destination; minimizing fuel con-
sumption and wear and tear; minimizing the trip time or cost; minimizing violations of trafﬁc
laws and disturbances to other drivers; maximizing safety and passenger comfort; maximiz-
ing proﬁts. Obviously, some of these goals conﬂict, so tradeoffs will be required.
Next, what is the driving environment that the taxi will face? Any taxi driver must
deal with a variety of roads, ranging from rural lanes and urban alleys to 12-lane freeways.
The roads contain other trafﬁc, pedestrians, stray animals, road works, police cars, puddles,
Section 2.3. The Nature of Environments 41
and potholes. The taxi must also interact with potential and actual passengers. There are also
some optional choices. The taxi might need to operate in Southern California, where snow
is seldom a problem, or in Alaska, where it seldom is not. It could always be driving on the
right, or we might want it to be ﬂexible enough to drive on the left when in Britain or Japan.
Obviously, the more restricted the environment, the easier the design problem.
The actuators for an automated taxi include those available to a human driver: control
over the engine through the accelerator and control over steering and braking. In addition, it
will need output to a display screen or voice synthesizer to talk back to the passengers, and
perhaps some way to communicate with other vehicles, politely or otherwise.
The basic sensors for the taxi will include one or more controllable video cameras so
that it can see the road; it might augment these with infrared or sonar sensors to detect dis-
tances to other cars and obstacles. To avoid speeding tickets, the taxi should have a speedome-
ter, and to control the vehicle properly, especially on curves, it should have an accelerometer.
To determine the mechanical state of the vehicle, it will need the usual array of engine, fuel,
and electrical system sensors. Like many human drivers, it might want a global positioning
system (GPS) so that it doesn’t get lost. Finally, it will need a keyboard or microphone for
the passenger to request a destination.
In Figure 2.5, we have sketched the basic PEAS elements for a number of additional
agent types. Further examples appear in Exercise 2.4. It may come as a surprise to some read-
ers that our list of agent types includes some programs that operate in the entirely artiﬁcial
environment deﬁned by keyboard input and character output on a screen. “Surely,” one might
say, “this is not a real environment, is it?” In fact, what matters is not the distinction between
“real” and “artiﬁcial” environments, but the complexity of the relationship among the behav-
ior of the agent, the percept sequence generated by the environment, and the performance
measure. Some “real” environments are actually quite simple. For example, a robot designed
to inspect parts as they come by on a conveyor belt can make use of a number of simplifying
assumptions: that the lighting is always just so, that the only thing on the conveyor belt will
be parts of a kind that it knows about, and that only two actions (accept or reject) are possible.
In contrast, some software agents (or software robots or softbots) exist in rich, unlim-
SOFTWARE AGENT
SOFTBOT ited domains. Imagine a softbot Web site operator designed to scan Internet news sources and
show the interesting items to its users, while selling advertising space to generate revenue.
To do well, that operator will need some natural language processing abilities, it will need
to learn what each user and advertiser is interested in, and it will need to change its plans
dynamically—for example, when the connection for one news source goes down or when a
new one comes online. The Internet is an environment whose complexity rivals that of the
physical world and whose inhabitants include many artiﬁcial and human agents.
2.3.2 Properties of task environments
The range of task environments that might arise in AI is obviously vast. We can, however,
identify a fairly small number of dimensions along which task environments can be catego-
rized. These dimensions determine, to a large extent, the appropriate agent design and the
applicability of each of the principal families of techniques for agent implementation. First,
42 Chapter 2. Intelligent Agents
Agent Type
 Performance
Measure
Environment
 Actuators
 Sensors
Medical
diagnosis system
Healthy patient,
reduced costs
Patient, hospital,
staff
Display of
questions, tests,
diagnoses,
treatments,
referrals
Keyboard entry
of symptoms,
ﬁndings, patient’s
answers
Satellite image
analysis system
Correct image
categorization
Downlink from
orbiting satellite
Display of scene
categorization
Color pixel
arrays
Part-picking
robot
Percentage of
parts in correct
bins
Conveyor belt
with parts; bins
Jointed arm and
hand
Camera, joint
angle sensors
Reﬁnery
controller
Purity, yield,
safety
Reﬁnery,
operators
Valves, pumps,
heaters, displays
Temperature,
pressure,
chemical sensors
Interactive
English tutor
Student’s score
on test
Set of students,
testing agency
Display of
exercises,
suggestions,
corrections
Keyboard entry
Figure 2.5 Examples of agent types and their PEAS descriptions.
we list the dimensions, then we analyze several task environments to illustrate the ideas. The
deﬁnitions here are informal; later chapters provide more precise statements and examples of
each kind of environment.
Fully observable vs. partially observable: If an agent’s sensors give it access to the
FULL Y OBSERVABLE
PA RT I A L LY
OBSERVABLE complete state of the environment at each point in time, then we say that the task environ-
ment is fully observable. A task environment is effectively fully observable if the sensors
detect all aspects that are relevant to the choice of action; relevance, in turn, depends on the
performance measure. Fully observable environments are convenient because the agent need
not maintain any internal state to keep track of the world. An environment might be partially
observable because of noisy and inaccurate sensors or because parts of the state are simply
missing from the sensor data—for example, a vacuum agent with only a local dirt sensor
cannot tell whether there is dirt in other squares, and an automated taxi cannot see what other
drivers are thinking. If the agent has no sensors at all then the environment is unobserv-
able. One might think that in such cases the agent’s plight is hopeless, but, as we discuss in
UNOBSERVABLE
Chapter 4, the agent’s goals may still be achievable, sometimes with certainty.
Single agent vs. multiagent: The distinction between single-agent and multiagent en-SINGLE AGENT
MULTIAGENT
Section 2.3. The Nature of Environments 43
vironments may seem simple enough. For example, an agent solving a crossword puzzle by
itself is clearly in a single-agent environment, whereas an agent playing chess is in a two-
agent environment. There are, however, some subtle issues. First, we have described how an
entity may be viewed as an agent, but we have not explained which entities must be viewed
as agents. Does an agent A (the taxi driver for example) have to treat an object B (another
vehicle) as an agent, or can it be treated merely as an object behaving according to the laws of
physics, analogous to waves at the beach or leaves blowing in the wind? The key distinction
is whether B’s behavior is best described as maximizing a performance measure whose value
depends on agent A’s behavior. For example, in chess, the opponent entity B is trying to
maximize its performance measure, which, by the rules of chess, minimizes agent A’s per-
formance measure. Thus, chess is a competitive multiagent environment. In the taxi-driving
COMPETITIVE
environment, on the other hand, avoiding collisions maximizes the performance measure of
all agents, so it is a partially cooperative multiagent environment. It is also partially com-COOPERA TIVE
petitive because, for example, only one car can occupy a parking space. The agent-design
problems in multiagent environments are often quite different from those in single-agent en-
vironments; for example, communication often emerges as a rational behavior in multiagent
environments; in some competitive environments, randomized behavior is rational because
it avoids the pitfalls of predictability.
Deterministic vs. stochastic. If the next state of the environment is completely deter-
DETERMINISTIC
STOCHASTIC mined by the current state and the action executed by the agent, then we say the environment
is deterministic; otherwise, it is stochastic. In principle, an agent need not worry about uncer-
tainty in a fully observable, deterministic environment. (In our deﬁnition, we ignore uncer-
tainty that arises purely from the actions of other agents in a multiagent environment; thus,
a game can be deterministic even though each agent may be unable to predict the actions of
the others.) If the environment is partially observable, however, then it could appear to be
stochastic. Most real situations are so complex that it is impossible to keep track of all the
unobserved aspects; for practical purposes, they must be treated as stochastic. Taxi driving is
clearly stochastic in this sense, because one can never predict the behavior of trafﬁc exactly;
moreover, one’s tires blow out and one’s engine seizes up without warning. The vacuum
world as we described it is deterministic, but variations can include stochastic elements such
as randomly appearing dirt and an unreliable suction mechanism (Exercise 2.13). We say an
environment is uncertain if it is not fully observable or not deterministic. One ﬁnal note:
UNCERT AIN
our use of the word “stochastic” generally implies that uncertainty about outcomes is quan-
tiﬁed in terms of probabilities; a nondeterministic environment is one in which actions areNONDETERMINISTIC
characterized by their possible outcomes, but no probabilities are attached to them. Nonde-
terministic environment descriptions are usually associated with performance measures that
require the agent to succeed for all possible outcomes of its actions.
Episodic vs. sequential: In an episodic task environment, the agent’s experience is
EPISODIC
SEQUENTIAL divided into atomic episodes. In each episode the agent receives a percept and then performs
a single action. Crucially, the next episode does not depend on the actions taken in previous
episodes. Many classiﬁcation tasks are episodic. For example, an agent that has to spot
defective parts on an assembly line bases each decision on the current part, regardless of
previous decisions; moreover, the current decision doesn’t affect whether the next part is
44 Chapter 2. Intelligent Agents
defective. In sequential environments, on the other hand, the current decision could affect
all future decisions.3 Chess and taxi driving are sequential: in both cases, short-term actions
can have long-term consequences. Episodic environments are much simpler than sequential
environments because the agent does not need to think ahead.
Static vs. dynamic: If the environment can change while an agent is deliberating, then
STA TIC
DYNAMIC we say the environment is dynamic for that agent; otherwise, it is static. Static environments
are easy to deal with because the agent need not keep looking at the world while it is deciding
on an action, nor need it worry about the passage of time. Dynamic environments, on the
other hand, are continuously asking the agent what it wants to do; if it hasn’t decided yet,
that counts as deciding to do nothing. If the environment itself does not change with the
passage of time but the agent’s performance score does, then we say the environment is
semidynamic. Taxi driving is clearly dynamic: the other cars and the taxi itself keep moving
SEMIDYNAMIC
while the driving algorithm dithers about what to do next. Chess, when played with a clock,
is semidynamic. Crossword puzzles are static.
Discrete vs. continuous: The discrete/continuous distinction applies to the state of theDISCRETE
CONTINUOUS environment, to the way time is handled, and to the percepts and actions of the agent. For
example, the chess environment has a ﬁnite number of distinct states (excluding the clock).
Chess also has a discrete set of percepts and actions. Taxi driving is a continuous-state and
continuous-time problem: the speed and location of the taxi and of the other vehicles sweep
through a range of continuous values and do so smoothly over time. Taxi-driving actions are
also continuous (steering angles, etc.). Input from digital cameras is discrete, strictly speak-
ing, but is typically treated as representing continuously varying intensities and locations.
Known vs. unknown: Strictly speaking, this distinction refers not to the environment
KNOWN
UNKNOWN itself but to the agent’s (or designer’s) state of knowledge about the “laws of physics” of
the environment. In a known environment, the outcomes (or outcome probabilities if the
environment is stochastic) for all actions are given. Obviously, if the environment is unknown,
the agent will have to learn how it works in order to make good decisions. Note that the
distinction between known and unknown environments is not the same as the one between
fully and partially observable environments. It is quite possible for a known environment
to be partially observable—for example, in solitaire card games, I know the rules but am
still unable to see the cards that have not yet been turned over. Conversely, an unknown
environment can be fully observable—in a new video game, the screen may show the entire
game state but I still don’t know what the buttons do until I try them.
As one might expect, the hardest case is partially observable, multiagent, stochastic,
sequential, dynamic, continuous,a n dunknown. Taxi driving is hard in all these senses, except
that for the most part the driver’s environment is known. Driving a rented car in a new country
with unfamiliar geography and trafﬁc laws is a lot more exciting.
Figure 2.6 lists the properties of a number of familiar environments. Note that the
answers are not always cut and dried. For example, we describe the part-picking robot as
episodic, because it normally considers each part in isolation. But if one day there is a large
3 The word “sequential” is also used in computer science as the antonym of “parallel.” The two meanings are
largely unrelated.
Section 2.3. The Nature of Environments 45
Task Environment
 Observable Agents Deterministic Episodic Static Discrete
Crossword puzzle
 Fully Single Deterministic Sequential Static Discrete
Chess with a clock
 Fully Multi Deterministic Sequential Semi Discrete
Poker
 Partially Multi Stochastic Sequential Static Discrete
Backgammon
 Fully Multi Stochastic Sequential Static Discrete
Taxi driving
 Partially Multi Stochastic Sequential Dynamic Continuous
Medical diagnosis
 Partially Single Stochastic Sequential Dynamic Continuous
Image analysis
 Fully Single Deterministic Episodic Semi Continuous
Part-picking robot
 Partially Single Stochastic Episodic Dynamic Continuous
Reﬁnery controller
 Partially Single Stochastic Sequential Dynamic Continuous
Interactive English tutor
 Partially Multi Stochastic Sequential Dynamic Discrete
Figure 2.6 Examples of task environments and their characteristics.
batch of defective parts, the robot should learn from several observations that the distribution
of defects has changed, and should modify its behavior for subsequent parts. We have not
included a “known/unknown” column because, as explained earlier, this is not strictly a prop-
erty of the environment. For some environments, such as chess and poker, it is quite easy to
supply the agent with full knowledge of the rules, but it is nonetheless interesting to consider
how an agent might learn to play these games without such knowledge.
Several of the answers in the table depend on how the task environment is deﬁned. We
have listed the medical-diagnosis task as single-agent because the disease process in a patient
is not proﬁtably modeled as an agent; but a medical-diagnosis system might also have to
deal with recalcitrant patients and skeptical staff, so the environment could have a multiagent
aspect. Furthermore, medical diagnosis is episodic if one conceives of the task as selecting a
diagnosis given a list of symptoms; the problem is sequential if the task can include proposing
a series of tests, evaluating progress over the course of treatment, and so on. Also, many
environments are episodic at higher levels than the agent’s individual actions. For example,
a chess tournament consists of a sequence of games; each game is an episode because (by
and large) the contribution of the moves in one game to the agent’s overall performance is
not affected by the moves in its previous game. On the other hand, decision making within a
single game is certainly sequential.
The code repository associated with this book (aima.cs.berkeley.edu) includes imple-
mentations of a number of environments, together with a general-purpose environment simu-
lator that places one or more agents in a simulated environment, observes their behavior over
time, and evaluates them according to a given performance measure. Such experiments are
often carried out not for a single environment but for many environments drawn from an en-
vironment class. For example, to evaluate a taxi driver in simulated trafﬁc, we would want to
ENVIRONMENT
CLASS
run many simulations with different trafﬁc, lighting, and weather conditions. If we designed
the agent for a single scenario, we might be able to take advantage of speciﬁc properties
of the particular case but might not identify a good design for driving in general. For this
46 Chapter 2. Intelligent Agents
reason, the code repository also includes an environment generator for each environmentENVIRONMENT
GENERA TOR
class that selects particular environments (with certain likelihoods) in which to run the agent.
For example, the vacuum environment generator initializes the dirt pattern and agent location
randomly. We are then interested in the agent’s average performance over the environment
class. A rational agent for a given environment class maximizes this average performance.
Exercises 2.8 to 2.13 take you through the process of developing an environment class and
evaluating various agents therein.
2.4 T HE STRUCTURE OF AGENTS
So far we have talked about agents by describingbehavior—the action that is performed after
any given sequence of percepts. Now we must bite the bullet and talk about how the insides
work. The job of AI is to design an agent program that implements the agent function—
AGENT PROGRAM
the mapping from percepts to actions. We assume this program will run on some sort of
computing device with physical sensors and actuators—we call this the architecture:ARCHITECTURE
agent = architecture+ program .
Obviously, the program we choose has to be one that is appropriate for the architecture. If the
program is going to recommend actions like Walk, the architecture had better have legs. The
architecture might be just an ordinary PC, or it might be a robotic car with several onboard
computers, cameras, and other sensors. In general, the architecture makes the percepts from
the sensors available to the program, runs the program, and feeds the program’s action choices
to the actuators as they are generated. Most of this book is about designing agent programs,
although Chapters 24 and 25 deal directly with the sensors and actuators.
2.4.1 Agent programs
The agent programs that we design in this book all have the same skeleton: they take the
current percept as input from the sensors and return an action to the actuators.
4 Notice the
difference between the agent program, which takes the current percept as input, and the agent
function, which takes the entire percept history. The agent program takes just the current
percept as input because nothing more is available from the environment; if the agent’s actions
need to depend on the entire percept sequence, the agent will have to remember the percepts.
We describe the agent programs in the simple pseudocode language that is deﬁned in
Appendix B. (The online code repository contains implementations in real programming
languages.) For example, Figure 2.7 shows a rather trivial agent program that keeps track of
the percept sequence and then uses it to index into a table of actions to decide what to do.
The table—an example of which is given for the vacuum world in Figure 2.3—represents
explicitly the agent function that the agent program embodies. To build a rational agent in
4 There are other choices for the agent program skelet on; for example, we could have the agent programs be
coroutines that run asynchronously with the environment. Each such coroutine has an input and output port and
consists of a loop that reads the input port for percepts and writes actions to the output port.
Section 2.4. The Structure of Agents 47
function TABLE -DRIVEN -AGENT (percept) returns an action
persistent: percepts, a sequence, initially empty
table, a table of actions, indexed by percept sequences, initially fully speciﬁed
append percept to the end of percepts
action←LOOKUP (percepts,table)
return action
Figure 2.7 The TABLE -DRIVEN -AGENT program is invoked for each new percept and
returns an action each time. It retains the complete percept sequence in memory.
this way, we as designers must construct a table that contains the appropriate action for every
possible percept sequence.
It is instructive to consider why the table-driven approach to agent construction is
doomed to failure. Let P be the set of possible percepts and let T be the lifetime of the
agent (the total number of percepts it will receive). The lookup table will contain ∑T
t=1 |P|t
entries. Consider the automated taxi: the visual input from a single camera comes in at the
rate of roughly 27 megabytes per second (30 frames per second, 640× 480 pixels with 24
bits of color information). This gives a lookup table with over 10250,000,000,000 entries for an
hour’s driving. Even the lookup table for chess—a tiny, well-behaved fragment of the real
world—would have at least 10
150 entries. The daunting size of these tables (the number of
atoms in the observable universe is less than 1080) means that (a) no physical agent in this
universe will have the space to store the table, (b) the designer would not have time to create
the table, (c) no agent could ever learn all the right table entries from its experience, and (d)
even if the environment is simple enough to yield a feasible table size, the designer still has
no guidance about how to ﬁll in the table entries.
Despite all this, T
ABLE -DRIVEN -AGENT does do what we want: it implements the
desired agent function. The key challenge for AI is to ﬁnd out how to write programs that,
to the extent possible, produce rational behavior from a smallish program rather than from
a vast table. We have many examples showing that this can be done successfully in other
areas: for example, the huge tables of square roots used by engineers and schoolchildren prior
to the 1970s have now been replaced by a ﬁve-line program for Newton’s method running
on electronic calculators. The question is, can AI do for general intelligent behavior what
Newton did for square roots? We believe the answer is yes.
In the remainder of this section, we outline four basic kinds of agent programs that
embody the principles underlying almost all intelligent systems:
•Simple reﬂex agents;
•Model-based reﬂex agents;
•Goal-based agents; and
•Utility-based agents.
Each kind of agent program combines particular components in particular ways to generate
actions. Section 2.4.6 explains in general terms how to convert all these agents into learning
48 Chapter 2. Intelligent Agents
function REFLEX -VACUUM -AGENT ([location,status]) returns an action
if status = Dirty then return Suck
else if location = A then return Right
else if location = B then return Left
Figure 2.8 The agent program for a simple reﬂex agent in the two-state vacuum environ-
ment. This program implements the agent function tabulated in Figure 2.3.
agents that can improve the performance of their components so as to generate better actions.
Finally, Section 2.4.7 describes the variety of ways in which the components themselves can
be represented within the agent. This variety provides a major organizing principle for the
ﬁeld and for the book itself.
2.4.2 Simple reﬂex agents
The simplest kind of agent is thesimple reﬂex agent. These agents select actions on the basisSIMPLE REFLEX
AGENT
of the current percept, ignoring the rest of the percept history. For example, the vacuum agent
whose agent function is tabulated in Figure 2.3 is a simple reﬂex agent, because its decision
is based only on the current location and on whether that location contains dirt. An agent
program for this agent is shown in Figure 2.8.
Notice that the vacuum agent program is very small indeed compared to the correspond-
ing table. The most obvious reduction comes from ignoring the percept history, which cuts
down the number of possibilities from 4
T to just 4. A further, small reduction comes from
the fact that when the current square is dirty, the action does not depend on the location.
Simple reﬂex behaviors occur even in more complex environments. Imagine yourself
as the driver of the automated taxi. If the car in front brakes and its brake lights come on, then
you should notice this and initiate braking. In other words, some processing is done on the
visual input to establish the condition we call “The car in front is braking.” Then, this triggers
some established connection in the agent program to the action “initiate braking.” We call
such a connection a condition–action rule,
5 written asCONDITION–ACTION
RULE
if car-in-front-is-braking then initiate-braking.
Humans also have many such connections, some of which are learned responses (as for driv-
ing) and some of which are innate reﬂexes (such as blinking when something approaches the
eye). In the course of the book, we show several different ways in which such connections
can be learned and implemented.
The program in Figure 2.8 is speciﬁc to one particular vacuum environment. A more
general and ﬂexible approach is ﬁrst to build a general-purpose interpreter for condition–
action rules and then to create rule sets for speciﬁc task environments. Figure 2.9 gives the
structure of this general program in schematic form, showing how the condition–action rules
allow the agent to make the connection from percept to action. (Do not worry if this seems
5 Also called situation–action rules, productions,o r if–then rules.
Section 2.4. The Structure of Agents 49
Agent
Environment
Sensors
What action I
should do nowCondition-action rules
Actuators
What the world
is like now
Figure 2.9 Schematic diagram of a simple reﬂex agent.
function SIMPLE -REFLEX -AGENT (percept) returns an action
persistent: rules, a set of condition–action rules
state←INTERPRET -INPUT (percept)
rule←RULE -MATCH(state,rules)
action←rule.ACTION
return action
Figure 2.10 A simple reﬂex agent. It acts according to a rule whose condition matches
the current state, as deﬁned by the percept.
trivial; it gets more interesting shortly.) We use rectangles to denote the current internal state
of the agent’s decision process, and ovals to represent the background information used in
the process. The agent program, which is also very simple, is shown in Figure 2.10. The
I
NTERPRET -INPUT function generates an abstracted description of the current state from the
percept, and the RULE -MATCH function returns the ﬁrst rule in the set of rules that matches
the given state description. Note that the description in terms of “rules” and “matching” is
purely conceptual; actual implementations can be as simple as a collection of logic gates
implementing a Boolean circuit.
Simple reﬂex agents have the admirable property of being simple, but they turn out to be
of limited intelligence. The agent in Figure 2.10 will work only if the correct decision can be
made on the basis of only the current percept—that is, only if the environment is fully observ-
able. Even a little bit of unobservability can cause serious trouble. For example, the braking
rule given earlier assumes that the condition car-in-front-is-braking can be determined from
the current percept—a single frame of video. This works if the car in front has a centrally
mounted brake light. Unfortunately, older models have different conﬁgurations of taillights,
50 Chapter 2. Intelligent Agents
brake lights, and turn-signal lights, and it is not always possible to tell from a single image
whether the car is braking. A simple reﬂex agent driving behind such a car would either brake
continuously and unnecessarily, or, worse, never brake at all.
We can see a similar problem arising in the vacuum world. Suppose that a simple reﬂex
vacuum agent is deprived of its location sensor and has only a dirt sensor. Such an agent
has just two possible percepts: [Dirty] and [Clean]. It can Suck in response to [Dirty];w h a t
should it do in response to[Clean]?M o v i n gLeft fails (forever) if it happens to start in square
A, and moving Right fails (forever) if it happens to start in squareB. Inﬁnite loops are often
unavoidable for simple reﬂex agents operating in partially observable environments.
Escape from inﬁnite loops is possible if the agent can randomize its actions. For ex-
RANDOMIZA TION
ample, if the vacuum agent perceives[Clean], it might ﬂip a coin to choose betweenLeft and
Right. It is easy to show that the agent will reach the other square in an average of two steps.
Then, if that square is dirty, the agent will clean it and the task will be complete. Hence, a
randomized simple reﬂex agent might outperform a deterministic simple reﬂex agent.
We mentioned in Section 2.3 that randomized behavior of the right kind can be rational
in some multiagent environments. In single-agent environments, randomization is usuallynot
rational. It is a useful trick that helps a simple reﬂex agent in some situations, but in most
cases we can do much better with more sophisticated deterministic agents.
2.4.3 Model-based reﬂex agents
The most effective way to handle partial observability is for the agent to keep track of the
part of the world it can’t see now . That is, the agent should maintain some sort of internal
state that depends on the percept history and thereby reﬂects at least some of the unobservedINTERNAL ST A TE
aspects of the current state. For the braking problem, the internal state is not too extensive—
just the previous frame from the camera, allowing the agent to detect when two red lights at
the edge of the vehicle go on or off simultaneously. For other driving tasks such as changing
lanes, the agent needs to keep track of where the other cars are if it can’t see them all at once.
And for any driving to be possible at all, the agent needs to keep track of where its keys are.
Updating this internal state information as time goes by requires two kinds of knowl-
edge to be encoded in the agent program. First, we need some information about how the
world evolves independently of the agent—for example, that an overtaking car generally will
be closer behind than it was a moment ago. Second, we need some information about how
the agent’s own actions affect the world—for example, that when the agent turns the steering
wheel clockwise, the car turns to the right, or that after driving for ﬁve minutes northbound
on the freeway, one is usually about ﬁve miles north of where one was ﬁve minutes ago. This
knowledge about “how the world works”—whether implemented in simple Boolean circuits
or in complete scientiﬁc theories—is called a model of the world. An agent that uses such a
model is called a model-based agent.
MODEL-BASED
AGENT
Figure 2.11 gives the structure of the model-based reﬂex agent with internal state, show-
ing how the current percept is combined with the old internal state to generate the updated
description of the current state, based on the agent’s model of how the world works. The agent
program is shown in Figure 2.12. The interesting part is the function U
PDATE-STATE,w h i c h
Section 2.4. The Structure of Agents 51
Agent
Environment
Sensors
State
How the world evolves
What my actions do
Condition-action rules
Actuators
What the world
is like now
What action I
should do now
Figure 2.11 A model-based reﬂex agent.
function MODEL -BASED -REFLEX -AGENT (percept) returns an action
persistent: state, the agent’s current conception of the world state
model, a description of how the next state depends on current state and action
rules, a set of condition–action rules
action, the most recent action, initially none
state←UPDATE -STATE(state,action,percept,model)
rule←RULE -MATCH(state,rules)
action←rule.ACTION
return action
Figure 2.12 A model-based reﬂex agent. It keeps track of the current state of the world,
using an internal model. It then chooses an action in the same way as the reﬂex agent.
is responsible for creating the new internal state description. The details of how models and
states are represented vary widely depending on the type of environment and the particular
technology used in the agent design. Detailed examples of models and updating algorithms
appear in Chapters 4, 12, 11, 15, 17, and 25.
Regardless of the kind of representation used, it is seldom possible for the agent to
determine the current state of a partially observable environment exactly. Instead, the box
labeled “what the world is like now” (Figure 2.11) represents the agent’s “best guess” (or
sometimes best guesses). For example, an automated taxi may not be able to see around the
large truck that has stopped in front of it and can only guess about what may be causing the
hold-up. Thus, uncertainty about the current state may be unavoidable, but the agent still has
to make a decision.
A perhaps less obvious point about the internal “state” maintained by a model-based
agent is that it does not have to describe “what the world is like now” in a literal sense. For
52 Chapter 2. Intelligent Agents
Agent
Environment
Sensors
What action I
should do now
State
How the world evolves
What my actions do
Actuators
What the world
is like now
What it will be like
  if I do action A
Goals
Figure 2.13 A model-based, goal-based agent. It keeps track of the world state as well as
a set of goals it is trying to achieve, and chooses an action that will (eventually) lead to the
achievement of its goals.
example, the taxi may be driving back home, and it may have a rule telling it to ﬁll up with
gas on the way home unless it has at least half a tank. Although “driving back home” may
seem to an aspect of the world state, the fact of the taxi’s destination is actually an aspect of
the agent’s internal state. If you ﬁnd this puzzling, consider that the taxi could be in exactly
the same place at the same time, but intending to reach a different destination.
2.4.4 Goal-based agents
Knowing something about the current state of the environment is not always enough to decide
what to do. For example, at a road junction, the taxi can turn left, turn right, or go straight
on. The correct decision depends on where the taxi is trying to get to. In other words, as well
as a current state description, the agent needs some sort of goal information that describes
GOAL
situations that are desirable—for example, being at the passenger’s destination. The agent
program can combine this with the model (the same information as was used in the model-
based reﬂex agent) to choose actions that achieve the goal. Figure 2.13 shows the goal-based
agent’s structure.
Sometimes goal-based action selection is straightforward—for example, when goal sat-
isfaction results immediately from a single action. Sometimes it will be more tricky—for
example, when the agent has to consider long sequences of twists and turns in order to ﬁnd a
way to achieve the goal. Search (Chapters 3 to 5) and planning (Chapters 10 and 11) are the
subﬁelds of AI devoted to ﬁnding action sequences that achieve the agent’s goals.
Notice that decision making of this kind is fundamentally different from the condition–
action rules described earlier, in that it involves consideration of the future—both “What will
happen if I do such-and-such?” and “Will that make me happy?” In the reﬂex agent designs,
this information is not explicitly represented, because the built-in rules map directly from
Section 2.4. The Structure of Agents 53
percepts to actions. The reﬂex agent brakes when it sees brake lights. A goal-based agent, in
principle, could reason that if the car in front has its brake lights on, it will slow down. Given
the way the world usually evolves, the only action that will achieve the goal of not hitting
other cars is to brake.
Although the goal-based agent appears less efﬁcient, it is more ﬂexible because the
knowledge that supports its decisions is represented explicitly and can be modiﬁed. If it starts
to rain, the agent can update its knowledge of how effectively its brakes will operate; this will
automatically cause all of the relevant behaviors to be altered to suit the new conditions.
For the reﬂex agent, on the other hand, we would have to rewrite many condition–action
rules. The goal-based agent’s behavior can easily be changed to go to a different destination,
simply by specifying that destination as the goal. The reﬂex agent’s rules for when to turn
and when to go straight will work only for a single destination; they must all be replaced to
go somewhere new.
2.4.5 Utility-based agents
Goals alone are not enough to generate high-quality behavior in most environments. For
example, many action sequences will get the taxi to its destination (thereby achieving the
goal) but some are quicker, safer, more reliable, or cheaper than others. Goals just provide a
crude binary distinction between “happy” and “unhappy” states. A more general performance
measure should allow a comparison of different world states according to exactly how happy
they would make the agent. Because “happy” does not sound very scientiﬁc, economists and
computer scientists use the term utility instead.
6UTILITY
We have already seen that a performance measure assigns a score to any given sequence
of environment states, so it can easily distinguish between more and less desirable ways of
getting to the taxi’s destination. An agent’s utility function is essentially an internalizationUTILITY FUNCTION
of the performance measure. If the internal utility function and the external performance
measure are in agreement, then an agent that chooses actions to maximize its utility will be
rational according to the external performance measure.
Let us emphasize again that this is not the only way to be rational—we have already
seen a rational agent program for the vacuum world (Figure 2.8) that has no idea what its
utility function is—but, like goal-based agents, a utility-based agent has many advantages in
terms of ﬂexibility and learning. Furthermore, in two kinds of cases, goals are inadequate but
a utility-based agent can still make rational decisions. First, when there are conﬂicting goals,
only some of which can be achieved (for example, speed and safety), the utility function
speciﬁes the appropriate tradeoff. Second, when there are several goals that the agent can
aim for, none of which can be achieved with certainty, utility provides a way in which the
likelihood of success can be weighed against the importance of the goals.
Partial observability and stochasticity are ubiquitous in the real world, and so, therefore,
is decision making under uncertainty. Technically speaking, a rational utility-based agent
chooses the action that maximizes the expected utility of the action outcomes—that is, the
EXPECTED UTILITY
utility the agent expects to derive, on average, given the probabilities and utilities of each
6 The word “utility” here refers to “the quality of being useful,” not to the electric company or waterworks.
54 Chapter 2. Intelligent Agents
Agent
Environment
Sensors
How happy I will be
in such a state
State
How the world evolves
What my actions do
Utility
Actuators
What action I
should do now
What it will be like
if I do action A
What the world
is like now
Figure 2.14 A model-based, utility-based agent. It uses a model of the world, along with
a utility function that measures its preferences among states of the world. Then it chooses the
action that leads to the best expected utility, where expected utility is computed by averaging
over all possible outcome states, weighted by the probability of the outcome.
outcome. (Appendix A deﬁnes expectation more precisely.) In Chapter 16, we show that any
rational agent must behave as if it possesses a utility function whose expected value it tries
to maximize. An agent that possesses an explicit utility function can make rational decisions
with a general-purpose algorithm that does not depend on the speciﬁc utility function being
maximized. In this way, the “global” deﬁnition of rationality—designating as rational those
agent functions that have the highest performance—is turned into a “local” constraint on
rational-agent designs that can be expressed in a simple program.
The utility-based agent structure appears in Figure 2.14. Utility-based agent programs
appear in Part IV, where we design decision-making agents that must handle the uncertainty
inherent in stochastic or partially observable environments.
At this point, the reader may be wondering, “Is it that simple? We just build agents that
maximize expected utility, and we’re done?” It’s true that such agents would be intelligent,
but it’s not simple. A utility-based agent has to model and keep track of its environment,
tasks that have involved a great deal of research on perception, representation, reasoning,
and learning. The results of this research ﬁll many of the chapters of this book. Choosing
the utility-maximizing course of action is also a difﬁcult task, requiring ingenious algorithms
that ﬁll several more chapters. Even with these algorithms, perfect rationality is usually
unachievable in practice because of computational complexity, as we noted in Chapter 1.
2.4.6 Learning agents
We have described agent programs with various methods for selecting actions. We have
not, so far, explained how the agent programs come into being . In his famous early paper,
Turing (1950) considers the idea of actually programming his intelligent machines by hand.
Section 2.4. The Structure of Agents 55
Performance standard
Agent
Environment
Sensors
Performance
element
changes
knowledge
learning
  goals
Problem
generator
feedback
  Learning
element
Critic
Actuators
Figure 2.15 A general learning agent.
He estimates how much work this might take and concludes “Some more expeditious method
seems desirable.” The method he proposes is to build learning machines and then to teach
them. In many areas of AI, this is now the preferred method for creating state-of-the-art
systems. Learning has another advantage, as we noted earlier: it allows the agent to operate
in initially unknown environments and to become more competent than its initial knowledge
alone might allow. In this section, we brieﬂy introduce the main ideas of learning agents.
Throughout the book, we comment on opportunities and methods for learning in particular
kinds of agents. Part V goes into much more depth on the learning algorithms themselves.
A learning agent can be divided into four conceptual components, as shown in Fig-
ure 2.15. The most important distinction is between the learning element ,w h i c hi sr e -
LEARNING ELEMENT
sponsible for making improvements, and the performance element, which is responsible forPERFORMANCE
ELEMENT
selecting external actions. The performance element is what we have previously considered
to be the entire agent: it takes in percepts and decides on actions. The learning element uses
feedback from the critic on how the agent is doing and determines how the performance
CRITIC
element should be modiﬁed to do better in the future.
The design of the learning element depends very much on the design of the performance
element. When trying to design an agent that learns a certain capability, the ﬁrst question is
not “How am I going to get it to learn this?” but “What kind of performance element will my
agent need to do this once it has learned how?” Given an agent design, learning mechanisms
can be constructed to improve every part of the agent.
The critic tells the learning element how well the agent is doing with respect to a ﬁxed
performance standard. The critic is necessary because the percepts themselves provide no
indication of the agent’s success. For example, a chess program could receive a percept
indicating that it has checkmated its opponent, but it needs a performance standard to know
that this is a good thing; the percept itself does not say so. It is important that the performance
56 Chapter 2. Intelligent Agents
standard be ﬁxed. Conceptually, one should think of it as being outside the agent altogether
because the agent must not modify it to ﬁt its own behavior.
The last component of the learning agent is the problem generator. It is responsiblePROBLEM
GENERA TOR
for suggesting actions that will lead to new and informative experiences. The point is that
if the performance element had its way, it would keep doing the actions that are best, given
what it knows. But if the agent is willing to explore a little and do some perhaps suboptimal
actions in the short run, it might discover much better actions for the long run. The problem
generator’s job is to suggest these exploratory actions. This is what scientists do when they
carry out experiments. Galileo did not think that dropping rocks from the top of a tower in
Pisa was valuable in itself. He was not trying to break the rocks or to modify the brains of
unfortunate passers-by. His aim was to modify his own brain by identifying a better theory
of the motion of objects.
To make the overall design more concrete, let us return to the automated taxi example.
The performance element consists of whatever collection of knowledge and procedures the
taxi has for selecting its driving actions. The taxi goes out on the road and drives, using
this performance element. The critic observes the world and passes information along to the
learning element. For example, after the taxi makes a quick left turn across three lanes of traf-
ﬁc, the critic observes the shocking language used by other drivers. From this experience, the
learning element is able to formulate a rule saying this was a bad action, and the performance
element is modiﬁed by installation of the new rule. The problem generator might identify
certain areas of behavior in need of improvement and suggest experiments, such as trying out
the brakes on different road surfaces under different conditions.
The learning element can make changes to any of the “knowledge” components shown
in the agent diagrams (Figures 2.9, 2.11, 2.13, and 2.14). The simplest cases involve learning
directly from the percept sequence. Observation of pairs of successive states of the environ-
ment can allow the agent to learn “How the world evolves,” and observation of the results of
its actions can allow the agent to learn “What my actions do.” For example, if the taxi exerts
a certain braking pressure when driving on a wet road, then it will soon ﬁnd out how much
deceleration is actually achieved. Clearly, these two learning tasks are more difﬁcult if the
environment is only partially observable.
The forms of learning in the preceding paragraph do not need to access the external
performance standard—in a sense, the standard is the universal one of making predictions
that agree with experiment. The situation is slightly more complex for a utility-based agent
that wishes to learn utility information. For example, suppose the taxi-driving agent receives
no tips from passengers who have been thoroughly shaken up during the trip. The external
performance standard must inform the agent that the loss of tips is a negative contribution to
its overall performance; then the agent might be able to learn that violent maneuvers do not
contribute to its own utility. In a sense, the performance standard distinguishes part of the
incoming percept as a reward (or penalty) that provides direct feedback on the quality of the
agent’s behavior. Hard-wired performance standards such as pain and hunger in animals can
be understood in this way. This issue is discussed further in Chapter 21.
In summary, agents have a variety of components, and those components can be repre-
sented in many ways within the agent program, so there appears to be great variety among
Section 2.4. The Structure of Agents 57
learning methods. There is, however, a single unifying theme. Learning in intelligent agents
can be summarized as a process of modiﬁcation of each component of the agent to bring the
components into closer agreement with the available feedback information, thereby improv-
ing the overall performance of the agent.
2.4.7 How the components of agent programs work
We have described agent programs (in very high-level terms) as consisting of various compo-
nents, whose function it is to answer questions such as: “What is the world like now?” “What
action should I do now?” “What do my actions do?” The next question for a student of AI
is, “How on earth do these components work?” It takes about a thousand pages to begin to
answer that question properly, but here we want to draw the reader’s attention to some basic
distinctions among the various ways that the components can represent the environment that
the agent inhabits.
Roughly speaking, we can place the representations along an axis of increasing com-
plexity and expressive power—atomic, factored,a n d structured. To illustrate these ideas,
it helps to consider a particular agent component, such as the one that deals with “What my
actions do.” This component describes the changes that might occur in the environment as
the result of taking an action, and Figure 2.16 provides schematic depictions of how those
transitions might be represented.
B C
(a) Atomic (b) Factored (b) Structured
BC
Figure 2.16 Three ways to represent states and the transitions between them. (a) Atomic
representation: a state (such as B or C) is a black box with no internal structure; (b) Factored
representation: a state consists of a vector of attribute values; values can be Boolean, real-
valued, or one of a ﬁxed set of symbols. (c) Structured representation: a state includes
objects, each of which may have attributes of its own as well as relationships to other objects.
In an atomic representation each state of the world is indivisible—it has no internalATO M I C
REPRESENTA TION
structure. Consider the problem of ﬁnding a driving route from one end of a country to the
other via some sequence of cities (we address this problem in Figure 3.2 on page 68). For the
purposes of solving this problem, it may sufﬁce to reduce the state of world to just the name
of the city we are in—a single atom of knowledge; a “black box” whose only discernible
property is that of being identical to or different from another black box. The algorithms
58 Chapter 2. Intelligent Agents
underlying search and game-playing (Chapters 3–5), Hidden Markov models (Chapter 15),
and Markov decision processes (Chapter 17) all work with atomic representations—or, at
least, they treat representations as if they were atomic.
Now consider a higher-ﬁdelity description for the same problem, where we need to be
concerned with more than just atomic location in one city or another; we might need to pay
attention to how much gas is in the tank, our current GPS coordinates, whether or not the oil
warning light is working, how much spare change we have for toll crossings, what station is
on the radio, and so on. A factored representation splits up each state into a ﬁxed set of
FACTORED
REPRESENTA TION
variables or attributes, each of which can have a value. While two different atomic statesVARIABLE
A TTRIBUTE
VALUE
have nothing in common—they are just different black boxes—two different factored states
can share some attributes (such as being at some particular GPS location) and not others (such
as having lots of gas or having no gas); this makes it much easier to work out how to turn
one state into another. With factored representations, we can also represent uncertainty—for
example, ignorance about the amount of gas in the tank can be represented by leaving that
attribute blank. Many important areas of AI are based on factored representations, including
constraint satisfaction algorithms (Chapter 6), propositional logic (Chapter 7), planning
(Chapters 10 and 11), Bayesian networks (Chapters 13–16), and the machine learning al-
gorithms in Chapters 18, 20, and 21.
For many purposes, we need to understand the world as having things in it that are
related to each other, not just variables with values. For example, we might notice that a
large truck ahead of us is reversing into the driveway of a dairy farm but a cow has got loose
and is blocking the truck’s path. A factored representation is unlikely to be pre-equipped
with the attributeTruckAheadBackingIntoDairyFarmDrivewayBlockedByLooseCowwith
value true or false. Instead, we would need a structured representation, in which ob-
STRUCTURED
REPRESENTA TION
jects such as cows and trucks and their various and varying relationships can be described
explicitly. (See Figure 2.16(c).) Structured representations underlie relational databases
and ﬁrst-order logic (Chapters 8, 9, and 12), ﬁrst-order probability models (Chapter 14),
knowledge-based learning (Chapter 19) and much of natural language understanding
(Chapters 22 and 23). In fact, almost everything that humans express in natural language
concerns objects and their relationships.
As we mentioned earlier, the axis along which atomic, factored, and structured repre-
sentations lie is the axis of increasing expressiveness. Roughly speaking, a more expressiveEXPRESSIVENESS
representation can capture, at least as concisely, everything a less expressive one can capture,
plus some more. Often, the more expressive language ismuch more concise; for example, the
rules of chess can be written in a page or two of a structured-representation language such
as ﬁrst-order logic but require thousands of pages when written in a factored-representation
language such as propositional logic. On the other hand, reasoning and learning become
more complex as the expressive power of the representation increases. To gain the beneﬁts
of expressive representations while avoiding their drawbacks, intelligent systems for the real
world may need to operate at all points along the axis simultaneously.
Section 2.5. Summary 59
2.5 S UMMARY
This chapter has been something of a whirlwind tour of AI, which we have conceived of as
the science of agent design. The major points to recall are as follows:
•An agent is something that perceives and acts in an environment. The agent function
for an agent speciﬁes the action taken by the agent in response to any percept sequence.
•The performance measure evaluates the behavior of the agent in an environment. A
rational agent acts so as to maximize the expected value of the performance measure,
given the percept sequence it has seen so far.
•A task environment speciﬁcation includes the performance measure, the external en-
vironment, the actuators, and the sensors. In designing an agent, the ﬁrst step must
always be to specify the task environment as fully as possible.
•Task environments vary along several signiﬁcant dimensions. They can be fully or
partially observable, single-agent or multiagent, deterministic or stochastic, episodic or
sequential, static or dynamic, discrete or continuous, and known or unknown.
•The agent program implements the agent function. There exists a variety of basic
agent-program designs reﬂecting the kind of information made explicit and used in the
decision process. The designs vary in efﬁciency, compactness, and ﬂexibility. The
appropriate design of the agent program depends on the nature of the environment.
•Simple reﬂex agents respond directly to percepts, whereas model-based reﬂex agents
maintain internal state to track aspects of the world that are not evident in the current
percept. Goal-based agents act to achieve their goals, and utility-based agents try to
maximize their own expected “happiness.”
•All agents can improve their performance through learning.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
The central role of action in intelligence—the notion of practical reasoning—goes back at
least as far as Aristotle’s Nicomachean Ethics. Practical reasoning was also the subject of
McCarthy’s (1958) inﬂuential paper “Programs with Common Sense.” The ﬁelds of robotics
and control theory are, by their very nature, concerned principally with physical agents. The
concept of a controller in control theory is identical to that of an agent in AI. Perhaps sur-
CONTROLLER
prisingly, AI has concentrated for most of its history on isolated components of agents—
question-answering systems, theorem-provers, vision systems, and so on—rather than on
whole agents. The discussion of agents in the text by Genesereth and Nilsson (1987) was an
inﬂuential exception. The whole-agent view is now widely accepted and is a central theme in
recent texts (Poole et al., 1998; Nilsson, 1998; Padgham and Winikoff, 2004; Jones, 2007).
Chapter 1 traced the roots of the concept of rationality in philosophy and economics. In
AI, the concept was of peripheral interest until the mid-1980s, when it began to suffuse many
60 Chapter 2. Intelligent Agents
discussions about the proper technical foundations of the ﬁeld. A paper by Jon Doyle (1983)
predicted that rational agent design would come to be seen as the core mission of AI, while
other popular topics would spin off to form new disciplines.
Careful attention to the properties of the environment and their consequences for ra-
tional agent design is most apparent in the control theory tradition—for example, classical
control systems (Dorf and Bishop, 2004; Kirk, 2004) handle fully observable, deterministic
environments; stochastic optimal control (Kumar and Varaiya, 1986; Bertsekas and Shreve,
2007) handles partially observable, stochastic environments; and hybrid control (Henzinger
and Sastry, 1998; Cassandras and Lygeros, 2006) deals with environments containing both
discrete and continuous elements. The distinction between fully and partially observable en-
vironments is also central in the dynamic programming literature developed in the ﬁeld of
operations research (Puterman, 1994), which we discuss in Chapter 17.
Reﬂex agents were the primary model for psychological behaviorists such as Skinner
(1953), who attempted to reduce the psychology of organisms strictly to input/output or stim-
ulus/response mappings. The advance from behaviorism to functionalism in psychology,
which was at least partly driven by the application of the computer metaphor to agents (Put-
nam, 1960; Lewis, 1966), introduced the internal state of the agent into the picture. Most
work in AI views the idea of pure reﬂex agents with state as too simple to provide much
leverage, but work by Rosenschein (1985) and Brooks (1986) questioned this assumption
(see Chapter 25). In recent years, a great deal of work has gone into ﬁnding efﬁcient algo-
rithms for keeping track of complex environments (Hamscheret al., 1992; Simon, 2006). The
Remote Agent program (described on page 28) that controlled the Deep Space One spacecraft
is a particularly impressive example (Muscettola et al., 1998; Jonsson et al., 2000).
Goal-based agents are presupposed in everything from Aristotle’s view of practical rea-
soning to McCarthy’s early papers on logical AI. Shakey the Robot (Fikes and Nilsson,
1971; Nilsson, 1984) was the ﬁrst robotic embodiment of a logical, goal-based agent. A
full logical analysis of goal-based agents appeared in Genesereth and Nilsson (1987), and a
goal-based programming methodology called agent-oriented programming was developed by
Shoham (1993). The agent-based approach is now extremely popular in software engineer-
ing (Ciancarini and Wooldridge, 2001). It has also inﬁltrated the area of operating systems,
where autonomic computing refers to computer systems and networks that monitor and con-
AUTONOMIC
COMPUTING
trol themselves with a perceive–act loop and machine learning methods (Kephart and Chess,
2003). Noting that a collection of agent programs designed to work well together in a true
multiagent environment necessarily exhibits modularity—the programs share no internal state
and communicate with each other only through the environment—it is common within the
ﬁeld of multiagent systems to design the agent program of a single agent as a collection of
MULTIAGENT
SYSTEMS
autonomous sub-agents. In some cases, one can even prove that the resulting system gives
the same optimal solutions as a monolithic design.
The goal-based view of agents also dominates the cognitive psychology tradition in the
area of problem solving, beginning with the enormously inﬂuential Human Problem Solv-
ing (Newell and Simon, 1972) and running through all of Newell’s later work (Newell, 1990).
Goals, further analyzed as desires (general) and intentions (currently pursued), are central to
the theory of agents developed by Bratman (1987). This theory has been inﬂuential both in
Exercises 61
natural language understanding and multiagent systems.
Horvitz et al. (1988) speciﬁcally suggest the use of rationality conceived as the maxi-
mization of expected utility as a basis for AI. The text by Pearl (1988) was the ﬁrst in AI to
cover probability and utility theory in depth; its exposition of practical methods for reasoning
and decision making under uncertainty was probably the single biggest factor in the rapid
shift towards utility-based agents in the 1990s (see Part IV).
The general design for learning agents portrayed in Figure 2.15 is classic in the machine
learning literature (Buchanan et al., 1978; Mitchell, 1997). Examples of the design, as em-
bodied in programs, go back at least as far as Arthur Samuel’s (1959, 1967) learning program
for playing checkers. Learning agents are discussed in depth in Part V.
Interest in agents and in agent design has risen rapidly in recent years, partly because of
the growth of the Internet and the perceived need for automated and mobile softbot (Etzioni
and Weld, 1994). Relevant papers are collected in Readings in Agents (Huhns and Singh,
1998) and F oundations of Rational Agency(Wooldridge and Rao, 1999). Texts on multiagent
systems usually provide a good introduction to many aspects of agent design (Weiss, 2000a;
Wooldridge, 2002). Several conference series devoted to agents began in the 1990s, including
the International Workshop on Agent Theories, Architectures, and Languages (ATAL), the
International Conference on Autonomous Agents (AGENTS), and the International Confer-
ence on Multi-Agent Systems (ICMAS). In 2002, these three merged to form the International
Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS). The journal
Autonomous Agents and Multi-Agent Systems was founded in 1998. Finally, Dung Beetle
Ecology (Hanski and Cambefort, 1991) provides a wealth of interesting information on the
behavior of dung beetles. YouTube features inspiring video recordings of their activities.
EXERCISES
2.1 Suppose that the performance measure is concerned with just the ﬁrst T time steps of
the environment and ignores everything thereafter. Show that a rational agent’s action may
depend not just on the state of the environment but also on the time step it has reached.
2.2 Let us examine the rationality of various vacuum-cleaner agent functions.
a. Show that the simple vacuum-cleaner agent function described in Figure 2.3 is indeed
rational under the assumptions listed on page 38.
b. Describe a rational agent function for the case in which each movement costs one point.
Does the corresponding agent program require internal state?
c. Discuss possible agent designs for the cases in which clean squares can become dirty
and the geography of the environment is unknown. Does it make sense for the agent to
learn from its experience in these cases? If so, what should it learn? If not, why not?
2.3 For each of the following assertions, say whether it is true or false and support your
answer with examples or counterexamples where appropriate.
a. An agent that senses only partial information about the state cannot be perfectly rational.
62 Chapter 2. Intelligent Agents
b. There exist task environments in which no pure reﬂex agent can behave rationally.
c. There exists a task environment in which every agent is rational.
d. The input to an agent program is the same as the input to the agent function.
e. Every agent function is implementable by some program/machine combination.
f. Suppose an agent selects its action uniformly at random from the set of possible actions.
There exists a deterministic task environment in which this agent is rational.
g. It is possible for a given agent to be perfectly rational in two distinct task environments.
h. Every agent is rational in an unobservable environment.
i. A perfectly rational poker-playing agent never loses.
2.4 For each of the following activities, give a PEAS description of the task environment
and characterize it in terms of the properties listed in Section 2.3.2.
•Playing soccer.
•Exploring the subsurface oceans of Titan.
•Shopping for used AI books on the Internet.
•Playing a tennis match.
•Practicing tennis against a wall.
•Performing a high jump.
•Knitting a sweater.
•Bidding on an item at an auction.
2.5 Deﬁne in your own words the following terms: agent, agent function, agent program,
rationality, autonomy, reﬂex agent, model-based agent, goal-based agent, utility-based agent,
learning agent.
2.6 This exercise explores the differences between agent functions and agent programs.
a. Can there be more than one agent program that implements a given agent function?
Give an example, or show why one is not possible.
b. Are there agent functions that cannot be implemented by any agent program?
c. Given a ﬁxed machine architecture, does each agent program implement exactly one
agent function?
d. Given an architecture with n bits of storage, how many different possible agent pro-
grams are there?
e. Suppose we keep the agent program ﬁxed but speed up the machine by a factor of two.
Does that change the agent function?
2.7 Write pseudocode agent programs for the goal-based and utility-based agents.
The following exercises all concern the implementation of environments and agents for the
vacuum-cleaner world.
Exercises 63
2.8 Implement a performance-measuring environment simulator for the vacuum-cleaner
world depicted in Figure 2.2 and speciﬁed on page 38. Your implementation should be modu-
lar so that the sensors, actuators, and environment characteristics (size, shape, dirt placement,
etc.) can be changed easily. (Note: for some choices of programming language and operating
system there are already implementations in the online code repository.)
2.9 Implement a simple reﬂex agent for the vacuum environment in Exercise 2.8. Run the
environment with this agent for all possible initial dirt conﬁgurations and agent locations.
Record the performance score for each conﬁguration and the overall average score.
2.10 Consider a modiﬁed version of the vacuum environment in Exercise 2.8, in which the
agent is penalized one point for each movement.
a. Can a simple reﬂex agent be perfectly rational for this environment? Explain.
b. What about a reﬂex agent with state? Design such an agent.
c. How do your answers to a and b change if the agent’s percepts give it the clean/dirty
status of every square in the environment?
2.11 Consider a modiﬁed version of the vacuum environment in Exercise 2.8, in which the
geography of the environment—its extent, boundaries, and obstacles—is unknown, as is the
initial dirt conﬁguration. (The agent can go Up and Down as well as Left and Right.)
a. Can a simple reﬂex agent be perfectly rational for this environment? Explain.
b. Can a simple reﬂex agent with a randomized agent function outperform a simple reﬂex
agent? Design such an agent and measure its performance on several environments.
c. Can you design an environment in which your randomized agent will perform poorly?
Show your results.
d. Can a reﬂex agent with state outperform a simple reﬂex agent? Design such an agent
and measure its performance on several environments. Can you design a rational agent
of this type?
2.12 Repeat Exercise 2.11 for the case in which the location sensor is replaced with a
“bump” sensor that detects the agent’s attempts to move into an obstacle or to cross the
boundaries of the environment. Suppose the bump sensor stops working; how should the
agent behave?
2.13 The vacuum environments in the preceding exercises have all been deterministic. Dis-
cuss possible agent programs for each of the following stochastic versions:
a. Murphy’s law: twenty-ﬁve percent of the time, the Suck action fails to clean the ﬂoor if
it is dirty and deposits dirt onto the ﬂoor if the ﬂoor is clean. How is your agent program
affected if the dirt sensor gives the wrong answer 10% of the time?
b. Small children: At each time step, each clean square has a 10% chance of becoming
dirty. Can you come up with a rational agent design for this case?


--- BOOK CHAPTER: 26_Philosophical_Foundations ---

26
PHILOSOPHICAL
FOUNDATIONS
In which we consider what it means to think and whether artifacts could and
should ever do so.
Philosophers have been around far longer than computers and have been trying to resolve
some questions that relate to AI: How do minds work? Is it possible for machines to act
intelligently in the way that people do, and if they did, would they have real, conscious
minds? What are the ethical implications of intelligent machines?
First, some terminology: the assertion that machines could actas if they were intelligent
is called the weak AI hypothesis by philosophers, and the assertion that machines that do so
WEAK AI
are actually thinking (not just simulating thinking) is called the strong AI hypothesis.STRONG AI
Most AI researchers take the weak AI hypothesis for granted, and don’t care about the
strong AI hypothesis—as long as their program works, they don’t care whether you call it a
simulation of intelligence or real intelligence. All AI researchers should be concerned with
the ethical implications of their work.
26.1 W EAK AI: C AN MACHINES ACT INTELLIGENTLY ?
The proposal for the 1956 summer workshop that deﬁned the ﬁeld of Artiﬁcial Intelligence
(McCarthy et al., 1955) made the assertion that “Every aspect of learning or any other feature
of intelligence can be so precisely described that a machine can be made to simulate it.” Thus,
AI was founded on the assumption that weak AI is possible. Others have asserted that weak
AI is impossible: “Artiﬁcial intelligence pursued within the cult of computationalism stands
not even a ghost of a chance of producing durable results” (Sayre, 1993).
Clearly, whether AI is impossible depends on how it is deﬁned. In Section 1.1, we de-
ﬁned AI as the quest for the best agent program on a given architecture. With this formulation,
AI is by deﬁnition possible: for any digital architecture with k bits of program storage there
are exactly 2
k agent programs, and all we have to do to ﬁnd the best one is enumerate and test
them all. This might not be feasible for large k, but philosophers deal with the theoretical,
not the practical.
1020
Section 26.1. Weak AI: Can Machines Act Intelligently? 1021
Our deﬁnition of AI works well for the engineering problem of ﬁnding a good agent,
given an architecture. Therefore, we’re tempted to end this section right now, answering the
title question in the afﬁrmative. But philosophers are interested in the problem of compar-
ing two architectures—human and machine. Furthermore, they have traditionally posed the
question not in terms of maximizing expected utility but rather as, “Can machines think?”
CAN MACHINES
THINK?
The computer scientist Edsger Dijkstra (1984) said that “The question of whether Ma-
chines Can Think ...i sa bout as relevant as the question of whether Submarines Can Swim .”CAN SUBMARINES
SWIM?
The American Heritage Dictionary’s ﬁrst deﬁnition of swim is “To move through water by
means of the limbs, ﬁns, or tail,” and most people agree that submarines, being limbless,
cannot swim. The dictionary also deﬁnes ﬂy as “To move through the air by means of wings
or winglike parts,” and most people agree that airplanes, having winglike parts, can ﬂy. How-
ever, neither the questions nor the answers have any relevance to the design or capabilities of
airplanes and submarines; rather they are about the usage of words in English. (The fact that
ships do swim in Russian only ampliﬁes this point.). The practical possibility of “thinking
machines” has been with us for only 50 years or so, not long enough for speakers of English to
settle on a meaning for the word “think”—does it require “a brain” or just “brain-like parts.”
Alan Turing, in his famous paper “Computing Machinery and Intelligence” (1950), sug-
gested that instead of asking whether machines can think, we should ask whether machines
can pass a behavioral intelligence test, which has come to be called theTuring Test. The test
TURING TEST
is for a program to have a conversation (via online typed messages) with an interrogator for
ﬁve minutes. The interrogator then has to guess if the conversation is with a program or a
person; the program passes the test if it fools the interrogator 30% of the time. Turing con-
jectured that, by the year 2000, a computer with a storage of 109 units could be programmed
well enough to pass the test. He was wrong—programs have yet to fool a sophisticated judge.
On the other hand, many people have been fooled when they didn’t know they might
be chatting with a computer. The E LIZA program and Internet chatbots such as M GONZ
(Humphrys, 2008) and N ATACHATA have fooled their correspondents repeatedly, and the
chatbot CYBER LOVER has attracted the attention of law enforcement because of its penchant
for tricking fellow chatters into divulging enough personal information that their identity can
be stolen. The Loebner Prize competition, held annually since 1991, is the longest-running
Turing Test-like contest. The competitions have led to better models of human typing errors.
Turing himself examined a wide variety of possible objections to the possibility of in-
telligent machines, including virtually all of those that have been raised in the half-century
since his paper appeared. We will look at some of them.
26.1.1 The argument from disability
The “argument from disability” makes the claim that “a machine can never do X.” As exam-
ples of X, Turing lists the following:
Be kind, resourceful, beautiful, friendly, have initiative, have a sense of humor, tell right
from wrong, make mistakes, fall in love, enjoy strawberries and cream, make someone
fall in love with it, learn from experience, use words properly, be the subject of its own
thought, have as much diversity of behavior as man, do something really new.
1022 Chapter 26. Philosophical Foundations
In retrospect, some of these are rather easy—we’re all familiar with computers that “make
mistakes.” We are also familiar with a century-old technology that has had a proven ability
to “make someone fall in love with it”—the teddy bear. Computer chess expert David Levy
predicts that by 2050 people will routinely fall in love with humanoid robots (Levy, 2007).
As for a robot falling in love, that is a common theme in ﬁction,
1 but there has been only lim-
ited speculation about whether it is in fact likely (Kim et al., 2007). Programs do play chess,
checkers and other games; inspect parts on assembly lines, steer cars and helicopters; diag-
nose diseases; and do hundreds of other tasks as well as or better than humans. Computers
have made small but signiﬁcant discoveries in astronomy, mathematics, chemistry, mineral-
ogy, biology, computer science, and other ﬁelds. Each of these required performance at the
level of a human expert.
Given what we now know about computers, it is not surprising that they do well at
combinatorial problems such as playing chess. But algorithms also perform at human levels
on tasks that seemingly involve human judgment, or as Turing put it, “learning from experi-
ence” and the ability to “tell right from wrong.” As far back as 1955, Paul Meehl (see also
Grove and Meehl, 1996) studied the decision-making processes of trained experts at subjec-
tive tasks such as predicting the success of a student in a training program or the recidivism
of a criminal. In 19 out of the 20 studies he looked at, Meehl found that simple statistical
learning algorithms (such as linear regression or naive Bayes) predict better than the experts.
The Educational Testing Service has used an automated program to grade millions of essay
questions on the GMAT exam since 1999. The program agrees with human graders 97% of
the time, about the same level that two human graders agree (Burstein et al., 2001).
It is clear that computers can do many things as well as or better than humans, including
things that people believe require great human insight and understanding. This does not mean,
of course, that computers use insight and understanding in performing these tasks—those are
not part of behavior, and we address such questions elsewhere—but the point is that one’s
ﬁrst guess about the mental processes required to produce a given behavior is often wrong. It
is also true, of course, that there are many tasks at which computers do not yet excel (to put
it mildly), including Turing’s task of carrying on an open-ended conversation.
26.1.2 The mathematical objection
It is well known, through the work of Turing (1936) and G¨ odel (1931), that certain math-
ematical questions are in principle unanswerable by particular formal systems. G¨ odel’s in-
completeness theorem (see Section 9.5) is the most famous example of this. Brieﬂy, for any
formal axiomatic system F powerful enough to do arithmetic, it is possible to construct a
so-called G¨odel sentence G(F) with the following properties:
•G(F) is a sentence of F , but cannot be proved within F .
•If F is consistent, then G(F) is true.
1 For example, the opera Copp´elia (1870), the novel Do Androids Dream of Electric Sheep? (1968), the movies
AI (2001) and Wal l - E(2008), and in song, Noel Coward’s 1955 version ofLet’s Do It: Let’s Fall in Love predicted
“probably we’ll live to see machines do it.” He didn’t.
Section 26.1. Weak AI: Can Machines Act Intelligently? 1023
Philosophers such as J. R. Lucas (1961) have claimed that this theorem shows that machines
are mentally inferior to humans, because machines are formal systems that are limited by the
incompleteness theorem—they cannot establish the truth of their own G¨odel sentence—while
humans have no such limitation. This claim has caused decades of controversy, spawning a
vast literature, including two books by the mathematician Sir Roger Penrose (1989, 1994)
that repeat the claim with some fresh twists (such as the hypothesis that humans are different
because their brains operate by quantum gravity). We will examine only three of the problems
with the claim.
First, G¨odel’s incompleteness theorem applies only to formal systems that are powerful
enough to do arithmetic. This includes Turing machines, and Lucas’s claim is in part based
on the assertion that computers are Turing machines. This is a good approximation, but is not
quite true. Turing machines are inﬁnite, whereas computers are ﬁnite, and any computer can
therefore be described as a (very large) system in propositional logic, which is not subject to
G¨odel’s incompleteness theorem. Second, an agent should not be too ashamed that it cannot
establish the truth of some sentence while other agents can. Consider the sentence
J. R. Lucas cannot consistently assert that this sentence is true.
If Lucas asserted this sentence, then he would be contradicting himself, so therefore Lucas
cannot consistently assert it, and hence it must be true. We have thus demonstrated that there
is a sentence that Lucas cannot consistently assert while other people (and machines) can. But
that does not make us think less of Lucas. To take another example, no human could compute
the sum of a billion 10 digit numbers in his or her lifetime, but a computer could do it in
seconds. Still, we do not see this as a fundamental limitation in the human’s ability to think.
Humans were behaving intelligently for thousands of years before they invented mathematics,
so it is unlikely that formal mathematical reasoning plays more than a peripheral role in what
it means to be intelligent.
Third, and most important, even if we grant that computers have limitations on what
they can prove, there is no evidence that humans are immune from those limitations. It is
all too easy to show rigorously that a formal system cannot do X, and then claim that hu-
mans can do X using their own informal method, without giving any evidence for this claim.
Indeed, it is impossible to prove that humans are not subject to G¨odel’s incompleteness theo-
rem, because any rigorous proof would require a formalization of the claimed unformalizable
human talent, and hence refute itself. So we are left with an appeal to intuition that humans
can somehow perform superhuman feats of mathematical insight. This appeal is expressed
with arguments such as “we must assume our own consistency, if thought is to be possible at
all” (Lucas, 1976). But if anything, humans are known to be inconsistent. This is certainly
true for everyday reasoning, but it is also true for careful mathematical thought. A famous
example is the four-color map problem. Alfred Kempe published a proof in 1879 that was
widely accepted and contributed to his election as a Fellow of the Royal Society. In 1890,
however, Percy Heawood pointed out a ﬂaw and the theorem remained unproved until 1977.
1024 Chapter 26. Philosophical Foundations
26.1.3 The argument from informality
One of the most inﬂuential and persistent criticisms of AI as an enterprise was raised by Tur-
ing as the “argument from informality of behavior.” Essentially, this is the claim that human
behavior is far too complex to be captured by any simple set of rules and that because com-
puters can do no more than follow a set of rules, they cannot generate behavior as intelligent
as that of humans. The inability to capture everything in a set of logical rules is called the
qualiﬁcation problem in AI.
QUALIFICA TION
PROBLEM
The principal proponent of this view has been the philosopher Hubert Dreyfus, who
has produced a series of inﬂuential critiques of artiﬁcial intelligence: What Computers Can’t
Do (1972), the sequel What Computers Still Can’t Do (1992), and, with his brother Stuart,
Mind Over Machine (1986).
The position they criticize came to be called “Good Old-Fashioned AI,” or GOFAI ,a
term coined by philosopher John Haugeland (1985). GOFAI is supposed to claim that all
intelligent behavior can be captured by a system that reasons logically from a set of facts and
rules describing the domain. It therefore corresponds to the simplest logical agent described
in Chapter 7. Dreyfus is correct in saying that logical agents are vulnerable to the qualiﬁcation
problem. As we saw in Chapter 13, probabilistic reasoning systems are more appropriate for
open-ended domains. The Dreyfus critique therefore is not addressed against computers per
se, but rather against one particular way of programming them. It is reasonable to suppose,
however, that a book called What First-Order Logical Rule-Based Systems Without Learning
Can’t Do might have had less impact.
Under Dreyfus’s view, human expertise does include knowledge of some rules, but only
as a “holistic context” or “background” within which humans operate. He gives the example
of appropriate social behavior in giving and receiving gifts: “Normally one simply responds
in the appropriate circumstances by giving an appropriate gift.” One apparently has “a direct
sense of how things are done and what to expect.” The same claim is made in the context of
chess playing: “A mere chess master might need to ﬁgure out what to do, but a grandmaster
just sees the board as demanding a certain move . . . the right response just pops into his or her
head.” It is certainly true that much of the thought processes of a present-giver or grandmaster
is done at a level that is not open to introspection by the conscious mind. But that does not
mean that the thought processes do not exist. The important question that Dreyfus does not
answer is how the right move gets into the grandmaster’s head. One is reminded of Daniel
Dennett’s (1984) comment,
It is rather as if philosophers were to proclaim themselves expert explainers of the meth-
ods of stage magicians, and then, when we ask how the magician does the sawing-the-
lady-in-half trick, they explain that it is really quite obvious: the magician doesn’t really
saw her in half; he simply makes it appear that he does. “But how does he do that?” we
ask. “Not our department,” say the philosophers.
Dreyfus and Dreyfus (1986) propose a ﬁve-stage process of acquiring expertise, beginning
with rule-based processing (of the sort proposed in GOFAI ) and ending with the ability to
select correct responses instantaneously. In making this proposal, Dreyfus and Dreyfus in
effect move from being AI critics to AI theorists—they propose a neural network architecture
Section 26.1. Weak AI: Can Machines Act Intelligently? 1025
organized into a vast “case library,” but point out several problems. Fortunately, all of their
problems have been addressed, some with partial success and some with total success. Their
problems include the following:
1. Good generalization from examples cannot be achieved without background knowl-
edge. They claim no one has any idea how to incorporate background knowledge into
the neural network learning process. In fact, we saw in Chapters 19 and 20 that there
are techniques for using prior knowledge in learning algorithms. Those techniques,
however, rely on the availability of knowledge in explicit form, something that Dreyfus
and Dreyfus strenuously deny. In our view, this is a good reason for a serious redesign
of current models of neural processing so that they can take advantage of previously
learned knowledge in the way that other learning algorithms do.
2. Neural network learning is a form of supervised learning (see Chapter 18), requiring
the prior identiﬁcation of relevant inputs and correct outputs. Therefore, they claim,
it cannot operate autonomously without the help of a human trainer. In fact, learning
without a teacher can be accomplished by unsupervised learning (Chapter 20) and
reinforcement learning (Chapter 21).
3. Learning algorithms do not perform well with many features, and if we pick a subset
of features, “there is no known way of adding new features should the current set prove
inadequate to account for the learned facts.” In fact, new methods such as support
vector machines handle large feature sets very well. With the introduction of large
Web-based data sets, many applications in areas such as language processing (Sha and
Pereira, 2003) and computer vision (Viola and Jones, 2002a) routinely handle millions
of features. We saw in Chapter 19 that there are also principled ways to generate new
features, although much more work is needed.
4. The brain is able to direct its sensors to seek relevant information and to process it
to extract aspects relevant to the current situation. But, Dreyfus and Dreyfus claim,
“Currently, no details of this mechanism are understood or even hypothesized in a way
that could guide AI research.” In fact, the ﬁeld of active vision, underpinned by the
theory of information value (Chapter 16), is concerned with exactly the problem of
directing sensors, and already some robots have incorporated the theoretical results
obtained. S
TANLEY ’s 132-mile trip through the desert (page 28) was made possible in
large part by an active sensing system of this kind.
In sum, many of the issues Dreyfus has focused on—background commonsense knowledge,
the qualiﬁcation problem, uncertainty, learning, compiled forms of decision making—are
indeed important issues, and have by now been incorporated into standard intelligent agent
design. In our view, this is evidence of AI’s progress, not of its impossibility.
One of Dreyfus’ strongest arguments is for situated agents rather than disembodied
logical inference engines. An agent whose understanding of “dog” comes only from a limited
set of logical sentences such as “ Dog(x) ⇒Mammal(x)” is at a disadvantage compared
to an agent that has watched dogs run, has played fetch with them, and has been licked by
one. As philosopher Andy Clark (1998) says, “Biological brains are ﬁrst and foremost the
control systems for biological bodies. Biological bodies move and act in rich real-world
1026 Chapter 26. Philosophical Foundations
surroundings.” To understand how human (or other animal) agents work, we have to consider
the whole agent, not just the agent program. Indeed, theembodied cognition approach claimsEMBODIED
COGNITION
that it makes no sense to consider the brain separately: cognition takes place within a body,
which is embedded in an environment. We need to study the system as a whole; the brain
augments its reasoning by referring to the environment, as the reader does in perceiving (and
creating) marks on paper to transfer knowledge. Under the embodied cognition program,
robotics, vision, and other sensors become central, not peripheral.
26.2 S TRONG AI: C AN MACHINES REALLY THINK ?
Many philosophers have claimed that a machine that passes the Turing Test would still not
be actually thinking, but would be only a simulation of thinking. Again, the objection was
foreseen by Turing. He cites a speech by Professor Geoffrey Jefferson (1949):
Not until a machine could write a sonnet or compose a concerto because of thoughts and
emotions felt, and not by the chance fall of symbols, could we agree that machine equals
brain—that is, not only write it but know that it had written it.
Turing calls this the argument from consciousness—the machine has to be aware of its own
mental states and actions. While consciousness is an important subject, Jefferson’s key point
actually relates to phenomenology, or the study of direct experience: the machine has to
actually feel emotions. Others focus on intentionality—that is, the question of whether the
machine’s purported beliefs, desires, and other representations are actually “about” some-
thing in the real world.
Turing’s response to the objection is interesting. He could have presented reasons that
machines can in fact be conscious (or have phenomenology, or have intentions). Instead, he
maintains that the question is just as ill-deﬁned as asking, “Can machines think?” Besides,
why should we insist on a higher standard for machines than we do for humans? After all,
in ordinary life we never have any direct evidence about the internal mental states of other
humans. Nevertheless, Turing says, “Instead of arguing continually over this point, it is usual
to have the polite convention that everyone thinks.”
Turing argues that Jefferson would be willing to extend the polite convention to ma-
chines if only he had experience with ones that act intelligently. He cites the following dialog,
which has become such a part of AI’s oral tradition that we simply have to include it:
HUMAN : In the ﬁrst line of your sonnet which reads “shall I compare thee to a summer’s
day,” would not a “spring day” do as well or better?
MACHINE : It wouldn’t scan.
HUMAN : How about “a winter’s day.” That would scan all right.
MACHINE : Yes, but nobody wants to be compared to a winter’s day.
HUMAN : Would you say Mr. Pickwick reminded you of Christmas?
MACHINE :I naw a y .
HUMAN : Yet Christmas is a winter’s day, and I do not think Mr. Pickwick would mind
the comparison.
Section 26.2. Strong AI: Can Machines Really Think? 1027
MACHINE : I don’t think you’re serious. By a winter’s day one means a typical winter’s
day, rather than a special one like Christmas.
One can easily imagine some future time in which such conversations with machines are
commonplace, and it becomes customary to make no linguistic distinction between “real”
and “artiﬁcial” thinking. A similar transition occurred in the years after 1848, when artiﬁcial
urea was synthesized for the ﬁrst time by Frederick W¨ohler. Prior to this event, organic and
inorganic chemistry were essentially disjoint enterprises and many thought that no process
could exist that would convert inorganic chemicals into organic material. Once the synthesis
was accomplished, chemists agreed that artiﬁcial urea was urea, because it had all the right
physical properties. Those who had posited an intrinsic property possessed by organic ma-
terial that inorganic material could never have were faced with the impossibility of devising
any test that could reveal the supposed deﬁciency of artiﬁcial urea.
For thinking, we have not yet reached our 1848 and there are those who believe that
artiﬁcial thinking, no matter how impressive, will never be real. For example, the philosopher
John Searle (1980) argues as follows:
No one supposes that a computer simulation of a storm will leave us all wet ... Why on
earth would anyone in his right mind suppose a computer simulation of mental processes
actually had mental processes? (pp. 37–38)
While it is easy to agree that computer simulations of storms do not make us wet, it is not
clear how to carry this analogy over to computer simulations of mental processes. After
all, a Hollywood simulation of a storm using sprinklers and wind machines does make the
actors wet, and a video game simulation of a storm does make the simulated characters wet.
Most people are comfortable saying that a computer simulation of addition is addition, and
of chess is chess. In fact, we typically speak of an implementation of addition or chess, not a
simulation. Are mental processes more like storms, or more like addition?
Turing’s answer—the polite convention—suggests that the issue will eventually go
away by itself once machines reach a certain level of sophistication. This would have the
effect of dissolving the difference between weak and strong AI. Against this, one may insist
that there is a factual issue at stake: humans do have real minds, and machines might or
might not. To address this factual issue, we need to understand how it is that humans have
real minds, not just bodies that generate neurophysiological processes. Philosophical efforts
to solve this mind–body problem are directly relevant to the question of whether machines
MIND–BODY
PROBLEM
could have real minds.
The mind–body problem was considered by the ancient Greek philosophers and by var-
ious schools of Hindu thought, but was ﬁrst analyzed in depth by the 17th-century French
philosopher and mathematician Ren´e Descartes. His Meditations on First Philosophy (1641)
considered the mind’s activity of thinking (a process with no spatial extent or material prop-
erties) and the physical processes of the body, concluding that the two must exist in separate
realms—what we would now call a dualist theory. The mind–body problem faced by du-
DUALISM
alists is the question of how the mind can control the body if the two are really separate.
Descartes speculated that the two might interact through the pineal gland, which simply begs
the question of how the mind controls the pineal gland.
1028 Chapter 26. Philosophical Foundations
The monist theory of mind, often called physicalism, avoids this problem by assertingMONISM
PHYSICALISM the mind is not separate from the body—that mental states are physical states. Most modern
philosophers of mind are physicalists of one form or another, and physicalism allows, at least
in principle, for the possibility of strong AI. The problem for physicalists is to explain how
physical states—in particular, the molecular conﬁgurations and electrochemical processes of
the brain—can simultaneously bemental states, such as being in pain, enjoying a hamburger,
MENTAL STA TES
knowing that one is riding a horse, or believing that Vienna is the capital of Austria.
26.2.1 Mental states and the brain in a vat
Physicalist philosophers have attempted to explicate what it means to say that a person—and,
by extension, a computer—is in a particular mental state. They have focused in particular on
intentional states. These are states, such as believing, knowing, desiring, fearing, and so on,
INTENTIONAL ST A TE
that refer to some aspect of the external world. For example, the knowledge that one is eating
a hamburger is a belief about the hamburger and what is happening to it.
If physicalism is correct, it must be the case that the proper description of a person’s
mental state is determined by that person’s brain state. Thus, if I am currently focused on
eating a hamburger in a mindful way, my instantaneous brain state is an instance of the class of
mental states “knowing that one is eating a hamburger.” Of course, the speciﬁc conﬁgurations
of all the atoms of my brain are not essential: there are many conﬁgurations of my brain, or
of other people’s brain, that would belong to the same class of mental states. The key point is
that the same brain state could not correspond to a fundamentally distinct mental state, such
as the knowledge that one is eating a banana.
The simplicity of this view is challenged by some simple thought experiments. Imag-
ine, if you will, that your brain was removed from your body at birth and placed in a mar-
velously engineered vat. The vat sustains your brain, allowing it to grow and develop. At the
same time, electronic signals are fed to your brain from a computer simulation of an entirely
ﬁctitious world, and motor signals from your brain are intercepted and used to modify the
simulation as appropriate.
2 In fact, the simulated life you live replicates exactly the life you
would have lived, had your brain not been placed in the vat, including simulated eating of
simulated hamburgers. Thus, you could have a brain state identical to that of someone who is
really eating a real hamburger, but it would be literally false to say that you have the mental
state “knowing that one is eating a hamburger.” You aren’t eating a hamburger, you have
never even experienced a hamburger, and you could not, therefore, have such a mental state.
This example seems to contradict the view that brain states determine mental states. One
way to resolve the dilemma is to say that the content of mental states can be interpreted from
two different points of view. The “ wide content” view interprets it from the point of view
WIDE CONTENT
of an omniscient outside observer with access to the whole situation, who can distinguish
differences in the world. Under this view, the content of mental states involves both the brain
state and the environment history. Narrow content, on the other hand, considers only theNARROW CONTENT
brain state. The narrow content of the brain states of a real hamburger-eater and a brain-in-a-
vat “hamburger”-“eater” is the same in both cases.
2 This situation may be familiar to those who have seen the 1999 ﬁlm The Matrix.
Section 26.2. Strong AI: Can Machines Really Think? 1029
Wide content is entirely appropriate if one’s goals are to ascribe mental states to others
who share one’s world, to predict their likely behavior and its effects, and so on. This is the
setting in which our ordinary language about mental content has evolved. On the other hand,
if one is concerned with the question of whether AI systems are really thinking and really
do have mental states, then narrow content is appropriate; it simply doesn’t make sense to
say that whether or not an AI system is really thinking depends on conditions outside that
system. Narrow content is also relevant if we are thinking about designing AI systems or
understanding their operation, because it is the narrow content of a brain state that determines
what will be the (narrow content of the) next brain state. This leads naturally to the idea that
what matters about a brain state—what makes it have one kind of mental content and not
another—is its functional role within the mental operation of the entity involved.
26.2.2 Functionalism and the brain replacement experiment
The theory of functionalism says that a mental state is any intermediate causal conditionFUNCTIONALISM
between input and output. Under functionalist theory, any two systems with isomorphic
causal processes would have the same mental states. Therefore, a computer program could
have the same mental states as a person. Of course, we have not yet said what “isomorphic”
really means, but the assumption is that there is some level of abstraction below which the
speciﬁc implementation does not matter.
The claims of functionalism are illustrated most clearly by the brain replacement ex-
periment. This thought experiment was introduced by the philosopher Clark Glymour and
was touched on by John Searle (1980), but is most commonly associated with roboticist Hans
Moravec (1988). It goes like this: Suppose neurophysiology has developed to the point where
the input–output behavior and connectivity of all the neurons in the human brain are perfectly
understood. Suppose further that we can build microscopic electronic devices that mimic this
behavior and can be smoothly interfaced to neural tissue. Lastly, suppose that some mirac-
ulous surgical technique can replace individual neurons with the corresponding electronic
devices without interrupting the operation of the brain as a whole. The experiment consists
of gradually replacing all the neurons in someone’s head with electronic devices.
We are concerned with both the external behavior and the internal experience of the
subject, during and after the operation. By the deﬁnition of the experiment, the subject’s
external behavior must remain unchanged compared with what would be observed if the
operation were not carried out.
3 Now although the presence or absence of consciousness
cannot easily be ascertained by a third party, the subject of the experiment ought at least to
be able to record any changes in his or her own conscious experience. Apparently, there is
a direct clash of intuitions as to what would happen. Moravec, a robotics researcher and
functionalist, is convinced his consciousness would remain unaffected. Searle, a philosopher
and biological naturalist, is equally convinced his consciousness would vanish:
You ﬁnd, to your total amazement, that you are indeed losing control of your external
behavior. You ﬁnd, for example, that when doctors test your vision, you hear them say
“We are holding up a red object in front of you; please tell us what you see.” You want
3 One can imagine using an identical “control” subject who is given a placebo operation, for comparison.
1030 Chapter 26. Philosophical Foundations
to cry out “I can’t see anything. I’m going totally blind.” But you hear your voice saying
in a way that is completely out of your control, “I see a red object in front of me.” ...
your conscious experience slowly shrinks to nothing, while your externally observable
behavior remains the same. (Searle, 1992)
One can do more than argue from intuition. First, note that, for the external behavior to re-
main the same while the subject gradually becomes unconscious, it must be the case that the
subject’s volition is removed instantaneously and totally; otherwise the shrinking of aware-
ness would be reﬂected in external behavior—“Help, I’m shrinking!” or words to that effect.
This instantaneous removal of volition as a result of gradual neuron-at-a-time replacement
seems an unlikely claim to have to make.
Second, consider what happens if we do ask the subject questions concerning his or
her conscious experience during the period when no real neurons remain. By the conditions
of the experiment, we will get responses such as “I feel ﬁne. I must say I’m a bit surprised
because I believed Searle’s argument.” Or we might poke the subject with a pointed stick and
observe the response, “Ouch, that hurt.” Now, in the normal course of affairs, the skeptic can
dismiss such outputs from AI programs as mere contrivances. Certainly, it is easy enough to
use a rule such as “If sensor 12 reads ‘High’ then output ‘Ouch.’ ” But the point here is that,
because we have replicated the functional properties of a normal human brain, we assume
that the electronic brain contains no such contrivances. Then we must have an explanation of
the manifestations of consciousness produced by the electronic brain that appeals only to the
functional properties of the neurons. And this explanation must also apply to the real brain,
which has the same functional properties. There are three possible conclusions:
1. The causal mechanisms of consciousness that generate these kinds of outputs in normal
brains are still operating in the electronic version, which is therefore conscious.
2. The conscious mental events in the normal brain have no causal connection to behavior,
and are missing from the electronic brain, which is therefore not conscious.
3. The experiment is impossible, and therefore speculation about it is meaningless.
Although we cannot rule out the second possibility, it reduces consciousness to what philoso-
phers call an epiphenomenal role—something that happens, but casts no shadow, as it were,
EPIPHENOMENON
on the observable world. Furthermore, if consciousness is indeed epiphenomenal, then it
cannot be the case that the subject says “Ouch” because it hurts—that is, because of the con-
scious experience of pain. Instead, the brain must contain a second, unconscious mechanism
that is responsible for the “Ouch.”
Patricia Churchland (1986) points out that the functionalist arguments that operate at
the level of the neuron can also operate at the level of any larger functional unit—a clump
of neurons, a mental module, a lobe, a hemisphere, or the whole brain. That means that if
you accept the notion that the brain replacement experiment shows that the replacement brain
is conscious, then you should also believe that consciousness is maintained when the entire
brain is replaced by a circuit that updates its state and maps from inputs to outputs via a huge
lookup table. This is disconcerting to many people (including Turing himself), who have
the intuition that lookup tables are not conscious—or at least, that the conscious experiences
generated during table lookup are not the same as those generated during the operation of a
Section 26.2. Strong AI: Can Machines Really Think? 1031
system that might be described (even in a simple-minded, computational sense) as accessing
and generating beliefs, introspections, goals, and so on.
26.2.3 Biological naturalism and the Chinese Room
A strong challenge to functionalism has been mounted by John Searle’s (1980) biological
naturalism, according to which mental states are high-level emergent features that are causedBIOLOGICAL
NA TURALISM
by low-level physical processes in the neurons , and it is the (unspeciﬁed) properties of the
neurons that matter. Thus, mental states cannot be duplicated just on the basis of some pro-
gram having the same functional structure with the same input–output behavior; we would
require that the program be running on an architecture with the same causal power as neurons.
To support his view, Searle describes a hypothetical system that is clearly running a program
and passes the Turing Test, but that equally clearly (according to Searle) does notunderstand
anything of its inputs and outputs. His conclusion is that running the appropriate program
(i.e., having the right outputs) is not a sufﬁcient condition for being a mind.
The system consists of a human, who understands only English, equipped with a rule
book, written in English, and various stacks of paper, some blank, some with indecipherable
inscriptions. (The human therefore plays the role of the CPU, the rule book is the program,
and the stacks of paper are the storage device.) The system is inside a room with a small
opening to the outside. Through the opening appear slips of paper with indecipherable sym-
bols. The human ﬁnds matching symbols in the rule book, and follows the instructions. The
instructions may include writing symbols on new slips of paper, ﬁnding symbols in the stacks,
rearranging the stacks, and so on. Eventually, the instructions will cause one or more symbols
to be transcribed onto a piece of paper that is passed back to the outside world.
So far, so good. But from the outside, we see a system that is taking input in the form
of Chinese sentences and generating answers in Chinese that are as “intelligent” as those
in the conversation imagined by Turing.
4 Searle then argues: the person in the room does
not understand Chinese (given). The rule book and the stacks of paper, being just pieces of
paper, do not understand Chinese. Therefore, there is no understanding of Chinese. Hence,
according to Searle, running the right program does not necessarily generate understanding.
Like Turing, Searle considered and attempted to rebuff a number of replies to his ar-
gument. Several commentators, including John McCarthy and Robert Wilensky, proposed
what Searle calls the systems reply. The objection is that asking if the human in the room
understands Chinese is analogous to asking if the CPU can take cube roots. In both cases,
the answer is no, and in both cases, according to the systems reply, the entire system does
have the capacity in question. Certainly, if one asks the Chinese Room whether it understands
Chinese, the answer would be afﬁrmative (in ﬂuent Chinese). By Turing’s polite convention,
this should be enough. Searle’s response is to reiterate the point that the understanding is not
in the human and cannot be in the paper, so there cannot be any understanding. He seems to
be relying on the argument that a property of the whole must reside in one of the parts. Yet
4 The fact that the stacks of paper might contain trillions of pages and the generation of answers would take
millions of years has no bearing on the logical structure of the argument. One aim of philosophical training is to
develop a ﬁnely honed sense of which objections are germane and which are not.
1032 Chapter 26. Philosophical Foundations
water is wet, even though neither H nor O2 is. The real claim made by Searle rests upon the
following four axioms (Searle, 1990):
1. Computer programs are formal (syntactic).
2. Human minds have mental contents (semantics).
3. Syntax by itself is neither constitutive of nor sufﬁcient for semantics.
4. Brains cause minds.
From the ﬁrst three axioms Searle concludes that programs are not sufﬁcient for minds. In
other words, an agent running a programmight be a mind, but it is notnecessarily am i n dj u s t
by virtue of running the program. From the fourth axiom he concludes “Any other system
capable of causing minds would have to have causal powers (at least) equivalent to those
of brains.” From there he infers that any artiﬁcial brain would have to duplicate the causal
powers of brains, not just run a particular program, and that human brains do not produce
mental phenomena solely by virtue of running a program.
The axioms are controversial. For example, axioms 1 and 2 rely on an unspeciﬁed
distinction between syntax and semantics that seems to be closely related to the distinction
between narrow and wide content. On the one hand, we can view computers as manipulating
syntactic symbols; on the other, we can view them as manipulating electric current, which
happens to be what brains mostly do (according to our current understanding). So it seems
we could equally say that brains are syntactic.
Assuming we are generous in interpreting the axioms, then the conclusion—that pro-
grams are not sufﬁcient for minds— does follow. But the conclusion is unsatisfactory—all
Searle has shown is that if you explicitly deny functionalism (that is what his axiom 3 does),
then you can’t necessarily conclude that non-brains are minds. This is reasonable enough—
almost tautological—so the whole argument comes down to whether axiom 3 can be ac-
cepted. According to Searle, the point of the Chinese Room argument is to provide intuitions
for axiom 3. The public reaction shows that the argument is acting as what Daniel Dennett
(1991) calls an intuition pump: it ampliﬁes one’s prior intuitions, so biological naturalists
INTUITION PUMP
are more convinced of their positions, and functionalists are convinced only that axiom 3 is
unsupported, or that in general Searle’s argument is unconvincing. The argument stirs up
combatants, but has done little to change anyone’s opinion. Searle remains undeterred, and
has recently started calling the Chinese Room a “refutation” of strong AI rather than just an
“argument” (Snell, 2008).
Even those who accept axiom 3, and thus accept Searle’s argument, have only their in-
tuitions to fall back on when deciding what entities are minds. The argument purports to show
that the Chinese Room is not a mind by virtue of running the program, but the argument says
nothing about how to decide whether the room (or a computer, some other type of machine,
or an alien) is a mind by virtue of some other reason. Searle himself says that some machines
do have minds: humans are biological machines with minds. According to Searle, human
brains may or may not be running something like an AI program, but if they are, that is not
the reason they are minds. It takes more to make a mind—according to Searle, something
equivalent to the causal powers of individual neurons. What these powers are is left unspec-
iﬁed. It should be noted, however, that neurons evolved to fulﬁll functional roles—creatures
Section 26.2. Strong AI: Can Machines Really Think? 1033
with neurons were learning and deciding long before consciousness appeared on the scene. It
would be a remarkable coincidence if such neurons just happened to generate consciousness
because of some causal powers that are irrelevant to their functional capabilities; after all, it
is the functional capabilities that dictate survival of the organism.
In the case of the Chinese Room, Searle relies on intuition, not proof: just look at the
room; what’s there to be a mind? But one could make the same argument about the brain:
just look at this collection of cells (or of atoms), blindly operating according to the laws of
biochemistry (or of physics)—what’s there to be a mind? Why can a hunk of brain be a mind
while a hunk of liver cannot? That remains the great mystery.
26.2.4 Consciousness, qualia, and the explanatory gap
Running through all the debates about strong AI—the elephant in the debating room, so
to speak—is the issue of consciousness. Consciousness is often broken down into aspects
CONSCIOUSNESS
such as understanding and self-awareness. The aspect we will focus on is that of subjective
experience: why it is that it feels like something to have certain brain states (e.g., while eating
a hamburger), whereas it presumably does not feel like anything to have other physical states
(e.g., while being a rock). The technical term for the intrinsic nature of experiences is qualiaQUALIA
(from the Latin word meaning, roughly, “such things”).
Qualia present a challenge for functionalist accounts of the mind because different
qualia could be involved in what are otherwise isomorphic causal processes. Consider, for
example, the inverted spectrum thought experiment, which the subjective experience of per-
INVERTED
SPECTRUM
son X when seeing red objects is the same experience that the rest of us experience when
seeing green objects, and vice versa.X still calls red objects “red,” stops for red trafﬁc lights,
and agrees that the redness of red trafﬁc lights is a more intense red than the redness of the
setting sun. Yet, X’s subjective experience is just different.
Qualia are challenging not just for functionalism but for all of science. Suppose, for the
sake of argument, that we have completed the process of scientiﬁc research on the brain—we
have found that neural process P
12 in neuron N177 transforms molecule A into molecule B,
and so on, and on. There is simply no currently accepted form of reasoning that would lead
from such ﬁndings to the conclusion that the entity owning those neurons has any particular
subjective experience. This explanatory gap has led some philosophers to conclude that
EXPLANA TORY GAP
humans are simply incapable of forming a proper understanding of their own consciousness.
Others, notably Daniel Dennett (1991), avoid the gap by denying the existence of qualia,
attributing them to a philosophical confusion.
Turing himself concedes that the question of consciousness is a difﬁcult one, but denies
that it has much relevance to the practice of AI: “I do not wish to give the impression that I
think there is no mystery about consciousness ... But I do not think these mysteries neces-
sarily need to be solved before we can answer the question with which we are concerned in
this paper.” We agree with Turing—we are interested in creating programs that behave intel-
ligently. The additional project of making them conscious is not one that we are equipped to
take on, nor one whose success we would be able to determine.
1034 Chapter 26. Philosophical Foundations
26.3 T HE ETHICS AND RISKS OF DEVELOPING ARTIFICIAL INTELLIGENCE
So far, we have concentrated on whether we can develop AI, but we must also consider
whether we should. If the effects of AI technology are more likely to be negative than positive,
then it would be the moral responsibility of workers in the ﬁeld to redirect their research.
Many new technologies have had unintended negative side effects: nuclear ﬁssion brought
Chernobyl and the threat of global destruction; the internal combustion engine brought air
pollution, global warming, and the paving-over of paradise. In a sense, automobiles are
robots that have conquered the world by making themselves indispensable.
All scientists and engineers face ethical considerations of how they should act on the
job, what projects should or should not be done, and how they should be handled. See the
handbook on the Ethics of Computing (Berleur and Brunnstein, 2001). AI, however, seems
to pose some fresh problems beyond that of, say, building bridges that don’t fall down:
•People might lose their jobs to automation.
•People might have too much (or too little) leisure time.
•People might lose their sense of being unique.
•AI systems might be used toward undesirable ends.
•The use of AI systems might result in a loss of accountability.
•The success of AI might mean the end of the human race.
We will look at each issue in turn.
People might lose their jobs to automation. The modern industrial economy has be-
come dependent on computers in general, and select AI programs in particular. For example,
much of the economy, especially in the United States, depends on the availability of con-
sumer credit. Credit card applications, charge approvals, and fraud detection are now done
by AI programs. One could say that thousands of workers have been displaced by these AI
programs, but in fact if you took away the AI programs these jobs would not exist, because
human labor would add an unacceptable cost to the transactions. So far, automation through
information technology in general and AI in particular has created more jobs than it has
eliminated, and has created more interesting, higher-paying jobs. Now that the canonical AI
program is an “intelligent agent” designed to assist a human, loss of jobs is less of a concern
than it was when AI focused on “expert systems” designed to replace humans. But some
researchers think that doing the complete job is the right goal for AI. In reﬂecting on the 25th
Anniversary of the AAAI, Nils Nilsson (2005) set as a challenge the creation of human-level
AI that could pass the employment test rather than the Turing Test—a robot that could learn
to do any one of a range of jobs. We may end up in a future where unemployment is high, but
even the unemployed serve as managers of their own cadre of robot workers.
People might have too much (or too little) leisure time. Alvin Tofﬂer wrote inFuture
Shock (1970), “The work week has been cut by 50 percent since the turn of the century. It
is not out of the way to predict that it will be slashed in half again by 2000.” Arthur C.
Clarke (1968b) wrote that people in 2001 might be “faced with a future of utter boredom,
where the main problem in life is deciding which of several hundred TV channels to select.”
Section 26.3. The Ethics and Risks of Developing Artiﬁcial Intelligence 1035
The only one of these predictions that has come close to panning out is the number of TV
channels. Instead, people working in knowledge-intensive industries have found themselves
part of an integrated computerized system that operates 24 hours a day; to keep up, they have
been forced to worklonger hours. In an industrial economy, rewards are roughly proportional
to the time invested; working 10% more would tend to mean a 10% increase in income. In
an information economy marked by high-bandwidth communication and easy replication of
intellectual property (what Frank and Cook (1996) call the “Winner-Take-All Society”), there
is a large reward for being slightly better than the competition; working 10% more could mean
a 100% increase in income. So there is increasing pressure on everyone to work harder. AI
increases the pace of technological innovation and thus contributes to this overall trend, but
AI also holds the promise of allowing us to take some time off and let our automated agents
handle things for a while. Tim Ferriss (2007) recommends using automation and outsourcing
to achieve a four-hour work week.
People might lose their sense of being unique. In Computer Power and Human Rea-
son, Weizenbaum (1976), the author of the E
LIZA program, points out some of the potential
threats that AI poses to society. One of Weizenbaum’s principal arguments is that AI research
makes possible the idea that humans are automata—an idea that results in a loss of autonomy
or even of humanity. We note that the idea has been around much longer than AI, going back
at least to L’Homme Machine (La Mettrie, 1748). Humanity has survived other setbacks to
our sense of uniqueness: De Revolutionibus Orbium Coelestium (Copernicus, 1543) moved
the Earth away from the center of the solar system, and Descent of Man (Darwin, 1871) put
Homo sapiens at the same level as other species. AI, if widely successful, may be at least as
threatening to the moral assumptions of 21st-century society as Darwin’s theory of evolution
was to those of the 19th century.
AI systems might be used toward undesirable ends. Advanced technologies have
often been used by the powerful to suppress their rivals. As the number theorist G. H. Hardy
wrote (Hardy, 1940), “A science is said to be useful if its development tends to accentuate the
existing inequalities in the distribution of wealth, or more directly promotes the destruction
of human life.” This holds for all sciences, AI being no exception. Autonomous AI systems
are now commonplace on the battleﬁeld; the U.S. military deployed over 5,000 autonomous
aircraft and 12,000 autonomous ground vehicles in Iraq (Singer, 2009). One moral theory
holds that military robots are like medieval armor taken to its logical extreme: no one would
have moral objections to a soldier wanting to wear a helmet when being attacked by large,
angry, axe-wielding enemies, and a teleoperated robot is like a very safe form of armor. On
the other hand, robotic weapons pose additional risks. To the extent that human decision
making is taken out of the ﬁring loop, robots may end up making decisions that lead to the
killing of innocent civilians. At a larger scale, the possession of powerful robots (like the
possession of sturdy helmets) may give a nation overconﬁdence, causing it to go to war more
recklessly than necessary. In most wars, at least one party is overconﬁdent in its military
abilities—otherwise the conﬂict would have been resolved peacefully.
Weizenbaum (1976) also pointed out that speech recognition technology could lead to
widespread wiretapping, and hence to a loss of civil liberties. He didn’t foresee a world with
terrorist threats that would change the balance of how much surveillance people are willing to
1036 Chapter 26. Philosophical Foundations
accept, but he did correctly recognize that AI has the potential to mass-produce surveillance.
His prediction has in part come true: the U.K. now has an extensive network of surveillance
cameras, and other countries routinely monitor Web trafﬁc and telephone calls. Some accept
that computerization leads to a loss of privacy—Sun Microsystems CEO Scott McNealy has
said “You have zero privacy anyway. Get over it.” David Brin (1998) argues that loss of
privacy is inevitable, and the way to combat the asymmetry of power of the state over the
individual is to make the surveillance accessible to all citizens. Etzioni (2004) argues for a
balancing of privacy and security; individual rights and community.
The use of AI systems might result in a loss of accountability. In the litigious atmo-
sphere that prevails in the United States, legal liability becomes an important issue. When a
physician relies on the judgment of a medical expert system for a diagnosis, who is at fault if
the diagnosis is wrong? Fortunately, due in part to the growing inﬂuence of decision-theoretic
methods in medicine, it is now accepted that negligence cannot be shown if the physician
performs medical procedures that have high expected utility, even if theactual result is catas-
trophic for the patient. The question should therefore be “Who is at fault if the diagnosis is
unreasonable?” So far, courts have held that medical expert systems play the same role as
medical textbooks and reference books; physicians are responsible for understanding the rea-
soning behind any decision and for using their own judgment in deciding whether to accept
the system’s recommendations. In designing medical expert systems as agents, therefore,
the actions should be thought of not as directly affecting the patient but as inﬂuencing the
physician’s behavior. If expert systems become reliably more accurate than human diagnosti-
cians, doctors might become legally liable if theydon’t use the recommendations of an expert
system. Atul Gawande (2002) explores this premise.
Similar issues are beginning to arise regarding the use of intelligent agents on the Inter-
net. Some progress has been made in incorporating constraints into intelligent agents so that
they cannot, for example, damage the ﬁles of other users (Weld and Etzioni, 1994). The prob-
lem is magniﬁed when money changes hands. If monetary transactions are made “on one’s
behalf” by an intelligent agent, is one liable for the debts incurred? Would it be possible for
an intelligent agent to have assets itself and to perform electronic trades on its own behalf?
So far, these questions do not seem to be well understood. To our knowledge, no program
has been granted legal status as an individual for the purposes of ﬁnancial transactions; at
present, it seems unreasonable to do so. Programs are also not considered to be “drivers”
for the purposes of enforcing trafﬁc regulations on real highways. In California law, at least,
there do not seem to be any legal sanctions to prevent an automated vehicle from exceeding
the speed limits, although the designer of the vehicle’s control mechanism would be liable in
the case of an accident. As with human reproductive technology, the law has yet to catch up
with the new developments.
The success of AI might mean the end of the human race. Almost any technology
has the potential to cause harm in the wrong hands, but with AI and robotics, we have the new
problem that the wrong hands might belong to the technology itself. Countless science ﬁction
stories have warned about robots or robot–human cyborgs running amok. Early examples
Section 26.3. The Ethics and Risks of Developing Artiﬁcial Intelligence 1037
include Mary Shelley’s Frankenstein, or the Modern Prometheus (1818)5 and Karel Capek’s
play R.U.R. (1921), in which robots conquer the world. In movies, we have The Terminator
(1984), which combines the cliches of robots-conquer-the-world with time travel, and The
Matrix (1999), which combines robots-conquer-the-world with brain-in-a-vat.
It seems that robots are the protagonists of so many conquer-the-world stories because
they represent the unknown, just like the witches and ghosts of tales from earlier eras, or the
Martians from The War of the Worlds (Wells, 1898). The question is whether an AI system
poses a bigger risk than traditional software. We will look at three sources of risk.
First, the AI system’s state estimation may be incorrect, causing it to do the wrong
thing. For example, an autonomous car might incorrectly estimate the position of a car in the
adjacent lane, leading to an accident that might kill the occupants. More seriously, a missile
defense system might erroneously detect an attack and launch a counterattack, leading to
the death of billions. These risks are not really risks of AI systems—in both cases the same
mistake could just as easily be made by a human as by a computer. The correct way to mitigate
these risks is to design a system with checks and balances so that a single state-estimation
error does not propagate through the system unchecked.
Second, specifying the right utility function for an AI system to maximize is not so
easy. For example, we might propose a utility function designed tominimize human suffering,
expressed as an additive reward function over time as in Chapter 17. Given the way humans
are, however, we’ll always ﬁnd a way to suffer even in paradise; so the optimal decision for
the AI system is to terminate the human race as soon as possible—no humans, no suffering.
With AI systems, then, we need to be very careful what we ask for, whereas humans would
have no trouble realizing that the proposed utility function cannot be taken literally. On the
other hand, computers need not be tainted by the irrational behaviors described in Chapter 16.
Humans sometimes use their intelligence in aggressive ways because humans have some
innately aggressive tendencies, due to natural selection. The machines we build need not be
innately aggressive, unless we decide to build them that way (or unless they emerge as the
end product of a mechanism design that encourages aggressive behavior). Fortunately, there
are techniques, such as apprenticeship learning, that allows us to specify a utility function by
example. One can hope that a robot that is smart enough to ﬁgure out how to terminate the
human race is also smart enough to ﬁgure out that that was not the intended utility function.
Third, the AI system’s learning function may cause it to evolve into a system with
unintended behavior. This scenario is the most serious, and is unique to AI systems, so we
will cover it in more depth. I. J. Good wrote (1965),
Let an ultraintelligent machine be deﬁned as a machine that can far surpass all theUL TRAINTELLIGENT
MACHINE
intellectual activities of any man however clever. Since the design of machines is one of
these intellectual activities, an ultraintelligent machine could design even better machines;
there would then unquestionably be an “intellig ence explosion,” and the intelligence of
man would be left far behind. Thus the ﬁrst ultraintelligent machine is the last invention
that man need ever make, provided that the machine is docile enough to tell us how to
keep it under control.
5 As a young man, Charles Babbage was inﬂuenced by reading Frankenstein.
1038 Chapter 26. Philosophical Foundations
The “intelligence explosion” has also been called the technological singularity by mathe-TECHNOLOGICAL
SINGULARITY
matics professor and science ﬁction author Vernor Vinge, who writes (1993), “Within thirty
years, we will have the technological means to create superhuman intelligence. Shortly after,
the human era will be ended.” Good and Vinge (and many others) correctly note that the curve
of technological progress (on many measures) is growing exponentially at present (consider
Moore’s Law). However, it is a leap to extrapolate that the curve will continue to a singularity
of near-inﬁnite growth. So far, every other technology has followed an S-shaped curve, where
the exponential growth eventually tapers off. Sometimes new technologies step in when the
old ones plateau; sometimes we hit hard limits. With less than a century of high-technology
history to go on, it is difﬁcult to extrapolate hundreds of years ahead.
Note that the concept of ultraintelligent machines assumes that intelligence is an es-
pecially important attribute, and if you have enough of it, all problems can be solved. But
we know there are limits on computability and computational complexity. If the problem
of deﬁning ultraintelligent machines (or even approximations to them) happens to fall in the
class of, say, NEXPTIME-complete problems, and if there are no heuristic shortcuts, then
even exponential progress in technology won’t help—the speed of light puts a strict upper
bound on how much computing can be done; problems beyond that limit will not be solved.
We still don’t know where those upper bounds are.
Vinge is concerned about the coming singularity, but some computer scientists and
futurists relish it. Hans Moravec (2000) encourages us to give every advantage to our “mind
children,” the robots we create, which may surpass us in intelligence. There is even a new
word—transhumanism—for the active social movement that looks forward to this future in
TRANSHUMANISM
which humans are merged with—or replaced by—robotic and biotech inventions. Sufﬁce it
to say that such issues present a challenge for most moral theorists, who take the preservation
of human life and the human species to be a good thing. Ray Kurzweil is currently the most
visible advocate for the singularity view, writing in The Singularity is Near (2005):
The Singularity will allow us to transcend these limitations of our biological bodies and
brain. We will gain power over our fates. Our mortality will be in our own hands. We
will be able to live as long as we want (a subtly different statement from saying we will
live forever). We will fully understand human thinking and will vastly extend and expand
its reach. By the end of this century, the nonbiological portion of our intelligence will be
trillions of trillions of times more powerful than unaided human intelligence.
Kurzweil also notes the potential dangers, writing “But the Singularity will also amplify the
ability to act on our destructive inclinations, so its full story has not yet been written.”
If ultraintelligent machines are a possibility, we humans would do well to make sure
that we design their predecessors in such a way that they design themselves to treat us well.
Science ﬁction writer Isaac Asimov (1942) was the ﬁrst to address this issue, with his three
laws of robotics:
1. A robot may not injure a human being or, through inaction, allow a human being to
come to harm.
2. A robot must obey orders given to it by human beings, except where such orders would
conﬂict with the First Law.
Section 26.3. The Ethics and Risks of Developing Artiﬁcial Intelligence 1039
3. A robot must protect its own existence as long as such protection does not conﬂict with
the First or Second Law.
These laws seem reasonable, at least to us humans.6 But the trick is how to implement these
laws. In the Asimov story Roundabout a robot is sent to fetch some selenium. Later the
robot is found wandering in a circle around the selenium source. Every time it heads toward
the source, it senses a danger, and the third law causes it to veer away. But every time it
veers away, the danger recedes, and the power of the second law takes over, causing it to
veer back towards the selenium. The set of points that deﬁne the balancing point between
the two laws deﬁnes a circle. This suggests that the laws are not logical absolutes, but rather
are weighed against each other, with a higher weighting for the earlier laws. Asimov was
probably thinking of an architecture based on control theory—perhaps a linear combination
of factors—while today the most likely architecture would be a probabilistic reasoning agent
that reasons over probability distributions of outcomes, and maximizes utility as deﬁned by
the three laws. But presumably we don’t want our robots to prevent a human from crossing
the street because of the nonzero chance of harm. That means that the negative utility for
harm to a human must be much greater than for disobeying, but that each of the utilities is
ﬁnite, not inﬁnite.
Yudkowsky (2008) goes into more detail about how to design aFriendly AI. He asserts
FRIENDL Y AI
that friendliness (a desire not to harm humans) should be designed in from the start, but that
the designers should recognize both that their own designs may be ﬂawed, and that the robot
will learn and evolve over time. Thus the challenge is one of mechanism design—to deﬁne a
mechanism for evolving AI systems under a system of checks and balances, and to give the
systems utility functions that will remain friendly in the face of such changes.
We can’t just give a program a static utility function, because circumstances, and our de-
sired responses to circumstances, change over time. For example, if technology had allowed
us to design a super-powerful AI agent in 1800 and endow it with the prevailing morals of
the time, it would be ﬁghting today to reestablish slavery and abolish women’s right to vote.
On the other hand, if we build an AI agent today and tell it to evolve its utility function, how
can we assure that it won’t reason that “Humans think it is moral to kill annoying insects, in
part because insect brains are so primitive. But human brains are primitive compared to my
powers, so it must be moral for me to kill humans.”
Omohundro (2008) hypothesizes that even an innocuous chess program could pose a
risk to society. Similarly, Marvin Minsky once suggested that an AI program designed to
solve the Riemann Hypothesis might end up taking over all the resources of Earth to build
more powerful supercomputers to help achieve its goal. The moral is that even if you only
want your program to play chess or prove theorems, if you give it the capability to learn
and alter itself, you need safeguards. Omohundro concludes that “Social structures which
cause individuals to bear the cost of their negative externalities would go a long way toward
ensuring a stable and positive future,” This seems to be an excellent idea for society in general,
regardless of the possibility of ultraintelligent machines.
6 A robot might notice the inequity that a human is allowed to kill another in self-defense, but a robot is required
to sacriﬁce its own life to save a human.
1040 Chapter 26. Philosophical Foundations
We should note that the idea of safeguards against change in utility function is not a
new one. In the Odyssey, Homer (ca. 700 B.C.) described Ulysses’ encounter with the sirens,
whose song was so alluring it compelled sailors to cast themselves into the sea. Knowing it
would have that effect on him, Ulysses ordered his crew to bind him to the mast so that he
could not perform the self-destructive act. It is interesting to think how similar safeguards
could be built into AI systems.
Finally, let us consider the robot’s point of view. If robots become conscious, then to
treat them as mere “machines” (e.g., to take them apart) might be immoral. Science ﬁction
writers have addressed the issue of robot rights. The movie A.I. (Spielberg, 2001) was based
on a story by Brian Aldiss about an intelligent robot who was programmed to believe that
he was human and fails to understand his eventual abandonment by his owner–mother. The
story (and the movie) argue for the need for a civil rights movement for robots.
26.4 S UMMARY
This chapter has addressed the following issues:
•Philosophers use the term weak AI for the hypothesis that machines could possibly
behave intelligently, and strong AI for the hypothesis that such machines would count
as having actual minds (as opposed to simulated minds).
•Alan Turing rejected the question “Can machines think?” and replaced it with a be-
havioral test. He anticipated many objections to the possibility of thinking machines.
Few AI researchers pay attention to the Turing Test, preferring to concentrate on their
systems’ performance on practical tasks, rather than the ability to imitate humans.
•There is general agreement in modern times that mental states are brain states.
•Arguments for and against strong AI are inconclusive. Few mainstream AI researchers
believe that anything signiﬁcant hinges on the outcome of the debate.
•Consciousness remains a mystery.
•We identiﬁed six potential threats to society posed by AI and related technology. We
concluded that some of the threats are either unlikely or differ little from threats posed
by “unintelligent” technologies. One threat in particular is worthy of further consider-
ation: that ultraintelligent machines might lead to a future that is very different from
today—we may not like it, and at that point we may not have a choice. Such consid-
erations lead inevitably to the conclusion that we must weigh carefully, and soon, the
possible consequences of AI research.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Sources for the various responses to Turing’s 1950 paper and for the main critics of weak
AI were given in the chapter. Although it became fashionable in the post-neural-network era
Bibliographical and Historical Notes 1041
to deride symbolic approaches, not all philosophers are critical of GOFAI . Some are, in fact,
ardent advocates and even practitioners. Zenon Pylyshyn (1984) has argued that cognition
can best be understood through a computational model, not only in principle but also as a
way of conducting research at present, and has speciﬁcally rebutted Dreyfus’s criticisms of
the computational model of human cognition (Pylyshyn, 1974). Gilbert Harman (1983), in
analyzing belief revision, makes connections with AI research on truth maintenance systems.
Michael Bratman has applied his “belief-desire-intention” model of human psychology (Brat-
man, 1987) to AI research on planning (Bratman, 1992). At the extreme end of strong AI,
Aaron Sloman (1978, p. xiii) has even described as “racialist” the claim by Joseph Weizen-
baum (1976) that intelligent machines can never be regarded as persons.
Proponents of the importance of embodiment in cognition include the philosophers
Merleau-Ponty, whose Phenomenology of Perception (1945) stressed the importance of the
body and the subjective interpretation of reality afforded by our senses, and Heidegger, whose
Being and Time (1927) asked what it means to actually be an agent, and criticized all of the
history of philosophy for taking this notion for granted. In the computer age, Alva Noe (2009)
and Andy Clark (1998, 2008) propose that our brains form a rather minimal representation
of the world, use the world itself in a just-in-time basis to maintain the illusion of a detailed
internal model, use props in the world (such as paper and pencil as well as computers) to
increase the capabilities of the mind. Pfeifer et al. (2006) and Lakoff and Johnson (1999)
present arguments for how the body helps shape cognition.
The nature of the mind has been a standard topic of philosophical theorizing from an-
cient times to the present. In the Phaedo, Plato speciﬁcally considered and rejected the idea
that the mind could be an “attunement” or pattern of organization of the parts of the body, a
viewpoint that approximates the functionalist viewpoint in modern philosophy of mind. He
decided instead that the mind had to be an immortal, immaterial soul, separable from the
body and different in substance—the viewpoint of dualism. Aristotle distinguished a variety
of souls (Greekψυχη) in living things, some of which, at least, he described in a functionalist
manner. (See Nussbaum (1978) for more on Aristotle’s functionalism.)
Descartes is notorious for his dualistic view of the human mind, but ironically his histor-
ical inﬂuence was toward mechanism and physicalism. He explicitly conceived of animals as
automata, and he anticipated the Turing Test, writing “it is not conceivable [that a machine]
should produce different arrangements of words so as to give an appropriately meaningful
answer to whatever is said in its presence, as even the dullest of men can do” (Descartes,
1637). Descartes’s spirited defense of the animals-as-automata viewpoint actually had the
effect of making it easier to conceive of humans as automata as well, even though he himself
did not take this step. The book L’Homme Machine (La Mettrie, 1748) did explicitly argue
that humans are automata.
Modern analytic philosophy has typically accepted physicalism, but the variety of views
on the content of mental states is bewildering. The identiﬁcation of mental states with brain
states is usually attributed to Place (1956) and Smart (1959). The debate between narrow-
content and wide-content views of mental states was triggered by Hilary Putnam (1975), who
introduced so-called twin earths (rather than brain-in-a-vat, as we did in the chapter) as a
TWIN EARTHS
device to generate identical brain states with different (wide) content.
1042 Chapter 26. Philosophical Foundations
Functionalism is the philosophy of mind most naturally suggested by AI. The idea that
mental states correspond to classes of brain states deﬁned functionally is due to Putnam
(1960, 1967) and Lewis (1966, 1980). Perhaps the most forceful proponent of functional-
ism is Daniel Dennett, whose ambitiously titled work Consciousness Explained (Dennett,
1991) has attracted many attempted rebuttals. Metzinger (2009) argues there is no such thing
as an objective self, that consciousness is the subjective appearance of a world. The inverted
spectrum argument concerning qualia was introduced by John Locke (1690). Frank Jack-
son (1982) designed an inﬂuential thought experiment involving Mary, a color scientist who
has been brought up in an entirely black-and-white world. There’s Something About Mary
(Ludlow et al., 2004) collects several papers on this topic.
Functionalism has come under attack from authors who claim that they do not account
for the qualia or “what it’s like” aspect of mental states (Nagel, 1974). Searle has focused
instead on the alleged inability of functionalism to account for intentionality (Searle, 1980,
1984, 1992). Churchland and Churchland (1982) rebut both these types of criticism. The
Chinese Room has been debated endlessly (Searle, 1980, 1990; Preston and Bishop, 2002).
We’ll just mention here a related work: Terry Bisson’s (1990) science ﬁction story They’re
Made out of Meat , in which alien robotic explorers who visit earth are incredulous to ﬁnd
thinking human beings whose minds are made of meat. Presumably, the robotic alien equiv-
alent of Searle believes that he can think due to the special causal powers of robotic circuits;
causal powers that mere meat-brains do not possess.
Ethical issues in AI predate the existence of the ﬁeld itself. I. J. Good’s (1965) ul-
traintelligent machine idea was foreseen a hundred years earlier by Samuel Butler (1863).
Written four years after the publication of Darwin’s On the Origins of Species and at a time
when the most sophisticated machines were steam engines, Butler’s article onDarwin Among
the Machines envisioned “the ultimate development of mechanical consciousness” by natural
selection. The theme was reiterated by George Dyson (1998) in a book of the same title.
The philosophical literature on minds, brains, and related topics is large and difﬁcult to
read without training in the terminology and methods of argument employed. The Encyclo-
pedia of Philosophy (Edwards, 1967) is an impressively authoritative and very useful aid in
this process. The Cambridge Dictionary of Philosophy (Audi, 1999) is a shorter and more
accessible work, and the online Stanford Encyclopedia of Philosophy offers many excellent
articles and up-to-date references. The MIT Encyclopedia of Cognitive Science (Wilson and
Keil, 1999) covers the philosophy of mind as well as the biology and psychology of mind.
There are several general introductions to the philosophical “AI question” (Boden, 1990;
Haugeland, 1985; Copeland, 1993; McCorduck, 2004; Minsky, 2007). The Behavioral and
Brain Sciences, abbreviated BBS, is a major journal devoted to philosophical and scientiﬁc
debates about AI and neuroscience. Topics of ethics and responsibility in AI are covered in
the journals AI and Society and Journal of Artiﬁcial Intelligence and Law .
Exercises 1043
EXERCISES
26.1 Go through Turing’s list of alleged “disabilities” of machines, identifying which have
been achieved, which are achievable in principle by a program, and which are still problem-
atic because they require conscious mental states.
26.2 Find and analyze an account in the popular media of one or more of the arguments to
the effect that AI is impossible.
26.3 In the brain replacement argument, it is important to be able to restore the subject’s
brain to normal, such that its external behavior is as it would have been if the operation had
not taken place. Can the skeptic reasonably object that this would require updating those
neurophysiological properties of the neurons relating to conscious experience, as distinct
from those involved in the functional behavior of the neurons?
26.4 Suppose that a Prolog program containing many clauses about the rules of British
citizenship is compiled and run on an ordinary computer. Analyze the “brain states” of the
computer under wide and narrow content.
26.5 Alan Perlis (1982) wrote, “A year spent in artiﬁcial intelligence is enough to make one
believe in God”. He also wrote, in a letter to Philip Davis, that one of the central dreams of
computer science is that “through the performance of computers and their programs we will
remove all doubt that there is only a chemical distinction between the living and nonliving
world.” To what extent does the progress made so far in artiﬁcial intelligence shed light on
these issues? Suppose that at some future date, the AI endeavor has been completely success-
ful; that is, we have build intelligent agents capable of carrying out any human cognitive task
at human levels of ability. To what extent would that shed light on these issues?
26.6 Compare the social impact of artiﬁcial intelligence in the last ﬁfty years with the social
impact of the introduction of electric appliances and the internal combustion engine in the
ﬁfty years between 1890 and 1940.
26.7 I. J. Good claims that intelligence is the most important quality, and that building
ultraintelligent machines will change everything. A sentient cheetah counters that “Actually
speed is more important; if we could build ultrafast machines, that would change everything,”
and a sentient elephant claims “You’re both wrong; what we need is ultrastrong machines.”
What do you think of these arguments?
26.8 Analyze the potential threats from AI technology to society. What threats are most se-
rious, and how might they be combated? How do they compare to the potential beneﬁts?
26.9 How do the potential threats from AI technology compare with those from other com-
puter science technologies, and to bio-, nano-, and nuclear technologies?
26.10 Some critics object that AI is impossible, while others object that it is too possible
and that ultraintelligent machines pose a threat. Which of these objections do you think is
more likely? Would it be a contradiction for someone to hold both positions?


END_INSTRUCTION
