
START_INSTRUCTION
You are an expert AI tutor creating a comprehensive study guide.
Your task is to rewrite and expand the "Lesson Notes" provided below into a cohesive, readable study text.

1. **Structure:** Follow the exact structure and order of the topics in the "Lesson Notes". Do not reorder them.
2. **Enrichment:** Use the "Textbook Context" provided to expand on every bullet point in the notes.
    - If the notes mention a concept (e.g., "Rationality"), define it precisely using the textbook.
    - If the notes list algorithms (e.g., "A*", "Simulated Annealing"), explain how they work, their complexity, and pros/cons using the textbook details.
    - Add examples from the textbook where appropriate.
3. **Tone:** Academic but accessible study material. Not just bullet points—write paragraphs where necessary to explain complex ideas.
4. **Language:** The output must be in **Czech** (since the lesson notes are in Czech), but you can keep standard English AI terminology (like "Overfitting") in parentheses.

--- LESSON NOTES (Skeleton) ---
Zpětnovazební učení

• Učení založené na interakci s prostředím:
○ je aktivní spíše než pasivní
○ interakce mají obvykle sekvenční charakter
○ je orientované na dosažení cíle
○ probíhá bez příkladů optimálního chování
○ Cílem je optimalizovat souhrnnou odměnu plynoucí z interakce s prostředím.
•
S každým stavem s je spojena odměna za jeho dosažení
○ Agent se učí na základě odměn a trestů, které mu dává prostředí
•
U nějakých problémů je složité "oštítkovat" učitelem všechny možné inputy
○ Pak se hodí učení se zpětnou vazbou
•
Zpětnovazební učení
Pasivní: cílem agenta je naučit se hodnoty užitku pro jednotlivé stavy U(s) za předpokladu fixní 
strategie π
•
• Aktivní: cílem agenta je naučit se i strategii
Obecný učící se agent
učící se komponenta
○ slouží ke zdokonalování agenta
•
výkonná komponenta
○ provádí příslušné akce agenta
•
kritika postupu učení
○ zpětnovazební řízení procesu učení
•
generátor
○ návrh akcí, které může agent provádět
•
•
Úloha posilovaného učení
•
Markovský rozhodovací proces (MDP)
matematický model používaný pro rozhodování v prostředí, kde výsledky akcí jsou částečně
náhodné a částečně pod kontrolou agenta
•
• pravděpodobnost přechodu do stavu s' závisí jen na aktuálním stavu s a akci a, ne na historii
Cílem agenta je najít optimální politiku π(s), což je pravidlo, které určuje, jakou akci a provést v 
každém stavu s, aby maximalizoval očekávaný kumulativní zisk.
•
MDP je sekvenční rozhodovací proces (diskrétní ve stavech i v čase) s plně pozorovatelným 
stochastickým prostředím, markovskou přechodovou funkcí a aditivní užitkovou funkcí.

--- TEXTBOOK CONTEXT (Source Material) ---


--- BOOK CHAPTER: 21_Reinforcement_Learning ---

21
REINFORCEMENT
LEARNING
In which we examine how an agent can learn from success and failure, from re-
ward and punishment.
21.1 I NTRODUCTION
Chapters 18, 19, and 20 covered methods that learn functions, logical theories, and probability
models from examples. In this chapter, we will study how agents can learn what to do in the
absence of labeled examples of what to do.
Consider, for example, the problem of learning to play chess. A supervised learning
agent needs to be told the correct move for each position it encounters, but such feedback is
seldom available. In the absence of feedback from a teacher, an agent can learn a transition
model for its own moves and can perhaps learn to predict the opponent’s moves, but without
some feedback about what is good and what is bad, the agent will have no grounds for decid-
ing which move to make. The agent needs to know that something good has happened when
it (accidentally) checkmates the opponent, and that something bad has happened when it is
checkmated—or vice versa, if the game is suicide chess. This kind of feedback is called a
reward,o r reinforcement. In games like chess, the reinforcement is received only at the end
REINFORCEMENT
of the game. In other environments, the rewards come more frequently. In ping-pong, each
point scored can be considered a reward; when learning to crawl, any forward motion is an
achievement. Our framework for agents regards the reward as part of the input percept, but
the agent must be “hardwired” to recognize that part as a reward rather than as just another
sensory input. Thus, animals seem to be hardwired to recognize pain and hunger as negative
rewards and pleasure and food intake as positive rewards. Reinforcement has been carefully
studied by animal psychologists for over 60 years.
Rewards were introduced in Chapter 17, where they served to deﬁne optimal policies
in Markov decision processes (MDPs). An optimal policy is a policy that maximizes the
expected total reward. The task ofreinforcement learningis to use observed rewards to learn
an optimal (or nearly optimal) policy for the environment. Whereas in Chapter 17 the agent
has a complete model of the environment and knows the reward function, here we assume no
830
Section 21.1. Introduction 831
prior knowledge of either. Imagine playing a new game whose rules you don’t know; after a
hundred or so moves, your opponent announces, “You lose.” This is reinforcement learning
in a nutshell.
In many complex domains, reinforcement learning is the only feasible way to train a
program to perform at high levels. For example, in game playing, it is very hard for a human
to provide accurate and consistent evaluations of large numbers of positions, which would be
needed to train an evaluation function directly from examples. Instead, the program can be
told when it has won or lost, and it can use this information to learn an evaluation function
that gives reasonably accurate estimates of the probability of winning from any given position.
Similarly, it is extremely difﬁcult to program an agent to ﬂy a helicopter; yet given appropriate
negative rewards for crashing, wobbling, or deviating from a set course, an agent can learn to
ﬂy by itself.
Reinforcement learning might be considered to encompass all of AI: an agent is placed
in an environment and must learn to behave successfully therein. To keep the chapter man-
ageable, we will concentrate on simple environments and simple agent designs. For the most
part, we will assume a fully observable environment, so that the current state is supplied by
each percept. On the other hand, we will assume that the agent does not know how the en-
vironment works or what its actions do, and we will allow for probabilistic action outcomes.
Thus, the agent faces an unknown Markov decision process. We will consider three of the
agent designs ﬁrst introduced in Chapter 2:
•A utility-based agent learns a utility function on states and uses it to select actions that
maximize the expected outcome utility.
•A Q-learning agent learns an action-utility function ,o r Q-function,g i v i n gt h ee x -
Q-LEARNING
Q-FUNCTION pected utility of taking a given action in a given state.
•A reﬂex agent learns a policy that maps directly from states to actions.
A utility-based agent must also have a model of the environment in order to make decisions,
because it must know the states to which its actions will lead. For example, in order to make
use of a backgammon evaluation function, a backgammon program must know what its legal
moves are and how they affect the board position . Only in this way can it apply the utility
function to the outcome states. A Q-learning agent, on the other hand, can compare the
expected utilities for its available choices without needing to know their outcomes, so it does
not need a model of the environment. On the other hand, because they do not know where
their actions lead, Q-learning agents cannot look ahead; this can seriously restrict their ability
to learn, as we shall see.
We begin in Section 21.2 with passive learning, where the agent’s policy is ﬁxed and
P ASSIVE LEARNING
the task is to learn the utilities of states (or state–action pairs); this could also involve learning
a model of the environment. Section 21.3 covers active learning, where the agent must alsoACTIVE LEARNING
learn what to do. The principal issue is exploration: an agent must experience as much asEXPLORA TION
possible of its environment in order to learn how to behave in it. Section 21.4 discusses how
an agent can use inductive learning to learn much faster from its experiences. Section 21.5
covers methods for learning direct policy representations in reﬂex agents. An understanding
of Markov decision processes (Chapter 17) is essential for this chapter.
832 Chapter 21. Reinforcement Learning
21.2 P ASSIVE REINFORCEMENT LEARNING
To keep things simple, we start with the case of a passive learning agent using a state-based
representation in a fully observable environment. In passive learning, the agent’s policy π
is ﬁxed: in state s, it always executes the action π(s). Its goal is simply to learn how good
the policy is—that is, to learn the utility function Uπ(s). We will use as our example the
4× 3 world introduced in Chapter 17. Figure 21.1 shows a policy for that world and the
corresponding utilities. Clearly, the passive learning task is similar to the policy evaluation
task, part of the policy iteration algorithm described in Section 17.3. The main difference
is that the passive learning agent does not know the transition model P(s′|s,a),w h i c h
speciﬁes the probability of reaching state s′ from state s after doing action a; nor does it
know the reward function R(s), which speciﬁes the reward for each state.
–1
+1
1
2
3
1234 123
1
2
3
–1
+ 1
4
0.611
0.812
0.655
0.762
0.918
0.705
0.660
0.868
 0.388
(a) (b)
Figure 21.1 (a) A policy π for the 4×3 world; this policy happens to be optimal with
rewards of R(s)= −0.04 in the nonterminal states and no discounting. (b) The utilities of
the states in the 4×3 world, given policy π.
The agent executes a set oftrials in the environment using its policyπ. In each trial, theTRIAL
agent starts in state (1,1) and experiences a sequence of state transitions until it reaches one
of the terminal states, (4,2) or (4,3). Its percepts supply both the current state and the reward
received in that state. Typical trials might look like this:
(1,1)-.04⇝(1,2)-.04 ⇝(1,3)-.04⇝(1,2)-.04⇝(1,3)-.04⇝(2,3)-.04 ⇝(3,3)-.04⇝(4,3)+1
(1,1)-.04⇝(1,2)-.04 ⇝(1,3)-.04⇝(2,3)-.04⇝(3,3)-.04⇝(3,2)-.04 ⇝(3,3)-.04⇝(4,3)+1
(1,1)-.04⇝(2,1)-.04 ⇝(3,1)-.04⇝(3,2)-.04⇝(4,2)-1 .
Note that each state percept is subscripted with the reward received. The object is to use the
information about rewards to learn the expected utilityUπ(s) associated with each nontermi-
nal state s. The utility is deﬁned to be the expected sum of (discounted) rewards obtained if
Section 21.2. Passive Reinforcement Learning 833
policy π is followed. As in Equation (17.2) on page 650, we write
Uπ(s)= E
[ ∞∑
t =0
γtR(St)
]
(21.1)
where R(s) is the reward for a state,St (a random variable) is the state reached at timet when
executing policy π,a n dS0 = s. We will include a discount factor γ in all of our equations,
but for the 4× 3 world we will set γ=1 .
21.2.1 Direct utility estimation
A simple method for direct utility estimation was invented in the late 1950s in the area ofDIRECT UTILITY
ESTIMA TION
adaptive control theory by Widrow and Hoff (1960). The idea is that the utility of a stateADAPTIVE CONTROL
THEORY
is the expected total reward from that state onward (called the expected reward-to-go), andREWARD-TO-GO
each trial provides a sample of this quantity for each state visited. For example, the ﬁrst trial
in the set of three given earlier provides a sample total reward of 0.72 for state (1,1), two
samples of 0.76 and 0.84 for (1,2), two samples of 0.80 and 0.88 for (1,3), and so on. Thus,
at the end of each sequence, the algorithm calculates the observed reward-to-go for each state
and updates the estimated utility for that state accordingly, just by keeping a running average
for each state in a table. In the limit of inﬁnitely many trials, the sample average will converge
to the true expectation in Equation (21.1).
It is clear that direct utility estimation is just an instance of supervised learning where
each example has the state as input and the observed reward-to-go as output. This means
that we have reduced reinforcement learning to a standard inductive learning problem, as
discussed in Chapter 18. Section 21.4 discusses the use of more powerful kinds of represen-
tations for the utility function. Learning techniques for those representations can be applied
directly to the observed data.
Direct utility estimation succeeds in reducing the reinforcement learning problem to
an inductive learning problem, about which much is known. Unfortunately, it misses a very
important source of information, namely, the fact that the utilities of states are not indepen-
dent! The utility of each state equals its own reward plus the expected utility of its successor
states. That is, the utility values obey the Bellman equations for a ﬁxed policy (see also
Equation (17.10)):
Uπ(s)= R(s)+ γ
∑
s′
P(s′| s,π(s))Uπ(s′) . (21.2)
By ignoring the connections between states, direct utility estimation misses opportunities for
learning. For example, the second of the three trials given earlier reaches the state (3,2),
which has not previously been visited. The next transition reaches (3,3), which is known
from the ﬁrst trial to have a high utility. The Bellman equation suggests immediately that
(3,2) is also likely to have a high utility, because it leads to (3,3), but direct utility estimation
learns nothing until the end of the trial. More broadly, we can view direct utility estimation
as searching for U in a hypothesis space that is much larger than it needs to be, in that it
includes many functions that violate the Bellman equations. For this reason, the algorithm
often converges very slowly.
834 Chapter 21. Reinforcement Learning
function PASSIVE -ADP-A GENT (percept) returns an action
inputs: percept, a percept indicating the current state s′ and reward signal r ′
persistent: π, a ﬁxed policy
mdp, an MDP with model P ,r e w a r d sR, discount γ
U , a table of utilities, initially empty
Nsa , a table of frequencies for state–action pairs, initially zero
Ns′|sa , a table of outcome frequencies given state–action pairs, initially zero
s, a, the previous state and action, initially null
if s′ is new then U [s′]←r ′; R[s′]←r ′
if s is not null then
increment Nsa [s,a]a n dNs′|sa [s′,s,a]
for each t such that Ns′|sa [t,s,a] is nonzero do
P(t|s,a)←Ns′|sa [t,s,a] / Nsa [s,a]
U←POLICY -EVA L UAT I O N(π,U ,mdp)
if s′.TERMINAL ? then s,a←null else s,a←s′,π[s′]
return a
Figure 21.2 A passive reinforcement learning agent based on adaptive dynamic program-
ming. The P OLICY -EVA L UAT I O Nfunction solves the ﬁxed-policy Bellman equations, as
described on page 657.
21.2.2 Adaptive dynamic programming
An adaptive dynamic programming (or ADP) agent takes advantage of the constraintsADAPTIVE DYNAMIC
PROGRAMMING
among the utilities of states by learning the transition model that connects them and solv-
ing the corresponding Markov decision process using a dynamic programming method. For
a passive learning agent, this means plugging the learned transition modelP(s
′|s,π(s)) and
the observed rewards R(s) into the Bellman equations (21.2) to calculate the utilities of the
states. As we remarked in our discussion of policy iteration in Chapter 17, these equations
are linear (no maximization involved) so they can be solved using any linear algebra pack-
age. Alternatively, we can adopt the approach of modiﬁed policy iteration (see page 657),
using a simpliﬁed value iteration process to update the utility estimates after each change to
the learned model. Because the model usually changes only slightly with each observation,
the value iteration process can use the previous utility estimates as initial values and should
converge quite quickly.
The process of learning the model itself is easy, because the environment is fully ob-
servable. This means that we have a supervised learning task where the input is a state–action
pair and the output is the resulting state. In the simplest case, we can represent the tran-
sition model as a table of probabilities. We keep track of how often each action outcome
occurs and estimate the transition probability P(s
′|s,a) from the frequency with which s′
is reached when executing a in s. For example, in the three trials given on page 832, Right
is executed three times in (1,3) and two out of three times the resulting state is (2,3), so
P((2,3)| (1,3),Right) is estimated to be 2/3.
Section 21.2. Passive Reinforcement Learning 835
0
0.2
0.4
0.6
0.8
1
0 20 40 60 80 100
Utility estimates
Number of trials
(1,1)
(1,3)
(3,2)
(3,3)
(4,3)
0
0.1
0.2
0.3
0.4
0.5
0.6
0 20 40 60 80 100
RMS error in utility
Number of trials
(a) (b)
Figure 21.3 The passive ADP learning curves for the4×3 world, given the optimal policy
shown in Figure 21.1. (a) The utility estimates for a selected subset of states, as a function
of the number of trials. Notice the large changes occurring around the 78th trial—this is the
ﬁrst time that the agent falls into the −1 terminal state at (4,2). (b) The root-mean-square
error (see Appendix A) in the estimate for U(1, 1), averaged over 20 runs of 100 trials each.
The full agent program for a passive ADP agent is shown in Figure 21.2. Its perfor-
mance on the 4× 3 world is shown in Figure 21.3. In terms of how quickly its value es-
timates improve, the ADP agent is limited only by its ability to learn the transition model.
In this sense, it provides a standard against which to measure other reinforcement learning
algorithms. It is, however, intractable for large state spaces. In backgammon, for example, it
would involve solving roughly 10
50 equations in 1050 unknowns.
A reader familiar with the Bayesian learning ideas of Chapter 20 will have noticed that
the algorithm in Figure 21.2 is using maximum-likelihood estimation to learn the transition
model; moreover, by choosing a policy based solely on the estimated model it is acting as
if the model were correct. This is not necessarily a good idea! For example, a taxi agent
that didn’t know about how trafﬁc lights might ignore a red light once or twice without no
ill effects and then formulate a policy to ignore red lights from then on. Instead, it might
be a good idea to choose a policy that, while not optimal for the model estimated by maxi-
mum likelihood, works reasonably well for the whole range of models that have a reasonable
chance of being the true model. There are two mathematical approaches that have this ﬂavor.
The ﬁrst approach, Bayesian reinforcement learning , assumes a prior probability
BAYESIAN
REINFORCEMENT
LEARNING
P(h) for each hypothesis h about what the true model is; the posterior probability P(h| e) is
obtained in the usual way by Bayes’ rule given the observations to date. Then, if the agent has
decided to stop learning, the optimal policy is the one that gives the highest expected utility.
Let u
π
h be the expected utility, averaged over all possible start states, obtained by executing
policy π in model h.T h e nw eh a v e
π∗ =a r g m a x
π
∑
h
P(h| e)uπ
h .
836 Chapter 21. Reinforcement Learning
In some special cases, this policy can even be computed! If the agent will continue learning
in the future, however, then ﬁnding an optimal policy becomes considerably more difﬁcult,
because the agent must consider the effects of future observations on its beliefs about the
transition model. The problem becomes a POMDP whose belief states are distributions over
models. This concept provides an analytical foundation for understanding the exploration
problem described in Section 21.3.
The second approach, derived from robust control theory, allows for a set of possible
ROBUST CONTROL
THEORY
modelsH and deﬁnes an optimal robust policy as one that gives the best outcome in theworst
case overH:
π∗ =a r g m a x
π
min
h
uπ
h .
Often, the setH will be the set of models that exceed some likelihood threshold on P(h| e),
so the robust and Bayesian approaches are related. Sometimes, the robust solution can be
computed efﬁciently. There are, moreover, reinforcement learning algorithms that tend to
produce robust solutions, although we do not cover them here.
21.2.3 Temporal-difference learning
Solving the underlying MDP as in the preceding section is not the only way to bring the
Bellman equations to bear on the learning problem. Another way is to use the observed
transitions to adjust the utilities of the observed states so that they agree with the constraint
equations. Consider, for example, the transition from (1,3) to (2,3) in the second trial on
page 832. Suppose that, as a result of the ﬁrst trial, the utility estimates are U
π(1,3) = 0.84
and Uπ(2,3) = 0.92. Now, if this transition occurred all the time, we would expect the utili-
ties to obey the equation
Uπ(1,3) =−0.04 +Uπ(2,3) ,
so Uπ(1,3) would be 0.88. Thus, its current estimate of 0.84 might be a little low and should
be increased. More generally, when a transition occurs from state s to state s′, we apply the
following update to Uπ(s):
Uπ(s)←Uπ(s)+ α(R(s)+ γU π(s′)−Uπ(s)) . (21.3)
Here, α is the learning rate parameter. Because this update rule uses the difference in utilities
between successive states, it is often called the temporal-difference, or TD, equation.TEMPORAL-
DIFFERENCE
All temporal-difference methods work by adjusting the utility estimates towards the
ideal equilibrium that holds locally when the utility estimates are correct. In the case of pas-
sive learning, the equilibrium is given by Equation (21.2). Now Equation (21.3) does in fact
cause the agent to reach the equilibrium given by Equation (21.2), but there is some subtlety
involved. First, notice that the update involves only the observed successor s
′, whereas the
actual equilibrium conditions involve all possible next states. One might think that this causes
an improperly large change in Uπ(s) when a very rare transition occurs; but, in fact, because
rare transitions occur only rarely, the average value of Uπ(s) will converge to the correct
value. Furthermore, if we change α from a ﬁxed parameter to a function that decreases as
the number of times a state has been visited increases, then Uπ(s) itself will converge to the
Section 21.2. Passive Reinforcement Learning 837
function PASSIVE -TD-A GENT (percept) returns an action
inputs: percept, a percept indicating the current state s′ and reward signal r ′
persistent: π, a ﬁxed policy
U , a table of utilities, initially empty
Ns, a table of frequencies for states, initially zero
s, a, r, the previous state, action, and reward, initially null
if s′ is new then U [s′]←r ′
if s is not null then
increment N s[s]
U [s]←U [s]+ α(Ns[s])(r + γU [s′] −U [s])
if s′.TERMINAL ? then s,a,r←null else s,a,r←s′,π[s′],r ′
return a
Figure 21.4 A passive reinforcement learning agent that learns utility estimates using tem-
poral differences. The step-size function α(n) is chosen to ensure convergence, as described
in the text.
correct value.1 This gives us the agent program shown in Figure 21.4. Figure 21.5 illustrates
the performance of the passive TD agent on the 4× 3 world. It does not learn quite as fast as
the ADP agent and shows much higher variability, but it is much simpler and requires much
less computation per observation. Notice thatTD does not need a transition model to perform
its updates. The environment supplies the connection between neighboring states in the form
of observed transitions.
The ADP approach and the TD approach are actually closely related. Both try to make
local adjustments to the utility estimates in order to make each state “agree” with its succes-
sors. One difference is that TD adjusts a state to agree with its observed successor (Equa-
tion (21.3)), whereas ADP adjusts the state to agree with all of the successors that might
occur, weighted by their probabilities (Equation (21.2)). This difference disappears when
the effects of TD adjustments are averaged over a large number of transitions, because the
frequency of each successor in the set of transitions is approximately proportional to its prob-
ability. A more important difference is that whereas TD makes a single adjustment per ob-
served transition, ADP makes as many as it needs to restore consistency between the utility
estimates U and the environment model P . Although the observed transition makes only a
local change in P , its effects might need to be propagated throughout U . Thus, TD can be
viewed as a crude but efﬁcient ﬁrst approximation to ADP.
Each adjustment made by ADP could be seen, from the TD point of view, as a re-
sult of a “pseudoexperience” generated by simulating the current environment model. It
is possible to extend the TD approach to use an environment model to generate several
pseudoexperiences—transitions that the TD agent can imaginemight happen, given its current
model. For each observed transition, the TD agent can generate a large number of imaginary
1 The technical conditions are given on page 725. In Figure 21.5 we have used α(n)=6 0/(59 + n),w h i c h
satisﬁes the conditions.
838 Chapter 21. Reinforcement Learning
0
0.2
0.4
0.6
0.8
1
0 100 200 300 400 500
Utility estimates
Number of trials
(1,1)
(1,3)
(2,1)
(3,3)
(4,3)
0
0.1
0.2
0.3
0.4
0.5
0.6
0 20 40 60 80 100
RMS error in utility
Number of trials
(a) (b)
Figure 21.5 The TD learning curves for the 4× 3 world. (a) The utility estimates for a
selected subset of states, as a function of the number of trials. (b) The root-mean-square error
in the estimate for U(1, 1), averaged over 20 runs of 500 trials each. Only the ﬁrst 100 trials
are shown to enable comparison with Figure 21.3.
transitions. In this way, the resulting utility estimates will approximate more and more closely
those of ADP—of course, at the expense of increased computation time.
In a similar vein, we can generate more efﬁcient versions of ADP by directly approxi-
mating the algorithms for value iteration or policy iteration. Even though the value iteration
algorithm is efﬁcient, it is intractable if we have, say, 10
100 states. However, many of the
necessary adjustments to the state values on each iteration will be extremely tiny. One pos-
sible approach to generating reasonably good answers quickly is to bound the number of
adjustments made after each observed transition. One can also use a heuristic to rank the pos-
sible adjustments so as to carry out only the most signiﬁcant ones. Theprioritized sweeping
PRIORITIZED
SWEEPING
heuristic prefers to make adjustments to states whose likely successors have just undergone a
large adjustment in their own utility estimates. Using heuristics like this, approximate ADP
algorithms usually can learn roughly as fast as full ADP, in terms of the number of training se-
quences, but can be several orders of magnitude more efﬁcient in terms of computation. (See
Exercise 21.3.) This enables them to handle state spaces that are far too large for full ADP.
Approximate ADP algorithms have an additional advantage: in the early stages of learning a
new environment, the environment model P often will be far from correct, so there is little
point in calculating an exact utility function to match it. An approximation algorithm can use
a minimum adjustment size that decreases as the environment model becomes more accurate.
This eliminates the very long value iterations that can occur early in learning due to large
changes in the model.
Section 21.3. Active Reinforcement Learning 839
21.3 A CTIVE REINFORCEMENT LEARNING
A passive learning agent has a ﬁxed policy that determines its behavior. An active agent must
decide what actions to take. Let us begin with the adaptive dynamic programming agent and
consider how it must be modiﬁed to handle this new freedom.
First, the agent will need to learn a complete model with outcome probabilities for all
actions, rather than just the model for the ﬁxed policy. The simple learning mechanism used
by P
ASSIVE -ADP-A GENT will do just ﬁne for this. Next, we need to take into account the
fact that the agent has a choice of actions. The utilities it needs to learn are those deﬁned by
the optimal policy; they obey the Bellman equations given on page 652, which we repeat here
for convenience:
U(s)= R(s)+ γ max
a
∑
s′
P(s′|s,a)U(s′) . (21.4)
These equations can be solved to obtain the utility function U using the value iteration or
policy iteration algorithms from Chapter 17. The ﬁnal issue is what to do at each step. Having
obtained a utility function U that is optimal for the learned model, the agent can extract an
optimal action by one-step look-ahead to maximize the expected utility; alternatively, if it
uses policy iteration, the optimal policy is already available, so it should simply execute the
action the optimal policy recommends. Or should it?
21.3.1 Exploration
Figure 21.6 shows the results of one sequence of trials for an ADP agent that follows the
recommendation of the optimal policy for the learned model at each step. The agent does
not learn the true utilities or the true optimal policy! What happens instead is that, in the
39th trial, it ﬁnds a policy that reaches the +1 reward along the lower route via (2,1), (3,1),
(3,2), and (3,3). (See Figure 21.6(b).) After experimenting with minor variations, from the
276th trial onward it sticks to that policy, never learning the utilities of the other states and
never ﬁnding the optimal route via (1,2), (1,3), and (2,3). We call this agent thegreedy agent.
GREEDY AGENT
Repeated experiments show that the greedy agentvery seldom converges to the optimal policy
for this environment and sometimes converges to really horrendous policies.
How can it be that choosing the optimal action leads to suboptimal results? The answer
is that the learned model is not the same as the true environment; what is optimal in the
learned model can therefore be suboptimal in the true environment. Unfortunately, the agent
does not know what the true environment is, so it cannot compute the optimal action for the
true environment. What, then, is to be done?
What the greedy agent has overlooked is that actions do more than provide rewards
according to the current learned model; they also contribute to learning the true model by af-
fecting the percepts that are received. By improving the model, the agent will receive greater
rewards in the future.
2 An agent therefore must make a tradeoff between exploitation toEXPLOITA TION
maximize its reward—as reﬂected in its current utility estimates—and exploration to maxi-EXPLORA TION
2 Notice the direct analogy to the theory of information value in Chapter 16.
840 Chapter 21. Reinforcement Learning
0
0.5
1
1.5
2
0 50 100 150 200 250 300 350 400 450 500
RMS error, policy loss
Number of trials
RMS error
Policy loss
123
1
2
3
–1
+1
4
(a) (b)
Figure 21.6 Performance of a greedy ADP agent that executes the action recommended
by the optimal policy for the learned model. (a) RMS error in the utility estimates averaged
over the nine nonterminal squares. (b) The suboptimal policy to which the greedy agent
converges in this particular sequence of trials.
mize its long-term well-being. Pure exploitation risks getting stuck in a rut. Pure exploration
to improve one’s knowledge is of no use if one never puts that knowledge into practice. In the
real world, one constantly has to decide between continuing in a comfortable existence and
striking out into the unknown in the hopes of discovering a new and better life. With greater
understanding, less exploration is necessary.
Can we be a little more precise than this? Is there an optimal exploration policy? This
question has been studied in depth in the subﬁeld of statistical decision theory that deals with
so-called bandit problems. (See sidebar.)
BANDIT PROBLEM
Although bandit problems are extremely difﬁcult to solve exactly to obtain an optimal
exploration method, it is nonetheless possible to come up with a reasonable scheme that
will eventually lead to optimal behavior by the agent. Technically, any such scheme needs
to be greedy in the limit of inﬁnite exploration, or GLIE. A GLIE scheme must try each
GLIE
action in each state an unbounded number of times to avoid having a ﬁnite probability that
an optimal action is missed because of an unusually bad series of outcomes. An ADP agent
using such a scheme will eventually learn the true environment model. A GLIE scheme must
also eventually become greedy, so that the agent’s actions become optimal with respect to the
learned (and hence the true) model.
There are several GLIE schemes; one of the simplest is to have the agent choose a ran-
dom action a fraction 1/t of the time and to follow the greedy policy otherwise. While this
does eventually converge to an optimal policy, it can be extremely slow. A more sensible
approach would give some weight to actions that the agent has not tried very often, while
tending to avoid actions that are believed to be of low utility. This can be implemented by
altering the constraint equation (21.4) so that it assigns a higher utility estimate to relatively
Section 21.3. Active Reinforcement Learning 841
EXPLORATION AND BANDITS
In Las Vegas, a one-armed bandit is a slot machine. A gambler can insert a coin,
pull the lever, and collect the winnings (if any). An n-armed bandit has n levers.
The gambler must choose which lever to play on each successive coin—the one
that has paid off best, or maybe one that has not been tried?
The n-armed bandit problem is a formal model for real problems in many vi-
tally important areas, such as deciding on the annual budget for AI research and
development. Each arm corresponds to an action (such as allocating $20 million
for the development of new AI textbooks), and the payoff from pulling the arm cor-
responds to the beneﬁts obtained from taking the action (immense). Exploration,
whether it is exploration of a new research ﬁeld or exploration of a new shopping
mall, is risky, is expensive, and has uncertain payoffs; on the other hand, failure to
explore at all means that one never discovers any actions that are worthwhile.
To formulate a bandit problem properly, one must deﬁne exactly what is meant
by optimal behavior. Most deﬁnitions in the literature assume that the aim is to
maximize the expected total reward obtained over the agent’s lifetime. These deﬁ-
nitions require that the expectation be taken over the possible worlds that the agent
could be in, as well as over the possible results of each action sequence in any given
world. Here, a “world” is deﬁned by the transition model P(s
′|s,a). Thus, in or-
der to act optimally, the agent needs a prior distribution over the possible models.
The resulting optimization problems are usually wildly intractable.
In some cases—for example, when the payoff of each machine is independent
and discounted rewards are used—it is possible to calculate a Gittins index for
each slot machine (Gittins, 1989). The index is a function only of the number of
times the slot machine has been played and how much it has paid off. The index for
each machine indicates how worthwhile it is to invest more; generally speaking, the
higher the expected return and the higher the uncertainty in the utility of a given
choice, the better. Choosing the machine with the highest index value gives an
optimal exploration policy. Unfortunately, no way has been found to extend Gittins
indices to sequential decision problems.
One can use the theory of n-armed bandits to argue for the reasonableness
of the selection strategy in genetic algorithms. (See Chapter 4.) If you consider
each arm in an n-armed bandit problem to be a possible string of genes, and the
investment of a coin in one arm to be the reproduction of those genes, then it can
be proven that genetic algorithms allocate coins optimally, given an appropriate set
of independence assumptions.

842 Chapter 21. Reinforcement Learning
unexplored state–action pairs. Essentially, this amounts to an optimistic prior over the possi-
ble environments and causes the agent to behave initially as if there were wonderful rewards
scattered all over the place. Let us use U
+(s) to denote the optimistic estimate of the utility
(i.e., the expected reward-to-go) of the state s,a n dl e tN(s,a) be the number of times action
a has been tried in state s. Suppose we are using value iteration in an ADP learning agent;
then we need to rewrite the update equation (Equation (17.6) on page 652) to incorporate the
optimistic estimate. The following equation does this:
U+(s)←R(s)+ γ max
a
f
⎞∑
s′
P(s′|s,a)U+(s′),N (s,a)
⎠
. (21.5)
Here, f(u, n) is called the exploration function . It determines how greed (preference forEXPLORA TION
FUNCTION
high values of u) is traded off against curiosity (preference for actions that have not been
tried often and have low n). The function f(u, n) should be increasing in u and decreasing
in n. Obviously, there are many possible functions that ﬁt these conditions. One particularly
simple deﬁnition is
f(u, n)=
{ R+ if n<N e
u otherwise
where R+ is an optimistic estimate of the best possible reward obtainable in any state andNe
is a ﬁxed parameter. This will have the effect of making the agent try each action–state pair
at least Ne times.
The fact that U+ rather than U appears on the right-hand side of Equation (21.5) is
very important. As exploration proceeds, the states and actions near the start state might well
be tried a large number of times. If we used U, the more pessimistic utility estimate, then
the agent would soon become disinclined to explore further aﬁeld. The use of U
+ means
that the beneﬁts of exploration are propagated back from the edges of unexplored regions,
so that actions that lead toward unexplored regions are weighted more highly, rather than
just actions that are themselves unfamiliar. The effect of this exploration policy can be seen
clearly in Figure 21.7, which shows a rapid convergence toward optimal performance, unlike
that of the greedy approach. A very nearly optimal policy is found after just 18 trials. Notice
that the utility estimates themselves do not converge as quickly. This is because the agent
stops exploring the unrewarding parts of the state space fairly soon, visiting them only “by
accident” thereafter. However, it makes perfect sense for the agent not to care about the exact
utilities of states that it knows are undesirable and can be avoided.
21.3.2 Learning an action-utility function
Now that we have an active ADP agent, let us consider how to construct an active temporal-
difference learning agent. The most obvious change from the passive case is that the agent
is no longer equipped with a ﬁxed policy, so, if it learns a utility function U, it will need to
learn a model in order to be able to choose an action based on U via one-step look-ahead.
The model acquisition problem for the TD agent is identical to that for the ADP agent. What
of the TD update rule itself? Perhaps surprisingly, the update rule (21.3) remains unchanged.
This might seem odd, for the following reason: Suppose the agent takes a step that normally
Section 21.3. Active Reinforcement Learning 843
0.6
0.8
1
1.2
1.4
1.6
1.8
2
2.2
0 20 40 60 80 100
Utility estimates
Number of trials
(1,1)
(1,2)
(1,3)
(2,3)
(3,2)
(3,3)
(4,3)
0
0.2
0.4
0.6
0.8
1
1.2
1.4
0 20 40 60 80 100
RMS error, policy loss
Number of trials
RMS error
Policy loss
(a) (b)
Figure 21.7 Performance of the exploratory ADP agent. using R+ =2 and Ne =5 .( a )
Utility estimates for selected states over time. (b) The RMS error in utility values and the
associated policy loss.
leads to a good destination, but because of nondeterminism in the environment the agent ends
up in a catastrophic state. The TD update rule will take this as seriously as if the outcome had
been the normal result of the action, whereas one might suppose that, because the outcome
was a ﬂuke, the agent should not worry about it too much. In fact, of course, the unlikely
outcome will occur only infrequently in a large set of training sequences; hence in the long
run its effects will be weighted proportionally to its probability, as we would hope. Once
again, it can be shown that the TD algorithm will converge to the same values as ADP as the
number of training sequences tends to inﬁnity.
There is an alternative TD method, called Q-learning, which learns an action-utility
representation instead of learning utilities. We will use the notation Q(s,a) to denote the
value of doing action a in state s. Q-values are directly related to utility values as follows:
U(s)=m a x
a
Q(s,a) . (21.6)
Q-functions may seem like just another way of storing utility information, but they have a
very important property: a TD agent that learns a Q-function does not need a model of the
form P(s′|s,a), either for learning or for action selection. For this reason, Q-learning is
called a model-free method. As with utilities, we can write a constraint equation that must
MODEL-FREE hold at equilibrium when the Q-values are correct:
Q(s,a)= R(s)+ γ
∑
s′
P(s′|s,a)m a x
a′
Q(s′,a ′) . (21.7)
As in the ADP learning agent, we can use this equation directly as an update equation for
an iteration process that calculates exact Q-values, given an estimated model. This does,
however, require that a model also be learned, because the equation uses P(s
′|s,a).T h e
temporal-difference approach, on the other hand, requires no model of state transitions—all
844 Chapter 21. Reinforcement Learning
function Q-L EARNING -AGENT (percept) returns an action
inputs: percept, a percept indicating the current state s′ and reward signal r ′
persistent: Q, a table of action values indexed by state and action, initially zero
Nsa , a table of frequencies for state–action pairs, initially zero
s, a, r, the previous state, action, and reward, initially null
if TERMINAL ?(s) then Q[s,None]←r ′
if s is not null then
increment Nsa [s,a]
Q[s,a]←Q[s,a]+ α(Nsa[s,a])(r + γ maxa′ Q[s′,a′] −Q[s,a])
s,a,r←s′,argmaxa′ f(Q[s′,a′],Nsa[s′,a ′]),r ′
return a
Figure 21.8 An exploratory Q-learning agent. It is an active learner that learns the value
Q(s, a) of each action in each situation. It uses the same exploration function f as the ex-
ploratory ADP agent, but avoids having to learn the transition model because the Q-value of
a state can be related directly to those of its neighbors.
it needs are the Q values. The update equation for TD Q-learning is
Q(s,a)←Q(s,a)+ α(R(s)+ γ max
a′
Q(s′,a ′)−Q(s,a)) , (21.8)
which is calculated whenever action a is executed in state s leading to state s′.
The complete agent design for an exploratory Q-learning agent using TD is shown in
Figure 21.8. Notice that it uses exactly the same exploration function f as that used by the
exploratory ADP agent—hence the need to keep statistics on actions taken (the table N). If
a simpler exploration policy is used—say, acting randomly on some fraction of steps, where
the fraction decreases over time—then we can dispense with the statistics.
Q-learning has a close relative called SARSA (for State-Action-Reward-State-Action).
SARSA
The update rule for SARSA is very similar to Equation (21.8):
Q(s,a)←Q(s,a)+ α(R(s)+ γQ (s′,a ′)−Q(s,a)) , (21.9)
where a′ is the action actually taken in state s′. The rule is applied at the end of each
s, a, r, s ′,a ′ quintuplet—hence the name. The difference from Q-learning is quite subtle:
whereas Q-learning backs up the best Q-value from the state reached in the observed transi-
tion, SARSA waits until an action is actually taken and backs up the Q-value for that action.
Now, for a greedy agent that always takes the action with best Q-value, the two algorithms
are identical. When exploration is happening, however, they differ signiﬁcantly. Because
Q-learning uses the best Q-value, it pays no attention to the actual policy being followed—it
is an off-policy learning algorithm, whereas SARSA is anon-policy algorithm. Q-learning is
OFF-POLICY
ON-POLICY more ﬂexible than SARSA, in the sense that a Q-learning agent can learn how to behave well
even when guided by a random or adversarial exploration policy. On the other hand, SARSA
is more realistic: for example, if the overall policy is even partly controlled by other agents, it
is better to learn a Q-function for what will actually happen rather than what the agent would
like to happen.
Section 21.4. Generalization in Reinforcement Learning 845
Both Q-learning and SARSA learn the optimal policy for the 4× 3 world, but do so
at a much slower rate than the ADP agent. This is because the local updates do not enforce
consistency among all the Q-values via the model. The comparison raises a general question:
is it better to learn a model and a utility function or to learn an action-utility function with
no model? In other words, what is the best way to represent the agent function? This is
an issue at the foundations of artiﬁcial intelligence. As we stated in Chapter 1, one of the
key historical characteristics of much of AI research is its (often unstated) adherence to the
knowledge-based approach. This amounts to an assumption that the best way to represent
the agent function is to build a representation of some aspects of the environment in which
the agent is situated.
Some researchers, both inside and outside AI, have claimed that the availability of
model-free methods such as Q-learning means that the knowledge-based approach is unnec-
essary. There is, however, little to go on but intuition. Our intuition, for what it’s worth, is that
as the environment becomes more complex, the advantages of a knowledge-based approach
become more apparent. This is borne out even in games such as chess, checkers (draughts),
and backgammon (see next section), where efforts to learn an evaluation function by means
of a model have met with more success than Q-learning methods.
21.4 G ENERALIZATION IN REINFORCEMENT LEARNING
So far, we have assumed that the utility functions and Q-functions learned by the agents are
represented in tabular form with one output value for each input tuple. Such an approach
works reasonably well for small state spaces, but the time to convergence and (for ADP) the
time per iteration increase rapidly as the space gets larger. With carefully controlled, approx-
imate ADP methods, it might be possible to handle 10,000 states or more. This sufﬁces for
two-dimensional maze-like environments, but more realistic worlds are out of the question.
Backgammon and chess are tiny subsets of the real world, yet their state spaces contain on
the order of 10
20 and 1040 states, respectively. It would be absurd to suppose that one must
visit all these states many times in order to learn how to play the game!
One way to handle such problems is to use function approximation, which simplyFUNCTION
APPROXIMA TION
means using any sort of representation for the Q-function other than a lookup table. The
representation is viewed as approximate because it might not be the case that the true utility
function or Q-function can be represented in the chosen form. For example, in Chapter 5 we
described an evaluation function for chess that is represented as a weighted linear function
of a set of features (or basis functions) f1,...,f n:BASIS FUNCTION
ˆUθ(s)= θ1 f1(s)+ θ2 f2(s)+ ··· + θn fn(s) .
A reinforcement learning algorithm can learn values for the parameters θ=θ1,...,θ n such
that the evaluation function ˆUθ approximates the true utility function. Instead of, say, 1040
values in a table, this function approximator is characterized by, say, n =2 0 parameters—
an enormous compression. Although no one knows the true utility function for chess, no
one believes that it can be represented exactly in 20 numbers. If the approximation is good
846 Chapter 21. Reinforcement Learning
enough, however, the agent might still play excellent chess.3 Function approximation makes
it practical to represent utility functions for very large state spaces, but that is not its principal
beneﬁt. The compression achieved by a function approximator allows the learning agent to
generalize from states it has visited to states it has not visited. That is, the most important
aspect of function approximation is not that it requires less space, but that it allows for induc-
tive generalization over input states. To give you some idea of the power of this effect: by
examining only one in every1012 of the possible backgammon states, it is possible to learn a
utility function that allows a program to play as well as any human (Tesauro, 1992).
On the ﬂip side, of course, there is the problem that there could fail to be any function
in the chosen hypothesis space that approximates the true utility function sufﬁciently well.
As in all inductive learning, there is a tradeoff between the size of the hypothesis space and
the time it takes to learn the function. A larger hypothesis space increases the likelihood that
a good approximation can be found, but also means that convergence is likely to be delayed.
Let us begin with the simplest case, which is direct utility estimation. (See Section 21.2.)
With function approximation, this is an instance of supervised learning. For example, sup-
pose we represent the utilities for the4× 3 world using a simple linear function. The features
of the squares are just their x and y coordinates, so we have
ˆU
θ(x, y)= θ0 + θ1x + θ2y. (21.10)
Thus, if (θ0,θ1,θ2)=( 0.5,0.2,0.1),t h e nˆUθ(1,1) = 0.8. Given a collection of trials, we ob-
tain a set of sample values of ˆUθ(x, y), and we can ﬁnd the best ﬁt, in the sense of minimizing
the squared error, using standard linear regression. (See Chapter 18.)
For reinforcement learning, it makes more sense to use an online learning algorithm
that updates the parameters after each trial. Suppose we run a trial and the total reward
obtained starting at (1,1) is 0.4. This suggests that ˆU
θ(1,1), currently 0.8, is too large and
must be reduced. How should the parameters be adjusted to achieve this? As with neural-
network learning, we write an error function and compute its gradient with respect to the
parameters. If uj(s) is the observed total reward from state s onward in the jth trial, then
the error is deﬁned as (half) the squared difference of the predicted total and the actual total:
E
j(s)=( ˆUθ(s)−uj(s))2/2. The rate of change of the error with respect to each parameter
θi is ∂Ej/∂θi, so to move the parameter in the direction of decreasing the error, we want
θi ←θi−α ∂Ej(s)
∂θi
= θi + α(uj(s)−ˆUθ(s))∂ˆUθ(s)
∂θi
. (21.11)
This is called the Widrow–Hoff rule,o rt h e delta rule , for online least-squares. For theWIDROW–HOFF RULE
DEL T A RULE linear function approximator ˆUθ(s) in Equation (21.10), we get three simple update rules:
θ0 ←θ0 + α(uj(s)−ˆUθ(s)) ,
θ1 ←θ1 + α(uj(s)−ˆUθ(s))x,
θ2 ←θ2 + α(uj(s)−ˆUθ(s))y.
3 We do know that the exact utility function can be represented in a page or two of Lisp, Java, or C++. That is,
it can be represented by a program that solves the game exactly every time it is called. We are interested only in
function approximators that use a reasonable amount of computation. It might in fact be better to learn a very
simple function approximator and combine it with a certain amount of look-ahead search. The tradeoffs involved
are currently not well understood.
Section 21.4. Generalization in Reinforcement Learning 847
We can apply these rules to the example where ˆUθ(1,1) is 0.8 and uj(1,1) is 0.4. θ0, θ1,
and θ2 are all decreased by 0.4α, which reduces the error for (1,1). Notice that changing the
parameters θin response to an observed transition between two states also changes the values
of ˆUθ for every other state! This is what we mean by saying that function approximation
allows a reinforcement learner to generalize from its experiences.
We expect that the agent will learn faster if it uses a function approximator, provided
that the hypothesis space is not too large, but includes some functions that are a reasonably
good ﬁt to the true utility function. Exercise 21.5 asks you to evaluate the performance of
direct utility estimation, both with and without function approximation. The improvement in
the 4× 3 world is noticeable but not dramatic, because this is a very small state space to begin
with. The improvement is much greater in a 10× 10 world with a +1 reward at (10,10). This
world is well suited for a linear utility function because the true utility function is smooth
and nearly linear. (See Exercise 21.8.) If we put the +1 reward at (5,5), the true utility is
more like a pyramid and the function approximator in Equation (21.10) will fail miserably.
All is not lost, however! Remember that what matters for linear function approximation
is that the function be linear in the parameters—the features themselves can be arbitrary
nonlinear functions of the state variables. Hence, we can include a term such asθ
3f3(x, y)=
θ3
√
(x−xg)2 +( y−yg)2 that measures the distance to the goal.
We can apply these ideas equally well to temporal-difference learners. All we need do is
adjust the parameters to try to reduce the temporal difference between successive states. The
new versions of the TD and Q-learning equations (21.3 on page 836 and 21.8 on page 844)
are given by
θ
i ←θi + α[R(s)+ γ ˆUθ(s′)−ˆUθ(s)]∂ˆUθ(s)
∂θi
(21.12)
for utilities and
θi ←θi + α[R(s)+ γ max
a′
ˆQθ(s′,a ′)−ˆQθ(s,a)]∂ˆQθ(s,a)
∂θi
(21.13)
for Q-values. For passive TD learning, the update rule can be shown to converge to the closest
possible4 approximation to the true function when the function approximator is linear in the
parameters. With active learning and nonlinear functions such as neural networks, all bets
are off: There are some very simple cases in which the parameters can go off to inﬁnity
even though there are good solutions in the hypothesis space. There are more sophisticated
algorithms that can avoid these problems, but at present reinforcement learning with general
function approximators remains a delicate art.
Function approximation can also be very helpful for learning a model of the environ-
ment. Remember that learning a model for an observable environment is a supervised learn-
ing problem, because the next percept gives the outcome state. Any of the supervised learning
methods in Chapter 18 can be used, with suitable adjustments for the fact that we need to pre-
dict a complete state description rather than just a Boolean classiﬁcation or a single real value.
For a partially observable environment, the learning problem is much more difﬁcult. If we
know what the hidden variables are and how they are causally related to each other and to the
4 The deﬁnition of distance between utility functions is rather technical; see Tsitsiklis and Van Roy (1997).
848 Chapter 21. Reinforcement Learning
observable variables, then we can ﬁx the structure of a dynamic Bayesian network and use the
EM algorithm to learn the parameters, as was described in Chapter 20. Inventing the hidden
variables and learning the model structure are still open problems. Some practical examples
are described in Section 21.6.
21.5 P OLICY SEARCH
The ﬁnal approach we will consider for reinforcement learning problems is called policy
search. In some ways, policy search is the simplest of all the methods in this chapter: thePOLICY SEARCH
idea is to keep twiddling the policy as long as its performance improves, then stop.
Let us begin with the policies themselves. Remember that a policy π is a function that
maps states to actions. We are interested primarily in parameterized representations of π that
have far fewer parameters than there are states in the state space (just as in the preceding
section). For example, we could represent π by a collection of parameterized Q-functions,
one for each action, and take the action with the highest predicted value:
π(s)=m a x
a
ˆQθ(s,a) . (21.14)
Each Q-function could be a linear function of the parameters θ, as in Equation (21.10),
or it could be a nonlinear function such as a neural network. Policy search will then ad-
just the parameters θ to improve the policy. Notice that if the policy is represented by Q-
functions, then policy search results in a process that learns Q-functions. This process is
not the same as Q-learning! In Q-learning with function approximation, the algorithm ﬁnds
a value of θ such that ˆQθ is “close” to Q∗, the optimal Q-function. Policy search, on the
other hand, ﬁnds a value of θthat results in good performance; the values found by the two
methods may differ very substantially. (For example, the approximate Q-function deﬁned
by ˆQ
θ(s,a)= Q∗(s,a)/10 gives optimal performance, even though it is not at all close to
Q∗.) Another clear instance of the difference is the case where π(s) is calculated using, say,
depth-10 look-ahead search with an approximate utility function ˆUθ.Av a l u eo fθthat gives
good results may be a long way from making ˆUθ resemble the true utility function.
One problem with policy representations of the kind given in Equation (21.14) is that
the policy is a discontinuous function of the parameters when the actions are discrete. (For a
continuous action space, the policy can be a smooth function of the parameters.) That is, there
will be values ofθsuch that an inﬁnitesimal change inθcauses the policy to switch from one
action to another. This means that the value of the policy may also change discontinuously,
which makes gradient-based search difﬁcult. For this reason, policy search methods often use
a stochastic policy representation π
θ(s,a), which speciﬁes the probability of selecting actionSTOCHASTIC POLICY
a in state s. One popular representation is the softmax function:SOFTMAX FUNCTION
πθ(s,a)= e
ˆQθ(s,a)/
∑
a′
e
ˆQθ(s,a′) .
Softmax becomes nearly deterministic if one action is much better than the others, but it
always gives a differentiable function of θ; hence, the value of the policy (which depends in
Section 21.5. Policy Search 849
a continuous fashion on the action selection probabilities) is a differentiable function of θ.
Softmax is a generalization of the logistic function (page 725) to multiple variables.
Now let us look at methods for improving the policy. We start with the simplest case: a
deterministic policy and a deterministic environment. Let ρ(θ) be the policy value, i.e., thePOLICY VALUE
expected reward-to-go whenπθ is executed. If we can derive an expression forρ(θ) in closed
form, then we have a standard optimization problem, as described in Chapter 4. We can follow
the policy gradient vector∇θρ(θ) provided ρ(θ) is differentiable. Alternatively, if ρ(θ) isPOLICY GRADIENT
not available in closed form, we can evaluate πθ simply by executing it and observing the
accumulated reward. We can follow theempirical gradient by hill climbing—i.e., evaluating
the change in policy value for small increments in each parameter. With the usual caveats,
this process will converge to a local optimum in policy space.
When the environment (or the policy) is stochastic, things get more difﬁcult. Suppose
we are trying to do hill climbing, which requires comparing ρ(θ) and ρ(θ+Δ θ) for some
small Δ θ. The problem is that the total reward on each trial may vary widely, so estimates
of the policy value from a small number of trials will be quite unreliable; trying to compare
two such estimates will be even more unreliable. One solution is simply to run lots of trials,
measuring the sample variance and using it to determine that enough trials have been run
to get a reliable indication of the direction of improvement for ρ(θ). Unfortunately, this is
impractical for many real problems where each trial may be expensive, time-consuming, and
perhaps even dangerous.
For the case of a stochastic policyπ
θ(s,a), it is possible to obtain an unbiased estimate
of the gradient at θ,∇θρ(θ), directly from the results of trials executed at θ. For simplicity,
we will derive this estimate for the simple case of a nonsequential environment in which the
reward R(a) is obtained immediately after doing action a in the start state s
0. In this case,
the policy value is just the expected value of the reward, and we have
∇θρ(θ)= ∇θ
∑
a
πθ(s0,a)R(a)=
∑
a
(∇θπθ(s0,a))R(a) .
Now we perform a simple trick so that this summation can be approximated by samples
generated from the probability distribution deﬁned by πθ(s0,a). Suppose that we have N
trials in all and the action taken on the jth trial is aj .T h e n
∇θρ(θ)=
∑
a
πθ(s0,a)· (∇θπθ(s0,a))R(a)
πθ(s0,a) ≈1
N
N∑
j =1
(∇θπθ(s0,aj))R(aj)
πθ(s0,aj) .
Thus, the true gradient of the policy value is approximated by a sum of terms involving
the gradient of the action-selection probability in each trial. For the sequential case, this
generalizes to
∇
θρ(θ)≈1
N
N∑
j =1
(∇θπθ(s,aj))Rj(s)
πθ(s,aj)
for each state s visited, where aj is executed in s on the jth trial and Rj(s) is the total
reward received from state s onwards in the jth trial. The resulting algorithm is called
REINFORCE (Williams, 1992); it is usually much more effective than hill climbing using
lots of trials at each value of θ. It is still much slower than necessary, however.
850 Chapter 21. Reinforcement Learning
Consider the following task: given two blackjack 5 programs, determine which is best.
One way to do this is to have each play against a standard “dealer” for a certain number of
hands and then to measure their respective winnings. The problem with this, as we have seen,
is that the winnings of each program ﬂuctuate widely depending on whether it receives good
or bad cards. An obvious solution is to generate a certain number of hands in advance and
have each program play the same set of hands. In this way, we eliminate the measurement
error due to differences in the cards received. This idea, called correlated sampling, un-CORRELA TED
SAMPLING
derlies a policy-search algorithm called P EGASUS (Ng and Jordan, 2000). The algorithm is
applicable to domains for which a simulator is available so that the “random” outcomes of
actions can be repeated. The algorithm works by generating in advance N sequences of ran-
dom numbers, each of which can be used to run a trial of any policy. Policy search is carried
out by evaluating each candidate policy using thesame set of random sequences to determine
the action outcomes. It can be shown that the number of random sequences required to ensure
that the value of every policy is well estimated depends only on the complexity of the policy
space, and not at all on the complexity of the underlying domain.
21.6 A PPLICATIONS OF REINFORCEMENT LEARNING
We now turn to examples of large-scale applications of reinforcement learning. We consider
applications in game playing, where the transition model is known and the goal is to learn the
utility function, and in robotics, where the model is usually unknown.
21.6.1 Applications to game playing
The ﬁrst signiﬁcant application of reinforcement learning was also the ﬁrst signiﬁcant learn-
ing program of any kind—the checkers program written by Arthur Samuel (1959, 1967).
Samuel ﬁrst used a weighted linear function for the evaluation of positions, using up to 16
terms at any one time. He applied a version of Equation (21.12) to update the weights. There
were some signiﬁcant differences, however, between his program and current methods. First,
he updated the weights using the difference between the current state and the backed-up value
generated by full look-ahead in the search tree. This works ﬁne, because it amounts to view-
ing the state space at a different granularity. A second difference was that the program did
not use any observed rewards! That is, the values of terminal states reached in self-play were
ignored. This means that it is theoretically possible for Samuel’s program not to converge, or
to converge on a strategy designed to lose rather than to win. He managed to avoid this fate
by insisting that the weight for material advantage should always be positive. Remarkably,
this was sufﬁcient to direct the program into areas of weight space corresponding to good
checkers play.
Gerry Tesauro’s backgammon program TD-G
AMMON (1992) forcefully illustrates the
potential of reinforcement learning techniques. In earlier work (Tesauro and Sejnowski,
1989), Tesauro tried learning a neural network representation of Q(s,a) directly from ex-
5 Also known as twenty-one or pontoon.
Section 21.6. Applications of Reinforcement Learning 851
x
θ
Figure 21.9 Setup for the problem of balancing a long pole on top of a moving cart. The
cart can be jerked left or right by a controller that observes x, θ, ˙x,a n d ˙θ.
amples of moves labeled with relative values by a human expert. This approach proved
extremely tedious for the expert. It resulted in a program, called N EUROGAMMON ,t h a tw a s
strong by computer standards, but not competitive with human experts. The TD-G AMMON
project was an attempt to learn from self-play alone. The only reward signal was given at
the end of each game. The evaluation function was represented by a fully connected neural
network with a single hidden layer containing 40 nodes. Simply by repeated application of
Equation (21.12), TD-G
AMMON learned to play considerably better than NEUROGAMMON ,
even though the input representation contained just the raw board position with no computed
features. This took about 200,000 training games and two weeks of computer time. Although
that may seem like a lot of games, it is only a vanishingly small fraction of the state space.
When precomputed features were added to the input representation, a network with 80 hidden
nodes was able, after 300,000 training games, to reach a standard of play comparable to that
of the top three human players worldwide. Kit Woolsey, a top player and analyst, said that
“There is no question in my mind that its positional judgment is far better than mine.”
21.6.2 Application to robot control
The setup for the famous cart–pole balancing problem, also known as the inverted pendu-CART –POLE
lum, is shown in Figure 21.9. The problem is to control the position x of the cart so thatINVERTED
PENDULUM
the pole stays roughly upright ( θ≈π/2), while staying within the limits of the cart track
as shown. Several thousand papers in reinforcement learning and control theory have been
published on this seemingly simple problem. The cart–pole problem differs from the prob-
lems described earlier in that the state variablesx, θ, ˙x,a n d ˙θare continuous. The actions are
usually discrete: jerk left or jerk right, the so-called bang-bang control regime.BANG-BANG
CONTROL
The earliest work on learning for this problem was carried out by Michie and Cham-
bers (1968). Their B OXES algorithm was able to balance the pole for over an hour after only
about 30 trials. Moreover, unlike many subsequent systems, BOXES was implemented with a
852 Chapter 21. Reinforcement Learning
real cart and pole, not a simulation. The algorithm ﬁrst discretized the four-dimensional state
space into boxes—hence the name. It then ran trials until the pole fell over or the cart hit the
end of the track. Negative reinforcement was associated with the ﬁnal action in the ﬁnal box
and then propagated back through the sequence. It was found that the discretization caused
some problems when the apparatus was initialized in a position different from those used in
training, suggesting that generalization was not perfect. Improved generalization and faster
learning can be obtained using an algorithm that adaptively partitions the state space accord-
ing to the observed variation in the reward, or by using a continuous-state, nonlinear function
approximator such as a neural network. Nowadays, balancing a triple inverted pendulum is a
common exercise—a feat far beyond the capabilities of most humans.
Still more impressive is the application of reinforcement learning to helicopter ﬂight
(Figure 21.10). This work has generally used policy search (Bagnell and Schneider, 2001)
as well as the P
EGASUS algorithm with simulation based on a learned transition model (Ng
et al., 2004). Further details are given in Chapter 25.
Figure 21.10 Superimposed time-lapse images of an autonomous helicopter performing
a very difﬁcult “nose-in circle” maneuver. The helicopter is under the control of a policy
developed by the P EGASUS policy-search algorithm. A simulator model was developed by
observing the effects of various control manipulations on the real helicopter; then the algo-
rithm was run on the simulator model overnight. A variety of controllers were developed for
different maneuvers. In all cases, performance far exceeded that of an expert human pilot
using remote control. (Image courtesy of Andrew Ng.)

Section 21.7. Summary 853
21.7 S UMMARY
This chapter has examined the reinforcement learning problem: how an agent can become
proﬁcient in an unknown environment, given only its percepts and occasional rewards. Rein-
forcement learning can be viewed as a microcosm for the entire AI problem, but it is studied
in a number of simpliﬁed settings to facilitate progress. The major points are:
•The overall agent design dictates the kind of information that must be learned. The
three main designs we covered were the model-based design, using a model P and a
utility function U ; the model-free design, using an action-utility function Q;a n dt h e
reﬂex design, using a policy π.
•Utilities can be learned using three approaches:
1. Direct utility estimation uses the total observed reward-to-go for a given state as
direct evidence for learning its utility.
2. Adaptive dynamic programming (ADP) learns a model and a reward function
from observations and then uses value or policy iteration to obtain the utilities or
an optimal policy. ADP makes optimal use of the local constraints on utilities of
states imposed through the neighborhood structure of the environment.
3. Temporal-difference(TD) methods update utility estimates to match those of suc-
cessor states. They can be viewed as simple approximations to the ADP approach
that can learn without requiring a transition model. Using a learned model to gen-
erate pseudoexperiences can, however, result in faster learning.
•Action-utility functions, or Q-functions, can be learned by an ADP approach or a TD
approach. With TD, Q-learning requires no model in either the learning or action-
selection phase. This simpliﬁes the learning problem but potentially restricts the ability
to learn in complex environments, because the agent cannot simulate the results of
possible courses of action.
•When the learning agent is responsible for selecting actions while it learns, it must
trade off the estimated value of those actions against the potential for learning useful
new information. An exact solution of the exploration problem is infeasible, but some
simple heuristics do a reasonable job.
•In large state spaces, reinforcement learning algorithms must use an approximate func-
tional representation in order to generalize over states. The temporal-difference signal
can be used directly to update parameters in representations such as neural networks.
•Policy-search methods operate directly on a representation of the policy, attempting
to improve it based on observed performance. The variation in the performance in a
stochastic domain is a serious problem; for simulated domains this can be overcome by
ﬁxing the randomness in advance.
Because of its potential for eliminating hand coding of control strategies, reinforcement learn-
ing continues to be one of the most active areas of machine learning research. Applications
in robotics promise to be particularly valuable; these will require methods for handling con-
854 Chapter 21. Reinforcement Learning
tinuous, high-dimensional, partially observable environments in which successful behaviors
may consist of thousands or even millions of primitive actions.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Turing (1948, 1950) proposed the reinforcement-learning approach, although he was not con-
vinced of its effectiveness, writing, “the use of punishments and rewards can at best be a part
of the teaching process.” Arthur Samuel’s work (1959) was probably the earliest successful
machine learning research. Although this work was informal and had a number of ﬂaws,
it contained most of the modern ideas in reinforcement learning, including temporal differ-
encing and function approximation. Around the same time, researchers in adaptive control
theory (Widrow and Hoff, 1960), building on work by Hebb (1949), were training simple net-
works using the delta rule. (This early connection between neural networks and reinforcement
learning may have led to the persistent misperception that the latter is a subﬁeld of the for-
mer.) The cart–pole work of Michie and Chambers (1968) can also be seen as a reinforcement
learning method with a function approximator. The psychological literature on reinforcement
learning is much older; Hilgard and Bower (1975) provide a good survey. Direct evidence for
the operation of reinforcement learning in animals has been provided by investigations into
the foraging behavior of bees; there is a clear neural correlate of the reward signal in the form
of a large neuron mapping from the nectar intake sensors directly to the motor cortex (Mon-
tague et al., 1995). Research using single-cell recording suggests that the dopamine system
in primate brains implements something resembling value function learning (Schultz et al.,
1997). The neuroscience text by Dayan and Abbott (2001) describes possible neural imple-
mentations of temporal-difference learning, while Dayan and Niv (2008) survey the latest
evidence from neuroscientiﬁc and behavioral experiments.
The connection between reinforcement learning and Markov decision processes was
ﬁrst made by Werbos (1977), but the development of reinforcement learning in AI stems
from work at the University of Massachusetts in the early 1980s (Barto et al. , 1981). The
paper by Sutton (1988) provides a good historical overview. Equation (21.3) in this chapter
is a special case for λ=0 of Sutton’s general TD (λ) algorithm. TD (λ) updates the utility
values of all states in a sequence leading up to each transition by an amount that drops off as
λ
t for states t steps in the past. TD (1) is identical to the Widrow–Hoff or delta rule. Boyan
(2002), building on work by Bradtke and Barto (1996), argues that TD (λ) and related algo-
rithms make inefﬁcient use of experiences; essentially, they are online regression algorithms
that converge much more slowly than ofﬂine regression. His LSTD (least-squares temporal
differencing) algorithm is an online algorithm for passive reinforcement learning that gives
the same results as ofﬂine regression. Least-squares policy iteration, or LSPI (Lagoudakis
and Parr, 2003), combines this idea with the policy iteration algorithm, yielding a robust,
statistically efﬁcient, model-free algorithm for learning policies.
The combination of temporal-difference learning with the model-based generation of
simulated experiences was proposed in Sutton’s D
YNA architecture (Sutton, 1990). The idea
of prioritized sweeping was introduced independently by Moore and Atkeson (1993) and
Bibliographical and Historical Notes 855
Peng and Williams (1993). Q-learning was developed in Watkins’s Ph.D. thesis (1989), while
SARSA appeared in a technical report by Rummery and Niranjan (1994).
Bandit problems, which model the problem of exploration for nonsequential decisions,
are analyzed in depth by Berry and Fristedt (1985). Optimal exploration strategies for several
settings are obtainable using the technique called Gittins indices (Gittins, 1989). A vari-
ety of exploration methods for sequential decision problems are discussed by Barto et al.
(1995). Kearns and Singh (1998) and Brafman and Tennenholtz (2000) describe algorithms
that explore unknown environments and are guaranteed to converge on near-optimal policies
in polynomial time. Bayesian reinforcement learning (Dearden et al., 1998, 1999) provides
another angle on both model uncertainty and exploration.
Function approximation in reinforcement learning goes back to the work of Samuel,
who used both linear and nonlinear evaluation functions and also used feature-selection meth-
ods to reduce the feature space. Later methods include the CMAC (Cerebellar Model Artic-
CMAC
ulation Controller) (Albus, 1975), which is essentially a sum of overlapping local kernel
functions, and the associative neural networks of Barto et al. (1983). Neural networks are
currently the most popular form of function approximator. The best-known application is
TD-Gammon (Tesauro, 1992, 1995), which was discussed in the chapter. One signiﬁcant
problem exhibited by neural-network-based TD learners is that they tend to forget earlier ex-
periences, especially those in parts of the state space that are avoided once competence is
achieved. This can result in catastrophic failure if such circumstances reappear. Function ap-
proximation based on instance-based learning can avoid this problem (Ormoneit and Sen,
2002; Forbes, 2002).
The convergence of reinforcement learning algorithms using function approximation is
an extremely technical subject. Results for TD learning have been progressively strength-
ened for the case of linear function approximators (Sutton, 1988; Dayan, 1992; Tsitsiklis and
Van Roy, 1997), but several examples of divergence have been presented for nonlinear func-
tions (see Tsitsiklis and Van Roy, 1997, for a discussion). Papavassiliou and Russell (1999)
describe a new type of reinforcement learning that converges with any form of function ap-
proximator, provided that a best-ﬁt approximation can be found for the observed data.
Policy search methods were brought to the fore by Williams (1992), who developed the
R
EINFORCE family of algorithms. Later work by Marbach and Tsitsiklis (1998), Suttonet al.
(2000), and Baxter and Bartlett (2000) strengthened and generalized the convergence results
for policy search. The method of correlated sampling for comparing different conﬁgurations
of a system was described formally by Kahn and Marshall (1953), but seems to have been
known long before that. Its use in reinforcement learning is due to Van Roy (1998) and Ng
and Jordan (2000); the latter paper also introduced the P
EGASUS algorithm and proved its
formal properties.
As we mentioned in the chapter, the performance of a stochastic policy is a continu-
ous function of its parameters, which helps with gradient-based search methods. This is not
the only beneﬁt: Jaakkola et al. (1995) argue that stochastic policies actually work better
than deterministic policies in partially observable environments, if both are limited to act-
ing based on the current percept. (One reason is that the stochastic policy is less likely to
get “stuck” because of some unseen hindrance.) Now, in Chapter 17 we pointed out that
856 Chapter 21. Reinforcement Learning
optimal policies in partially observable MDPs are deterministic functions of the belief state
rather than the current percept, so we would expect still better results by keeping track of the
belief state using the ﬁltering methods of Chapter 15. Unfortunately, belief-state space is
high-dimensional and continuous, and effective algorithms have not yet been developed for
reinforcement learning with belief states.
Real-world environments also exhibit enormous complexity in terms of the number
of primitive actions required to achieve signiﬁcant reward. For example, a robot playing
soccer might make a hundred thousand individual leg motions before scoring a goal. One
common method, used originally in animal training, is calledreward shaping. This involves
REWARD SHAPING
supplying the agent with additional rewards, called pseudorewards, for “making progress.”PSEUDOREWARD
For example, in soccer the real reward is for scoring a goal, but pseudorewards might be
given for making contact with the ball or for kicking it toward the goal. Such rewards can
speed up learning enormously and are simple to provide, but there is a risk that the agent
will learn to maximize the pseudorewards rather than the true rewards; for example, standing
next to the ball and “vibrating” causes many contacts with the ball. Ng et al. (1999) show
that the agent will still learn the optimal policy provided that the pseudoreward F(s, a, s
′)
satisﬁes F(s, a, s′)= γΦ(s′)−Φ(s),w h e r eΦ is an arbitrary function of the state. Φ can be
constructed to reﬂect any desirable aspects of the state, such as achievement of subgoals or
distance to a goal state.
The generation of complex behaviors can also be facilitated byhierarchical reinforce-
ment learning methods, which attempt to solve problems at multiple levels of abstraction—
HIERARCHICAL
REINFORCEMENT
LEARNING
much like the HTN planning methods of Chapter 11. For example, “scoring a goal” can be
broken down into “obtain possession,” “dribble towards the goal,” and “shoot;” and each of
these can be broken down further into lower-level motor behaviors. The fundamental result
in this area is due to Forestier and Varaiya (1978), who proved that lower-level behaviors
of arbitrary complexity can be treated just like primitive actions (albeit ones that can take
varying amounts of time) from the point of view of the higher-level behavior that invokes
them. Current approaches (Parr and Russell, 1998; Dietterich, 2000; Sutton et al. , 2000;
Andre and Russell, 2002) build on this result to develop methods for supplying an agent
with a partial program that constrains the agent’s behavior to have a particular hierarchical
PARTIAL PROGRAM
structure. The partial-programming language for agent programs extends an ordinary pro-
gramming language by adding primitives for unspeciﬁed choices that must be ﬁlled in by
learning. Reinforcement learning is then applied to learn the best behavior consistent with
the partial program. The combination of function approximation, shaping, and hierarchical
reinforcement learning has been shown to solve large-scale problems—for example, policies
that execute for10
4 steps in state spaces of10100 states with branching factors of1030 (Marthi
et al., 2005). One key result (Dietterich, 2000) is that the hierarchical structure provides a
natural additive decomposition of the overall utility function into terms that depend on small
subsets of the variables deﬁning the state space. This is somewhat analogous to the represen-
tation theorems underlying the conciseness of Bayes nets (Chapter 14).
The topic of distributed and multiagent reinforcement learning was not touched upon in
the chapter but is of great current interest. In distributed RL, the aim is to devise methods by
which multiple, coordinated agents learn to optimize a common utility function. For example,
Bibliographical and Historical Notes 857
can we devise methods whereby separate subagents for robot navigation and robot obstacleSUBAGENT
avoidance could cooperatively achieve a combined control system that is globally optimal?
Some basic results in this direction have been obtained (Guestrin et al., 2002; Russell and
Zimdars, 2003). The basic idea is that each subagent learns its own Q-function from its
own stream of rewards. For example, a robot-navigation component can receive rewards for
making progress towards the goal, while the obstacle-avoidance component receives negative
rewards for every collision. Each global decision maximizes the sum of Q-functions and the
whole process converges to globally optimal solutions.
Multiagent RL is distinguished from distributed RL by the presence of agents who
cannot coordinate their actions (except by explicit communicative acts) and who may not
share the same utility function. Thus, multiagent RL deals with sequential game-theoretic
problems or Markov games, as deﬁned in Chapter 17. The consequent requirement for ran-
domized policies is not a signiﬁcant complication, as we saw on page 848. What does cause
problems is the fact that, while an agent is learning to defeat its opponent’s policy, the op-
ponent is changing its policy to defeat the agent. Thus, the environment is nonstationary
(see page 568). Littman (1994) noted this difﬁculty when introducing the ﬁrst RL algorithms
for zero-sum Markov games. Hu and Wellman (2003) present a Q-learning algorithm for
general-sum games that converges when the Nash equilibrium is unique; when there are mul-
tiple equilibria, the notion of convergence is not so easy to deﬁne (Shoham et al., 2004).
Sometimes the reward function is not easy to deﬁne. Consider the task of driving a car.
There are extreme states (such as crashing the car) that clearly should have a large penalty.
But beyond that, it is difﬁcult to be precise about the reward function. However, it is easy
enough for a human to drive for a while and then tell a robot “do it like that.” The robot then
has the task of apprenticeship learning; learning from an example of the task done right,
APPRENTICESHIP
LEARNING
without explicit rewards. Ng et al. (2004) and Coates et al. (2009) show how this technique
works for learning to ﬂy a helicopter; see Figure 25.25 on page 1002 for an example of the
acrobatics the resulting policy is capable of. Russell (1998) describes the task of inverse
reinforcement learning—ﬁguring out what the reward function must be from an example
INVERSE
REINFORCEMENT
LEARNING
path through that state space. This is useful as a part of apprenticeship learning, or as a part
of doing science—we can understand an animal or robot by working backwards from what it
does to what its reward function must be.
This chapter has dealt only with atomic states—all the agent knows about a state is the
set of available actions and the utilities of the resulting states (or of state-action pairs). But
it is also possible to apply reinforcement learning to structured representations rather than
atomic ones; this is called relational reinforcement learning (Tadepalli et al., 2004).
RELA TIONAL
REINFORCEMENT
LEARNING
The survey by Kaelbling et al. (1996) provides a good entry point to the literature. The
text by Sutton and Barto (1998), two of the ﬁeld’s pioneers, focuses on architectures and algo-
rithms, showing how reinforcement learning weaves together the ideas of learning, planning,
and acting. The somewhat more technical work by Bertsekas and Tsitsiklis (1996) gives a
rigorous grounding in the theory of dynamic programming and stochastic convergence. Re-
inforcement learning papers are published frequently in Machine Learning,i nt h eJournal of
Machine Learning Research, and in the International Conferences on Machine Learning and
the Neural Information Processing Systems meetings.
858 Chapter 21. Reinforcement Learning
EXERCISES
21.1 Implement a passive learning agent in a simple environment, such as the 4× 3 world.
For the case of an initially unknown environment model, compare the learning performance
of the direct utility estimation, TD, and ADP algorithms. Do the comparison for the optimal
policy and for several random policies. For which do the utility estimates converge faster?
What happens when the size of the environment is increased? (Try environments with and
without obstacles.)
21.2 Chapter 17 deﬁned a proper policy for an MDP as one that is guaranteed to reach a
terminal state. Show that it is possible for a passive ADP agent to learn a transition model
for which its policy π is improper even if π is proper for the true MDP; with such models,
the P
OLICY -EVA L UAT I O Nstep may fail if γ=1 . Show that this problem cannot arise if
POLICY -EVA L UAT I O Nis applied to the learned model only at the end of a trial.
21.3 Starting with the passive ADP agent, modify it to use an approximate ADP algorithm
as discussed in the text. Do this in two steps:
a. Implement a priority queue for adjustments to the utility estimates. Whenever a state is
adjusted, all of its predecessors also become candidates for adjustment and should be
added to the queue. The queue is initialized with the state from which the most recent
transition took place. Allow only a ﬁxed number of adjustments.
b. Experiment with various heuristics for ordering the priority queue, examining their ef-
fect on learning rates and computation time.
21.4 Write out the parameter update equations for TD learning with
ˆU(x, y)= θ
0 + θ1x + θ2y + θ3
√
(x−xg)2 +( y−yg)2 .
21.5 Implement an exploring reinforcement learning agent that uses direct utility estima-
tion. Make two versions—one with a tabular representation and one using the function ap-
proximator in Equation (21.10). Compare their performance in three environments:
a.T h e 4× 3 world described in the chapter.
b.A 10× 10 world with no obstacles and a +1 reward at (10,10).
c.A 10× 10 world with no obstacles and a +1 reward at (5,5).
21.6 Devise suitable features for reinforcement learning in stochastic grid worlds (general-
izations of the 4× 3 world) that contain multiple obstacles and multiple terminal states with
rewards of +1 or−1.
21.7 Extend the standard game-playing environment (Chapter 5) to incorporate a reward
signal. Put two reinforcement learning agents into the environment (they may, of course,
share the agent program) and have them play against each other. Apply the generalized TD
update rule (Equation (21.12)) to update the evaluation function. You might wish to start with
a simple linear weighted evaluation function and a simple game, such as tic-tac-toe.
Exercises 859
21.8 Compute the true utility function and the best linear approximation in x and y (as in
Equation (21.10)) for the following environments:
a.A 10× 10 world with a single +1 terminal state at (10,10).
b. As in (a), but add a −1 terminal state at (10,1).
c. As in (b), but add obstacles in 10 randomly selected squares.
d. As in (b), but place a wall stretching from (5,2) to (5,9).
e. As in (a), but with the terminal state at (5,5).
The actions are deterministic moves in the four directions. In each case, compare the results
using three-dimensional plots. For each environment, propose additional features (besides x
and y) that would improve the approximation and show the results.
21.9 Implement the R EINFORCE and P EGASUS algorithms and apply them to the 4× 3
world, using a policy family of your own choosing. Comment on the results.
21.10 Is reinforcement learning an appropriate abstract model for evolution? What connec-
tion exists, if any, between hardwired reward signals and evolutionary ﬁtness?


--- BOOK CHAPTER: 17_Making_Complex_Decisions ---

17
MAKING COMPLEX
DECISIONS
In which we examine methods for deciding what to do today, given that we may
decide again tomorrow.
In this chapter, we address the computational issues involved in making decisions in a stochas-
tic environment. Whereas Chapter 16 was concerned with one-shot or episodic decision
problems, in which the utility of each action’s outcome was well known, we are concerned
here with sequential decision problems, in which the agent’s utility depends on a sequence
SEQUENTIAL
DECISION PROBLEM
of decisions. Sequential decision problems incorporate utilities, uncertainty, and sensing,
and include search and planning problems as special cases. Section 17.1 explains how se-
quential decision problems are deﬁned, and Sections 17.2 and 17.3 explain how they can
be solved to produce optimal behavior that balances the risks and rewards of acting in an
uncertain environment. Section 17.4 extends these ideas to the case of partially observable
environments, and Section 17.4.3 develops a complete design for decision-theoretic agents in
partially observable environments, combining dynamic Bayesian networks from Chapter 15
with decision networks from Chapter 16.
The second part of the chapter covers environments with multiple agents. In such en-
vironments, the notion of optimal behavior is complicated by the interactions among the
agents. Section 17.5 introduces the main ideas of game theory, including the idea that ra-
tional agents might need to behave randomly. Section 17.6 looks at how multiagent systems
can be designed so that multiple agents can achieve a common goal.
17.1 S EQUENTIAL DECISION PROBLEMS
Suppose that an agent is situated in the4× 3 environment shown in Figure 17.1(a). Beginning
in the start state, it must choose an action at each time step. The interaction with the environ-
ment terminates when the agent reaches one of the goal states, marked +1 or –1. Just as for
search problems, the actions available to the agent in each state are given by A
CTIONS (s),
sometimes abbreviated to A(s);i nt h e4× 3 environment, the actions in every state are Up,
Down, Left,a n dRight. We assume for now that the environment is fully observable,s ot h a t
the agent always knows where it is.
645
646 Chapter 17. Making Complex Decisions
1
2
3
1234
START
0.8
0.10.1
(a) (b)
–1
+ 1
Figure 17.1 (a) A simple 4×3 environment that presents the agent with a sequential
decision problem. (b) Illustration of the transition model of the environment: the “intended”
outcome occurs with probability 0.8, but with probability 0.2 the agent moves at right angles
to the intended direction. A collision with a wall results in no movement. The two terminal
states have reward +1 and –1, respectively, and all other states have a reward of –0.04.
If the environment were deterministic, a solution would be easy: [Up, Up, Right, Right,
Right]. Unfortunately, the environment won’t always go along with this solution, because the
actions are unreliable. The particular model of stochastic motion that we adopt is illustrated
in Figure 17.1(b). Each action achieves the intended effect with probability 0.8, but the rest
of the time, the action moves the agent at right angles to the intended direction. Furthermore,
if the agent bumps into a wall, it stays in the same square. For example, from the start square
(1,1), the action Up moves the agent to (1,2) with probability 0.8, but with probability 0.1, it
moves right to (2,1), and with probability 0.1, it moves left, bumps into the wall, and stays in
(1,1). In such an environment, the sequence [Up,Up,Right,Right,Right] goes up around
the barrier and reaches the goal state at (4,3) with probability 0.8
5 =0.32768. There is also a
small chance of accidentally reaching the goal by going the other way around with probability
0.1
4× 0.8, for a grand total of 0.32776. (See also Exercise 17.1.)
As in Chapter 3, the transition model (or just “model,” whenever no confusion can
arise) describes the outcome of each action in each state. Here, the outcome is stochastic,
so we write P(s
′|s,a) to denote the probability of reaching state s′ if action a is done in
state s. We will assume that transitions areMarkovian in the sense of Chapter 15, that is, the
probability of reachings′ from s depends only ons and not on the history of earlier states. For
now, you can think of P(s′|s,a) as a big three-dimensional table containing probabilities.
Later, in Section 17.4.3, we will see that the transition model can be represented as adynamic
Bayesian network, just as in Chapter 15.
To complete the deﬁnition of the task environment, we must specify the utility function
for the agent. Because the decision problem is sequential, the utility function will depend
on a sequence of states—an environment history—rather than on a single state. Later in
this section, we investigate how such utility functions can be speciﬁed in general; for now,
we simply stipulate that in each state s, the agent receives a reward R(s),w h i c hm a yb e
REWARD
positive or negative, but must be bounded. For our particular example, the reward is −0.04
in all states except the terminal states (which have rewards +1 and –1). The utility of an
Section 17.1. Sequential Decision Problems 647
environment history is just (for now) the sum of the rewards received. For example, if the
agent reaches the +1 state after 10 steps, its total utility will be 0.6. The negative reward of
–0.04 gives the agent an incentive to reach (4,3) quickly, so our environment is a stochastic
generalization of the search problems of Chapter 3. Another way of saying this is that the
agent does not enjoy living in this environment and so wants to leave as soon as possible.
To sum up: a sequential decision problem for a fully observable, stochastic environment
with a Markovian transition model and additive rewards is called aMarkov decision process,
MARKOV DECISION
PROCESS
or MDP, and consists of a set of states (with an initial state s0); a set ACTIONS (s) of actions
in each state; a transition model P(s′|s,a); and a reward function R(s).1
The next question is, what does a solution to the problem look like? We have seen that
any ﬁxed action sequence won’t solve the problem, because the agent might end up in a state
other than the goal. Therefore, a solution must specify what the agent should do for any state
that the agent might reach. A solution of this kind is called apolicy. It is traditional to denote
POLICY
a policy by π,a n dπ(s) is the action recommended by the policy π for state s. If the agent
has a complete policy, then no matter what the outcome of any action, the agent will always
know what to do next.
Each time a given policy is executed starting from the initial state, the stochastic nature
of the environment may lead to a different environment history. The quality of a policy is
therefore measured by the expected utility of the possible environment histories generated
by that policy. An optimal policy is a policy that yields the highest expected utility. We
OPTIMAL POLICY
use π∗ to denote an optimal policy. Given π∗, the agent decides what to do by consulting
its current percept, which tells it the current state s, and then executing the action π∗(s).A
policy represents the agent function explicitly and is therefore a description of a simple reﬂex
agent, computed from the information used for a utility-based agent.
An optimal policy for the world of Figure 17.1 is shown in Figure 17.2(a). Notice
that, because the cost of taking a step is fairly small compared with the penalty for ending
up in (4,2) by accident, the optimal policy for the state (3,1) is conservative. The policy
recommends taking the long way round, rather than taking the shortcut and thereby risking
entering (4,2).
The balance of risk and reward changes depending on the value of R(s) for the nonter-
minal states. Figure 17.2(b) shows optimal policies for four different ranges of R(s).W h e n
R(s)≤−1.6284, life is so painful that the agent heads straight for the nearest exit, even if
the exit is worth –1. When −0.4278≤R(s)≤−0.0850, life is quite unpleasant; the agent
takes the shortest route to the +1 state and is willing to risk falling into the –1 state by acci-
dent. In particular, the agent takes the shortcut from (3,1). When life is only slightly dreary
(−0.0221 <R (s) < 0), the optimal policy takes no risks at all . In (4,1) and (3,2), the agent
heads directly away from the –1 state so that it cannot fall in by accident, even though this
means banging its head against the wall quite a few times. Finally, if R(s) > 0, then life is
positively enjoyable and the agent avoids both exits. As long as the actions in (4,1), (3,2),
1 Some deﬁnitions of MDPs allow the reward to depend on the action and outcome too, so the reward function
is R(s, a, s′). This simpliﬁes the description of some environments but does not change the problem in any
fundamental way, as shown in Exercise 17.4.
648 Chapter 17. Making Complex Decisions
123
1
2
3+  1
–1
4
–1
+1
 R(s) < –1.6284
(a) (b)
– 0.0221 < R(s) < 0 
–1
+1
–1
+1
–1
+1
R(s) > 0 
– 0.4278 < R(s) < – 0.0850
Figure 17.2 (a) An optimal policy for the stochastic environment with R(s)= −0.04 in
the nonterminal states. (b) Optimal policies for four different ranges of R(s).
and (3,3) are as shown, every policy is optimal, and the agent obtains inﬁnite total reward be-
cause it never enters a terminal state. Surprisingly, it turns out that there are six other optimal
policies for various ranges of R(s); Exercise 17.5 asks you to ﬁnd them.
The careful balancing of risk and reward is a characteristic of MDPs that does not
arise in deterministic search problems; moreover, it is a characteristic of many real-world
decision problems. For this reason, MDPs have been studied in several ﬁelds, including
AI, operations research, economics, and control theory. Dozens of algorithms have been
proposed for calculating optimal policies. In sections 17.2 and 17.3 we describe two of the
most important algorithm families. First, however, we must complete our investigation of
utilities and policies for sequential decision problems.
17.1.1 Utilities over time
In the MDP example in Figure 17.1, the performance of the agent was measured by a sum of
rewards for the states visited. This choice of performance measure is not arbitrary, but it is
not the only possibility for the utility function on environment histories, which we write as
U
h([s0,s1,...,s n]). Our analysis draws on multiattribute utility theory (Section 16.4) and
is somewhat technical; the impatient reader may wish to skip to the next section.
The ﬁrst question to answer is whether there is a ﬁnite horizon or an inﬁnite horizonFINITE HORIZON
INFINITE HORIZON for decision making. A ﬁnite horizon means that there is a ﬁxed time N after which nothing
matters—the game is over, so to speak. Thus, Uh([s0,s1,...,s N+k]) =Uh([s0,s1,...,s N])
for all k> 0. For example, suppose an agent starts at (3,1) in the 4× 3 world of Figure 17.1,
and suppose that N =3 . Then, to have any chance of reaching the +1 state, the agent must
head directly for it, and the optimal action is to go Up. On the other hand, if N = 100,
then there is plenty of time to take the safe route by going Left. So, with a ﬁnite horizon,
Section 17.1. Sequential Decision Problems 649
the optimal action in a given state could change over time. We say that the optimal policy
for a ﬁnite horizon is nonstationary. With no ﬁxed time limit, on the other hand, there isNONST A TIONARY
POLICY
no reason to behave differently in the same state at different times. Hence, the optimal ac-
tion depends only on the current state, and the optimal policy is stationary. Policies for theSTA TIONARY POLICY
inﬁnite-horizon case are therefore simpler than those for the ﬁnite-horizon case, and we deal
mainly with the inﬁnite-horizon case in this chapter. (We will see later that for partially ob-
servable environments, the inﬁnite-horizon case is not so simple.) Note that “inﬁnite horizon”
does not necessarily mean that all state sequences are inﬁnite; it just means that there is no
ﬁxed deadline. In particular, there can be ﬁnite state sequences in an inﬁnite-horizon MDP
containing a terminal state.
The next question we must decide is how to calculate the utility of state sequences. In
the terminology of multiattribute utility theory, each states
i can be viewed as an attribute of
the state sequence [s0,s1,s2 ... ]. To obtain a simple expression in terms of the attributes, we
will need to make some sort of preference-independence assumption. The most natural as-
sumption is that the agent’s preferences between state sequences are stationary. Stationarity
STA TIONARY
PREFERENCE
for preferences means the following: if two state sequences[s0,s1,s2,... ] and [s′
0,s ′
1
,s ′
2
,... ]
begin with the same state (i.e.,s0 = s′
0), then the two sequences should be preference-ordered
the same way as the sequences [s1,s2,... ] and [s′
1,s ′
2
,... ]. In English, this means that if you
prefer one future to another starting tomorrow, then you should still prefer that future if it
were to start today instead. Stationarity is a fairly innocuous-looking assumption with very
strong consequences: it turns out that under stationarity there are just two coherent ways to
assign utilities to sequences:
1. Additive rewards: The utility of a state sequence is
ADDITIVE REWARD
Uh([s0,s1,s2,... ]) = R(s0)+ R(s1)+ R(s2)+ ··· .
The 4× 3 world in Figure 17.1 uses additive rewards. Notice that additivity was used
implicitly in our use of path cost functions in heuristic search algorithms (Chapter 3).
2. Discounted rewards: The utility of a state sequence isDISCOUNTED
REWARD
Uh([s0,s1,s2,... ]) = R(s0)+ γR(s1)+ γ2R(s2)+ ··· ,
where the discount factor γis a number between 0 and 1. The discount factor describesDISCOUNT FACTOR
the preference of an agent for current rewards over future rewards. When γ is close
to 0, rewards in the distant future are viewed as insigniﬁcant. When γ is 1, discounted
rewards are exactly equivalent to additive rewards, so additive rewards are a special
case of discounted rewards. Discounting appears to be a good model of both animal
and human preferences over time. A discount factor ofγ is equivalent to an interest rate
of (1/γ)−1.
For reasons that will shortly become clear, we assume discounted rewards in the remainder
of the chapter, although sometimes we allow γ=1 .
Lurking beneath our choice of inﬁnite horizons is a problem: if the environment does
not contain a terminal state, or if the agent never reaches one, then all environment histories
will be inﬁnitely long, and utilities with additive, undiscounted rewards will generally be
650 Chapter 17. Making Complex Decisions
inﬁnite. While we can agree that+∞is better than−∞, comparing two state sequences with
+∞utility is more difﬁcult. There are three solutions, two of which we have seen already:
1. With discounted rewards, the utility of an inﬁnite sequence is ﬁnite. In fact, if γ< 1
and rewards are bounded by±Rmax,w eh a v e
Uh([s0,s1,s2,... ]) =
∞∑
t=0
γtR(st)≤
∞∑
t=0
γtRmax = Rmax/(1−γ) , (17.1)
using the standard formula for the sum of an inﬁnite geometric series.
2. If the environment contains terminal states and if the agent is guaranteed to get to one
eventually, then we will never need to compare inﬁnite sequences. A policy that is
guaranteed to reach a terminal state is called a proper policy. With proper policies, wePROPER POLICY
can use γ=1 (i.e., additive rewards). The ﬁrst three policies shown in Figure 17.2(b)
are proper, but the fourth is improper. It gains inﬁnite total reward by staying away from
the terminal states when the reward for the nonterminal states is positive. The existence
of improper policies can cause the standard algorithms for solving MDPs to fail with
additive rewards, and so provides a good reason for using discounted rewards.
3. Inﬁnite sequences can be compared in terms of the average reward obtained per time
AVERAGE REWARD
step. Suppose that square (1,1) in the 4× 3 world has a reward of 0.1 while the other
nonterminal states have a reward of 0.01. Then a policy that does its best to stay in
(1,1) will have higher average reward than one that stays elsewhere. Average reward is
a useful criterion for some problems, but the analysis of average-reward algorithms is
beyond the scope of this book.
In sum, discounted rewards present the fewest difﬁculties in evaluating state sequences.
17.1.2 Optimal policies and the utilities of states
Having decided that the utility of a given state sequence is the sum of discounted rewards
obtained during the sequence, we can compare policies by comparing the expected utilities
obtained when executing them. We assume the agent is in some initial state s and deﬁne St
(a random variable) to be the state the agent reaches at time t when executing a particular
policy π. (Obviously, S0 =s, the state the agent is in now.) The probability distribution over
state sequences S1,S2,..., is determined by the initial states, the policyπ, and the transition
model for the environment.
The expected utility obtained by executing π starting in s is given by
Uπ(s)= E
[ ∞∑
t =0
γtR(St)
]
, (17.2)
where the expectation is with respect to the probability distribution over state sequences de-
termined by s and π. Now, out of all the policies the agent could choose to execute starting in
s, one (or more) will have higher expected utilities than all the others. We’ll useπ∗
s to denote
one of these policies:
π∗
s =a r g m a x
π
Uπ(s) . (17.3)
Section 17.1. Sequential Decision Problems 651
Remember that π∗
s is a policy, so it recommends an action for every state; its connection
with s in particular is that it’s an optimal policy when s is the starting state. A remarkable
consequence of using discounted utilities with inﬁnite horizons is that the optimal policy is
independent of the starting state. (Of course, the action sequence won’t be independent;
remember that a policy is a function specifying an action for each state.) This fact seems
intuitively obvious: if policy π
∗
a is optimal starting in a and policy π∗
b is optimal starting in b,
then, when they reach a third state c, there’s no good reason for them to disagree with each
other, or with π∗
c , about what to do next.2 So we can simply write π∗ for an optimal policy.
Given this deﬁnition, the true utility of a state is just Uπ∗
(s)—that is, the expected
sum of discounted rewards if the agent executes an optimal policy. We write this as U(s),
matching the notation used in Chapter 16 for the utility of an outcome. Notice that U(s) and
R(s) are quite different quantities; R(s) is the “short term” reward for being in s, whereas
U(s) is the “long term” total reward from s onward. Figure 17.3 shows the utilities for the
4× 3 world. Notice that the utilities are higher for states closer to the +1 exit, because fewer
steps are required to reach the exit.
123
1
2
3
–1
+ 1
4
0.611
0.812
0.655
0.762
0.918
0.705
0.660
0.868
 0.388
Figure 17.3 The utilities of the states in the 4×3 world, calculated with γ=1 and
R(s)= −0.04 for nonterminal states.
The utility function U(s) allows the agent to select actions by using the principle of
maximum expected utility from Chapter 16—that is, choose the action that maximizes the
expected utility of the subsequent state:
π
∗(s) = argmax
a∈A(s)
∑
s′
P(s′|s,a)U(s′) . (17.4)
The next two sections describe algorithms for ﬁnding optimal policies.
2 Although this seems obvious, it does not hold for ﬁnite-horizon policies or for other ways of combining
rewards over time. The proof follows directly from the uniqueness of the utility function on states, as shown in
Section 17.2.
652 Chapter 17. Making Complex Decisions
17.2 V ALUE ITERATION
In this section, we present an algorithm, called value iteration, for calculating an optimalVALUE ITERA TION
policy. The basic idea is to calculate the utility of each state and then use the state utilities to
select an optimal action in each state.
17.2.1 The Bellman equation for utilities
Section 17.1.2 deﬁned the utility of being in a state as the expected sum of discounted rewards
from that point onwards. From this, it follows that there is a direct relationship between the
utility of a state and the utility of its neighbors: the utility of a state is the immediate reward
for that state plus the expected discounted utility of the next state, assuming that the agent
chooses the optimal action. That is, the utility of a state is given by
U(s)= R(s)+ γ max
a∈A(s)
∑
s′
P(s′| s,a)U(s′) . (17.5)
This is called the Bellman equation , after Richard Bellman (1957). The utilities of theBELLMAN EQUA TION
states—deﬁned by Equation (17.2) as the expected utility of subsequent state sequences—are
solutions of the set of Bellman equations. In fact, they are the unique solutions, as we show
in Section 17.2.3.
Let us look at one of the Bellman equations for the 4× 3 world. The equation for the
state (1,1) is
U(1,1) =−0.04 +γ max[ 0 .8U(1,2) + 0.1U(2,1) + 0.1U(1,1), (Up)
0.9U(1,1) + 0.1U(1,2), (Left)
0.9U(1,1) + 0.1U(2,1), (Down)
0.8U(2,1) + 0.1U(1,2) + 0.1U(1,1) ]. (Right)
When we plug in the numbers from Figure 17.3, we ﬁnd that Up is the best action.
17.2.2 The value iteration algorithm
The Bellman equation is the basis of the value iteration algorithm for solving MDPs. If there
are n possible states, then there are n Bellman equations, one for each state. The n equations
contain n unknowns—the utilities of the states. So we would like to solve these simultaneous
equations to ﬁnd the utilities. There is one problem: the equations are nonlinear, because the
“max” operator is not a linear operator. Whereas systems of linear equations can be solved
quickly using linear algebra techniques, systems of nonlinear equations are more problematic.
One thing to try is an iterative approach. We start with arbitrary initial values for the utilities,
calculate the right-hand side of the equation, and plug it into the left-hand side—thereby
updating the utility of each state from the utilities of its neighbors. We repeat this until we
reach an equilibrium. Let U
i(s) be the utility value for states at the ith iteration. The iteration
step, called a Bellman update, looks like this:BELLMAN UPDA TE
Ui+1(s)←R(s)+ γ max
a∈A(s)
∑
s′
P(s′|s,a)Ui(s′) , (17.6)
Section 17.2. Value Iteration 653
function VALUE -ITERATION (mdp,ϵ) returns a utility function
inputs: mdp, an MDP with states S, actions A(s), transition model P(s′| s, a),
rewards R(s), discount γ
ϵ, the maximum error allowed in the utility of any state
local variables: U , U ′, vectors of utilities for states in S, initially zero
δ, the maximum change in the utility of any state in an iteration
repeat
U←U ′; δ←0
for each state s in S do
U ′[s]←R(s)+ γ max
a ∈ A(s)
∑
s′
P(s′| s, a) U [s′]
if|U ′[s] −U [s]| >δ then δ←|U ′[s] −U [s]|
until δ<ϵ (1−γ)/γ
return U
Figure 17.4 The value iteration algorithm for calculating utilities of states. The termina-
tion condition is from Equation (17.8).
-0.2
0
0.2
0.4
0.6
0.8
1
0 5 10 15 20 25 30
Utility estimates
Number of iterations
(4,3)
(3,3)
(1,1)
(3,1)
(4,1)
1
10
100
1000
10000
100000
1e+06
1e+07
0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
Iterations required
Discount factor γ
c = 0.0001
c = 0.001
c = 0.01
c = 0.1
(a) (b)
Figure 17.5 (a) Graph showing the evolution of the utilities of selected states using value
iteration. (b) The number of value iterations k required to guarantee an error of at most
ϵ =c· Rmax, for different values of c, as a function of the discount factor γ.
where the update is assumed to be applied simultaneously to all the states at each iteration.
If we apply the Bellman update inﬁnitely often, we are guaranteed to reach an equilibrium
(see Section 17.2.3), in which case the ﬁnal utility values must be solutions to the Bellman
equations. In fact, they are also the unique solutions, and the corresponding policy (obtained
using Equation (17.4)) is optimal. The algorithm, called V
ALUE -ITERATION ,i ss h o w ni n
Figure 17.4.
We can apply value iteration to the 4× 3 world in Figure 17.1(a). Starting with initial
values of zero, the utilities evolve as shown in Figure 17.5(a). Notice how the states at differ-
654 Chapter 17. Making Complex Decisions
ent distances from (4,3) accumulate negative reward until a path is found to (4,3), whereupon
the utilities start to increase. We can think of the value iteration algorithm as propagating
information through the state space by means of local updates.
17.2.3 Convergence of value iteration
We said that value iteration eventually converges to a unique set of solutions of the Bellman
equations. In this section, we explain why this happens. We introduce some useful mathe-
matical ideas along the way, and we obtain some methods for assessing the error in the utility
function returned when the algorithm is terminated early; this is useful because it means that
we don’t have to run forever. This section is quite technical.
The basic concept used in showing that value iteration converges is the notion of acon-
traction. Roughly speaking, a contraction is a function of one argument that, when applied to
CONTRACTION
two different inputs in turn, produces two output values that are “closer together,” by at least
some constant factor, than the original inputs. For example, the function “divide by two” is
a contraction, because, after we divide any two numbers by two, their difference is halved.
Notice that the “divide by two” function has a ﬁxed point, namely zero, that is unchanged by
the application of the function. From this example, we can discern two important properties
of contractions:
•A contraction has only one ﬁxed point; if there were two ﬁxed points they would not
get closer together when the function was applied, so it would not be a contraction.
•When the function is applied to any argument, the value must get closer to the ﬁxed
point (because the ﬁxed point does not move), so repeated application of a contraction
always reaches the ﬁxed point in the limit.
Now, suppose we view the Bellman update (Equation (17.6)) as an operatorB that is applied
simultaneously to update the utility of every state. Let U
i denote the vector of utilities for all
the states at the ith iteration. Then the Bellman update equation can be written as
Ui+1 ←BU i .
Next, we need a way to measure distances between utility vectors. We will use themax norm,MAX NORM
which measures the “length” of a vector by the absolute value of its biggest component:
||U|| =m a x
s
|U(s)| .
With this deﬁnition, the “distance” between two vectors, ||U −U ′||, is the maximum dif-
ference between any two corresponding elements. The main result of this section is the
following: Let U
i and U ′
i be any two utility vectors. Then we have
||BU i−BU ′
i||≤γ||Ui−U ′
i|| . (17.7)
That is, the Bellman update is a contraction by a factor of γ on the space of utility vectors.
(Exercise 17.6 provides some guidance on proving this claim.) Hence, from the properties of
contractions in general, it follows that value iteration always converges to a unique solution
of the Bellman equations whenever γ< 1.
Section 17.2. Value Iteration 655
We can also use the contraction property to analyze the rate of convergence to a solu-
tion. In particular, we can replace U ′
i in Equation (17.7) with the true utilities U,f o rw h i c h
BU = U. Then we obtain the inequality
||BU i−U||≤γ||Ui−U|| .
So, if we view||Ui−U|| as the error in the estimate Ui, we see that the error is reduced by a
factor of at least γ on each iteration. This means that value iteration converges exponentially
fast. We can calculate the number of iterations required to reach a speciﬁed error bound ϵ
as follows: First, recall from Equation (17.1) that the utilities of all states are bounded by
±R
max/(1−γ). This means that the maximum initial error ||U0 −U|| ≤2Rmax/(1−γ).
Suppose we run for N iterations to reach an error of at most ϵ. Then, because the error is
reduced by at least γ each time, we require γN · 2Rmax/(1−γ)≤ϵ. Taking logs, we ﬁnd
N =⌈log(2Rmax/ϵ(1−γ))/log(1/γ)⌉
iterations sufﬁce. Figure 17.5(b) shows how N varies with γ, for different values of the ratio
ϵ/Rmax. The good news is that, because of the exponentially fast convergence, N does not
depend much on the ratio ϵ/Rmax. The bad news is that N grows rapidly as γ becomes close
to 1. We can get fast convergence if we make γ small, but this effectively gives the agent a
short horizon and could miss the long-term effects of the agent’s actions.
The error bound in the preceding paragraph gives some idea of the factors inﬂuencing
the run time of the algorithm, but is sometimes overly conservative as a method of deciding
when to stop the iteration. For the latter purpose, we can use a bound relating the error
to the size of the Bellman update on any given iteration. From the contraction property
(Equation (17.7)), it can be shown that if the update is small (i.e., no state’s utility changes by
much), then the error, compared with the true utility function, also is small. More precisely,
if ||U
i+1−Ui|| <ϵ (1−γ)/γ then ||Ui+1−U|| <ϵ. (17.8)
This is the termination condition used in the VALUE -ITERATION algorithm of Figure 17.4.
So far, we have analyzed the error in the utility function returned by the value iteration
algorithm. What the agent really cares about, however , is how well it will do if it makes its
decisions on the basis of this utility function. Suppose that after i iterations of value iteration,
the agent has an estimate Ui of the true utility U and obtains the MEU policy πi based on
one-step look-ahead using Ui (as in Equation (17.4)). Will the resulting behavior be nearly
as good as the optimal behavior? This is a crucial question for any real agent, and it turns out
that the answer is yes. U
πi(s) is the utility obtained if πi is executed starting in s,a n dt h e
policy loss||Uπi −U|| is the most the agent can lose by executing πi instead of the optimalPOLICY LOSS
policy π∗. The policy loss of πi is connected to the error in Ui by the following inequality:
if ||Ui−U|| <ϵ then ||Uπi −U|| < 2ϵγ/(1−γ) . (17.9)
In practice, it often occurs thatπi becomes optimal long beforeUi has converged. Figure 17.6
shows how the maximum error in Ui and the policy loss approach zero as the value iteration
process proceeds for the4× 3 environment withγ=0.9. The policy πi is optimal wheni=4 ,
even though the maximum error in Ui is still 0.46.
Now we have everything we need to use value iteration in practice. We know that
it converges to the correct utilities, we can bound the error in the utility estimates if we
656 Chapter 17. Making Complex Decisions
stop after a ﬁnite number of iterations, and we can bound the policy loss that results from
executing the corresponding MEU policy. As a ﬁnal note, all of the results in this section
depend on discounting with γ< 1.I f γ=1 and the environment contains terminal states,
then a similar set of convergence results and error bounds can be derived whenever certain
technical conditions are satisﬁed.
17.3 P OLICY ITERATION
In the previous section, we observed that it is possible to get an optimal policy even when
the utility function estimate is inaccurate. If one action is clearly better than all others, then
the exact magnitude of the utilities on the states involved need not be precise. This insight
suggests an alternative way to ﬁnd optimal policies. Thepolicy iteration algorithm alternates
POLICY ITERA TION
the following two steps, beginning from some initial policy π0:
•Policy evaluation: given a policy πi, calculate Ui = Uπi , the utility of each state if πiPOLICY EVALUA TION
were to be executed.
•Policy improvement: Calculate a new MEU policy πi+1, using one-step look-aheadPOLICY
IMPROVEMENT
based on Ui (as in Equation (17.4)).
The algorithm terminates when the policy improvement step yields no change in the utilities.
At this point, we know that the utility function Ui is a ﬁxed point of the Bellman update, so
it is a solution to the Bellman equations, and πi must be an optimal policy. Because there are
only ﬁnitely many policies for a ﬁnite state space, and each iteration can be shown to yield a
better policy, policy iteration must terminate. The algorithm is shown in Figure 17.7.
The policy improvement step is obviously straightforward, but how do we implement
the P
OLICY -EVA L UAT I O Nroutine? It turns out that doing so is much simpler than solving
the standard Bellman equations (which is what value iteration does), because the action in
each state is ﬁxed by the policy. At theith iteration, the policy πi speciﬁes the action πi(s) in
0
0.2
0.4
0.6
0.8
1
0 2 4 6 8 10 12 14
Max error/Policy loss
Number of iterations
Max error
Policy loss
Figure 17.6 The maximum error ||Ui −U|| of the utility estimates and the policy loss
||Uπi −U||, as a function of the number of iterations of value iteration.

Section 17.3. Policy Iteration 657
state s. This means that we have a simpliﬁed version of the Bellman equation (17.5) relating
the utility of s (under πi) to the utilities of its neighbors:
Ui(s)= R(s)+ γ
∑
s′
P(s′| s,πi(s))Ui(s′) . (17.10)
For example, suppose πi is the policy shown in Figure 17.2(a). Then we have πi(1,1) =Up,
πi(1,2) =Up, and so on, and the simpliﬁed Bellman equations are
Ui(1,1) = −0.04 + 0.8Ui(1,2) + 0.1Ui(1,1) + 0.1Ui(2,1) ,
Ui(1,2) = −0.04 + 0.8Ui(1,3) + 0.2Ui(1,2) ,
...
The important point is that these equations are linear, because the “max” operator has been
removed. For n states, we have n linear equations with n unknowns, which can be solved
exactly in time O(n3) by standard linear algebra methods.
For small state spaces, policy evaluation using exact solution methods is often the most
efﬁcient approach. For large state spaces, O(n3) time might be prohibitive. Fortunately, it
is not necessary to do exact policy evaluation. Instead, we can perform some number of
simpliﬁed value iteration steps (simpliﬁed because the policy is ﬁxed) to give a reasonably
good approximation of the utilities. The simpliﬁed Bellman update for this process is
U
i+1(s)←R(s)+ γ
∑
s′
P(s′|s,πi(s))Ui(s′) ,
and this is repeated k times to produce the next utility estimate. The resulting algorithm is
called modiﬁed policy iteration. It is often much more efﬁcient than standard policy iterationMODIFIED POLICY
ITERA TION
or value iteration.
function POLICY -ITERATION (mdp) returns a policy
inputs: mdp, an MDP with states S, actions A(s), transition model P(s′| s, a)
local variables: U , a vector of utilities for states in S, initially zero
π, a policy vector indexed by state, initially random
repeat
U←POLICY -EVA L UAT I O N(π,U ,mdp)
unchanged?←true
for each state s in S do
if max
a ∈ A(s)
∑
s′
P(s′| s, a) U [s′] >
∑
s′
P(s′| s, π[s]) U [s′] then do
π[s]←argmax
a ∈ A(s)
∑
s′
P(s′| s, a) U [s′]
unchanged?←false
until unchanged?
return π
Figure 17.7 The policy iteration algorithm for calculating an optimal policy.

658 Chapter 17. Making Complex Decisions
The algorithms we have described so far require updating the utility or policy for all
states at once. It turns out that this is not strictly necessary. In fact, on each iteration, we
can pick any subset of states and apply either kind of updating (policy improvement or sim-
pliﬁed value iteration) to that subset. This very general algorithm is called asynchronous
policy iteration. Given certain conditions on the initial policy and initial utility function,
ASYNCHRONOUS
POLICY ITERA TION
asynchronous policy iteration is guaranteed to converge to an optimal policy. The freedom
to choose any states to work on means that we can design much more efﬁcient heuristic
algorithms—for example, algorithms that concentrate on updating the values of states that
are likely to be reached by a good policy. This makes a lot of sense in real life: if one has no
intention of throwing oneself off a cliff, one should not spend time worrying about the exact
value of the resulting states.
17.4 P ARTIALLY OBSERV ABLE MDP S
The description of Markov decision processes in Section 17.1 assumed that the environment
was fully observable. With this assumption, the agent always knows which state it is in.
This, combined with the Markov assumption for the transition model, means that the optimal
policy depends only on the current state. When the environment is onlypartially observable,
the situation is, one might say, much less clear. The agent does not necessarily know which
state it is in, so it cannot execute the actionπ(s) recommended for that state. Furthermore, the
utility of a state s and the optimal action in s depend not just on s, but also on how much the
agent knows when it is in s. For these reasons, partially observable MDPs (or POMDPs—
PA RT I A L LY
OBSERVABLE MDP
pronounced “pom-dee-pees”) are usually viewed as much more difﬁcult than ordinary MDPs.
We cannot avoid POMDPs, however, because the real world is one.
17.4.1 Deﬁnition of POMDPs
To get a handle on POMDPs, we must ﬁrst deﬁne them properly. A POMDP has the same
elements as an MDP—the transition model P(s′|s,a), actions A(s), and reward function
R(s)—but, like the partially observable search problems of Section 4.4, it also has a sensor
model P(e| s). Here, as in Chapter 15, the sensor model speciﬁes the probability of perceiv-
ing evidence e in state s.3 For example, we can convert the 4× 3 world of Figure 17.1 into
a POMDP by adding a noisy or partial sensor instead of assuming that the agent knows its
location exactly. Such a sensor might measure the number of adjacent walls , which happens
to be 2 in all the nonterminal squares except for those in the third column, where the value
is 1; a noisy version might give the wrong value with probability 0.1.
In Chapters 4 and 11, we studied nondeterministic and partially observable planning
problems and identiﬁed the belief state—the set of actual states the agent might be in—as a
key concept for describing and calculating solutions. In POMDPs, the belief stateb becomes a
probability distribution over all possible states, just as in Chapter 15. For example, the initial
3 As with the reward function for MDPs, the sensor model can also depend on the action and outcome state, but
again this change is not fundamental.
Section 17.4. Partially Observable MDPs 659
belief state for the 4× 3 POMDP could be the uniform distribution over the nine nonterminal
states, i.e., ⟨1
9, 1
9, 1
9, 1
9, 1
9, 1
9, 1
9, 1
9, 1
9,0,0⟩. We write b(s) for the probability assigned to the
actual states by belief stateb. The agent can calculate its current belief state as the conditional
probability distribution over the actual states given the sequence of percepts and actions so
far. This is essentially the ﬁltering task described in Chapter 15. The basic recursive ﬁltering
equation (15.5 on page 572) shows how to calculate the new belief state from the previous
belief state and the new evidence. For POMDPs, we also have an action to consider, but the
result is essentially the same. If b(s) was the previous belief state, and the agent does action
a and then perceives evidence e, then the new belief state is given by
b
′(s′)= αP (e| s′)
∑
s
P(s′|s,a)b(s) ,
where α is a normalizing constant that makes the belief state sum to 1. By analogy with the
update operator for ﬁltering (page 572), we can write this as
b′ = FORW ARD(b, a, e) . (17.11)
In the4× 3 POMDP, suppose the agent movesLeft and its sensor reports 1 adjacent wall; then
it’s quite likely (although not guaranteed, because both the motion and the sensor are noisy)
that the agent is now in (3,1). Exercise 17.13 asks you to calculate the exact probability values
for the new belief state.
The fundamental insight required to understand POMDPs is this: the optimal action
depends only on the agent’s current belief state. That is, the optimal policy can be described
by a mapping π∗(b) from belief states to actions. It does not depend on the actual state the
agent is in. This is a good thing, because the agent does not know its actual state; all it knows
is the belief state. Hence, the decision cycle of a POMDP agent can be broken down into the
following three steps:
1. Given the current belief state b, execute the action a= π
∗(b).
2. Receive percept e.
3. Set the current belief state to F ORW ARD(b, a, e) and repeat.
Now we can think of POMDPs as requiring a search in belief-state space, just like the meth-
ods for sensorless and contingency problems in Chapter 4. The main difference is that the
POMDP belief-state space is continuous, because a POMDP belief state is a probability dis-
tribution. For example, a belief state for the 4× 3 world is a point in an 11-dimensional
continuous space. An action changes the belief state, not just the physical state. Hence, the
action is evaluated at least in part according to the information the agent acquires as a result.
POMDPs therefore include the value of information (Section 16.6) as one component of the
decision problem.
Let’s look more carefully at the outcome of actions. In particular, let’s calculate the
probability that an agent in belief state b reaches belief state b
′ after executing action a.N o w ,
if we knew the action and the subsequent percept , then Equation (17.11) would provide a
deterministic update to the belief state: b′ = FORW ARD(b, a, e). Of course, the subsequent
percept is not yet known, so the agent might arrive in one of several possible belief states b′,
depending on the percept that is received. The probability of perceiving e, given that a was
660 Chapter 17. Making Complex Decisions
performed starting in belief state b, is given by summing over all the actual states s′ that the
agent might reach:
P(e|a, b)=
∑
s′
P(e|a, s′,b)P(s′|a, b)
=
∑
s′
P(e| s′)P(s′|a, b)
=
∑
s′
P(e| s′)
∑
s
P(s′| s,a)b(s) .
Let us write the probability of reaching b′ from b, given action a,a s P(b′|b, a)). Then that
gives us
P(b′|b, a)= P(b′|a, b)=
∑
e
P(b′|e, a, b)P(e|a, b)
=
∑
e
P(b′|e, a, b)
∑
s′
P(e| s′)
∑
s
P(s′|s,a)b(s) , (17.12)
where P(b′|e, a, b) is 1 if b′ = FORW ARD(b, a, e) and 0 otherwise.
Equation (17.12) can be viewed as deﬁning a transition model for the belief-state space.
We can also deﬁne a reward function for belief states (i.e., the expected reward for the actual
states the agent might be in):
ρ(b)=
∑
s
b(s)R(s) .
Together, P(b′|b, a) and ρ(b) deﬁne an observable MDP on the space of belief states. Fur-
thermore, it can be shown that an optimal policy for this MDP,π∗(b), is also an optimal policy
for the original POMDP. In other words, solving a POMDP on a physical state space can be
reduced to solving an MDP on the corresponding belief-state space. This fact is perhaps less
surprising if we remember that the belief state is always observable to the agent, by deﬁnition.
Notice that, although we have reduced POMDPs to MDPs, the MDP we obtain has a
continuous (and usually high-dimensional) state space. None of the MDP algorithms de-
scribed in Sections 17.2 and 17.3 applies directly to such MDPs. The next two subsec-
tions describe a value iteration algorithm designed speciﬁcally for POMDPs and an online
decision-making algorithm, similar to those developed for games in Chapter 5.
17.4.2 Value iteration for POMDPs
Section 17.2 described a value iteration algorithm that computed one utility value for each
state. With inﬁnitely many belief states, we need to be more creative. Consider an optimal
policy π
∗ and its application in a speciﬁc belief state b: the policy generates an action, then,
for each subsequent percept, the belief state is updated and a new action is generated, and so
on. For this speciﬁc b, therefore, the policy is exactly equivalent to aconditional plan,a sd e -
ﬁned in Chapter 4 for nondeterministic and partially observable problems. Instead of thinking
about policies, let us think about conditional plans and how the expected utility of executing
a ﬁxed conditional plan varies with the initial belief state. We make two observations:
Section 17.4. Partially Observable MDPs 661
1. Let the utility of executing aﬁxed conditional planp starting in physical states be αp(s).
Then the expected utility of executing p in belief state b is just ∑
s b(s)αp(s),o r b· αp
if we think of them both as vectors. Hence, the expected utility of a ﬁxed conditional
plan varies linearly with b; that is, it corresponds to a hyperplane in belief space.
2. At any given belief state b, the optimal policy will choose to execute the conditional
plan with highest expected utility; and the expected utility ofb under the optimal policy
is just the utility of that conditional plan:
U(b)= Uπ∗
(b)=m a x
p
b· αp .
If the optimal policyπ∗ chooses to executep starting at b, then it is reasonable to expect
that it might choose to execute p in belief states that are very close to b; in fact, if we
bound the depth of the conditional plans, then there are only ﬁnitely many such plans
and the continuous space of belief states will generally be divided into regions, each
corresponding to a particular conditional plan that is optimal in that region.
From these two observations, we see that the utility function U(b) on belief states, being the
maximum of a collection of hyperplanes, will be piecewise linear and convex.
To illustrate this, we use a simple two-state world. The states are labeled 0 and 1, with
R(0) = 0and R(1) = 1. There are two actions: Stay stays put with probability 0.9 and Go
switches to the other state with probability 0.9. For now we will assume the discount factor
γ=1 . The sensor reports the correct state with probability 0.6. Obviously, the agent should
Stay when it thinks it’s in state 1 and Go when it thinks it’s in state 0.
The advantage of a two-state world is that the belief space can be viewed as one-
dimensional, because the two probabilities must sum to 1. In Figure 17.8(a), the x-axis
represents the belief state, deﬁned byb(1), the probability of being in state 1. Now let us con-
sider the one-step plans [Stay] and [Go], each of which receives the reward for the current
state followed by the (discounted) reward for the state reached after the action:
α
[Stay](0) = R(0) +γ(0.9R(0) + 0.1R(1)) = 0.1
α[Stay](1) = R(1) +γ(0.9R(1) + 0.1R(0)) = 1.9
α[Go](0) = R(0) +γ(0.9R(1) + 0.1R(0)) = 0.9
α[Go](1) = R(1) +γ(0.9R(0) + 0.1R(1)) = 1.1
The hyperplanes (lines, in this case) forb·α[Stay] and b·α[Go] are shown in Figure 17.8(a) and
their maximum is shown in bold. The bold line therefore represents the utility function for
the ﬁnite-horizon problem that allows just one action, and in each “piece” of the piecewise
linear utility function the optimal action is the ﬁrst action of the corresponding conditional
plan. In this case, the optimal one-step policy is to Stay when b(1) > 0.5 and Go otherwise.
Once we have utilities α
p(s) for all the conditional plans p of depth 1 in each physical
state s, we can compute the utilities for conditional plans of depth 2 by considering each
possible ﬁrst action, each possible subsequent percept, and then each way of choosing a
depth-1 plan to execute for each percept:
[Stay; if Percept=0 then Stay else Stay]
[Stay; if Percept=0 then Stay else Go]...
662 Chapter 17. Making Complex Decisions
 0
 0.5
 1
 1.5
 2
 2.5
 3
 0  0.2  0.4  0.6  0.8  1
Utility
Probability of state 1
[Stay]
[Go]
 0
 0.5
 1
 1.5
 2
 2.5
 3
 0  0.2  0.4  0.6  0.8  1
Utility
Probability of state 1
(a) (b)
 0
 0.5
 1
 1.5
 2
 2.5
 3
 0  0.2  0.4  0.6  0.8  1
Utility
Probability of state 1
 4.5
 5
 5.5
 6
 6.5
 7
 7.5
 0  0.2  0.4  0.6  0.8  1
Utility
Probability of state 1
(c) (d)
Figure 17.8 (a) Utility of two one-step plans as a function of the initial belief state b(1)
for the two-state world, with the corres ponding utility function shown in bold. (b) Utilities
for 8 distinct two-step plans. (c) Utilities f or four undominated two- step plans. (d) Utility
function for optimal eight-step plans.
There are eight distinct depth-2 plans in all, and their utilities are shown in Figure 17.8(b).
Notice that four of the plans, shown as dashed lines, are suboptimal across the entire belief
space—we say these plans are dominated, and they need not be considered further. There
DOMINA TED PLAN
are four undominated plans, each of which is optimal in a speciﬁc region, as shown in Fig-
ure 17.8(c). The regions partition the belief-state space.
We repeat the process for depth 3, and so on. In general, let p be a depth-d conditional
plan whose initial action is a and whose depth-d−1 subplan for percept e is p.e;t h e n
αp(s)= R(s)+ γ
⎞∑
s′
P(s′|s,a)
∑
e
P(e| s′)αp.e(s′)
⎠
. (17.13)
This recursion naturally gives us a value iteration algorithm, which is sketched in Figure 17.9.
The structure of the algorithm and its error analysis are similar to those of the basic value iter-
ation algorithm in Figure 17.4 on page 653; the main difference is that instead of computing
one utility number for each state, POMDP-V
ALUE -ITERATION maintains a collection of
Section 17.4. Partially Observable MDPs 663
function POMDP-V ALUE -ITERATION (pomdp,ϵ) returns a utility function
inputs: pomdp, a POMDP with states S, actions A(s), transition model P(s′| s, a),
sensor model P(e|s),r e w a r d sR(s), discount γ
ϵ, the maximum error allowed in the utility of any state
local variables: U , U ′,s e t so fp l a n sp with associated utility vectors αp
U ′←a set containing just the empty plan [] , with α[] (s)= R(s)
repeat
U←U ′
U ′←the set of all plans consisting of an action and, for each possible next percept,
ap l a ni nU with utility vectors computed according to Equation (17.13)
U ′←REMOVE -DOMINATED -PLANS (U ′)
until MAX-DIFFERENCE (U ,U ′) <ϵ (1−γ)/γ
return U
Figure 17.9 A high-level sketch of the value iteration algorithm for POMDPs. The
REMOVE -DOMINATED -PLANS step and MAX-DIFFERENCE test are typically implemented
as linear programs.
undominated plans with their utility hyperplanes. The algorithm’s complexity depends pri-
marily on how many plans get generated. Given|A| actions and|E| possible observations, it
is easy to show that there are|A|O(|E|d− 1) distinct depth-d plans. Even for the lowly two-state
world with d=8 , the exact number is 2255. The elimination of dominated plans is essential
for reducing this doubly exponential growth: the number of undominated plans with d=8 is
just 144. The utility function for these 144 plans is shown in Figure 17.8(d).
Notice that even though state 0 has lower utility than state 1, the intermediate belief
states have even lower utility because the agent lacks the information needed to choose a
good action. This is why information has value in the sense deﬁned in Section 16.6 and
optimal policies in POMDPs often include information-gathering actions.
Given such a utility function, an executable policy can be extracted by looking at which
hyperplane is optimal at any given belief state b and executing the ﬁrst action of the corre-
sponding plan. In Figure 17.8(d), the corresponding optimal policy is still the same as for
depth-1 plans: Stay when b(1) > 0.5 and Go otherwise.
In practice, the value iteration algorithm in Figure 17.9 is hopelessly inefﬁcient for
larger problems—even the 4× 3 POMDP is too hard. The main reason is that, given n con-
ditional plans at level d, the algorithm constructs |A|· n
|E| conditional plans at level d +1
before eliminating the dominated ones. Since the 1970s, when this algorithm was developed,
there have been several advances including more efﬁcient forms of value iteration and various
kinds of policy iteration algorithms. Some of these are discussed in the notes at the end of the
chapter. For general POMDPs, however, ﬁnding optimal policies is very difﬁcult (PSPACE-
hard, in fact—i.e., very hard indeed). Problems with a few dozen states are often infeasible.
The next section describes a different, approximate method for solving POMDPs, one based
on look-ahead search.
664 Chapter 17. Making Complex Decisions
Xt–1
At–1
Rt–1
At
Rt
At+2
Rt+2
At+1
Rt+1
At–2
Et–1
Xt+1
Et+1
Xt+2
Et+2
Xt+3
Et+3
Ut+3Xt
Et
Figure 17.10 The generic structure of a dynamic decision network. Variables with known
values are shaded. The current time ist and the agent must decide what to do—that is, choose
av a l u ef o rAt. The network has been unrolled into the future for three steps and represents
future rewards, as well as the utility of the state at the look-ahead horizon.
17.4.3 Online agents for POMDPs
In this section, we outline a simple approach to agent design for partially observable, stochas-
tic environments. The basic elements of the design are already familiar:
•The transition and sensor models are represented by a dynamic Bayesian network
(DBN), as described in Chapter 15.
•The dynamic Bayesian network is extended with decision and utility nodes, as used in
decision networks in Chapter 16. The resulting model is called a dynamic decision
network, or DDN.DYNAMIC DECISION
NETWORK
•A ﬁltering algorithm is used to incorporate each new percept and action and to update
the belief state representation.
•Decisions are made by projecting forward possible action sequences and choosing the
best one.
DBNs are factored representations in the terminology of Chapter 2; they typically have
an exponential complexity advantage over atomic representations and can model quite sub-
stantial real-world problems. The agent design is therefore a practical implementation of the
utility-based agent sketched in Chapter 2.
In the DBN, the single state S
t becomes a set of state variables Xt, and there may be
multiple evidence variablesEt. We will useAt to refer to the action at timet, so the transition
model becomes P(Xt+1|Xt,At) and the sensor model becomes P(Et|Xt). We will use Rt to
refer to the reward received at time t and Ut to refer to the utility of the state at time t.( B o t h
of these are random variables.) With this notation, a dynamic decision network looks like the
one shown in Figure 17.10.
Dynamic decision networks can be used as inputs for any POMDP algorithm, including
those for value and policy iteration methods. In this section, we focus on look-ahead methods
that project action sequences forward from the current belief state in much the same way as do
the game-playing algorithms of Chapter 5. The network in Figure 17.10 has been projected
three steps into the future; the current and future decisions A and the future observations
Section 17.4. Partially Observable MDPs 665
. . .
... ... ...
.........
. . .
.........
.........
. . .
...
. . .
......
. . .
...
. . .
At in P(Xt | E1:t)
At+1 in P(Xt+1 | E1:t+1)
At+2 in P(Xt+2 | E1:t+2)
U(Xt+3)
Et+1
Et+2
Et+3
10 4 6 3
Figure 17.11 Part of the look-ahead solution of the DDN in Figure 17.10. Each decision
will be taken in the belief state indicated.
E and rewards R are all unknown. Notice that the network includes nodes for the rewards
for Xt+1 and Xt+2,b u tt h eutility for Xt+3. This is because the agent must maximize the
(discounted) sum of all future rewards, and U(Xt+3) represents the reward for Xt+3 and all
subsequent rewards. As in Chapter 5, we assume thatU is available only in some approximate
form: if exact utility values were available, look-ahead beyond depth 1 would be unnecessary.
Figure 17.11 shows part of the search tree corresponding to the three-step look-ahead
DDN in Figure 17.10. Each of the triangular nodes is a belief state in which the agent makes
a decision A
t+i for i=0,1,2,... . The round (chance) nodes correspond to choices by the
environment, namely, what evidence Et+i arrives. Notice that there are no chance nodes
corresponding to the action outcomes; this is because the belief-state update for an action is
deterministic regardless of the actual outcome.
The belief state at each triangular node can be computed by applying a ﬁltering al-
gorithm to the sequence of percepts and actions leading to it. In this way, the algorithm
takes into account the fact that, for decision At+i, the agent will have available percepts
Et+1, ..., Et+i, even though at time t it does not know what those percepts will be. In this
way, a decision-theoretic agent automatically takes into account the value of information and
will execute information-gathering actions where appropriate.
A decision can be extracted from the search tree by backing up the utility values from
the leaves, taking an average at the chance nodes and taking the maximum at the decision
nodes. This is similar to the E
XPECTIMINIMAX algorithm for game trees with chance nodes,
except that (1) there can also be rewards at non-leaf states and (2) the decision nodes corre-
spond to belief states rather than actual states. The time complexity of an exhaustive search
to depth d is O(|A|
d·| E|d),w h e r e|A| is the number of available actions and|E| is the num-
ber of possible percepts. (Notice that this is far less than the number of depth- d conditional
666 Chapter 17. Making Complex Decisions
plans generated by value iteration.) For problems in which the discount factor γ is not too
close to 1, a shallow search is often good enough to give near-optimal decisions. It is also
possible to approximate the averaging step at the chance nodes, by sampling from the set of
possible percepts instead of summing over all possible percepts. There are various other ways
of ﬁnding good approximate solutions quickly, but we defer them to Chapter 21.
Decision-theoretic agents based on dynamic decision networks have a number of advan-
tages compared with other, simpler agent designs presented in earlier chapters. In particular,
they handle partially observable, uncertain environments and can easily revise their “plans” to
handle unexpected evidence. With appropriate sensor models, they can handle sensor failure
and can plan to gather information. They exhibit “graceful degradation” under time pressure
and in complex environments, using various approximation techniques. So what is missing?
One defect of our DDN-based algorithm is its reliance on forward search through state space,
rather than using the hierarchical and other advanced planning techniques described in Chap-
ter 11. There have been attempts to extend these techniques into the probabilistic domain, but
so far they have proved to be inefﬁcient. A second, related problem is the basically proposi-
tional nature of the DDN language. We would like to be able to extend some of the ideas for
ﬁrst-order probabilistic languages to the problem of decision making. Current research has
shown that this extension is possible and has signiﬁcant beneﬁts, as discussed in the notes at
the end of the chapter.
17.5 D ECISIONS WITH MULTIPLE AGENTS :G AME THEORY
This chapter has concentrated on making decisions in uncertain environments. But what if
the uncertainty is due to other agents and the decisions they make? And what if the decisions
of those agents are in turn inﬂuenced by our decisions? We addressed this question once
before, when we studied games in Chapter 5. There, however, we were primarily concerned
with turn-taking games in fully observable environments, for which minimax search can be
used to ﬁnd optimal moves. In this section we study the aspects of game theory that analyze
GAME THEORY
games with simultaneous moves and other sources of partial observability. (Game theorists
use the terms perfect information and imperfect information rather than fully and partially
observable.) Game theory can be used in at least two ways:
1. Agent design: Game theory can analyze the agent’s decisions and compute the expected
utility for each decision (under the assumption that other agents are acting optimally
according to game theory). For example, in the game two-ﬁnger Morra, two players,
O and E, simultaneously display one or two ﬁngers. Let the total number of ﬁngers
be f.I f f is odd, O collects f dollars from E;a n di f f is even, E collects f dollars
from O. Game theory can determine the best strategy against a rational player and the
expected return for each player.
4
4 Morra is a recreational version of an inspection game. In such games, an inspector chooses a day to inspect a
facility (such as a restaurant or a biological weapons plant), and the facility operator chooses a day to hide all the
nasty stuff. The inspector wins if the days are different, and the facility operator wins if they are the same.
Section 17.5. Decisions with Multiple Agents: Game Theory 667
2. Mechanism design : When an environment is inhabited by many agents, it might be
possible to deﬁne the rules of the environment (i.e., the game that the agents must
play) so that the collective good of all agents is maximized when each agent adopts the
game-theoretic solution that maximizes its own utility. For example, game theory can
help design the protocols for a collection of Internet trafﬁc routers so that each router
has an incentive to act in such a way that global throughput is maximized. Mechanism
design can also be used to construct intelligent multiagent systems that solve complex
problems in a distributed fashion.
17.5.1 Single-move games
We start by considering a restricted set of games: ones where all players take action simulta-
neously and the result of the game is based on this single set of actions. (Actually, it is not
crucial that the actions take place at exactly the same time; what matters is that no player has
knowledge of the other players’ choices.) The restriction to a single move (and the very use
of the word “game”) might make this seem trivial, but in fact, game theory is serious busi-
ness. It is used in decision-making situations including the auctioning of oil drilling rights
and wireless frequency spectrum rights, bankruptcy proceedings, product development and
pricing decisions, and national defense—situations involving billions of dollars and hundreds
of thousands of lives. A single-move game is deﬁned by three components:
•Players or agents who will be making decisions. Two-player games have received the
PLAYER
most attention, although n-player games for n> 2 are also common. We give players
capitalized names, like Alice and Bob or O and E.
•Actions that the players can choose. We will give actions lowercase names, likeone orACTION
testify. The players may or may not have the same set of actions available.
•A payoff function that gives the utility to each player for each combination of actionsP AYOFF FUNCTION
by all the players. For single-move games the payoff function can be represented by a
matrix, a representation known as the strategic form (also called normal form). TheSTRA TEGIC FORM
payoff matrix for two-ﬁnger Morra is as follows:
O: one
 O: two
E: one
 E =+ 2,O =−2
 E =−3,O =+ 3
E: two
 E =−3,O =+ 3
 E =+ 4,O =−4
For example, the lower-right corner shows that when player O chooses action two and
E also chooses two, the payoff is +4 for E and−4 for O.
Each player in a game must adopt and then execute a strategy (which is the name used inSTRA TEGY
game theory for a policy). A pure strategy is a deterministic policy; for a single-move game,PURE STRA TEGY
a pure strategy is just a single action. For many games an agent can do better with a mixed
strategy, which is a randomized policy that selects actions according to a probability distri-MIXED STRA TEGY
bution. The mixed strategy that chooses action a with probability p and action b otherwise
is written [p:a;( 1−p):b]. For example, a mixed strategy for two-ﬁnger Morra might be
[0.5:one;0.5:two].A strategy proﬁle is an assignment of a strategy to each player; givenSTRA TEGY PROFILE
the strategy proﬁle, the game’s outcome is a numeric value for each player.OUTCOME
668 Chapter 17. Making Complex Decisions
A solution to a game is a strategy proﬁle in which each player adopts a rational strategy.SOLUTION
We will see that the most important issue in game theory is to deﬁne what “rational” means
when each agent chooses only part of the strategy proﬁle that determines the outcome. It is
important to realize that outcomes are actual results of playing a game, while solutions are
theoretical constructs used to analyze a game. We will see that some games have a solution
only in mixed strategies. But that does not mean that a player must literally be adopting a
mixed strategy to be rational.
Consider the following story: Two alleged burglars, Alice and Bob, are caught red-
handed near the scene of a burglary and are interrogated separately. A prosecutor offers each
a deal: if you testify against your partner as the leader of a burglary ring, you’ll go free for
being the cooperative one, while your partner will serve 10 years in prison. However, if you
both testify against each other, you’ll both get 5 years. Alice and Bob also know that if both
refuse to testify they will serve only 1 year each for the lesser charge of possessing stolen
property. Now Alice and Bob face the so-called prisoner’s dilemma: should they testify
PRISONER’S
DILEMMA
or refuse? Being rational agents, Alice and Bob each want to maximize their own expected
utility. Let’s assume that Alice is callously unconcerned about her partner’s fate, so her utility
decreases in proportion to the number of years she will spend in prison, regardless of what
happens to Bob. Bob feels exactly the same way. To help reach a rational decision, they both
construct the following payoff matrix:
Alice:testify
 Alice:refuse
Bob:testify
 A =−5,B =−5
 A =−10,B =0
Bob:refuse
 A =0 ,B =−10
 A =−1,B =−1
Alice analyzes the payoff matrix as follows: “Suppose Bob testiﬁes. Then I get 5 years if I
testify and 10 years if I don’t, so in that case testifying is better. On the other hand, if Bob
refuses, then I get 0 years if I testify and 1 year if I refuse, so in that case as well testifying is
better. So in either case, it’s better for me to testify, so that’s what I must do.”
Alice has discovered that testify is a dominant strategy for the game. We say that a
DOMINANT
STRA TEGY
strategy s for player p strongly dominates strategy s′ if the outcome for s is better for p thanSTRONG
DOMINA TION
the outcome for s′, for every choice of strategies by the other player(s). Strategy s weakly
dominates s′ if s is better than s′ on at least one strategy proﬁle and no worse on any other.WEAK DOMINA TION
A dominant strategy is a strategy that dominates all others. It is irrational to play a dominated
strategy, and irrational not to play a dominant strategy if one exists. Being rational, Alice
chooses the dominant strategy. We need just a bit more terminology: we say that an outcome
is Pareto optimal5 if there is no other outcome that all players would prefer. An outcome isP ARETO OPTIMAL
Pareto dominated by another outcome if all players would prefer the other outcome.P ARETO DOMINA TED
If Alice is clever as well as rational, she will continue to reason as follows: Bob’s
dominant strategy is also to testify. Therefore, he will testify and we will both get ﬁve years.
When each player has a dominant strategy, the combination of those strategies is called a
dominant strategy equilibrium . In general, a strategy proﬁle forms an equilibrium if no
DOMINANT
STRA TEGY
EQUILIBRIUM
EQUILIBRIUM player can beneﬁt by switching strategies, given that every other player sticks with the same
5 Pareto optimality is named after the economist Vilfredo Pareto (1848–1923).
Section 17.5. Decisions with Multiple Agents: Game Theory 669
strategy. An equilibrium is essentially a local optimum in the space of policies; it is the top
of a peak that slopes downward along every dimension, where a dimension corresponds to a
player’s strategy choices.
The mathematician John Nash (1928–) proved that every game has at least one equi-
librium. The general concept of equilibrium is now called Nash equilibrium in his honor.
Clearly, a dominant strategy equilibrium is a Nash equilibrium (Exercise 17.16), but someNASH EQUILIBRIUM
games have Nash equilibria but no dominant strategies.
The dilemma in the prisoner’s dilemma is that the equilibrium outcome is worse for
both players than the outcome they would get if they both refused to testify. In other words,
(testify,testify) is Pareto dominated by the (-1, -1) outcome of(refuse,refuse).I st h e r ea n y
way for Alice and Bob to arrive at the (-1, -1) outcome? It is certainly an allowable option
for both of them to refuse to testify, but is is hard to see how rational agents can get there,
given the deﬁnition of the game. Either player contemplating playing refuse will realize that
he or she would do better by playing testify. That is the attractive power of an equilibrium
point. Game theorists agree that being a Nash equilibrium is a necessary condition for being
a solution—although they disagree whether it is a sufﬁcient condition.
It is easy enough to get to the (refuse,refuse) solution if we modify the game. For
example, we could change to a repeated game in which the players know that they will meet
again. Or the agents might have moral beliefs that encourage cooperation and fairness. That
means they have a different utility function, necessitating a different payoff matrix, making
it a different game. We will see later that agents with limited computational powers, rather
than the ability to reason absolutely rationally, can reach non-equilibrium outcomes, as can an
agent that knows that the other agent has limited rationality. In each case, we are considering
a different game than the one described by the payoff matrix above.
Now let’s look at a game that has no dominant strategy. Acme, a video game console
manufacturer, has to decide whether its next game machine will use Blu-ray discs or DVDs.
Meanwhile, the video game software producer Best needs to decide whether to produce its
next game on Blu-ray or DVD. The proﬁts for both will be positive if they agree and negative
if they disagree, as shown in the following payoff matrix:
Acme:bluray
 Acme:dvd
Best:bluray
 A =+ 9,B =+ 9
 A =−4,B =−1
Best:dvd
 A =−3,B =−1
 A =+ 5,B =+ 5
There is no dominant strategy equilibrium for this game, but there are two Nash equilibria:
(bluray, bluray)a n d( dvd, dvd). We know these are Nash equilibria because if either player
unilaterally moves to a different strategy, that player will be worse off. Now the agents have
a problem: there are multiple acceptable solutions, but if each agent aims for a different
solution, then both agents will suffer . How can they agree on a solution? One answer is
that both should choose the Pareto-optimal solution ( bluray, bluray); that is, we can restrict
the deﬁnition of “solution” to the unique Pareto-optimal Nash equilibrium provided that one
exists. Every game has at least one Pareto-optimal solution, but a game might have several,
or they might not be equilibrium points. For example, if ( bluray, bluray) had payoff (5,
5), then there would be two equal Pareto-optimal equilibrium points. To choose between
670 Chapter 17. Making Complex Decisions
them the agents can either guess or communicate, which can be done either by establishing
a convention that orders the solutions before the game begins or by negotiating to reach a
mutually beneﬁcial solution during the game (which would mean including communicative
actions as part of a sequential game). Communication thus arises in game theory for exactly
the same reasons that it arose in multiagent planning in Section 11.4. Games in which players
need to communicate like this are called coordination games.
COORDINA TION
GAME
A game can have more than one Nash equilibrium; how do we know that every game
must have at least one? Some games have no pure-strategy Nash equilibria. Consider, for
example, any pure-strategy proﬁle for two-ﬁnger Morra (page 666). If the total number of
ﬁngers is even, thenO will want to switch; on the other hand (so to speak), if the total is odd,
then E will want to switch. Therefore, no pure strategy proﬁle can be an equilibrium and we
must look to mixed strategies instead.
But which mixed strategy? In 1928, von Neumann developed a method for ﬁnding the
optimal mixed strategy for two-player, zero-sum games—games in which the sum of the
ZERO-SUM GAME
payoffs is always zero.6 Clearly, Morra is such a game. For two-player, zero-sum games, we
know that the payoffs are equal and opposite, so we need consider the payoffs of only one
player, who will be the maximizer (just as in Chapter 5). For Morra, we pick the even player
E to be the maximizer, so we can deﬁne the payoff matrix by the valuesU
E(e, o)—the payoff
to E if E does e and O does o. (For convenience we call player E “her” and O “him.”) V on
Neumann’s method is called the the maximin technique, and it works as follows:MAXIMIN
•Suppose we change the rules as follows: ﬁrst E picks her strategy and reveals it to
O.T h e n O picks his strategy, with knowledge of E’s strategy. Finally, we evaluate
the expected payoff of the game based on the chosen strategies. This gives us a turn-
taking game to which we can apply the standard minimax algorithm from Chapter 5.
Let’s suppose this gives an outcome U
E,O . Clearly, this game favors O, so the true
utility U of the original game (from E’s point of view) is at least UE,O . For example,
if we just look at pure strategies, the minimax game tree has a root value of −3 (see
Figure 17.12(a)), so we know that U≥−3.
•Now suppose we change the rules to force O to reveal his strategy ﬁrst, followed byE.
Then the minimax value of this game isUO,E , and because this game favorsE we know
that U is at most UO,E . With pure strategies, the value is +2 (see Figure 17.12(b)), so
we know U≤+2.
Combining these two arguments, we see that the true utility U of the solution to the original
game must satisfy
UE,O ≤U≤UO,E or in this case, −3≤U≤2 .
To pinpoint the value ofU, we need to turn our analysis to mixed strategies. First, observe the
following: once the ﬁrst player has revealed his or her strategy, the second player might as
well choose a pure strategy. The reason is simple: if the second player plays a mixed strategy,
[p:one;( 1−p):two], its expected utility is a linear combination(p·uone +( 1−p)·utwo) of
6 or a constant—see page 162.
Section 17.5. Decisions with Multiple Agents: Game Theory 671
one
oneone
two
twotwo
E
O
one
oneone
two
twotwo
O
E
one two
E
O
one two
O
E
+4
+3
+2
+1
 0
–1
–2
–3
1
two
one
U
p
+4
+3
+2
+1
 0
–1
–2
–3
1
two
one
U
q
(a) (b)
(c) (d)
(e) (f)
[p: one; (1 – p): two][ q: one; (1 – q): two]
2p – 3(1 – p)2 q – 3(1 – q)3p + 4(1 – p)3 q + 4(1 – q)
2- 3
-3
-3
-3
-3
4 2
2
2
-3 -3 4
4
Figure 17.12 (a) and (b): Minimax game trees for two-ﬁnger Morra if the players take
turns playing pure strategies. (c) and (d): Parameterized game trees where the ﬁrst player
plays a mixed strategy. The payoffs depend on the probability parameter ( p or q)i nt h e
mixed strategy. (e) and (f): For any particular value of the probability parameter, the second
player will choose the “better” of the two actions, so the value of the ﬁrst player’s mixed
strategy is given by the heavy lines. The ﬁrst player will choose the probability parameter for
the mixed strategy at the intersection point.
the utilities of the pure strategies, uone and utwo . This linear combination can never be better
than the better of uone and utwo , so the second player can just choose the better one.
With this observation in mind, the minimax trees can be thought of as having inﬁnitely
many branches at the root, corresponding to the inﬁnitely many mixed strategies the ﬁrst
672 Chapter 17. Making Complex Decisions
player can choose. Each of these leads to a node with two branches corresponding to the
pure strategies for the second player. We can depict these inﬁnite trees ﬁnitely by having one
“parameterized” choice at the root:
•If E chooses ﬁrst, the situation is as shown in Figure 17.12(c). E chooses the strategy
[p:one;( 1−p):two] at the root, and thenO chooses a pure strategy (and hence a move)
given the value ofp.I f O chooses one, the expected payoff (toE)i s2p−3(1−p)=5 p−
3;i f O chooses two, the expected payoff is −3p +4 ( 1−p)=4 −7p. We can draw
these two payoffs as straight lines on a graph, wherep ranges from 0 to 1 on thex-axis,
as shown in Figure 17.12(e). O, the minimizer, will always choose the lower of the two
lines, as shown by the heavy lines in the ﬁgure. Therefore, the best that E can do at the
root is to choose p to be at the intersection point, which is where
5p−3=4 −7p ⇒ p =7 /12 .
The utility for E at this point is U
E,O =−1/12.
•If O moves ﬁrst, the situation is as shown in Figure 17.12(d). O chooses the strategy
[q:one;( 1−q):two] at the root, and then E chooses a move given the value of q.T h e
payoffs are2q−3(1−q)=5 q−3 and−3q+4(1−q)=4 −7q.7 Again, Figure 17.12(f)
shows that the best O can do at the root is to choose the intersection point:
5q−3=4 −7q ⇒ q =7 /12 .
The utility for E at this point is UO,E =−1/12.
Now we know that the true utility of the original game lies between −1/12 and−1/12,t h a t
is, it is exactly −1/12! (The moral is that it is better to be O than E if you are playing this
game.) Furthermore, the true utility is attained by the mixed strategy [7/12:one;5/12:two],
which should be played by both players. This strategy is called the maximin equilibrium ofMAXIMIN
EQUILIBRIUM
the game, and is a Nash equilibrium. Note that each component strategy in an equilibrium
mixed strategy has the same expected utility. In this case, both one and two have the same
expected utility,−1/12, as the mixed strategy itself.
Our result for two-ﬁnger Morra is an example of the general result by von Neumann:
every two-player zero-sum game has a maximin equilibrium when you allow mixed strategies.
Furthermore, every Nash equilibrium in a zero-sum game is a maximin for both players. A
player who adopts the maximin strategy has two guarantees: First, no other strategy can do
better against an opponent who plays well (although some other strategies might be better at
exploiting an opponent who makes irrational mistakes). Second, the player continues to do
just as well even if the strategy is revealed to the opponent.
The general algorithm for ﬁnding maximin equilibria in zero-sum games is somewhat
more involved than Figures 17.12(e) and (f) might suggest. When there aren possible actions,
a mixed strategy is a point in n-dimensional space and the lines become hyperplanes. It’s
also possible for some pure strategies for the second player to be dominated by others, so
that they are not optimal against any strategy for the ﬁrst player. After removing all such
strategies (which might have to be done repeatedly), the optimal choice at the root is the
7 It is a coincidence that these equations are the same as those for p; the coincidence arises because
UE(one, two)= UE(two, one)= − 3. This also explains why the optimal strategy is the same for both players.
Section 17.5. Decisions with Multiple Agents: Game Theory 673
highest (or lowest) intersection point of the remaining hyperplanes. Finding this choice is
an example of a linear programming problem: maximizing an objective function subject to
linear constraints. Such problems can be solved by standard techniques in time polynomial
in the number of actions (and in the number of bits used to specify the reward function, if you
want to get technical).
The question remains, what should a rational agent actuallydo in playing a single game
of Morra? The rational agent will have derived the fact that [7/12:one;5/12: two] is the
maximin equilibrium strategy, and will assume that this is mutual knowledge with a rational
opponent. The agent could use a 12-sided die or a random number generator to pick randomly
according to this mixed strategy, in which case the expected payoff would be -1/12 forE.O r
the agent could just decide to play one,o r two. In either case, the expected payoff remains
-1/12 for E. Curiously, unilaterally choosing a particular action does not harm one’s expected
payoff, but allowing the other agent to know that one has made such a unilateral decisiondoes
affect the expected payoff, because then the opponent can adjust his strategy accordingly.
Finding equilibria in non-zero-sum games is somewhat more complicated. The general
approach has two steps: (1) Enumerate all possible subsets of actions that might form mixed
strategies. For example, ﬁrst try all strategy proﬁles where each player uses a single action,
then those where each player uses either one or two actions, and so on. This is exponential
in the number of actions, and so only applies to relatively small games. (2) For each strategy
proﬁle enumerated in (1), check to see if it is an equilibrium. This is done by solving a set of
equations and inequalities that are similar to the ones used in the zero-sum case. For two play-
ers these equations are linear and can be solved with basic linear programming techniques,
but for three or more players they are nonlinear and may be very difﬁcult to solve.
17.5.2 Repeated games
So far we have looked only at games that last a single move. The simplest kind of multiple-
move game is the repeated game, in which players face the same choice repeatedly, but each
REPEA TED GAME
time with knowledge of the history of all players’ previous choices. A strategy proﬁle for a
repeated game speciﬁes an action choice for each player at each time step for every possible
history of previous choices. As with MDPs, payoffs are additive over time.
Let’s consider the repeated version of the prisoner’s dilemma. Will Alice and Bob work
together and refuse to testify, knowing they will meet again? The answer depends on the
details of the engagement. For example, suppose Alice and Bob know that they must play
exactly 100 rounds of prisoner’s dilemma. Then they both know that the 100th round will not
be a repeated game—that is, its outcome can have no effect on future rounds—and therefore
they will both choose the dominant strategy, testify, in that round. But once the 100th round
is determined, the 99th round can have no effect on subsequent rounds, so it too will have
a dominant strategy equilibrium at (testify,testify). By induction, both players will choose
testify on every round, earning a total jail sentence of 500 years each.
We can get different solutions by changing the rules of the interaction. For example,
suppose that after each round there is a 99% chance that the players will meet again. Then
the expected number of rounds is still 100, but neither player knows for sure which round
674 Chapter 17. Making Complex Decisions
will be the last. Under these conditions, more cooperative behavior is possible. For example,
one equilibrium strategy is for each player to refuse unless the other player has ever played
testify. This strategy could be called perpetual punishment . Suppose both players havePERPETUAL
PUNISHMENT
adopted this strategy, and this is mutual knowledge. Then as long as neither player has played
testify, then at any point in time the expected future total payoff for each player is
∞∑
t=0
0.99t· (−1) =−100 .
A player who deviates from the strategy and choosestestify will gain a score of 0 rather than
−1 on the very next move, but from then on both players will play testify and the player’s
total expected future payoff becomes
0+
∞∑
t=1
0.99t· (−5) =−495 .
Therefore, at every step, there is no incentive to deviate from (refuse,refuse). Perpetual
punishment is the “mutually assured destruction” strategy of the prisoner’s dilemma: once
either player decides to testify, it ensures that both players suffer a great deal. But it works
as a deterrent only if the other player believes you have adopted this strategy—or at least that
you might have adopted it.
Other strategies are more forgiving. The most famous, called tit-for-tat, calls for start-
TIT -FOR-T A T
ing with refuse and then echoing the other player’s previous move on all subsequent moves.
So Alice would refuse as long as Bob refuses and would testify the move after Bob testiﬁed,
but would go back to refusing if Bob did. Although very simple, this strategy has proven to
be highly robust and effective against a wide variety of strategies.
We can also get different solutions by changing the agents, rather than changing the
rules of engagement. Suppose the agents are ﬁnite-state machines with n states and they
a r ep l a y i n gag a m ew i t hm>n total steps. The agents are thus incapable of representing
the number of remaining steps, and must treat it as an unknown. Therefore, they cannot do
the induction, and are free to arrive at the more favorable ( refuse, refuse) equilibrium. In
this case, ignorance is bliss—or rather, having your opponent believe that you are ignorant is
bliss. Your success in these repeated games depends on the other player’s perception of you
as a bully or a simpleton, and not on your actual characteristics.
17.5.3 Sequential games
In the general case, a game consists of a sequence of turns that need not be all the same. Such
games are best represented by a game tree, which game theorists call theextensive form.T h eEXTENSIVE FORM
tree includes all the same information we saw in Section 5.1: an initial state S0, a function
PLAYER (s) that tells which player has the move, a function A CTIONS (s) enumerating the
possible actions, a function R ESULT (s,a) that deﬁnes the transition to a new state, and a
partial function U TILITY (s,p ), which is deﬁned only on terminal states, to give the payoff
for each player.
To represent stochastic games, such as backgammon, we add a distinguished player,
chance, that can take random actions. Chance’s “strategy” is part of the deﬁnition of the
Section 17.5. Decisions with Multiple Agents: Game Theory 675
game, speciﬁed as a probability distribution over actions (the other players get to choose
their own strategy). To represent games with nondeterministic actions, such as billiards, we
break the action into two pieces: the player’s action itself has a deterministic result, and then
chance has a turn to react to the action in its own capricious way. To represent simultaneous
moves, as in the prisoner’s dilemma or two-ﬁnger Morra, we impose an arbitrary order on the
players, but we have the option of asserting that the earlier player’s actions are not observable
to the subsequent players: e.g., Alice must choose refuse or testify ﬁrst, then Bob chooses,
but Bob does not know what choice Alice made at that time (we can also represent the fact
that the move is revealed later). However, we assume the players always remember all their
own previous actions; this assumption is called perfect recall.
The key idea of extensive form that sets it apart from the game trees of Chapter 5 is
the representation of partial observability. We saw in Section 5.6 that a player in a partially
observable game such as Kriegspiel can create a game tree over the space of belief states.
With that tree, we saw that in some cases a player can ﬁnd a sequence of moves (a strategy)
that leads to a forced checkmate regardless of what actual state we started in, and regardless of
what strategy the opponent uses. However, the techniques of Chapter 5 could not tell a player
what to do when there is no guaranteed checkmate. If the player’s best strategy depends
on the opponent’s strategy and vice versa, then minimax (or alpha–beta) by itself cannot
ﬁnd a solution. The extensive form does allow us to ﬁnd solutions because it represents the
belief states (game theorists call them information sets)o f all players at once. From that
INFORMA TION SETS
representation we can ﬁnd equilibrium solutions, just as we did with normal-form games.
As a simple example of a sequential game, place two agents in the 4× 3 world of Fig-
ure 17.1 and have them move simultaneously until one agent reaches an exit square, and gets
the payoff for that square. If we specify that no movement occurs when the two agents try
to move into the same square simultaneously (a common problem at many trafﬁc intersec-
tions), then certain pure strategies can get stuck forever. Thus, agents need a mixed strategy
to perform well in this game: randomly choose between moving ahead and staying put. This
is exactly what is done to resolve packet collisions in Ethernet networks.
Next we’ll consider a very simple variant of poker. The deck has only four cards, two
aces and two kings. One card is dealt to each player. The ﬁrst player then has the option
to raise the stakes of the game from 1 point to 2, or to check. If player 1 checks, the game
is over. If he raises, then player 2 has the option to call, accepting that the game is worth 2
points, or fold, conceding the 1 point. If the game does not end with a fold, then the payoff
depends on the cards: it is zero for both players if they have the same card; otherwise the
player with the king pays the stakes to the player with the ace.
The extensive-form tree for this game is shown in Figure 17.13. Nonterminal states are
shown as circles, with the player to move inside the circle; player 0 is chance. Each action is
depicted as an arrow with a label, corresponding to araise, check, call, or fold,o r ,f o rchance,
the four possible deals (“AK” means that player 1 gets an ace and player 2 a king). Terminal
states are rectangles labeled by their payoff to player 1 and player 2. Information sets are
shown as labeled dashed boxes; for example, I
1,1 is the information set where it is player
1’s turn, and he knows he has an ace (but does not know what player 2 has). In information
set I
2,1, it is player 2’s turn and she knows that she has an ace and that player 1 has raised,
676 Chapter 17. Making Complex Decisions
0 
1 
1 
1 
1 
2 
2 
2 
2 
0,0!
+1,-1!
0,0!
-1,+1 !
1/6: AA
r 
k 
r 
k 
r 
k 
r 
k 
+1,-1 !
+1,-1 !
+1,-1!
+1,-1!
0,0!
+2,-2
0,0
-2,+2
c 
f 
c 
f 
c 
f 
c 
f 
1/3: KA
1/3: AK
1/6: KK
2
I1,1 
I1,2 I2,1 
I2,2 
I2,1 
Figure 17.13 Extensive form of a simpliﬁed version of poker.
but does not know what card player 1 has. (Due to the limits of two-dimensional paper, this
information set is shown as two boxes rather than one.)
One way to solve an extensive game is to convert it to a normal-form game. Recall that
the normal form is a matrix, each row of which is labeled with a pure strategy for player 1, and
each column by a pure strategy for player 2. In an extensive game a pure strategy for player
i corresponds to an action for each information set involving that player. So in Figure 17.13,
one pure strategy for player 1 is “raise when in I
1,1 (that is, when I have an ace), and check
when in I1,2 (when I have a king).” In the payoff matrix below, this strategy is called rk.
Similarly, strategy cf for player 2 means “call when I have an ace and fold when I have a
king.” Since this is a zero-sum game, the matrix below gives only the payoff for player 1;
player 2 always has the opposite payoff:
2:cc
 2:cf
 2:ff
 2:fc
1:rr
 0
 -1/6
 1
 7/6
1:kr
 -1/3
 -1/6
 5/6
 2/3
1:rk
 1/3
 0
 1/6
 1/2
1:kk
 0
 0
 0
 0
This game is so simple that it has two pure-strategy equilibria, shown in bold: cf for player
2a n d rk or kk for player 1. But in general we can solve extensive games by converting
to normal form and then ﬁnding a solution (usually a mixed strategy) using standard linear
programming methods. That works in theory. But if a player has I information sets and
a actions per set, then that player will have aI pure strategies. In other words, the size of
the normal-form matrix is exponential in the number of information sets, so in practice the
Section 17.5. Decisions with Multiple Agents: Game Theory 677
approach works only for very small game trees, on the order of a dozen states. A game like
Texas hold’em poker has about1018 states, making this approach completely infeasible.
What are the alternatives? In Chapter 5 we saw how alpha–beta search could handle
games of perfect information with huge game trees by generating the tree incrementally, by
pruning some branches, and by heuristically evaluating nonterminal nodes. But that approach
does not work well for games with imperfect information, for two reasons: ﬁrst, it is harder
to prune, because we need to consider mixed strategies that combine multiple branches, not a
pure strategy that always chooses the best branch. Second, it is harder to heuristically evaluate
a nonterminal node, because we are dealing with information sets, not individual states.
Koller et al. (1996) come to the rescue with an alternative representation of extensive
games, called the sequence form, that is only linear in the size of the tree, rather than ex-
SEQUENCE FORM
ponential. Rather than represent strategies, it represents paths through the tree; the number
of paths is equal to the number of terminal nodes. Standard linear programming methods
can again be applied to this representation. The resulting system can solve poker variants
with 25,000 states in a minute or two. This is an exponential speedup over the normal-form
approach, but still falls far short of handling full poker, with10
18 states.
If we can’t handle 1018 states, perhaps we can simplify the problem by changing the
game to a simpler form. For example, if I hold an ace and am considering the possibility that
the next card will give me a pair of aces, then I don’t care about the suit of the next card; any
suit will do equally well. This suggests forming an abstraction of the game, one in which
ABSTRACTION
suits are ignored. The resulting game tree will be smaller by a factor of 4! = 24. Suppose I
can solve this smaller game; how will the solution to that game relate to the original game?
If no player is going for a ﬂush (or blufﬁng so), then the suits don’t matter to any player, and
the solution for the abstraction will also be a solution for the original game. However, if any
player is contemplating a ﬂush, then the abstraction will be only an approximate solution (but
it is possible to compute bounds on the error).
There are many opportunities for abstraction. For example, at the point in a game where
each player has two cards, if I hold a pair of queens, then the other players’ hands could be
abstracted into three classes: better (only a pair of kings or a pair of aces), same (pair of
queens) or worse (everything else). However, this abstraction might be too coarse. A better
abstraction would divide worse into, say, medium pair (nines through jacks), low pair,a n d
no pair. These examples are abstractions of states; it is also possible to abstract actions. For
example, instead of having a bet action for each integer from 1 to 1000, we could restrict the
bets to 10
0, 101, 102 and 103. Or we could cut out one of the rounds of betting altogether.
We can also abstract over chance nodes, by considering only a subset of the possible deals.
This is equivalent to the rollout technique used in Go programs. Putting all these abstractions
together, we can reduce the 10
18 states of poker to 107 states, a size that can be solved with
current techniques.
Poker programs based on this approach can easily defeat novice and some experienced
human players, but are not yet at the level of master players. Part of the problem is that
the solution these programs approximate—the equilibrium solution—is optimal only against
an opponent who also plays the equilibrium strategy. Against fallible human players it is
important to be able to exploit an opponent’s deviation from the equilibrium strategy. As
678 Chapter 17. Making Complex Decisions
Gautam Rao (aka “The Count”), the world’s leading online poker player, said (Billingset al.,
2003), “You have a very strong program. Once you add opponent modeling to it, it will kill
everyone.” However, good models of human fallability remain elusive.
In a sense, extensive game form is the one of the most complete representations we have
seen so far: it can handle partially observable, multiagent, stochastic, sequential, dynamic
environments—most of the hard cases from the list of environment properties on page 42.
However, there are two limitations of game theory. First, it does not deal well with continuous
states and actions (although there have been some extensions to the continuous case; for
example, the theory of Cournot competition uses game theory to solve problems where two
COURNOT
COMPETITION
companies choose prices for their products from a continuous space). Second, game theory
assumes the game is known. Parts of the game may be speciﬁed as unobservable to some of
the players, but it must be known what parts are unobservable. In cases in which the players
learn the unknown structure of the game over time, the model begins to break down. Let’s
examine each source of uncertainty, and whether each can be represented in game theory.
Actions: There is no easy way to represent a game where the players have to discover
what actions are available. Consider the game between computer virus writers and security
experts. Part of the problem is anticipating what action the virus writers will try next.
Strategies: Game theory is very good at representing the idea that the other players’
strategies are initially unknown—as long as we assume all agents are rational. The theory
itself does not say what to do when the other players are less than fully rational. The notion
of a Bayes–Nash equilibrium partially addresses this point: it is an equilibrium with respect
BAYES–NASH
EQUILIBRIUM
to a player’s prior probability distribution over the other players’ strategies—in other words,
it expresses a player’s beliefs about the other players’ likely strategies.
Chance: If a game depends on the roll of a die, it is easy enough to model a chance node
with uniform distribution over the outcomes. But what if it is possible that the die is unfair?
We can represent that with another chance node, higher up in the tree, with two branches for
“die is fair” and “die is unfair,” such that the corresponding nodes in each branch are in the
same information set (that is, the players don’t know if the die is fair or not). And what if we
suspect the other opponent does know? Then we add another chance node, with one branch
representing the case where the opponent does know, and one where he doesn’t.
Utilities: What if we don’t know our opponent’s utilities? Again, that can be modeled
with a chance node, such that the other agent knows its own utilities in each branch, but we
don’t. But what if we don’t know our own utilities? For example, how do I know if it is
rational to order the Chef’s salad if I don’t know how much I will like it? We can model that
with yet another chance node specifying an unobservable “intrinsic quality” of the salad.
Thus, we see that game theory is good at representing most sources of uncertainty—but
at the cost of doubling the size of the tree every time we add another node; a habit which
quickly leads to intractably large trees. Because of these and other problems, game theory
has been used primarily toanalyze environments that are at equilibrium, rather than tocontrol
agents within an environment. Next we shall see how it can help design environments.
Section 17.6. Mechanism Design 679
17.6 M ECHANISM DESIGN
In the previous section, we asked, “Given a game, what is a rational strategy?” In this sec-
tion, we ask, “Given that agents pick rational strategies, what game should we design?” More
speciﬁcally, we would like to design a game whose solutions, consisting of each agent pursu-
ing its own rational strategy, result in the maximization of some global utility function. This
problem is called mechanism design, or sometimes inverse game theory . Mechanism de-
MECHANISM DESIGN
sign is a staple of economics and political science. Capitalism 101 says that if everyone tries
to get rich, the total wealth of society will increase. But the examples we will discuss show
that proper mechanism design is necessary to keep the invisible hand on track. For collections
of agents, mechanism design allows us to construct smart systems out of a collection of more
limited systems—even uncooperative systems—in much the same way that teams of humans
can achieve goals beyond the reach of any individual.
Examples of mechanism design include auctioning off cheap airline tickets, routing
TCP packets between computers, deciding how medical interns will be assigned to hospitals,
and deciding how robotic soccer players will cooperate with their teammates. Mechanism
design became more than an academic subject in the 1990s when several nations, faced with
the problem of auctioning off licenses to broadcast in various frequency bands, lost hundreds
of millions of dollars in potential revenue as a result of poor mechanism design. Formally,
a mechanism consists of (1) a language for describing the set of allowable strategies that
MECHANISM
agents may adopt, (2) a distinguished agent, called thecenter, that collects reports of strategyCENTER
choices from the agents in the game, and (3) an outcome rule, known to all agents, that the
center uses to determine the payoffs to each agent, given their strategy choices.
17.6.1 Auctions
Let’s consider auctions ﬁrst. An auction is a mechanism for selling some goods to membersAUCTION
of a pool of bidders. For simplicity, we concentrate on auctions with a single item for sale.
Each bidder i has a utility value vi for having the item. In some cases, each bidder has a
private value for the item. For example, the ﬁrst item sold on eBay was a broken laser
pointer, which sold for $14.83 to a collector of broken laser pointers. Thus, we know that the
collector has v
i ≥$14.83, but most other people would have vj ≪ $14.83. In other cases,
such as auctioning drilling rights for an oil tract, the item has a common value—the tract
will produce some amount of money, X, and all bidders value a dollar equally—but there is
uncertainty as to what the actual value of X is. Different bidders have different information,
and hence different estimates of the item’s true value. In either case, bidders end up with their
own vi.G i v e nvi, each bidder gets a chance, at the appropriate time or times in the auction,
to make a bid bi. The highest bid, bmax wins the item, but the price paid need not be bmax;
that’s part of the mechanism design.
The best-known auction mechanism is the ascending-bid,8 or English auction ,i nASCENDING-BID
ENGLISH AUCTION which the center starts by asking for a minimum (or reserve)b i d bmin. If some bidder is
8 The word “auction” comes from the Latin augere, to increase.
680 Chapter 17. Making Complex Decisions
willing to pay that amount, the center then asks for bmin + d, for some increment d,a n d
continues up from there. The auction ends when nobody is willing to bid anymore; then the
last bidder wins the item, paying the price he bid.
How do we know if this is a good mechanism? One goal is to maximize expected
revenue for the seller. Another goal is to maximize a notion of global utility. These goals
overlap to some extent, because one aspect of maximizing global utility is to ensure that the
winner of the auction is the agent who values the item the most (and thus is willing to pay
the most). We say an auction is efﬁcient if the goods go to the agent who values them most.
EFFICIENT
The ascending-bid auction is usually both efﬁcient and revenue maximizing, but if the reserve
price is set too high, the bidder who values it most may not bid, and if the reserve is set too
low, the seller loses net revenue.
Probably the most important things that an auction mechanism can do is encourage a
sufﬁcient number of bidders to enter the game and discourage them from engaging in collu-
sion. Collusion is an unfair or illegal agreement by two or more bidders to manipulate prices.
COLLUSION
It can happen in secret backroom deals or tacitly, within the rules of the mechanism.
For example, in 1999, Germany auctioned ten blocks of cell-phone spectrum with a
simultaneous auction (bids were taken on all ten blocks at the same time), using the rule that
any bid must be a minimum of a 10% raise over the previous bid on a block. There were only
two credible bidders, and the ﬁrst, Mannesman, entered the bid of 20 million deutschmark
on blocks 1-5 and 18.18 million on blocks 6-10. Why 18.18M? One of T-Mobile’s managers
said they “interpreted Mannesman’s ﬁrst bid as an offer.” Both parties could compute that
a 10% raise on 18.18M is 19.99M; thus Mannesman’s bid was interpreted as saying “we
can each get half the blocks for 20M; let’s not spoil it by bidding the prices up higher.”
And in fact T-Mobile bid 20M on blocks 6-10 and that was the end of the bidding. The
German government got less than they expected, because the two competitors were able to
use the bidding mechanism to come to a tacit agreement on how not to compete. From
the government’s point of view, a better result could have been obtained by any of these
changes to the mechanism: a higher reserve price; a sealed-bid ﬁrst-price auction, so that
the competitors could not communicate through their bids; or incentives to bring in a third
bidder. Perhaps the 10% rule was an error in mechanism design, because it facilitated the
precise signaling from Mannesman to T-Mobile.
In general, both the seller and the global utility function beneﬁt if there are more bid-
ders, although global utility can suffer if you count the cost of wasted time of bidders that
have no chance of winning. One way to encourage more bidders is to make the mechanism
easier for them. After all, if it requires too much research or computation on the part of the
bidders, they may decide to take their money elsewhere. So it is desirable that the bidders
have a dominant strategy. Recall that “dominant” means that the strategy works against all
other strategies, which in turn means that an agent can adopt it without regard for the other
strategies. An agent with a dominant strategy can just bid, without wasting time contemplat-
ing other agents’ possible strategies. A mechanism where agents have a dominant strategy
is called a strategy-proof mechanism. If, as is usually the case, that strategy involves the
STRA TEGY -PROOF
bidders revealing their true value,vi,t h e ni ti sc a l l e datruth-revealing,o r truthful, auction;TRUTH-REVEALING
the term incentive compatible is also used. The revelation principle states that any mecha-REVELA TION
PRINCIPLE
Section 17.6. Mechanism Design 681
nism can be transformed into an equivalent truth-revealing mechanism, so part of mechanism
design is ﬁnding these equivalent mechanisms.
It turns out that the ascending-bid auction has most of the desirable properties. The
bidder with the highest value vi gets the goods at a price of bo + d,w h e r ebo is the highest
bid among all the other agents and d is the auctioneer’s increment.9 Bidders have a simple
dominant strategy: keep bidding as long as the current cost is below yourvi. The mechanism
is not quite truth-revealing, because the winning bidder reveals only that his vi ≥bo + d;w e
have a lower bound on vi but not an exact amount.
A disadvantage (from the point of view of the seller) of the ascending-bid auction is
that it can discourage competition. Suppose that in a bid for cell-phone spectrum there is
one advantaged company that everyone agrees would be able to leverage existing customers
and infrastructure, and thus can make a larger proﬁt than anyone else. Potential competitors
can see that they have no chance in an ascending-bid auction, because the advantaged com-
pany can always bid higher. Thus, the competitors may not enter at all, and the advantaged
company ends up winning at the reserve price.
Another negative property of the English auction is its high communication costs. Either
the auction takes place in one room or all bidders have to have high-speed, secure communi-
cation lines; in either case they have to have the time available to go through several rounds of
bidding. An alternative mechanism, which requires much less communication, is the sealed-
bid auction. Each bidder makes a single bid and communicates it to the auctioneer, without
SEALED-BID
AUCTION
the other bidders seeing it. With this mechanism, there is no longer a simple dominant strat-
egy. If your value is vi and you believe that the maximum of all the other agents’ bids will
be bo, then you should bid bo + ϵ, for some small ϵ, if that is less than vi. Thus, your bid
depends on your estimation of the other agents’ bids, requiring you to do more work. Also,
note that the agent with the highest v
i might not win the auction. This is offset by the fact
that the auction is more competitive, reducing the bias toward an advantaged bidder.
A small change in the mechanism for sealed-bid auctions produces the sealed-bid
second-price auction, also known as a Vickrey auction.10 In such auctions, the winner pays
SEALED-BID
SECOND-PRICE
AUCTION
VICKREY AUCTION
the price of the second-highest bid, bo, rather than paying his own bid. This simple modiﬁ-
cation completely eliminates the complex deliberations required for standard (or ﬁrst-price)
sealed-bid auctions, because the dominant strategy is now simply to bidvi; the mechanism is
truth-revealing. Note that the utility of agent i in terms of his bid bi, his value vi, and the best
bid among the other agents, bo,i s
ui =
{ (vi−bo) if bi >b o
0 otherwise.
To see that bi = vi is a dominant strategy, note that when (vi −bo) is positive, any bid
that wins the auction is optimal, and bidding vi in particular wins the auction. On the other
hand, when (vi −bo) is negative, any bid that loses the auction is optimal, and bidding vi in
9 There is actually a small chance that the agent with highest vi fails to get the goods, in the case in which
bo <v i <b o + d. The chance of this can be made arbitrarily small by decreasing the increment d.
10 Named after William Vickrey (1914–1996), who won the 1996 Nobel Prize in economics for this work and
died of a heart attack three days later.
682 Chapter 17. Making Complex Decisions
particular loses the auction. So bidding vi is optimal for all possible values of bo, and in fact,
vi is the only bid that has this property. Because of its simplicity and the minimal computation
requirements for both seller and bidders, the Vickrey auction is widely used in constructing
distributed AI systems. Also, Internet search engines conduct over a billion auctions a day
to sell advertisements along with their search results, and online auction sites handle $100
billion a year in goods, all using variants of the Vickrey auction. Note that the expected value
to the seller is b
o, which is the same expected return as the limit of the English auction as
the increment d goes to zero. This is actually a very general result: the revenue equivalence
theorem states that, with a few minor caveats, any auction mechanism where risk-neutral
REVENUE
EQUIVALENCE
THEOREM
bidders have values vi known only to themselves (but know a probability distribution from
which those values are sampled), will yield the same expected revenue. This principle means
that the various mechanisms are not competing on the basis of revenue generation, but rather
on other qualities.
Although the second-price auction is truth-revealing, it turns out that extending the idea
to multiple goods and using a next-price auction is not truth-revealing. Many Internet search
engines use a mechanism where they auction k slots for ads on a page. The highest bidder
wins the top spot, the second highest gets the second spot, and so on. Each winner pays the
price bid by the next-lower bidder, with the understanding that payment is made only if the
searcher actually clicks on the ad. The top slots are considered more valuable because they
are more likely to be noticed and clicked on. Imagine that three bidders, b
1,b2 and b3,h a v e
valuations for a click of v1 = 200,v2 = 180, and v3 = 100,a n dt h a tk =2 slots are available,
where it is known that the top spot is clicked on 5% of the time and the bottom spot 2%. If
all bidders bid truthfully, then b
1 wins the top slot and pays 180, and has an expected return
of (200−180)× 0.05 = 1. The second slot goes to b2.B u tb1 can see that if she were to bid
anything in the range 101–179, she would concede the top slot tob2, win the second slot, and
yield an expected return of(200−100)×.02 = 2. Thus, b1 can double her expected return by
bidding less than her true value in this case. In general, bidders in this multislot auction must
spend a lot of energy analyzing the bids of others to determine their best strategy; there is no
simple dominant strategy. Aggarwal et al. (2006) show that there is a unique truthful auction
mechanism for this multislot problem, in which the winner of slot j pays the full price for
slot j just for those additional clicks that are available at slot j and not at slot j +1 .T h e
winner pays the price for the lower slot for the remaining clicks. In our example, b
1 would
bid 200 truthfully, and would pay 180 for the additional.05−.02 =.03 clicks in the top slot,
but would pay only the cost of the bottom slot, 100, for the remaining .02 clicks. Thus, the
total return to b1 would be (200−180)× .03 + (200−100)× .02 = 2.6.
Another example of where auctions can come into play within AI is when a collection
of agents are deciding whether to cooperate on a joint plan. Hunsberger and Grosz (2000)
show that this can be accomplished efﬁciently with an auction in which the agents bid for
roles in the joint plan.
Section 17.6. Mechanism Design 683
17.6.2 Common goods
Now let’s consider another type of game, in which countries set their policy for controlling
air pollution. Each country has a choice: they can reduce pollution at a cost of -10 points for
implementing the necessary changes, or they can continue to pollute, which gives them a net
utility of -5 (in added health costs, etc.) and also contributes -1 points to every other country
(because the air is shared across countries). Clearly, the dominant strategy for each country
is “continue to pollute,” but if there are 100 countries and each follows this policy, then each
country gets a total utility of -104, whereas if every country reduced pollution, they would
each have a utility of -10. This situation is called the tragedy of the commons : if nobody
TRAGEDY OF THE
COMMONS
has to pay for using a common resource, then it tends to be exploited in a way that leads to
a lower total utility for all agents. It is similar to the prisoner’s dilemma: there is another
solution to the game that is better for all parties, but there appears to be no way for rational
agents to arrive at that solution.
The standard approach for dealing with the tragedy of the commons is to change the
mechanism to one that charges each agent for using the commons. More generally, we need
to ensure that all externalities—effects on global utility that are not recognized in the in-
EXTERNALITIES
dividual agents’ transactions—are made explicit. Setting the prices correctly is the difﬁcult
part. In the limit, this approach amounts to creating a mechanism in which each agent is
effectively required to maximize global utility, but can do so by making a local decision. For
this example, a carbon tax would be an example of a mechanism that charges for use of the
commons in a way that, if implemented well, maximizes global utility.
As a ﬁnal example, consider the problem of allocating some common goods. Suppose a
city decides it wants to install some free wireless Internet transceivers. However, the number
of transceivers they can afford is less than the number of neighborhoods that want them. The
city wants to allocate the goods efﬁciently, to the neighborhoods that would value them the
most. That is, they want to maximize the global utility V = ∑
i vi. The problem is that if
they just ask each neighborhood council “how much do you value this free gift?” they would
all have an incentive to lie, and report a high value. It turns out there is a mechanism, known
as the Vickrey-Clarke-Groves,o r VCG, mechanism, that makes it a dominant strategy for
VICKREY -CLARKE-
GROVES
VCG each agent to report its true utility and that achieves an efﬁcient allocation of the goods. The
trick is that each agent pays a tax equivalent to the loss in global utility that occurs because
of the agent’s presence in the game. The mechanism works like this:
1. The center asks each agent to report its value for receiving an item. Call this bi.
2. The center allocates the goods to a subset of the bidders. We call this subset A, and use
the notation bi(A) to mean the result to i under this allocation: bi if i is in A (that is, i
is a winner), and 0 otherwise. The center chooses A to maximize total reported utility
B = ∑
i bi(A).
3. The center calculates (for each i) the sum of the reported utilities for all the winners
except i. We use the notation B− i = ∑
j̸=i bj(A). The center also computes (for each
i) the allocation that would maximize total global utility if i were not in the game; call
that sum W− i.
4. Each agent i pays a tax equal to W− i−B− i.
684 Chapter 17. Making Complex Decisions
In this example, the VCG rule means that each winner would pay a tax equal to the highest
reported value among the losers. That is, if I report my value as 5, and that causes someone
with value 2 to miss out on an allocation, then I pay a tax of 2. All winners should be happy
because they pay a tax that is less than their value, and all losers are as happy as they can be,
because they value the goods less than the required tax.
Why is it that this mechanism is truth-revealing? First, consider the payoff to agent i,
which is the value of getting an item, minus the tax:
v
i(A)−(W− i−B− i) . (17.14)
Here we distinguish the agent’s true utility, vi, from his reported utility bi (but we are trying
to show that a dominant strategy is bi = vi). Agent i knows that the center will maximize
global utility using the reported values,
∑
j
bj(A)= bi(A)+
∑
j̸=i
bj(A)
whereas agent i wants the center to maximize (17.14), which can be rewritten as
vi(A)+
∑
j̸=i
bj(A)−W− i .
Since agent i cannot affect the value of W− i (it depends only on the other agents), the only
way i can make the center optimize what i wants is to report the true utility, bi =vi.
17.7 S UMMARY
This chapter shows how to use knowledge about the world to make decisions even when the
outcomes of an action are uncertain and the rewards for acting might not be reaped until many
actions have passed. The main points are as follows:
•Sequential decision problems in uncertain environments, also called Markov decision
processes, or MDPs, are deﬁned by a transition model specifying the probabilistic
outcomes of actions and a reward function specifying the reward in each state.
•The utility of a state sequence is the sum of all the rewards over the sequence, possibly
discounted over time. The solution of an MDP is a policy that associates a decision
with every state that the agent might reach. An optimal policy maximizes the utility of
the state sequences encountered when it is executed.
•The utility of a state is the expected utility of the state sequences encountered when
an optimal policy is executed, starting in that state. The value iteration algorithm for
solving MDPs works by iteratively solving the equations relating the utility of each state
to those of its neighbors.
•Policy iteration alternates between calculating the utilities of states under the current
policy and improving the current policy with respect to the current utilities.
•Partially observable MDPs, or POMDPs, are much more difﬁcult to solve than are
MDPs. They can be solved by conversion to an MDP in the continuous space of belief
Bibliographical and Historical Notes 685
states; both value iteration and policy iteration algorithms have been devised. Optimal
behavior in POMDPs includes information gathering to reduce uncertainty and there-
fore make better decisions in the future.
•A decision-theoretic agent can be constructed for POMDP environments. The agent
uses a dynamic decision network to represent the transition and sensor models, to
update its belief state, and to project forward possible action sequences.
•Game theory describes rational behavior for agents in situations in which multiple
agents interact simultaneously. Solutions of games are Nash equilibria—strategy pro-
ﬁles in which no agent has an incentive to deviate from the speciﬁed strategy.
•Mechanism design can be used to set the rules by which agents will interact, in order
to maximize some global utility through the operation of individually rational agents.
Sometimes, mechanisms exist that achieve this goal without requiring each agent to
consider the choices made by other agents.
We shall return to the world of MDPs and POMDP in Chapter 21, when we study rein-
forcement learning methods that allow an agent to improve its behavior from experience in
sequential, uncertain environments.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Richard Bellman developed the ideas underlying the modern approach to sequential decision
problems while working at the RAND Corporation beginning in 1949. According to his au-
tobiography (Bellman, 1984), he coined the exciting term “dynamic programming” to hide
from a research-phobic Secretary of Defense, Charles Wilson, the fact that his group was
doing mathematics. (This cannot be strictly true, because his ﬁrst paper using the term (Bell-
man, 1952) appeared before Wilson became Secretary of Defense in 1953.) Bellman’s book,
Dynamic Programming (1957), gave the new ﬁeld a solid foundation and introduced the basic
algorithmic approaches. Ron Howard’s Ph.D. thesis (1960) introduced policy iteration and
the idea of average reward for solving inﬁnite-horizon problems. Several additional results
were introduced by Bellman and Dreyfus (1962). Modiﬁed policy iteration is due to van
Nunen (1976) and Puterman and Shin (1978). Asynchronous policy iteration was analyzed
by Williams and Baird (1993), who also proved the policy loss bound in Equation (17.9). The
analysis of discounting in terms of stationary preferences is due to Koopmans (1972). The
texts by Bertsekas (1987), Puterman (1994), and Bertsekas and Tsitsiklis (1996) provide a
rigorous introduction to sequential decision problems. Papadimitriou and Tsitsiklis (1987)
describe results on the computational complexity of MDPs.
Seminal work by Sutton (1988) and Watkins (1989) on reinforcement learning methods
for solving MDPs played a signiﬁcant role in introducing MDPs into the AI community, as
did the later survey by Barto et al. (1995). (Earlier work by Werbos (1977) contained many
similar ideas, but was not taken up to the same extent.) The connection between MDPs and
AI planning problems was made ﬁrst by Sven Koenig (1991), who showed how probabilistic
S
TRIPS operators provide a compact representation for transition models (see also Wellman,
686 Chapter 17. Making Complex Decisions
1990b). Work by Dean et al. (1993) and Tash and Russell (1994) attempted to overcome
the combinatorics of large state spaces by using a limited search horizon and abstract states.
Heuristics based on the value of information can be used to select areas of the state space
where a local expansion of the horizon will yield a signiﬁcant improvement in decision qual-
ity. Agents using this approach can tailor their effort to handle time pressure and generate
some interesting behaviors such as using familiar “beaten paths” to ﬁnd their way around the
state space quickly without having to recompute optimal decisions at each point.
As one might expect, AI researchers have pushed MDPs in the direction of more ex-
pressive representations that can accommodate much larger problems than the traditional
atomic representations based on transition matrices. The use of a dynamic Bayesian network
to represent transition models was an obvious idea, but work on factored MDPs (Boutilier
FACTORED MDP
et al. , 2000; Koller and Parr, 2000; Guestrin et al. , 2003b) extends the idea to structured
representations of the value function with provable improvements in complexity. Relational
MDPs (Boutilier et al. , 2001; Guestrin et al. , 2003a) go one step further, using structuredRELA TIONAL MDP
representations to handle domains with many related objects.
The observation that a partially observable MDP can be transformed into a regular MDP
over belief states is due to Astrom (1965) and Aoki (1965). The ﬁrst complete algorithm for
the exact solution of POMDPs—essentially the value iteration algorithm presented in this
chapter—was proposed by Edward Sondik (1971) in his Ph.D. thesis. (A later journal paper
by Smallwood and Sondik (1973) contains some errors, but is more accessible.) Lovejoy
(1991) surveyed the ﬁrst twenty-ﬁve years of POMDP research, reaching somewhat pes-
simistic conclusions about the feasibility of solving large problems. The ﬁrst signiﬁcant
contribution within AI was the Witness algorithm (Cassandra et al., 1994; Kaelbling et al.,
1998), an improved version of POMDP value iteration. Other algorithms soon followed, in-
cluding an approach due to Hansen (1998) that constructs a policy incrementally in the form
of a ﬁnite-state automaton. In this policy representation, the belief state corresponds directly
to a particular state in the automaton. More recent work in AI has focused on point-based
value iteration methods that, at each iteration, generate conditional plans and α-vectors for
a ﬁnite set of belief states rather than for the entire belief space. Lovejoy (1991) proposed
such an algorithm for a ﬁxed grid of points, an approach taken also by Bonet (2002). An
inﬂuential paper by Pineau et al. (2003) suggested generating reachable points by simulat-
ing trajectories in a somewhat greedy fashion; Spaan and Vlassis (2005) observe that one
need generate plans for only a small, randomly selected subset of points to improve on the
plans from the previous iteration for all points in the set. Current point-based methods—
such as point-based policy iteration (Ji et al., 2007)—can generate near-optimal solutions for
POMDPs with thousands of states. Because POMDPs are PSPACE-hard (Papadimitriou and
Tsitsiklis, 1987), further progress may require taking advantage of various kinds of structure
within a factored representation.
The online approach—using look-ahead search to select an action for the current belief
state—was ﬁrst examined by Satia and Lave (1973). The use of sampling at chance nodes
was explored analytically by Kearns et al. (2000) and Ng and Jordan (2000). The basic
ideas for an agent architecture using dynamic decision networks were proposed by Dean
and Kanazawa (1989a). The book Planning and Control by Dean and Wellman (1991) goes
Bibliographical and Historical Notes 687
into much greater depth, making connections between DBN/DDN models and the classical
control literature on ﬁltering. Tatman and Shachter (1990) showed how to apply dynamic
programming algorithms to DDN models. Russell (1998) explains various ways in which
such agents can be scaled up and identiﬁes a number of open research issues.
The roots of game theory can be traced back to proposals made in the 17th century
by Christiaan Huygens and Gottfried Leibniz to study competitive and cooperative human
interactions scientiﬁcally and mathematically. Throughout the 19th century, several leading
economists created simple mathematical examples to analyze particular examples of com-
petitive situations. The ﬁrst formal results in game theory are due to Zermelo (1913) (who
had, the year before, suggested a form of minimax search for games, albeit an incorrect one).
Emile Borel (1921) introduced the notion of a mixed strategy. John von Neumann (1928)
proved that every two-person, zero-sum game has a maximin equilibrium in mixed strategies
and a well-deﬁned value. V on Neumann’s collaboration with the economist Oskar Morgen-
stern led to the publication in 1944 of the Theory of Games and Economic Behavior ,t h e
deﬁning book for game theory. Publication of the book was delayed by the wartime paper
shortage until a member of the Rockefeller family personally subsidized its publication.
In 1950, at the age of 21, John Nash published his ideas concerning equilibria in general
(non-zero-sum) games. His deﬁnition of an equilibrium solution, although originating in the
work of Cournot (1838), became known as Nash equilibrium. After a long delay because
of the schizophrenia he suffered from 1959 onward, Nash was awarded the Nobel Memorial
Prize in Economics (along with Reinhart Selten and John Harsanyi) in 1994. The Bayes–Nash
equilibrium is described by Harsanyi (1967) and discussed by Kadane and Larkey (1982).
Some issues in the use of game theory for agent control are covered by Binmore (1982).
The prisoner’s dilemma was invented as a classroom exercise by Albert W. Tucker in
1950 (based on an example by Merrill Flood and Melvin Dresher) and is covered extensively
by Axelrod (1985) and Poundstone (1993). Repeated games were introduced by Luce and
Raiffa (1957), and games of partial information in extensive form by Kuhn (1953). The ﬁrst
practical algorithm for sequential, partial-information games was developed within AI by
Koller et al. (1996); the paper by Koller and Pfeffer (1997) provides a readable introduction
to the ﬁeld and describe a working system for representing and solving sequential games.
The use of abstraction to reduce a game tree to a size that can be solved with Koller’s
technique is discussed by Billings et al. (2003). Bowling et al. (2008) show how to use
importance sampling to get a better estimate of the value of a strategy. Waugh et al. (2009)
show that the abstraction approach is vulnerable to making systematic errors in approximating
the equilibrium solution, meaning that the whole approach is on shaky ground: it works for
some games but not others. Korb et al. (1999) experiment with an opponent model in the
form of a Bayesian network. It plays ﬁve-card stud about as well as experienced humans.
(Zinkevich et al., 2008) show how an approach that minimizes regret can ﬁnd approximate
equilibria for abstractions with 10
12 states, 100 times more than previous methods.
Game theory and MDPs are combined in the theory of Markov games, also called
stochastic games (Littman, 1994; Hu and Wellman, 1998). Shapley (1953) actually described
the value iteration algorithm independently of Bellman, but his results were not widely ap-
preciated, perhaps because they were presented in the context of Markov games. Evolu-
688 Chapter 17. Making Complex Decisions
tionary game theory (Smith, 1982; Weibull, 1995) looks at strategy drift over time: if your
opponent’s strategy is changing, how should you react? Textbooks on game theory from
an economics point of view include those by Myerson (1991), Fudenberg and Tirole (1991),
Osborne (2004), and Osborne and Rubinstein (1994); Mailath and Samuelson (2006) concen-
trate on repeated games. From an AI perspective we have Nisan et al. (2007), Leyton-Brown
and Shoham (2008), and Shoham and Leyton-Brown (2009).
The 2007 Nobel Memorial Prize in Economics went to Hurwicz, Maskin, and Myerson
“for having laid the foundations of mechanism design theory” (Hurwicz, 1973). The tragedy
of the commons, a motivating problem for the ﬁeld, was presented by Hardin (1968). The rev-
elation principle is due to Myerson (1986), and the revenue equivalence theorem was devel-
oped independently by Myerson (1981) and Riley and Samuelson (1981). Two economists,
Milgrom (1997) and Klemperer (2002), write about the multibillion-dollar spectrum auctions
they were involved in.
Mechanism design is used in multiagent planning (Hunsberger and Grosz, 2000; Stone
et al., 2009) and scheduling (Rassenti et al., 1982). Varian (1995) gives a brief overview with
connections to the computer science literature, and Rosenschein and Zlotkin (1994) present a
book-length treatment with applications to distributed AI. Related work on distributed AI also
goes under other names, including collective intelligence (Tumer and Wolpert, 2000; Segaran,
2007) and market-based control (Clearwater, 1996). Since 2001 there has been an annual
Trading Agents Competition (TAC), in which agents try to make the best proﬁt on a series
of auctions (Wellman et al., 2001; Arunachalam and Sadeh, 2005). Papers on computational
issues in auctions often appear in the ACM Conferences on Electronic Commerce.
EXERCISES
17.1 For the 4× 3 world shown in Figure 17.1, calculate which squares can be reached
from (1,1) by the action sequence[Up,Up,Right,Right,Right] and with what probabilities.
Explain how this computation is related to the prediction task (see Section 15.2.1) for a hidden
Markov model.
17.2 Select a speciﬁc member of the set of policies that are optimal for R(s) > 0 as shown
in Figure 17.2(b), and calculate the fraction of time the agent spends in each state, in the limit,
if the policy is executed forever. ( Hint: Construct the state-to-state transition probability
matrix corresponding to the policy and see Exercise 15.2.)
17.3 Suppose that we deﬁne the utility of a state sequence to be the maximum reward ob-
tained in any state in the sequence. Show that this utility function does not result in stationary
preferences between state sequences. Is it still possible to deﬁne a utility function on states
such that MEU decision making gives optimal behavior?
17.4 Sometimes MDPs are formulated with a reward function R(s,a) that depends on the
action taken or with a reward function R(s, a, s
′) that also depends on the outcome state.
a. Write the Bellman equations for these formulations.
Exercises 689
b. Show how an MDP with reward function R(s, a, s′) can be transformed into a different
MDP with reward function R(s,a), such that optimal policies in the new MDP corre-
spond exactly to optimal policies in the original MDP.
c. Now do the same to convert MDPs with R(s,a) into MDPs with R(s).
17.5 For the environment shown in Figure 17.1, ﬁnd all the threshold values for R(s) such
that the optimal policy changes when the threshold is crossed. You will need a way to calcu-
late the optimal policy and its value for ﬁxed R(s).( Hint: Prove that the value of any ﬁxed
policy varies linearly with R(s).)
17.6 Equation (17.7) on page 654 states that the Bellman operator is a contraction.
a. Show that, for any functions f and g,
|max
a
f(a)−max
a
g(a)|≤max
a
|f(a)−g(a)| .
b. Write out an expression for |(BU i −BU ′
i)(s)| and then apply the result from (a) to
complete the proof that the Bellman operator is a contraction.
17.7 This exercise considers two-player MDPs that correspond to zero-sum, turn-taking
games like those in Chapter 5. Let the players be A and B,a n dl e tR(s) be the reward for
player A in state s.( T h er e w a r df o rB is always equal and opposite.)
a.L e t UA(s) be the utility of state s when it is A’s turn to move ins,a n dl e tUB(s) be the
utility of state s when it is B’s turn to move ins. All rewards and utilities are calculated
from A’s point of view (just as in a minimax game tree). Write down Bellman equations
deﬁning UA(s) and UB(s).
b. Explain how to do two-player value iteration with these equations, and deﬁne a suitable
termination criterion.
c. Consider the game described in Figure 5.17 on page 197. Draw the state space (rather
than the game tree), showing the moves by A as solid lines and moves by B as dashed
lines. Mark each state with R(s). You will ﬁnd it helpful to arrange the states (sA,sB)
on a two-dimensional grid, using sA and sB as “coordinates.”
d. Now apply two-player value iteration to solve this game, and derive the optimal policy.
17.8 Consider the 3× 3 world shown in Figure 17.14(a). The transition model is the same
as in the 4× 3 Figure 17.1: 80% of the time the agent goes in the direction it selects; the rest
of the time it moves at right angles to the intended direction.
Implement value iteration for this world for each value of r below. Use discounted
rewards with a discount factor of 0.99. Show the policy obtained in each case. Explain
intuitively why the value of r leads to each policy.
a. r = 100
b. r =−3
c. r =0
d. r =+ 3
690 Chapter 17. Making Complex Decisions
-50 +1 +1 +1 +1 +1 +1 +1
+50 -1 -1 -1 -1 -1 -1 -1
···
···
···
Start
r -1
-1
-1
+10
-1
-1
-1
-1
(a) (b)
Figure 17.14 (a) 3×3 world for Exercise 17.8. The reward for each state is indicated.
The upper right square is a terminal state. (b) 101×3 world for Exercise 17.9 (omitting 93
identical columns in the middle). The start state has reward 0.
17.9 Consider the 101× 3 world shown in Figure 17.14(b). In the start state the agent has
a choice of two deterministic actions, Up or Down, but in the other states the agent has one
deterministic action, Right. Assuming a discounted reward function, for what values of the
discount γ should the agent choose Up and for which Down? Compute the utility of each
action as a function of γ. (Note that this simple example actually reﬂects many real-world
situations in which one must weigh the value of an immediate action versus the potential
continual long-term consequences, such as choosing to dump pollutants into a lake.)
17.10 Consider an undiscounted MDP having three states, (1, 2, 3), with rewards −1,−2,
0, respectively. State 3 is a terminal state. In states 1 and 2 there are two possible actions: a
and b. The transition model is as follows:
•In state 1, action a moves the agent to state 2 with probability 0.8 and makes the agent
stay put with probability 0.2.
•In state 2, action a moves the agent to state 1 with probability 0.8 and makes the agent
stay put with probability 0.2.
•In either state 1 or state 2, action b moves the agent to state 3 with probability 0.1 and
makes the agent stay put with probability 0.9.
Answer the following questions:
a. What can be determined qualitatively about the optimal policy in states 1 and 2?
b. Apply policy iteration, showing each step in full, to determine the optimal policy and
the values of states 1 and 2. Assume that the initial policy has action b in both states.
c. What happens to policy iteration if the initial policy has action a in both states? Does
discounting help? Does the optimal policy depend on the discount factor?
17.11 Consider the 4× 3 world shown in Figure 17.1.
a. Implement an environment simulator for this environment, such that the speciﬁc geog-
raphy of the environment is easily altered. Some code for doing this is already in the
online code repository.
Exercises 691
b. Create an agent that uses policy iteration, and measure its performance in the environ-
ment simulator from various starting states. Perform several experiments from each
starting state, and compare the average total reward received per run with the utility of
the state, as determined by your algorithm.
c. Experiment with increasing the size of the environment. How does the run time for
policy iteration vary with the size of the environment?
17.12 How can the value determination algorithm be used to calculate the expected loss
experienced by an agent using a given set of utility estimates U and an estimated model P,
compared with an agent using correct values?
17.13 Let the initial belief state b
0 for the 4× 3 POMDP on page 658 be the uniform dis-
tribution over the nonterminal states, i.e., ⟨1
9, 1
9, 1
9, 1
9, 1
9, 1
9, 1
9, 1
9, 1
9,0,0⟩. Calculate the exact
belief state b1 after the agent moves Left and its sensor reports 1 adjacent wall. Also calculate
b2 assuming that the same thing happens again.
17.14 What is the time complexity of d steps of POMDP value iteration for a sensorless
environment?
17.15 Consider a version of the two-state POMDP on page 661 in which the sensor is
90% reliable in state 0 but provides no information in state 1 (that is, it reports 0 or 1 with
equal probability). Analyze, either qualitatively or quantitatively, the utility function and the
optimal policy for this problem.
17.16 Show that a dominant strategy equilibrium is a Nash equilibrium, but not vice versa.
17.17 In the children’s game of rock–paper–scissors each player reveals at the same time
a choice of rock, paper, or scissors. Paper wraps rock, rock blunts scissors, and scissors cut
paper. In the extended version rock–paper–scissors–ﬁre–water, ﬁre beats rock, paper, and
scissors; rock, paper, and scissors beat water; and water beats ﬁre. Write out the payoff
matrix and ﬁnd a mixed-strategy solution to this game.
17.18 The following payoff matrix, from Blinder (1983) by way of Bernstein (1996), shows
a game between politicians and the Federal Reserve.
Fed: contract
 Fed: do nothing
 Fed: expand
Pol: contract
 F =7 ,P =1
 F =9 ,P =4
 F =6 ,P =6
Pol: do nothing
 F =8 ,P =2
 F =5 ,P =5
 F =4 ,P =9
Pol: expand
 F =3 ,P =3
 F =2 ,P =7
 F =1 ,P =8
Politicians can expand or contract ﬁscal policy, while the Fed can expand or contract mon-
etary policy. (And of course either side can choose to do nothing.) Each side also has pref-
erences for who should do what—neither side wants to look like the bad guys. The payoffs
shown are simply the rank orderings: 9 for ﬁrst choice through 1 for last choice. Find the
Nash equilibrium of the game in pure strategies. Is this a Pareto-optimal solution? You might
wish to analyze the policies of recent administrations in this light.
692 Chapter 17. Making Complex Decisions
17.19 A Dutch auction is similar in an English auction, but rather than starting the bidding
at a low price and increasing, in a Dutch auction the seller starts at a high price and gradually
lowers the price until some buyer is willing to accept that price. (If multiple bidders accept
the price, one is arbitrarily chosen as the winner.) More formally, the seller begins with a
price p and gradually lowers p by increments of d until at least one buyer accepts the price.
Assuming all bidders act rationally, is it true that for arbitrarily small d, a Dutch auction will
always result in the bidder with the highest value for the item obtaining the item? If so, show
mathematically why. If not, explain how it may be possible for the bidder with highest value
for the item not to obtain it.
17.20 Imagine an auction mechanism that is just like an ascending-bid auction, except that
at the end, the winning bidder, the one who bid b
max, pays only bmax/2 rather than bmax.
Assuming all agents are rational, what is the expected revenue to the auctioneer for this
mechanism, compared with a standard ascending-bid auction?
17.21 Teams in the National Hockey League historically received 2 points for winning a
game and 0 for losing. If the game is tied, an overtime period is played; if nobody wins in
overtime, the game is a tie and each team gets 1 point. But league ofﬁcials felt that teams
were playing too conservatively in overtime (to avoid a loss), and it would be more exciting
if overtime produced a winner. So in 1999 the ofﬁcials experimented in mechanism design:
the rules were changed, giving a team that loses in overtime 1 point, not 0. It is still 2 points
for a win and 1 for a tie.
a. Was hockey a zero-sum game before the rule change? After?
b. Suppose that at a certain time t in a game, the home team has probability p of winning
in regulation time, probability 0.78−p of losing, and probability 0.22 of going into
overtime, where they have probability q of winning, .9−q of losing, and .1 of tying.
Give equations for the expected value for the home and visiting teams.
c. Imagine that it were legal and ethical for the two teams to enter into a pact where they
agree that they will skate to a tie in regulation time, and then both try in earnest to win
in overtime. Under what conditions, in terms of p and q, would it be rational for both
teams to agree to this pact?
d. Longley and Sankaran (2005) report that since the rule change, the percentage of games
with a winner in overtime went up 18.2%, as desired, but the percentage of overtime
games also went up 3.6%. What does that suggest about possible collusion or conser-
vative play after the rule change?


END_INSTRUCTION
