
START_INSTRUCTION
You are an expert AI tutor creating a comprehensive study guide.
Your task is to rewrite and expand the "Lesson Notes" provided below into a cohesive, readable study text.

1. **Structure:** Follow the exact structure and order of the topics in the "Lesson Notes". Do not reorder them.
2. **Enrichment:** Use the "Textbook Context" provided to expand on every bullet point in the notes.
    - If the notes mention a concept (e.g., "Rationality"), define it precisely using the textbook.
    - If the notes list algorithms (e.g., "A*", "Simulated Annealing"), explain how they work, their complexity, and pros/cons using the textbook details.
    - Add examples from the textbook where appropriate.
3. **Tone:** Academic but accessible study material. Not just bullet points—write paragraphs where necessary to explain complex ideas.
4. **Language:** The output must be in **Czech** (since the lesson notes are in Czech), but you can keep standard English AI terminology (like "Overfitting") in parentheses.

--- LESSON NOTES (Skeleton) ---
Řešení úloh ve stavovém prostoru

Máme:
• Definici problému
• Definici cíle
• Akce, které můžeme provádět
Př. Hanojské věže
Cíl:
○ Přesunout disky z 1. tyčky na 3. tyčku
Pravidla:
○ Lze přemístit jen 1 disk
○ Větší nelze na menší
Akce:
○ Přesun z tyčky X na tyčku Y
Obecná abstraktní reprezentace
• Generalizace - zobecnění skupiny problémů→ nalezení společných rysů
• Abstrakce - zjednodušení zanedbáním poddružných detailů
Stavový prostor
○ abstraktní model úlohy a postupu jejího řešení
○ stavy - odpovídají situacím, které mohou dostat
○ Akce - přechody mezi stavy
Definován:
▪ explicitně (mapová navigace)
▪ implicitně (šachy)
○
•
Máme tedy:
○ Množinu stavů S={s}
○ Množinu přechodů φ
○ Počáteční stav S0
○ Množina koncových stavů G={g}
•
Řešením úlohy je sekvence přechodů (akcí) začínajících v počátečním stavu, vedoucích ke 
koncovému stavu
•
• Optimální sekvence má nejnižší cenu cesty
Algoritmy prohledávání
• Algoritmy se liší ve způsobu, kterým volí uzel, který má být rozvíjen v daném kroku.
Založené na opakované expanzi zvoleného uzlu
○ Rozvinutí = aplikace všech použitelných operátorů
•
Algo používají dva seznamy
○ Seznam rozvinutých uzlů (closed)
○ Seznam nerozvinutých uzlů (fringe)
•
Pro každý uzel udržuji tyto info:
○ Stav odpovídající uzlu
○ Rodiče - uzel který byl rozvinut
○ Akci - operátor, který byl aplikován na rodiče
○ Cenu kroku
○ Cena cesty
•
Slepé algoritmy
• Pracují jen se stavovým prostorem, následnící rozvinutého uzlu jsou považováni za rovnocenné
• Algoritmy:
BFS - FIFO
▪ máme jistotu, že vždy nalezneme koncový stav
musíme projít hodně uzlů
□ procházíme všechny, které mají menší hloubku než koncový
▪
○
DFS - LIFO
▪ Nemusíme najít koncový uzel - Problém "nekonečné" větve
▪ V reálných využitích často ale výhodnější
○
Omezené do hloubky
▪ Hledáme řešení jen do nějaké hloubky (procházet můžeme i hlouběji)
○
Iterativní prohlubování - (DFS)
▪ Postupně zvětšujeme hloubku
▪ opakované omezené prohledávání do hloubky s rostoucí hloubkou
○
Obousměrné prohledávání - (BFS)
▪ Stavový prostor musí být explicitně zadán
Hledám od počátečního i koncového stavu
□ Od počátečního - klasický algo
Od konce
 rozvinutí koncového stavu - nalezení všech předchůdců
□
▪
▪ Pokud nás obě techniky přivedou do stejného stavu - nalezl jsem cestu
Dvě úlohy s poloviční složitostí - menší složitost než jedna úloha s normální 
složitostí
▪
○
Se stejnoměrnou cenou - (BFS)
▪ Řeší číselné ohodnocení přechodů
Fringe uspořádáno podle path-cost
□ v každém kroku přerovnám
▪
▪ V případě stejných hodnot cen kroku se jedná o prosté BFS
▪ Algo končí, až se koncový uzel dostane na první místo ve fringe
○
•
Informované (Heuristické) algoritmy
používají heuristickou fci. - číslo vyjadřující kvalitu uzlu z hlediska řešení úlohy
○ lze chápat jako odhad ceny, kterou budu potřebovat z daného uzlu do cíle
Pro jednu úlohu můžeme mít víc fcí.
○ Říkáme že lepší fce je informovanější
○
○ Př. autoavigace - nejkratší a nejrychlejší cesta
•
Algoritmy:
Paprskové (beam search) - (BFS)
○ „paralelní“ heuristická varianta prohledávání do šířky
▪ rozvinou se všechny uzly dané hloubky
▪ do fringe se zařadí jen k nejlepších
▪ Problém BFS - velikost bobtnající fronty
○
•
▪ Problém BFS - velikost bobtnající fronty
▪ Tato metoda má zafixovanou maximální velikost fronty
Uspořádané (greedy best-first search) - (BFS)
▪ Heuristická varianta prohledávání do šířky
▪ Rozvineme uzel, zařadíme nalezené uzly
▪ Seřadíme celý fringe podle hodnoty heuristické fce
○
A*
každý uzel S hodnotíme součtem:
□ g(s) - známé ceny cesty od počátku do S
□ h(s) - heuristickou fcí. odhadující cenu z S do koncového uzlu
▪
▪ Do seznamu closed přidáme i uzel, který tam již je, ale s vyšší hodnotu g(s)
▪ fringe uspořádáme podle f(s)
○
Gradientní (hill-climbing s pamětí) (DFS)
následníky rozvíjeného uzlu seřadíme vzestupně podle hodnoty heurist. fce a 
zařadíme do fringe
▪
○
Náhodné algoritmy
• vybírají uzly náhodně
Algoritmy:
○ genetické algoritmy
•
Simulované žíhání
• Samostatná technika prohledávání
můžeme použít pro řešení problému uváznutí v lokálním extrému u gradientního 
prohledávání.
•
inspirovaná žíháním kovových slitin k získání jejich optimálních vlastností
○ materiály se zahřívají a pak pomalu ochlazují
•
Princip:
Inicializace:
• Zvolí se počáteční řešení problému.
Nastaví se počáteční "teplota" T, která určuje pravděpodobnost přijetí horších 
řešení.
•
○
Iterační proces:
Generování nového řešení:
○ V okolí aktuálního řešení se náhodně vybere nové řešení.
•
Hodnocení řešení:
Spočítá se změna kvality (např. energie) ΔE mezi novým a aktuálním 
řešením.
○
•
Rozhodnutí o přijetí:
○ Pokud je nové řešení lepší (ΔE>0), přijme se.
Pokud je nové řešení horší (ΔE<0), přijme se s pravděpodobností 
P=e^(−ΔE/T).
○
•
• Snížení teploty:
Snížení teploty:
Teplota T se postupně snižuje podle zvoleného chladicího schématu (např. 
geometrickou řadou Tnové=αT, kde 0<α<1).
○
•
Koncové podmínky:
Proces pokračuje, dokud není dosaženo koncové teploty nebo maximálního počtu 
iterací.
•
○
• Proč přijímat horší řešení?
Přijetí horších řešení s určitou pravděpodobností umožňuje algoritmu překonat lokální minima 
a prozkoumat globální prostor řešení. To zvyšuje šanci na nalezení globálního optima.
• Shrnutí:
Simulované žíhání je efektivní metoda pro hledání přibližných řešení složitých problémů, kde 
tradiční deterministické metody selhávají nebo jsou výpočetně náročné. 
General Problem Solver
algoritmus pro řešení úloh metodou postupného rozkladu úlohy na podúlohy
○ na základě výpočtu diferencí mezi aktuálním a cílovým stavem
•
tři základní procedury:
○ TRANSFORM
○ REDUCE
○ APPLY
•
Další systémy pro řešení úloh
STRIPS
○ následník systému GPS
○ stavy úlohy popsány formulemi predikátové logiky
•
Planner
○ rozlišuje tvrzení a teorémy
•
• SOAR

--- TEXTBOOK CONTEXT (Source Material) ---


--- BOOK CHAPTER: 3_Solving_Problems_by_Searching ---

3
SOLVING PROBLEMS BY
SEARCHING
In which we see how an agent can ﬁnd a sequence of actions that achieves its
goals when no single action will do.
The simplest agents discussed in Chapter 2 were the reﬂex agents, which base their actions on
a direct mapping from states to actions. Such agents cannot operate well in environments for
which this mapping would be too large to store and would take too long to learn. Goal-based
agents, on the other hand, consider future actions and the desirability of their outcomes.
This chapter describes one kind of goal-based agent called a problem-solving agent.
PROBLEM-SOLVING
AGENT
Problem-solving agents use atomic representations, as described in Section 2.4.7—that is,
states of the world are considered as wholes, with no internal structure visible to the problem-
solving algorithms. Goal-based agents that use more advanced factored or structured rep-
resentations are usually called planning agents and are discussed in Chapters 7 and 10.
Our discussion of problem solving begins with precise deﬁnitions ofproblems and their
solutions and give several examples to illustrate these deﬁnitions. We then describe several
general-purpose search algorithms that can be used to solve these problems. We will see
several uninformed search algorithms—algorithms that are given no information about the
problem other than its deﬁnition. Although some of these algorithms can solve any solvable
problem, none of them can do so efﬁciently. Informed search algorithms, on the other hand,
can do quite well given some guidance on where to look for solutions.
In this chapter, we limit ourselves to the simplest kind of task environment, for which
the solution to a problem is always aﬁxed sequence of actions. The more general case—where
the agent’s future actions may vary depending on future percepts—is handled in Chapter 4.
This chapter uses the concepts of asymptotic complexity (that is, O() notation) and
NP-completeness. Readers unfamiliar with these concepts should consult Appendix A.
3.1 P ROBLEM -SOLVING AGENTS
Intelligent agents are supposed to maximize their performance measure. As we mentioned
in Chapter 2, achieving this is sometimes simpliﬁed if the agent can adopt a goal and aim at
satisfying it. Let us ﬁrst look at why and how an agent might do this.
64
Section 3.1. Problem-Solving Agents 65
Imagine an agent in the city of Arad, Romania, enjoying a touring holiday. The agent’s
performance measure contains many factors: it wants to improve its suntan, improve its Ro-
manian, take in the sights, enjoy the nightlife (such as it is), avoid hangovers, and so on. The
decision problem is a complex one involving many tradeoffs and careful reading of guide-
books. Now, suppose the agent has a nonrefundable ticket to ﬂy out of Bucharest the follow-
ing day. In that case, it makes sense for the agent to adopt the goal of getting to Bucharest.
Courses of action that don’t reach Bucharest on time can be rejected without further consid-
eration and the agent’s decision problem is greatly simpliﬁed. Goals help organize behavior
by limiting the objectives that the agent is trying to achieve and hence the actions it needs
to consider. Goal formulation, based on the current situation and the agent’s performance
GOAL FORMULA TION
measure, is the ﬁrst step in problem solving.
We will consider a goal to be a set of world states—exactly those states in which the
goal is satisﬁed. The agent’s task is to ﬁnd out how to act, now and in the future, so that it
reaches a goal state. Before it can do this, it needs to decide (or we need to decide on its
behalf) what sorts of actions and states it should consider. If it were to consider actions at
the level of “move the left foot forward an inch” or “turn the steering wheel one degree left,”
the agent would probably never ﬁnd its way out of the parking lot, let alone to Bucharest,
because at that level of detail there is too much uncertainty in the world and there would be
too many steps in a solution. Problem formulation is the process of deciding what actions
PROBLEM
FORMULA TION
and states to consider, given a goal. We discuss this process in more detail later. For now, let
us assume that the agent will consider actions at the level of driving from one major town to
another. Each state therefore corresponds to being in a particular town.
Our agent has now adopted the goal of driving to Bucharest and is considering where
to go from Arad. Three roads lead out of Arad, one toward Sibiu, one to Timisoara, and one
to Zerind. None of these achieves the goal, so unless the agent is familiar with the geography
of Romania, it will not know which road to follow.
1 In other words, the agent will not know
which of its possible actions is best, because it does not yet know enough about the state
that results from taking each action. If the agent has no additional information—i.e., if the
environment is unknown in the sense deﬁned in Section 2.3—then it is has no choice but to
try one of the actions at random. This sad situation is discussed in Chapter 4.
But suppose the agent has a map of Romania. The point of a map is to provide the
agent with information about the states it might get itself into and the actions it can take. The
agent can use this information to consider subsequent stages of a hypothetical journey via
each of the three towns, trying to ﬁnd a journey that eventually gets to Bucharest. Once it has
found a path on the map from Arad to Bucharest, it can achieve its goal by carrying out the
driving actions that correspond to the legs of the journey. In general, an agent with several
immediate options of unknown value can decide what to do by ﬁrst examining future actions
that eventually lead to states of known value.
To be more speciﬁc about what we mean by “examining future actions,” we have to
be more speciﬁc about properties of the environment, as deﬁned in Section 2.3. For now,
1 We are assuming that most readers are in the same position and can easily imagine themselves to be as clueless
as our agent. We apologize to Romanian readers who are unable to take advantage of this pedagogical device.
66 Chapter 3. Solving Problems by Searching
we assume that the environment is observable, so the agent always knows the current state.
For the agent driving in Romania, it’s reasonable to suppose that each city on the map has a
sign indicating its presence to arriving drivers. We also assume the environment is discrete,
so at any given state there are only ﬁnitely many actions to choose from. This is true for
navigating in Romania because each city is connected to a small number of other cities. We
will assume the environment is known, so the agent knows which states are reached by each
action. (Having an accurate map sufﬁces to meet this condition for navigation problems.)
Finally, we assume that the environment is deterministic, so each action has exactly one
outcome. Under ideal conditions, this is true for the agent in Romania—it means that if it
chooses to drive from Arad to Sibiu, it does end up in Sibiu. Of course, conditions are not
always ideal, as we show in Chapter 4.
Under these assumptions, the solution to any problem is a ﬁxed sequence of actions.
“Of course!” one might say, “What else could it be?” Well, in general it could be a branching
strategy that recommends different actions in the future depending on what percepts arrive.
For example, under less than ideal conditions, the agent might plan to drive from Arad to
Sibiu and then to Rimnicu Vilcea but may also need to have a contingency plan in case it
arrives by accident in Zerind instead of Sibiu. Fortunately, if the agent knows the initial state
and the environment is known and deterministic, it knows exactly where it will be after the
ﬁrst action and what it will perceive. Since only one percept is possible after the ﬁrst action,
the solution can specify only one possible second action, and so on.
The process of looking for a sequence of actions that reaches the goal is called search.
SEARCH
A search algorithm takes a problem as input and returns a solution in the form of an actionSOLUTION
sequence. Once a solution is found, the actions it recommends can be carried out. This
is called the execution phase. Thus, we have a simple “formulate, search, execute” designEXECUTION
for the agent, as shown in Figure 3.1. After formulating a goal and a problem to solve,
the agent calls a search procedure to solve it. It then uses the solution to guide its actions,
doing whatever the solution recommends as the next thing to do—typically, the ﬁrst action of
the sequence—and then removing that step from the sequence. Once the solution has been
executed, the agent will formulate a new goal.
Notice that while the agent is executing the solution sequence it ignores its percepts
when choosing an action because it knows in advance what they will be. An agent that
carries out its plans with its eyes closed, so to speak, must be quite certain of what is going
on. Control theorists call this an open-loop system, because ignoring the percepts breaks the
OPEN-LOOP
loop between agent and environment.
We ﬁrst describe the process of problem formulation, and then devote the bulk of the
chapter to various algorithms for the S EARCH function. We do not discuss the workings of
the UPDATE-STATE and FORMULATE -GOAL functions further in this chapter.
3.1.1 Well-deﬁned problems and solutions
A problem can be deﬁned formally by ﬁve components:PROBLEM
•The initial state that the agent starts in. For example, the initial state for our agent inINITIAL ST A TE
Romania might be described as In(Arad).
Section 3.1. Problem-Solving Agents 67
function SIMPLE -PROBLEM -SOLVING -AGENT (percept) returns an action
persistent: seq, an action sequence, initially empty
state, some description of the current world state
goal, a goal, initially null
problem, a problem formulation
state←UPDATE -STATE(state,percept)
if seq is empty then
goal←FORMULATE -GOAL(state)
problem←FORMULATE -PROBLEM (state,goal)
seq←SEARCH (problem)
if seq = failure then return a null action
action←FIRST (seq)
seq←REST (seq)
return action
Figure 3.1 A simple problem-solving agent. It ﬁrst formulates a goal and a problem,
searches for a sequence of actions that would solve the problem, and then executes the actions
one at a time. When this is complete, it formulates another goal and starts over.
•A description of the possible actions available to the agent. Given a particular state s,ACTIONS
ACTIONS (s) returns the set of actions that can be executed in s. We say that each of
these actions is applicable in s. For example, from the state In(Arad), the applicableAPPLICABLE
actions are{Go(Sibiu),Go(Timisoara),Go(Zerind)}.
•A description of what each action does; the formal name for this is the transition
model, speciﬁed by a function R ESULT (s,a) that returns the state that results fromTRANSITION MODEL
doing action a in state s. We also use the term successor to refer to any state reachableSUCCESSOR
from a given state by a single action.2 For example, we have
RESULT (In(Arad),Go(Zerind)) = In(Zerind) .
Together, the initial state, actions, and transition model implicitly deﬁne thestate spaceS TAT E S PAC E
of the problem—the set of all states reachable from the initial state by any sequence
of actions. The state space forms a directed network or graph in which the nodesGRAPH
are states and the links between nodes are actions. (The map of Romania shown in
Figure 3.2 can be interpreted as a state-space graph if we view each road as standing
for two driving actions, one in each direction.) A path in the state space is a sequence
PAT H
of states connected by a sequence of actions.
•The goal test, which determines whether a given state is a goal state. Sometimes thereGOAL TEST
is an explicit set of possible goal states, and the test simply checks whether the given
state is one of them. The agent’s goal in Romania is the singleton set{In(Bucharest)}.
2 Many treatments of problem solving, including previous editions of this book, use asuccessor function,w h i c h
returns the set of all successors, instead of separate A CTIONS and R ESULT functions. The successor function
makes it difﬁcult to describe an agent that knows what actions it can try but not what they achieve. Also, note
some author use R
ESULT (a,s) instead of RESULT (s,a), and some use DO instead of RESULT .
68 Chapter 3. Solving Problems by Searching
Giurgiu
Urziceni
Hirsova
Eforie
Neamt
Oradea
Zerind
Arad
Timisoara
Lugoj
Mehadia
Drobeta
Craiova
Sibiu Fagaras
Pitesti
Vaslui
Iasi
Rimnicu Vilcea
Bucharest
71
75
118
111
70
75
120
151
140
99
80
97
101
211
138
146 85
90
98
142
92
87
86
Figure 3.2 A simpliﬁed road map of part of Romania.
Sometimes the goal is speciﬁed by an abstract property rather than an explicitly enumer-
ated set of states. For example, in chess, the goal is to reach a state called “checkmate,”
where the opponent’s king is under attack and can’t escape.
•A path cost function that assigns a numeric cost to each path. The problem-solving
PAT H C O S T
agent chooses a cost function that reﬂects its own performance measure. For the agent
trying to get to Bucharest, time is of the essence, so the cost of a path might be its length
in kilometers. In this chapter, we assume that the cost of a path can be described as the
sum of the costs of the individual actions along the path.
3 The step cost of taking actionSTEP COST
a in state s to reach state s′ is denoted by c(s, a, s′). The step costs for Romania are
shown in Figure 3.2 as route distances. We assume that step costs are nonnegative.4
The preceding elements deﬁne a problem and can be gathered into a single data structure
that is given as input to a problem-solving algorithm. A solution to a problem is an action
sequence that leads from the initial state to a goal state. Solution quality is measured by the
path cost function, and an optimal solution has the lowest path cost among all solutions.
OPTIMAL SOLUTION
3.1.2 Formulating problems
In the preceding section we proposed a formulation of the problem of getting to Bucharest in
terms of the initial state, actions, transition model, goal test, and path cost. This formulation
seems reasonable, but it is still a model—an abstract mathematical description—and not the
3 This assumption is algorithmically convenient but also theoretically justiﬁable—see page 649 in Chapter 17.
4 The implications of negative costs are explored in Exercise 3.8.
Section 3.2. Example Problems 69
real thing. Compare the simple state description we have chosen,In(Arad), to an actual cross-
country trip, where the state of the world includes so many things: the traveling companions,
the current radio program, the scenery out of the window, the proximity of law enforcement
ofﬁcers, the distance to the next rest stop, the condition of the road, the weather, and so on.
All these considerations are left out of our state descriptions because they are irrelevant to the
problem of ﬁnding a route to Bucharest. The process of removing detail from a representation
is called abstraction.
ABSTRACTION
In addition to abstracting the state description, we must abstract the actions themselves.
A driving action has many effects. Besides changing the location of the vehicle and its oc-
cupants, it takes up time, consumes fuel, generates pollution, and changes the agent (as they
say, travel is broadening). Our formulation takes into account only the change in location.
Also, there are many actions that we omit altogether: turning on the radio, looking out of
the window, slowing down for law enforcement ofﬁcers, and so on. And of course, we don’t
specify actions at the level of “turn steering wheel to the left by one degree.”
Can we be more precise about deﬁning the appropriate level of abstraction? Think of the
abstract states and actions we have chosen as corresponding to large sets of detailed world
states and detailed action sequences. Now consider a solution to the abstract problem: for
example, the path from Arad to Sibiu to Rimnicu Vilcea to Pitesti to Bucharest. This abstract
solution corresponds to a large number of more detailed paths. For example, we could drive
with the radio on between Sibiu and Rimnicu Vilcea, and then switch it off for the rest of
the trip. The abstraction is valid if we can expand any abstract solution into a solution in the
more detailed world; a sufﬁcient condition is that for every detailed state that is “in Arad,”
there is a detailed path to some state that is “in Sibiu,” and so on.
5 The abstraction is useful
if carrying out each of the actions in the solution is easier than the original problem; in this
case they are easy enough that they can be carried out without further search or planning by
an average driving agent. The choice of a good abstraction thus involves removing as much
detail as possible while retaining validity and ensuring that the abstract actions are easy to
carry out. Were it not for the ability to construct useful abstractions, intelligent agents would
be completely swamped by the real world.
3.2 E XAMPLE PROBLEMS
The problem-solving approach has been applied to a vast array of task environments. We
list some of the best known here, distinguishing between toy and real-world problems. A
toy problem is intended to illustrate or exercise various problem-solving methods. It can beTOY PROBLEM
given a concise, exact description and hence is usable by different researchers to compare the
performance of algorithms. A real-world problem is one whose solutions people actuallyREAL-WORLD
PROBLEM
care about. Such problems tend not to have a single agreed-upon description, but we can give
the general ﬂavor of their formulations.
5 See Section 11.2 for a more complete set of deﬁnitions and algorithms.
70 Chapter 3. Solving Problems by Searching
R
L
S S
SS
R
L
R
L
R
L
S
SS
S
L
L
LL R
R
R
R
Figure 3.3 The state space for the vacuum world. Links denote actions: L = Left,R=
Right,S= Suck.
3.2.1 T oy problems
The ﬁrst example we examine is the vacuum world ﬁrst introduced in Chapter 2. (See
Figure 2.2.) This can be formulated as a problem as follows:
•States: The state is determined by both the agent location and the dirt locations. The
agent is in one of two locations, each of which might or might not contain dirt. Thus,
there are 2× 2
2 =8 possible world states. A larger environment with n locations has
n· 2n states.
•Initial state: Any state can be designated as the initial state.
•Actions: In this simple environment, each state has just three actions: Left, Right,a n d
Suck. Larger environments might also include Up and Down.
•Transition model: The actions have their expected effects, except that moving Left in
the leftmost square, movingRight in the rightmost square, andSucking in a clean square
have no effect. The complete state space is shown in Figure 3.3.
•Goal test: This checks whether all the squares are clean.
•Path cost: Each step costs 1, so the path cost is the number of steps in the path.
Compared with the real world, this toy problem has discrete locations, discrete dirt, reliable
cleaning, and it never gets any dirtier. Chapter 4 relaxes some of these assumptions.
The 8-puzzle, an instance of which is shown in Figure 3.4, consists of a 3×3 board with8-PUZZLE
eight numbered tiles and a blank space. A tile adjacent to the blank space can slide into the
space. The object is to reach a speciﬁed goal state, such as the one shown on the right of the
ﬁgure. The standard formulation is as follows:
Section 3.2. Example Problems 71
2
Start State Goal State
1
3 4
6 7
5
1
2
3
4
6
7
8
5
8
Figure 3.4 A typical instance of the 8-puzzle.
•States: A state description speciﬁes the location of each of the eight tiles and the blank
in one of the nine squares.
•Initial state: Any state can be designated as the initial state. Note that any given goal
can be reached from exactly half of the possible initial states (Exercise 3.4).
•Actions: The simplest formulation deﬁnes the actions as movements of the blank space
Left, Right, Up,o r Down. Different subsets of these are possible depending on where
the blank is.
•Transition model: Given a state and action, this returns the resulting state; for example,
if we apply Left to the start state in Figure 3.4, the resulting state has the 5 and the blank
switched.
•Goal test: This checks whether the state matches the goal conﬁguration shown in Fig-
ure 3.4. (Other goal conﬁgurations are possible.)
•Path cost: Each step costs 1, so the path cost is the number of steps in the path.
What abstractions have we included here? The actions are abstracted to their beginning and
ﬁnal states, ignoring the intermediate locations where the block is sliding. We have abstracted
away actions such as shaking the board when pieces get stuck and ruled out extracting the
pieces with a knife and putting them back again. We are left with a description of the rules of
the puzzle, avoiding all the details of physical manipulations.
The 8-puzzle belongs to the family of sliding-block puzzles, which are often used as
SLIDING-BLOCK
PUZZLES
test problems for new search algorithms in AI. This family is known to be NP-complete,
so one does not expect to ﬁnd methods signiﬁcantly better in the worst case than the search
algorithms described in this chapter and the next. The 8-puzzle has9!/2 = 181,440 reachable
states and is easily solved. The 15-puzzle (on a4×4 board) has around 1.3 trillion states, and
random instances can be solved optimally in a few milliseconds by the best search algorithms.
The 24-puzzle (on a 5× 5 board) has around 10
25 states, and random instances take several
hours to solve optimally.
The goal of the 8-queens problem is to place eight queens on a chessboard such that8-QUEENS PROBLEM
no queen attacks any other. (A queen attacks any piece in the same row, column or diago-
nal.) Figure 3.5 shows an attempted solution that fails: the queen in the rightmost column is
attacked by the queen at the top left.
72 Chapter 3. Solving Problems by Searching
Figure 3.5 Almost a solution to the 8-queens problem. (Solution is left as an exercise.)
Although efﬁcient special-purpose algorithms exist for this problem and for the whole
n-queens family, it remains a useful test problem for search algorithms. There are two main
kinds of formulation. An incremental formulation involves operators that augment the stateINCREMENT AL
FORMULA TION
description, starting with an empty state; for the 8-queens problem, this means that each
action adds a queen to the state. A complete-state formulation starts with all 8 queens onCOMPLETE-ST A TE
FORMULA TION
the board and moves them around. In either case, the path cost is of no interest because only
the ﬁnal state counts. The ﬁrst incremental formulation one might try is the following:
•States: Any arrangement of 0 to 8 queens on the board is a state.
•Initial state: No queens on the board.
•Actions: Add a queen to any empty square.
•Transition model: Returns the board with a queen added to the speciﬁed square.
•Goal test: 8 queens are on the board, none attacked.
In this formulation, we have 64· 63··· 57≈1.8× 1014 possible sequences to investigate. A
better formulation would prohibit placing a queen in any square that is already attacked:
•States: All possible arrangements of n queens (0 ≤n ≤8), one per column in the
leftmost n columns, with no queen attacking another.
•Actions: Add a queen to any square in the leftmost empty column such that it is not
attacked by any other queen.
This formulation reduces the 8-queens state space from1.8× 1014 to just 2,057, and solutions
are easy to ﬁnd. On the other hand, for 100 queens the reduction is from roughly10400 states
to about 1052 states (Exercise 3.5)—a big improvement, but not enough to make the problem
tractable. Section 4.1 describes the complete-state formulation, and Chapter 6 gives a simple
algorithm that solves even the million-queens problem with ease.
Section 3.2. Example Problems 73
Our ﬁnal toy problem was devised by Donald Knuth (1964) and illustrates how inﬁnite
state spaces can arise. Knuth conjectured that, starting with the number 4, a sequence of fac-
torial, square root, and ﬂoor operations will reach any desired positive integer. For example,
we can reach 5 from 4 as follows:
⌊
√
√
√
√
√
(4!)!
⌋
=5 .
The problem deﬁnition is very simple:
•States: Positive numbers.
•Initial state:4 .
•Actions: Apply factorial, square root, or ﬂoor operation (factorial for integers only).
•Transition model: As given by the mathematical deﬁnitions of the operations.
•Goal test: State is the desired positive integer.
To our knowledge there is no bound on how large a number might be constructed in the pro-
cess of reaching a given target—for example, the number 620,448,401,733,239,439,360,000
is generated in the expression for 5—so the state space for this problem is inﬁnite. Such
state spaces arise frequently in tasks involving the generation of mathematical expressions,
circuits, proofs, programs, and other recursively deﬁned objects.
3.2.2 Real-world problems
We have already seen how the route-ﬁnding problem is deﬁned in terms of speciﬁed loca-ROUTE-FINDING
PROBLEM
tions and transitions along links between them. Route-ﬁnding algorithms are used in a variety
of applications. Some, such as Web sites and in-car systems that provide driving directions,
are relatively straightforward extensions of the Romania example. Others, such as routing
video streams in computer networks, military operations planning, and airline travel-planning
systems, involve much more complex speciﬁcations. Consider the airline travel problems that
must be solved by a travel-planning Web site:
•States: Each state obviously includes a location (e.g., an airport) and the current time.
Furthermore, because the cost of an action (a ﬂight segment) may depend on previous
segments, their fare bases, and their status as domestic or international, the state must
record extra information about these “historical” aspects.
•Initial state: This is speciﬁed by the user’s query.
•Actions: Take any ﬂight from the current location, in any seat class, leaving after the
current time, leaving enough time for within-airport transfer if needed.
•Transition model: The state resulting from taking a ﬂight will have the ﬂight’s desti-
nation as the current location and the ﬂight’s arrival time as the current time.
•Goal test: Are we at the ﬁnal destination speciﬁed by the user?
•Path cost: This depends on monetary cost, waiting time, ﬂight time, customs and im-
migration procedures, seat quality, time of day, type of airplane, frequent-ﬂyer mileage
awards, and so on.
74 Chapter 3. Solving Problems by Searching
Commercial travel advice systems use a problem formulation of this kind, with many addi-
tional complications to handle the byzantine fare structures that airlines impose. Any sea-
soned traveler knows, however, that not all air travel goes according to plan. A really good
system should include contingency plans—such as backup reservations on alternate ﬂights—
to the extent that these are justiﬁed by the cost and likelihood of failure of the original plan.
Touring problems are closely related to route-ﬁnding problems, but with an impor-
TOURING PROBLEM
tant difference. Consider, for example, the problem “Visit every city in Figure 3.2 at least
once, starting and ending in Bucharest.” As with route ﬁnding, the actions correspond
to trips between adjacent cities. The state space, however, is quite different. Each state
must include not just the current location but also the set of cities the agent has visited .
So the initial state would be In(Bucharest),Visited({Bucharest}), a typical intermedi-
ate state would be In(Vaslui),Visited({Bucharest,Urziceni,Vaslui}), and the goal test
would check whether the agent is in Bucharest and all 20 cities have been visited.
The traveling salesperson problem (TSP) is a touring problem in which each city
TRAVELING
SALESPERSON
PROBLEM
must be visited exactly once. The aim is to ﬁnd the shortest tour. The problem is known to
be NP-hard, but an enormous amount of effort has been expended to improve the capabilities
of TSP algorithms. In addition to planning trips for traveling salespersons, these algorithms
have been used for tasks such as planning movements of automatic circuit-board drills and of
stocking machines on shop ﬂoors.
A VLSI layout problem requires positioning millions of components and connections
VLSI LAYOUT
on a chip to minimize area, minimize circuit delays, minimize stray capacitances, and max-
imize manufacturing yield. The layout problem comes after the logical design phase and is
usually split into two parts: cell layout and channel routing. In cell layout, the primitive
components of the circuit are grouped into cells, each of which performs some recognized
function. Each cell has a ﬁxed footprint (size and shape) and requires a certain number of
connections to each of the other cells. The aim is to place the cells on the chip so that they do
not overlap and so that there is room for the connecting wires to be placed between the cells.
Channel routing ﬁnds a speciﬁc route for each wire through the gaps between the cells. These
search problems are extremely complex, but deﬁnitely worth solving. Later in this chapter,
we present some algorithms capable of solving them.
Robot navigation is a generalization of the route-ﬁnding problem described earlier.
ROBOT NAVIGA TION
Rather than following a discrete set of routes, a robot can move in a continuous space with
(in principle) an inﬁnite set of possible actions and states. For a circular robot moving on a
ﬂat surface, the space is essentially two-dimensional. When the robot has arms and legs or
wheels that must also be controlled, the search space becomes many-dimensional. Advanced
techniques are required just to make the search space ﬁnite. We examine some of these
methods in Chapter 25. In addition to the complexity of the problem, real robots must also
deal with errors in their sensor readings and motor controls.
Automatic assembly sequencing of complex objects by a robot was ﬁrst demonstrated
AUTOMA TIC
ASSEMBL Y
SEQUENCING
by FREDDY (Michie, 1972). Progress since then has been slow but sure, to the point where
the assembly of intricate objects such as electric motors is economically feasible. In assembly
problems, the aim is to ﬁnd an order in which to assemble the parts of some object. If the
wrong order is chosen, there will be no way to add some part later in the sequence without
Section 3.3. Searching for Solutions 75
undoing some of the work already done. Checking a step in the sequence for feasibility is a
difﬁcult geometrical search problem closely related to robot navigation. Thus, the generation
of legal actions is the expensive part of assembly sequencing. Any practical algorithm must
avoid exploring all but a tiny fraction of the state space. Another important assembly problem
is protein design, in which the goal is to ﬁnd a sequence of amino acids that will fold into a
PROTEIN DESIGN
three-dimensional protein with the right properties to cure some disease.
3.3 S EARCHING FOR SOLUTIONS
Having formulated some problems, we now need to solve them. A solution is an action
sequence, so search algorithms work by considering various possible action sequences. The
possible action sequences starting at the initial state form a search tree with the initial state
SEARCH TREE
at the root; the branches are actions and the nodes correspond to states in the state space ofNODE
the problem. Figure 3.6 shows the ﬁrst few steps in growing the search tree for ﬁnding a route
from Arad to Bucharest. The root node of the tree corresponds to the initial state, In(Arad).
The ﬁrst step is to test whether this is a goal state. (Clearly it is not, but it is important to
check so that we can solve trick problems like “starting in Arad, get to Arad.”) Then we
need to consider taking various actions. We do this by expanding the current state; that is,
EXP ANDING
applying each legal action to the current state, thereby generating a new set of states. InGENERA TING
this case, we add three branches from the parent node In(Arad) leading to three new childP ARENT NODE
nodes: In(Sibiu), In(Timisoara), and In(Zerind). Now we must choose which of these threeCHILD NODE
possibilities to consider further.
This is the essence of search—following up one option now and putting the others aside
for later, in case the ﬁrst choice does not lead to a solution. Suppose we choose Sibiu ﬁrst.
We check to see whether it is a goal state (it is not) and then expand it to get In(Arad),
In(Fagaras), In(Oradea),a n dIn(RimnicuVilcea). We can then choose any of these four or go
back and choose Timisoara or Zerind. Each of these six nodes is a leaf node, that is, a node
LEAF NODE
with no children in the tree. The set of all leaf nodes available for expansion at any given
point is called the frontier. (Many authors call it the open list, which is both geographicallyFRONTIER
OPEN LIST less evocative and less accurate, because other data structures are better suited than a list.) In
Figure 3.6, the frontier of each tree consists of those nodes with bold outlines.
The process of expanding nodes on the frontier continues until either a solution is found
or there are no more states to expand. The general T REE -SEARCH algorithm is shown infor-
mally in Figure 3.7. Search algorithms all share this basic structure; they vary primarily
according to how they choose which state to expand next—the so-called search strategy.
SEARCH STRA TEGY
The eagle-eyed reader will notice one peculiar thing about the search tree shown in Fig-
ure 3.6: it includes the path from Arad to Sibiu and back to Arad again! We say thatIn(Arad)
is a repeated state in the search tree, generated in this case by a loopy path. ConsideringREPEA TED STA TE
LOOPY P A TH such loopy paths means that the complete search tree for Romania is inﬁnite because there
is no limit to how often one can traverse a loop. On the other hand, the state space—the
map shown in Figure 3.2—has only 20 states. As we discuss in Section 3.4, loops can cause
76 Chapter 3. Solving Problems by Searching
certain algorithms to fail, making otherwise solvable problems unsolvable. Fortunately, there
is no need to consider loopy paths. We can rely on more than intuition for this: because path
costs are additive and step costs are nonnegative, a loopy path to any given state is never
better than the same path with the loop removed.
Loopy paths are a special case of the more general concept of redundant paths,w h i c h
REDUNDANT P A TH
exist whenever there is more than one way to get from one state to another. Consider the paths
Arad–Sibiu (140 km long) and Arad–Zerind–Oradea–Sibiu (297 km long). Obviously, the
second path is redundant—it’s just a worse way to get to the same state. If you are concerned
about reaching the goal, there’s never any reason to keep more than one path to any given
state, because any goal state that is reachable by extending one path is also reachable by
extending the other.
In some cases, it is possible to deﬁne the problem itself so as to eliminate redundant
paths. For example, if we formulate the 8-queens problem (page 71) so that a queen can be
placed in any column, then each state withn queens can be reached by n! different paths; but
if we reformulate the problem so that each new queen is placed in the leftmost empty column,
then each state can be reached only through one path.
(a) The initial state
(b) After expanding Arad
(c) After expanding Sibiu
Rimnicu Vilcea LugojArad Fagaras Oradea AradArad Oradea
Rimnicu Vilcea Lugoj
ZerindSibiu
Arad Fagaras Oradea
Timisoara
AradArad Oradea
Lugoj AradArad Oradea
Zerind
Arad
Sibiu Timisoara
Arad
Rimnicu Vilcea
Zerind
Arad
Sibiu
Arad Fagaras Oradea
Timisoara
Figure 3.6 Partial search trees for ﬁnding a route from Arad to Bucharest. Nodes that
have been expanded are shaded; nodes that have been generated but not yet expanded are
outlined in bold; nodes that have not yet been generated are shown in faint dashed lines.

Section 3.3. Searching for Solutions 77
function TREE -SEARCH (problem) returns a solution, or failure
initialize the frontier using the initial state of problem
loop do
if the frontier is empty then return failure
choose a leaf node and remove it from the frontier
if the node contains a goal state then return the corresponding solution
expand the chosen node, adding the resulting nodes to the frontier
function GRAPH -SEARCH (problem) returns a solution, or failure
initialize the frontier using the initial state of problem
initialize the explored set to be empty
loop do
if the frontier is empty then return failure
choose a leaf node and remove it from the frontier
if the node contains a goal state then return the corresponding solution
add the node to the explored set
expand the chosen node, adding the resulting nodes to the frontier
only if not in the frontier or explored set
Figure 3.7 An informal description of the general tree-search and graph-search algo-
rithms. The parts of G RAPH -SEARCH marked in bold italic are the additions needed to
handle repeated states.
In other cases, redundant paths are unavoidable. This includes all problems where
the actions are reversible, such as route-ﬁnding problems and sliding-block puzzles. Route-
ﬁnding on a rectangular grid (like the one used later for Figure 3.9) is a particularly impor-
RECT ANGULAR GRID
tant example in computer games. In such a grid, each state has four successors, so a search
tree of depthd that includes repeated states has4d leaves; but there are only about2d2 distinct
states within d steps of any given state. Ford =2 0, this means about a trillion nodes but only
about 800 distinct states. Thus, following redundant paths can cause a tractable problem to
become intractable. This is true even for algorithms that know how to avoid inﬁnite loops.
As the saying goes, algorithms that forget their history are doomed to repeat it. The
way to avoid exploring redundant paths is to remember where one has been. To do this, we
augment the T REE -SEARCH algorithm with a data structure called the explored set (alsoEXPLORED SET
known as the closed list), which remembers every expanded node. Newly generated nodesCLOSED LIST
that match previously generated nodes—ones in the explored set or the frontier—can be dis-
carded instead of being added to the frontier. The new algorithm, called GRAPH -SEARCH ,i s
shown informally in Figure 3.7. The speciﬁc algorithms in this chapter draw on this general
design.
Clearly, the search tree constructed by the G
RAPH -SEARCH algorithm contains at most
one copy of each state, so we can think of it as growing a tree directly on the state-space graph,
as shown in Figure 3.8. The algorithm has another nice property: the frontier separates the
SEP ARA TOR
state-space graph into the explored region and the unexplored region, so that every path from
78 Chapter 3. Solving Problems by Searching
Figure 3.8 A sequence of search trees generated by a graph search on the Romania prob-
lem of Figure 3.2. At each stage, we have extended each path by one step. Notice that at the
third stage, the northernmost city (Oradea) has become a dead end: both of its successors are
already explored via other paths.
(c)(b)(a)
Figure 3.9 The separation property of GRAPH -SEARCH , illustrated on a rectangular-grid
problem. The frontier (white nodes) always separates the explored region of the state space
(black nodes) from the unexplored region (gray nodes). In (a), just the root has been ex-
panded. In (b), one leaf node has been expanded. In (c), the remaining successors of the root
have been expanded in clockwise order.
the initial state to an unexplored state has to pass through a state in the frontier. (If this
seems completely obvious, try Exercise 3.13 now.) This property is illustrated in Figure 3.9.
As every step moves a state from the frontier into the explored region while moving some
states from the unexplored region into the frontier, we see that the algorithm issystematically
examining the states in the state space, one by one, until it ﬁnds a solution.
3.3.1 Infrastructure for search algorithms
Search algorithms require a data structure to keep track of the search tree that is being con-
structed. For each node n of the tree, we have a structure that contains four components:
•n.S
TATE: the state in the state space to which the node corresponds;
•n.PARENT : the node in the search tree that generated this node;
•n.ACTION : the action that was applied to the parent to generate the node;
•n.PATH-COST : the cost, traditionally denoted by g(n), of the path from the initial state
to the node, as indicated by the parent pointers.
Section 3.3. Searching for Solutions 79
1
23
45
6
7
81
23
45
6
7
8
Node
STATE
PARENT
ACTION = Right
PATH-COST = 6
Figure 3.10 Nodes are the data structures from which the search tree is constructed. Each
has a parent, a state, and various bookkeeping ﬁelds. Arrows point from child to parent.
Given the components for a parent node, it is easy to see how to compute the necessary
components for a child node. The function C HILD -NODE takes a parent node and an action
and returns the resulting child node:
function CHILD -NODE (problem,parent,action) returns a node
return a node with
STATE = problem.RESULT (parent.STATE,action),
PARENT = parent,A CTION = action,
PATH-COST = parent.PATH-COST + problem.STEP -COST (parent.STATE,action)
The node data structure is depicted in Figure 3.10. Notice how the P ARENT pointers
string the nodes together into a tree structure. These pointers also allow the solution path to be
extracted when a goal node is found; we use the S
OLUTION function to return the sequence
of actions obtained by following parent pointers back to the root.
Up to now, we have not been very careful to distinguish between nodes and states, but in
writing detailed algorithms it’s important to make that distinction. A node is a bookkeeping
data structure used to represent the search tree. A state corresponds to a conﬁguration of the
world. Thus, nodes are on particular paths, as deﬁned by P
ARENT pointers, whereas states
are not. Furthermore, two different nodes can contain the same world state if that state is
generated via two different search paths.
Now that we have nodes, we need somewhere to put them. The frontier needs to be
stored in such a way that the search algorithm can easily choose the next node to expand
according to its preferred strategy. The appropriate data structure for this is a queue.T h e
QUEUE
operations on a queue are as follows:
•EMPTY ?(queue) returns true only if there are no more elements in the queue.
•POP(queue) removes the ﬁrst element of the queue and returns it.
•INSERT (element, queue) inserts an element and returns the resulting queue.
80 Chapter 3. Solving Problems by Searching
Queues are characterized by the order in which they store the inserted nodes. Three common
variants are the ﬁrst-in, ﬁrst-out or FIFO queue, which pops the oldest element of the queue;FIFO QUEUE
the last-in, ﬁrst-out or LIFO queue (also known as a stack), which pops the newest elementLIFO QUEUE
of the queue; and the priority queue, which pops the element of the queue with the highestPRIORITY QUEUE
priority according to some ordering function.
The explored set can be implemented with a hash table to allow efﬁcient checking for
repeated states. With a good implementation, insertion and lookup can be done in roughly
constant time no matter how many states are stored. One must take care to implement the
hash table with the right notion of equality between states. For example, in the traveling
salesperson problem (page 74), the hash table needs to know that the set of visited cities
{Bucharest,Urziceni,Vaslui} is the same as{Urziceni,Vaslui,Bucharest}. Sometimes this can
be achieved most easily by insisting that the data structures for states be in some canonical
form; that is, logically equivalent states should map to the same data structure. In the case
CANONICAL FORM
of states described by sets, for example, a bit-vector representation or a sorted list without
repetition would be canonical, whereas an unsorted list would not.
3.3.2 Measuring problem-solving performance
Before we get into the design of speciﬁc search algorithms, we need to consider the criteria
that might be used to choose among them. We can evaluate an algorithm’s performance in
four ways:
•Completeness: Is the algorithm guaranteed to ﬁnd a solution when there is one?
COMPLETENESS
•Optimality: Does the strategy ﬁnd the optimal solution, as deﬁned on page 68?OPTIMALITY
•Time complexity: How long does it take to ﬁnd a solution?TIME COMPLEXITY
•Space complexity: How much memory is needed to perform the search?SP ACE COMPLEXITY
Time and space complexity are always considered with respect to some measure of the prob-
lem difﬁculty. In theoretical computer science, the typical measure is the size of the state
space graph,|V| +|E|,w h e r eV is the set of vertices (nodes) of the graph and E is the set
of edges (links). This is appropriate when the graph is an explicit data structure that is input
to the search program. (The map of Romania is an example of this.) In AI, the graph is often
represented implicitly by the initial state, actions, and transition model and is frequently inﬁ-
nite. For these reasons, complexity is expressed in terms of three quantities:b,t h ebranching
factor or maximum number of successors of any node; d,t h e depth of the shallowest goalBRANCHING FACTOR
DEPTH node (i.e., the number of steps along the path from the root); and m, the maximum length of
any path in the state space. Time is often measured in terms of the number of nodes generated
during the search, and space in terms of the maximum number of nodes stored in memory.
For the most part, we describe time and space complexity for search on a tree; for a graph,
the answer depends on how “redundant” the paths in the state space are.
To assess the effectiveness of a search algorithm, we can consider just thesearch cost—
SEARCH COST
which typically depends on the time complexity but can also include a term for memory
usage—or we can use the total cost, which combines the search cost and the path cost of theTOTAL COST
solution found. For the problem of ﬁnding a route from Arad to Bucharest, the search cost
is the amount of time taken by the search and the solution cost is the total length of the path
Section 3.4. Uninformed Search Strategies 81
in kilometers. Thus, to compute the total cost, we have to add milliseconds and kilometers.
There is no “ofﬁcial exchange rate” between the two, but it might be reasonable in this case to
convert kilometers into milliseconds by using an estimate of the car’s average speed (because
time is what the agent cares about). This enables the agent to ﬁnd an optimal tradeoff point
at which further computation to ﬁnd a shorter path becomes counterproductive. The more
general problem of tradeoffs between different goods is taken up in Chapter 16.
3.4 U NINFORMED SEARCH STRATEGIES
This section covers several search strategies that come under the heading of uninformed
search (also called blind search ). The term means that the strategies have no additionalUNINFORMED
SEARCH
BLIND SEARCH information about states beyond that provided in the problem deﬁnition. All they can do is
generate successors and distinguish a goal state from a non-goal state. All search strategies
are distinguished by the order in which nodes are expanded. Strategies that know whether
one non-goal state is “more promising” than another are calledinformed search or heuristic
INFORMED SEARCH
search strategies; they are covered in Section 3.5.HEURISTIC SEARCH
3.4.1 Breadth-ﬁrst search
Breadth-ﬁrst search is a simple strategy in which the root node is expanded ﬁrst, then all theBREADTH-FIRST
SEARCH
successors of the root node are expanded next, then their successors, and so on. In general,
all the nodes are expanded at a given depth in the search tree before any nodes at the next
level are expanded.
Breadth-ﬁrst search is an instance of the general graph-search algorithm (Figure 3.7) in
which the shallowest unexpanded node is chosen for expansion. This is achieved very simply
by using a FIFO queue for the frontier. Thus, new nodes (which are always deeper than their
parents) go to the back of the queue, and old nodes, which are shallower than the new nodes,
get expanded ﬁrst. There is one slight tweak on the general graph-search algorithm, which is
that the goal test is applied to each node when it isgenerated rather than when it is selected for
expansion. This decision is explained below, where we discuss time complexity. Note also
that the algorithm, following the general template for graph search, discards any new path to
a state already in the frontier or explored set; it is easy to see that any such path must be at
least as deep as the one already found. Thus, breadth-ﬁrst search always has the shallowest
path to every node on the frontier.
Pseudocode is given in Figure 3.11. Figure 3.12 shows the progress of the search on a
simple binary tree.
How does breadth-ﬁrst search rate according to the four criteria from the previous sec-
tion? We can easily see that it iscomplete—if the shallowest goal node is at some ﬁnite depth
d, breadth-ﬁrst search will eventually ﬁnd it after generating all shallower nodes (provided
the branching factor b is ﬁnite). Note that as soon as a goal node is generated, we know it
is the shallowest goal node because all shallower nodes must have been generated already
and failed the goal test. Now, the shallowest goal node is not necessarily the optimal one;
82 Chapter 3. Solving Problems by Searching
function BREADTH -FIRST -SEARCH (problem) returns a solution, or failure
node←a node with STATE = problem.INITIAL -STATE,P ATH-COST =0
if problem.GOAL -TEST (node.STATE) then return SOLUTION (node)
frontier←a FIFO queue with node as the only element
explored←an empty set
loop do
if EMPTY ?(frontier) then return failure
node←POP(frontier) /* chooses the shallowest node in frontier */
add node.STATE to explored
for each action in problem.ACTIONS (node.STATE) do
child←CHILD -NODE (problem,node,action)
if child.STATE is not in explored or frontier then
if problem.GOAL -TEST (child.STATE) then return SOLUTION (child)
frontier←INSERT (child,frontier)
Figure 3.11 Breadth-ﬁrst search on a graph.
technically, breadth-ﬁrst search is optimal if the path cost is a nondecreasing function of the
depth of the node. The most common such scenario is that all actions have the same cost.
So far, the news about breadth-ﬁrst search has been good. The news about time and
space is not so good. Imagine searching a uniform tree where every state has b successors.
The root of the search tree generatesb nodes at the ﬁrst level, each of which generatesb more
nodes, for a total of b2 at the second level. Each of these generates b more nodes, yielding b3
nodes at the third level, and so on. Now suppose that the solution is at depth d.I nt h ew o r s t
case, it is the last node generated at that level. Then the total number of nodes generated is
b + b2 + b3 +··· + bd = O(bd) .
(If the algorithm were to apply the goal test to nodes when selected for expansion, rather than
when generated, the whole layer of nodes at depth d would be expanded before the goal was
detected and the time complexity would be O(bd+1).)
As for space complexity: for any kind of graph search, which stores every expanded
node in the explored set, the space complexity is always within a factor of b of the time
complexity. For breadth-ﬁrst graph search in particular, every node generated remains in
memory. There will be O(bd−1) nodes in the explored set and O(bd) nodes in the frontier,
A
B C
E F GD
A
B
D E F G
C
A
C
D E F G
BB C
D E F G
A
Figure 3.12 Breadth-ﬁrst search on a simple binary tree. At each stage, the node to be
expanded next is indicated by a marker.

Section 3.4. Uninformed Search Strategies 83
so the space complexity is O(bd), i.e., it is dominated by the size of the frontier. Switching
to a tree search would not save much space, and in a state space with many redundant paths,
switching could cost a great deal of time.
An exponential complexity bound such as O(b
d) is scary. Figure 3.13 shows why. It
lists, for various values of the solution depth d, the time and memory required for a breadth-
ﬁrst search with branching factor b =1 0. The table assumes that 1 million nodes can be
generated per second and that a node requires 1000 bytes of storage. Many search problems
ﬁt roughly within these assumptions (give or take a factor of 100) when run on a modern
personal computer.
Depth Nodes Time Memory
2 110 .11 milliseconds 107 kilobytes
4 11,110 11 milliseconds 10.6 megabytes
6 106 1.1 seconds 1 gigabyte
8 108 2 minutes 103 gigabytes
10 1010 3 hours 10 terabytes
12 1012 13 days 1 petabyte
14 1014 3.5 years 99 petabytes
16 1016 350 years 10 exabytes
Figure 3.13 Time and memory requirements for breadth-ﬁrst search. The numbers shown
assume branching factor b =1 0; 1 million nodes/second; 1000 bytes/node.
Two lessons can be learned from Figure 3.13. First, the memory requirements are a
bigger problem for breadth-ﬁrst search than is the execution time. One might wait 13 days
for the solution to an important problem with search depth 12, but no personal computer has
the petabyte of memory it would take. Fortunately, other strategies require less memory.
The second lesson is that time is still a major factor. If your problem has a solution at
depth 16, then (given our assumptions) it will take about 350 years for breadth-ﬁrst search (or
indeed any uninformed search) to ﬁnd it. In general, exponential-complexity search problems
cannot be solved by uninformed methods for any but the smallest instances.
3.4.2 Uniform-cost search
When all step costs are equal, breadth-ﬁrst search is optimal because it always expands the
shallowest unexpanded node. By a simple extension, we can ﬁnd an algorithm that is optimal
with any step-cost function. Instead of expanding the shallowest node, uniform-cost searchUNIFORM-COST
SEARCH
expands the node n with the lowest path cost g(n). This is done by storing the frontier as a
priority queue ordered by g. The algorithm is shown in Figure 3.14.
In addition to the ordering of the queue by path cost, there are two other signiﬁcant
differences from breadth-ﬁrst search. The ﬁrst is that the goal test is applied to a node when
it is selected for expansion (as in the generic graph-search algorithm shown in Figure 3.7)
rather than when it is ﬁrst generated. The reason is that the ﬁrst goal node that is generated
84 Chapter 3. Solving Problems by Searching
function UNIFORM -COST-SEARCH (problem) returns a solution, or failure
node←a node with STATE = problem.INITIAL -STATE,P ATH-COST =0
frontier←a priority queue ordered by PATH-COST , with node as the only element
explored←an empty set
loop do
if EMPTY ?(frontier) then return failure
node←POP(frontier) /* chooses the lowest-cost node in frontier */
if problem.GOAL -TEST (node.STATE) then return SOLUTION (node)
add node.STATE to explored
for each action in problem.ACTIONS (node.STATE) do
child←CHILD -NODE (problem,node,action)
if child.STATE is not in explored or frontier then
frontier←INSERT (child,frontier)
else if child.STATE is in frontier with higher PATH-COST then
replace that frontier node with child
Figure 3.14 Uniform-cost search on a graph. The algorithm is identical to the general
graph search algorithm in Figure 3.7, except for the use of a priority queue and the addition
of an extra check in case a shorter path to a frontier state is discovered. The data structure for
frontier needs to support efﬁcient membership testing, so it should combine the capabilities
of a priority queue and a hash table.
Sibiu Fagaras
Pitesti
Rimnicu Vilcea
Bucharest
99
80
97
101
211
Figure 3.15 Part of the Romania state space, selected to illustrate uniform-cost search.
may be on a suboptimal path. The second difference is that a test is added in case a better
path is found to a node currently on the frontier.
Both of these modiﬁcations come into play in the example shown in Figure 3.15, where
the problem is to get from Sibiu to Bucharest. The successors of Sibiu are Rimnicu Vilcea and
Fagaras, with costs 80 and 99, respectively. The least-cost node, Rimnicu Vilcea, is expanded
next, adding Pitesti with cost 80 + 97 = 177. The least-cost node is now Fagaras, so it is
expanded, adding Bucharest with cost 99 + 211 = 310. Now a goal node has been generated,
but uniform-cost search keeps going, choosing Pitesti for expansion and adding a second path
Section 3.4. Uninformed Search Strategies 85
to Bucharest with cost80+97+101 = 278 . Now the algorithm checks to see if this new path
is better than the old one; it is, so the old one is discarded. Bucharest, now with g-cost 278,
is selected for expansion and the solution is returned.
It is easy to see that uniform-cost search is optimal in general. First, we observe that
whenever uniform-cost search selects a node n for expansion, the optimal path to that node
has been found. (Were this not the case, there would have to be another frontier node n′ on
the optimal path from the start node to n, by the graph separation property of Figure 3.9;
by deﬁnition, n′ would have lower g-cost than n and would have been selected ﬁrst.) Then,
because step costs are nonnegative, paths never get shorter as nodes are added. These two
facts together imply that uniform-cost search expands nodes in order of their optimal path
cost. Hence, the ﬁrst goal node selected for expansion must be the optimal solution.
Uniform-cost search does not care about the number of steps a path has, but only about
their total cost. Therefore, it will get stuck in an inﬁnite loop if there is a path with an inﬁnite
sequence of zero-cost actions—for example, a sequence of NoOp actions.
6 Completeness is
guaranteed provided the cost of every step exceeds some small positive constantϵ.
Uniform-cost search is guided by path costs rather than depths, so its complexity is not
easily characterized in terms of b and d. Instead, let C∗ be the cost of the optimal solution,7
and assume that every action costs at least ϵ. Then the algorithm’s worst-case time and space
complexity is O(b1+⌊C∗/ϵ⌋), which can be much greater than bd. This is because uniform-
cost search can explore large trees of small steps before exploring paths involving large and
perhaps useful steps. When all step costs are equal, b1+⌊C∗/ϵ⌋ is just bd+1. When all step
costs are the same, uniform-cost search is similar to breadth-ﬁrst search, except that the latter
stops as soon as it generates a goal, whereas uniform-cost search examines all the nodes at
the goal’s depth to see if one has a lower cost; thus uniform-cost search does strictly more
work by expanding nodes at depth d unnecessarily.
3.4.3 Depth-ﬁrst search
Depth-ﬁrst search always expands the deepest node in the current frontier of the search tree.DEPTH-FIRST
SEARCH
The progress of the search is illustrated in Figure 3.16. The search proceeds immediately
to the deepest level of the search tree, where the nodes have no successors. As those nodes
are expanded, they are dropped from the frontier, so then the search “backs up” to the next
deepest node that still has unexplored successors.
The depth-ﬁrst search algorithm is an instance of the graph-search algorithm in Fig-
ure 3.7; whereas breadth-ﬁrst-search uses a FIFO queue, depth-ﬁrst search uses a LIFO queue.
A LIFO queue means that the most recently generated node is chosen for expansion. This
must be the deepest unexpanded node because it is one deeper than its parent—which, in turn,
was the deepest unexpanded node when it was selected.
As an alternative to the G
RAPH -SEARCH -style implementation, it is common to im-
plement depth-ﬁrst search with a recursive function that calls itself on each of its children in
turn. (A recursive depth-ﬁrst algorithm incorporating a depth limit is shown in Figure 3.17.)
6 NoOp, or “no operation,” is the name of an assembly language instruction that does nothing.
7 Here, and throughout the book, the “star” in C∗ means an optimal value for C.
86 Chapter 3. Solving Problems by Searching
A
C
F G
M N O
A
C
F G
L M N O
A
C
F G
L M N O
C
F G
L M N O
A
B C
E F G
K L M N O
A
C
E F G
J K L M N O
A
C
E F G
J K L M N O
A
B C
D E F G
I J K L M N O
A
B C
D E F G
H I J K L M N O
A
B C
D E F G
H I J K L M N O
A
B C
D E F G
H I J K L M N O
A
B C
D E F G
H I J K L M N O
Figure 3.16 Depth-ﬁrst search on a binary tree. The unexplored region is shown in light
gray. Explored nodes with no descendants in the frontier are removed from memory. Nodes
at depth 3 have no successors and M is the only goal node.
The properties of depth-ﬁrst search depend strongly on whether the graph-search or
tree-search version is used. The graph-search version, which avoids repeated states and re-
dundant paths, is complete in ﬁnite state spaces because it will eventually expand every node.
The tree-search version, on the other hand, is not complete—for example, in Figure 3.6 the
algorithm will follow the Arad–Sibiu–Arad–Sibiu loop forever. Depth-ﬁrst tree search can be
modiﬁed at no extra memory cost so that it checks new states against those on the path from
the root to the current node; this avoids inﬁnite loops in ﬁnite state spaces but does not avoid
the proliferation of redundant paths. In inﬁnite state spaces, both versions fail if an inﬁnite
non-goal path is encountered. For example, in Knuth’s 4 problem, depth-ﬁrst search would
keep applying the factorial operator forever.
For similar reasons, both versions are nonoptimal. For example, in Figure 3.16, depth-
ﬁrst search will explore the entire left subtree even if node C is a goal node. If node J were
also a goal node, then depth-ﬁrst search would return it as a solution instead of C,w h i c h
would be a better solution; hence, depth-ﬁrst search is not optimal.
Section 3.4. Uninformed Search Strategies 87
The time complexity of depth-ﬁrst graph search is bounded by the size of the state space
(which may be inﬁnite, of course). A depth-ﬁrst tree search, on the other hand, may generate
a l lo ft h eO(b
m) nodes in the search tree, where m is the maximum depth of any node; this
can be much greater than the size of the state space. Note that m itself can be much larger
than d (the depth of the shallowest solution) and is inﬁnite if the tree is unbounded.
So far, depth-ﬁrst search seems to have no clear advantage over breadth-ﬁrst search,
so why do we include it? The reason is the space complexity. For a graph search, there is
no advantage, but a depth-ﬁrst tree search needs to store only a single path from the root
to a leaf node, along with the remaining unexpanded sibling nodes for each node on the
path. Once a node has been expanded, it can be removed from memory as soon as all its
descendants have been fully explored. (See Figure 3.16.) For a state space with branching
factor b and maximum depth m, depth-ﬁrst search requires storage of only O(bm) nodes.
Using the same assumptions as for Figure 3.13 and assuming that nodes at the same depth as
the goal node have no successors, we ﬁnd that depth-ﬁrst search would require 156 kilobytes
instead of 10 exabytes at depth d =1 6 , a factor of 7 trillion times less space. This has
led to the adoption of depth-ﬁrst tree search as the basic workhorse of many areas of AI,
including constraint satisfaction (Chapter 6), propositional satisﬁability (Chapter 7), and logic
programming (Chapter 9). For the remainder of this section, we focus primarily on the tree-
search version of depth-ﬁrst search.
A variant of depth-ﬁrst search called backtracking search uses still less memory. (See
BACKTRACKING
SEARCH
Chapter 6 for more details.) In backtracking, only one successor is generated at a time rather
than all successors; each partially expanded node remembers which successor to generate
next. In this way, only O(m) memory is needed rather than O(bm). Backtracking search
facilitates yet another memory-saving (and time-saving) trick: the idea of generating a suc-
cessor by modifying the current state description directly rather than copying it ﬁrst. This
reduces the memory requirements to just one state description and O(m) actions. For this to
work, we must be able to undo each modiﬁcation when we go back to generate the next suc-
cessor. For problems with large state descriptions, such as robotic assembly, these techniques
are critical to success.
3.4.4 Depth-limited search
The embarrassing failure of depth-ﬁrst search in inﬁnite state spaces can be alleviated by
supplying depth-ﬁrst search with a predetermined depth limit ℓ. That is, nodes at depth ℓ are
treated as if they have no successors. This approach is called depth-limited search.T h eDEPTH-LIMITED
SEARCH
depth limit solves the inﬁnite-path problem. Unfortunately, it also introduces an additional
source of incompleteness if we choose ℓ<d , that is, the shallowest goal is beyond the depth
limit. (This is likely when d is unknown.) Depth-limited search will also be nonoptimal if
we choose ℓ>d . Its time complexity is O(bℓ) and its space complexity is O(bℓ). Depth-ﬁrst
search can be viewed as a special case of depth-limited search with ℓ=∞.
Sometimes, depth limits can be based on knowledge of the problem. For example, on
the map of Romania there are 20 cities. Therefore, we know that if there is a solution, it must
be of length 19 at the longest, so ℓ =1 9 is a possible choice. But in fact if we studied the
88 Chapter 3. Solving Problems by Searching
function DEPTH -LIMITED -SEARCH (problem,limit) returns a solution, or failure/cutoff
return RECURSIVE -DLS(M AKE -NODE (problem.INITIAL -STATE),problem,limit)
function RECURSIVE -DLS( node,problem,limit) returns a solution, or failure/cutoff
if problem.GOAL -TEST (node.STATE) then return SOLUTION (node)
else if limit =0 then return cutoﬀ
else
cutoﬀ
 occurred?←false
for each action in problem.ACTIONS (node.STATE) do
child←CHILD -NODE (problem,node,action)
result←RECURSIVE -DLS( child,problem,limit−1)
if result = cutoﬀ then cutoﬀ
 occurred?←true
else if result̸= failure then return result
if cutoﬀ
 occurred? then return cutoﬀ else return failure
Figure 3.17 A recursive implementation of depth-limited tree search.
map carefully, we would discover that any city can be reached from any other city in at most
9 steps. This number, known as the diameter of the state space, gives us a better depth limit,DIAMETER
which leads to a more efﬁcient depth-limited search. For most problems, however, we will
not know a good depth limit until we have solved the problem.
Depth-limited search can be implemented as a simple modiﬁcation to the general tree-
or graph-search algorithm. Alternatively, it can be implemented as a simple recursive al-
gorithm as shown in Figure 3.17. Notice that depth-limited search can terminate with two
kinds of failure: the standard failure value indicates no solution; the cutoﬀ value indicates
no solution within the depth limit.
3.4.5 Iterative deepening depth-ﬁrst search
Iterative deepening search (or iterative deepening depth-ﬁrst search) is a general strategy,ITERA TIVE
DEEPENING SEARCH
often used in combination with depth-ﬁrst tree search, that ﬁnds the best depth limit. It does
this by gradually increasing the limit—ﬁrst 0, then 1, then 2, and so on—until a goal is found.
This will occur when the depth limit reaches d, the depth of the shallowest goal node. The
algorithm is shown in Figure 3.18. Iterative deepening combines the beneﬁts of depth-ﬁrst
and breadth-ﬁrst search. Like depth-ﬁrst search, its memory requirements are modest: O(bd)
to be precise. Like breadth-ﬁrst search, it is complete when the branching factor is ﬁnite and
optimal when the path cost is a nondecreasing function of the depth of the node. Figure 3.19
shows four iterations of I
TERATIVE -DEEPENING -SEARCH on a binary search tree, where the
solution is found on the fourth iteration.
Iterative deepening search may seem wasteful because states are generated multiple
times. It turns out this is not too costly. The reason is that in a search tree with the same (or
nearly the same) branching factor at each level, most of the nodes are in the bottom level,
so it does not matter much that the upper levels are generated multiple times. In an iterative
deepening search, the nodes on the bottom level (depth d) are generated once, those on the
Section 3.4. Uninformed Search Strategies 89
function ITERATIVE -DEEPENING -SEARCH (problem) returns a solution, or failure
for depth =0 to∞do
result←DEPTH -LIMITED -SEARCH (problem,depth)
if result̸= cutoff then return result
Figure 3.18 The iterative deepening search algorithm, which repeatedly applies depth-
limited search with increasing limits. It terminates when a solution is found or if the depth-
limited search returns failure, meaning that no solution exists.
Limit = 3
Limit = 2
Limit = 1
Limit = 0 A A
A
B C
A
B C
A
B C
A
B C
A
B C
D E F G
A
B C
D E F G
A
B C
D E F G
A
B C
D E F G
A
B C
D E F G
A
B C
D E F G
A
B C
D E F G
A
B C
D E F G
A
B C
D E F G
H I J K L M N O
A
B C
D E F G
H I J K L M N O
A
B C
D E F G
H I J K L M N O
A
B C
D E F G
H I J K L M N O
A
B C
D E F G
H I J K L M N O
A
B C
D E F G
H I J K L M N O
A
B C
D E F G
H I J K L M N O
A
B C
D E F G
H I J K L M N O
A
B C
D E F G
H I J K L M N O
A
B C
D E F G
H I J K L M N O
A
B C
D E F G
H J K L M N OI
A
B C
D E F G
H I J K L M N O
Figure 3.19 Four iterations of iterative deepening search on a binary tree.

90 Chapter 3. Solving Problems by Searching
next-to-bottom level are generated twice, and so on, up to the children of the root, which are
generated d times. So the total number of nodes generated in the worst case is
N(IDS)=( d)b +( d−1)b2 +··· +( 1 )bd ,
which gives a time complexity of O(bd)—asymptotically the same as breadth-ﬁrst search.
There is some extra cost for generating the upper levels multiple times, but it is not large. For
example, if b =1 0 and d =5 , the numbers are
N(IDS) = 50 + 400 + 3,000 + 20,000 + 100,000 = 123,450
N(BFS) = 10 + 100 + 1,000 + 10,000 + 100,000 = 111,110 .
If you are really concerned about repeating the repetition, you can use a hybrid approach
that runs breadth-ﬁrst search until almost all the available memory is consumed, and then
runs iterative deepening from all the nodes in the frontier. In general, iterative deepening is
the preferred uninformed search method when the search space is large and the depth of the
solution is not known.
Iterative deepening search is analogous to breadth-ﬁrst search in that it explores a com-
plete layer of new nodes at each iteration before going on to the next layer. It would seem
worthwhile to develop an iterative analog to uniform-cost search, inheriting the latter algo-
rithm’s optimality guarantees while avoiding its memory requirements. The idea is to use
increasing path-cost limits instead of increasing depth limits. The resulting algorithm, called
iterative lengthening search , is explored in Exercise 3.17. It turns out, unfortunately, that
ITERA TIVE
LENGTHENING
SEARCH
iterative lengthening incurs substantial overhead compared to uniform-cost search.
3.4.6 Bidirectional search
The idea behind bidirectional search is to run two simultaneous searches—one forward from
the initial state and the other backward from the goal—hoping that the two searches meet in
the middle (Figure 3.20). The motivation is that b
d/2 + bd/2 is much less than bd,o ri nt h e
ﬁgure, the area of the two small circles is less than the area of one big circle centered on the
start and reaching to the goal.
Bidirectional search is implemented by replacing the goal test with a check to see
whether the frontiers of the two searches intersect; if they do, a solution has been found.
(It is important to realize that the ﬁrst such solution found may not be optimal, even if the
two searches are both breadth-ﬁrst; some additional search is required to make sure there
isn’t another short-cut across the gap.) The check can be done when each node is generated
or selected for expansion and, with a hash table, will take constant time. For example, if a
problem has solution depth d=6 , and each direction runs breadth-ﬁrst search one node at a
time, then in the worst case the two searches meet when they have generated all of the nodes
at depth 3. For b=1 0, this means a total of 2,220 node generations, compared with 1,111,110
for a standard breadth-ﬁrst search. Thus, the time complexity of bidirectional search using
breadth-ﬁrst searches in both directions is O(b
d/2). The space complexity is also O(bd/2).
We can reduce this by roughly half if one of the two searches is done by iterative deepening,
but at least one of the frontiers must be kept in memory so that the intersection check can be
done. This space requirement is the most signiﬁcant weakness of bidirectional search.
Section 3.4. Uninformed Search Strategies 91
GoalStart
Figure 3.20 A schematic view of a bidirectional search that is about to succeed when a
branch from the start node meets a branch from the goal node.
The reduction in time complexity makes bidirectional search attractive, but how do we
search backward? This is not as easy as it sounds. Let the predecessors of a state x be allPREDECESSOR
those states that have x as a successor. Bidirectional search requires a method for computing
predecessors. When all the actions in the state space are reversible, the predecessors of x are
just its successors. Other cases may require substantial ingenuity.
Consider the question of what we mean by “the goal” in searching “backward from the
goal.” For the 8-puzzle and for ﬁnding a route in Romania, there is just one goal state, so the
backward search is very much like the forward search. If there are several explicitly listed
goal states—for example, the two dirt-free goal states in Figure 3.3—then we can construct a
new dummy goal state whose immediate predecessors are all the actual goal states. But if the
goal is an abstract description, such as the goal that “no queen attacks another queen” in the
n-queens problem, then bidirectional search is difﬁcult to use.
3.4.7 Comparing uninformed search strategies
Figure 3.21 compares search strategies in terms of the four evaluation criteria set forth in
Section 3.3.2. This comparison is for tree-search versions. For graph searches, the main
differences are that depth-ﬁrst search is complete for ﬁnite state spaces and that the space and
time complexities are bounded by the size of the state space.
Criterion
 Breadth- Uniform- Depth- Depth- Iterative Bidirectional
First Cost First Limited Deepening (if applicable)
Complete?
 Yesa Yesa,b No No Yes a Yesa,d
Time
 O(bd) O(b1+⌊C∗/ϵ⌋) O(bm) O(bℓ) O(bd) O(bd/2)
Space
 O(bd) O(b1+⌊C∗/ϵ⌋) O(bm) O(bℓ) O(bd) O(bd/2)
Optimal?
 Yesc Yes No No Yes c Yesc,d
Figure 3.21 Evaluation of tree-search strategies. b is the branching factor; d is the depth
of the shallowest solution; m is the maximum depth of the search tree; l is the depth limit.
Superscript caveats are as follows: a complete if b is ﬁnite; b complete if step costs ≥ϵ for
positive ϵ; c optimal if step costs are all identical;d if both directions use breadth-ﬁrst search.

92 Chapter 3. Solving Problems by Searching
3.5 I NFORMED (HEURISTIC )S EARCH STRATEGIES
This section shows how aninformed search strategy—one that uses problem-speciﬁc knowl-INFORMED SEARCH
edge beyond the deﬁnition of the problem itself—can ﬁnd solutions more efﬁciently than can
an uninformed strategy.
The general approach we consider is called best-ﬁrst search. Best-ﬁrst search is anBEST -FIRST SEARCH
instance of the general T REE -SEARCH or G RAPH -SEARCH algorithm in which a node is
selected for expansion based on an evaluation function, f(n). The evaluation function isEVALUA TION
FUNCTION
construed as a cost estimate, so the node with the lowest evaluation is expanded ﬁrst. The
implementation of best-ﬁrst graph search is identical to that for uniform-cost search (Fig-
ure 3.14), except for the use of f instead of g to order the priority queue.
The choice of f determines the search strategy. (For example, as Exercise 3.21 shows,
best-ﬁrst tree search includes depth-ﬁrst search as a special case.) Most best-ﬁrst algorithms
include as a component of f a heuristic function, denoted h(n):
HEURISTIC
FUNCTION
h(n)= estimated cost of the cheapest path from the state at node n to a goal state.
(Notice that h(n) takes a node as input, but, unlike g(n), it depends only on the state at that
node.) For example, in Romania, one might estimate the cost of the cheapest path from Arad
to Bucharest via the straight-line distance from Arad to Bucharest.
Heuristic functions are the most common form in which additional knowledge of the
problem is imparted to the search algorithm. We study heuristics in more depth in Section 3.6.
For now, we consider them to be arbitrary, nonnegative, problem-speciﬁc functions, with one
constraint: if n is a goal node, then h(n)=0 . The remainder of this section covers two ways
to use heuristic information to guide search.
3.5.1 Greedy best-ﬁrst search
Greedy best-ﬁrst search8 tries to expand the node that is closest to the goal, on the groundsGREEDY BEST -FIRST
SEARCH
that this is likely to lead to a solution quickly. Thus, it evaluates nodes by using just the
heuristic function; that is, f(n)= h(n).
Let us see how this works for route-ﬁnding problems in Romania; we use the straight-
line distance heuristic, which we will call hSLD . If the goal is Bucharest, we need toSTRAIGHT -LINE
DIST ANCE
know the straight-line distances to Bucharest, which are shown in Figure 3.22. For exam-
ple, hSLD(In(Arad)) = 366. Notice that the values of hSLD cannot be computed from the
problem description itself. Moreover, it takes a certain amount of experience to know that
hSLD is correlated with actual road distances and is, therefore, a useful heuristic.
Figure 3.23 shows the progress of a greedy best-ﬁrst search using hSLD to ﬁnd a path
from Arad to Bucharest. The ﬁrst node to be expanded from Arad will be Sibiu because it
is closer to Bucharest than either Zerind or Timisoara. The next node to be expanded will
be Fagaras because it is closest. Fagaras in turn generates Bucharest, which is the goal. For
this particular problem, greedy best-ﬁrst search using h
SLD ﬁnds a solution without ever
8 Our ﬁrst edition called this greedy search; other authors have called it best-ﬁrst search. Our more general
usage of the latter term follows Pearl (1984).
Section 3.5. Informed (Heuristic) Search Strategies 93
Urziceni
Neamt
Oradea
Zerind
Timisoara
Mehadia
Sibiu
Pitesti
Rimnicu Vilcea
Vaslui
Bucharest
Giurgiu
Hirsova
Eforie
Arad
Lugoj
Drobeta
Craiova
Fagaras
Iasi
 0
160
242
161
77
151
366
244
226
176
241
253
329
80
199
380
234
374
100
193
Figure 3.22 Values ofhSLD —straight-line distances to Bucharest.
expanding a node that is not on the solution path; hence, its search cost is minimal. It is
not optimal, however: the path via Sibiu and Fagaras to Bucharest is 32 kilometers longer
than the path through Rimnicu Vilcea and Pitesti. This shows why the algorithm is called
“greedy”—at each step it tries to get as close to the goal as it can.
Greedy best-ﬁrst tree search is also incomplete even in a ﬁnite state space, much like
depth-ﬁrst search. Consider the problem of getting from Iasi to Fagaras. The heuristic sug-
gests that Neamt be expanded ﬁrst because it is closest to Fagaras, but it is a dead end. The
solution is to go ﬁrst to Vaslui—a step that is actually farther from the goal according to
the heuristic—and then to continue to Urziceni, Bucharest, and Fagaras. The algorithm will
never ﬁnd this solution, however, because expanding Neamt puts Iasi back into the frontier,
Iasi is closer to Fagaras than Vaslui is, and so Iasi will be expanded again, leading to an inﬁ-
nite loop. (The graph search version is complete in ﬁnite spaces, but not in inﬁnite ones.) The
worst-case time and space complexity for the tree version isO(b
m),w h e r em is the maximum
depth of the search space. With a good heuristic function, however, the complexity can be
reduced substantially. The amount of the reduction depends on the particular problem and on
the quality of the heuristic.
3.5.2 A* search: Minimizing the total estimated solution cost
The most widely known form of best-ﬁrst search is called A∗ search (pronounced “A-starA∗ SEARCH
search”). It evaluates nodes by combiningg(n), the cost to reach the node, andh(n), the cost
to get from the node to the goal:
f(n)= g(n)+ h(n) .
Since g(n) gives the path cost from the start node to node n,a n dh(n) is the estimated cost
of the cheapest path from n to the goal, we have
f(n)= estimated cost of the cheapest solution through n.
Thus, if we are trying to ﬁnd the cheapest solution, a reasonable thing to try ﬁrst is the
node with the lowest value of g(n)+ h(n). It turns out that this strategy is more than just
reasonable: provided that the heuristic function h(n) satisﬁes certain conditions, A∗ search is
both complete and optimal. The algorithm is identical to U NIFORM -COST-SEARCH except
that A∗ uses g + h instead of g.
94 Chapter 3. Solving Problems by Searching
Rimnicu Vilcea
Zerind
Arad
Sibiu
Arad Fagaras Oradea
Timisoara
Sibiu Bucharest
329 374
366 380 193
253 0
Rimnicu Vilcea
Arad
Sibiu
Arad Fagaras Oradea
Timisoara
329
Zerind
374
366 176 380 193
Zerind
Arad
Sibiu Timisoara
253 329 374
Arad
366
(a) The initial state
(b) After expanding Arad
(c) After expanding Sibiu
(d) After expanding Fagaras
Figure 3.23 Stages in a greedy best-ﬁrst tree search for Bucharest with the straight-line
distance heuristic hSLD . Nodes are labeled with their h-values.
Conditions for optimality: Admissibility and consistency
The ﬁrst condition we require for optimality is that h(n) be an admissible heuristic .A nADMISSIBLE
HEURISTIC
admissible heuristic is one that never overestimates the cost to reach the goal. Because g(n)
is the actual cost to reach n along the current path, and f(n)= g(n)+ h(n),w eh a v ea sa n
immediate consequence that f(n) never overestimates the true cost of a solution along the
current path through n.
Admissible heuristics are by nature optimistic because they think the cost of solving
the problem is less than it actually is. An obvious example of an admissible heuristic is the
straight-line distance h
SLD that we used in getting to Bucharest. Straight-line distance is
admissible because the shortest path between any two points is a straight line, so the straight
Section 3.5. Informed (Heuristic) Search Strategies 95
line cannot be an overestimate. In Figure 3.24, we show the progress of an A∗ tree search for
Bucharest. The values of g are computed from the step costs in Figure 3.2, and the values of
hSLD are given in Figure 3.22. Notice in particular that Bucharest ﬁrst appears on the frontier
at step (e), but it is not selected for expansion because its f-cost (450) is higher than that of
Pitesti (417). Another way to say this is that there might be a solution through Pitesti whose
cost is as low as 417, so the algorithm will not settle for a solution that costs 450.
A second, slightly stronger condition called consistency (or sometimes monotonicity)CONSISTENCY
MONOTONICITY is required only for applications of A∗ to graph search.9 A heuristic h(n) is consistent if, for
every node n and every successor n′ of n generated by any action a, the estimated cost of
reaching the goal from n is no greater than the step cost of getting to n′ plus the estimated
cost of reaching the goal from n′:
h(n)≤c(n, a, n′)+ h(n′) .
This is a form of the general triangle inequality, which stipulates that each side of a triangleTRIANGLE
INEQUALITY
cannot be longer than the sum of the other two sides. Here, the triangle is formed by n, n′,
and the goal Gn closest to n. For an admissible heuristic, the inequality makes perfect sense:
if there were a route from n to Gn via n′ that was cheaper than h(n), that would violate the
property that h(n) is a lower bound on the cost to reach Gn.
It is fairly easy to show (Exercise 3.29) that every consistent heuristic is also admissible.
Consistency is therefore a stricter requirement than admissibility, but one has to work quite
hard to concoct heuristics that are admissible but not consistent. All the admissible heuristics
we discuss in this chapter are also consistent. Consider, for example, h
SLD . We know that
the general triangle inequality is satisﬁed when each side is measured by the straight-line
distance and that the straight-line distance between n and n
′ is no greater than c(n, a, n′).
Hence, hSLD is a consistent heuristic.
Optimality of A*
As we mentioned earlier, A ∗ has the following properties: the tree-search version of A∗ is
optimal if h(n) is admissible, while the graph-search version is optimal if h(n) is consistent.
We show the second of these two claims since it is more useful. The argument es-
sentially mirrors the argument for the optimality of uniform-cost search, with g replaced by
f—just as in the A∗ algorithm itself.
The ﬁrst step is to establish the following: if h(n) is consistent, then the values of
f(n) along any path are nondecreasing. The proof follows directly from the deﬁnition of
consistency. Suppose n′ is a successor of n;t h e ng(n′)= g(n)+ c(n, a, n′) for some action
a, and we have
f(n′)= g(n′)+ h(n′)= g(n)+ c(n, a, n′)+ h(n′)≥g(n)+ h(n)= f(n) .
The next step is to prove that whenever A∗ selects a node n for expansion, the optimal path
to that node has been found. Were this not the case, there would have to be another frontier
node n′ on the optimal path from the start node to n, by the graph separation property of
9 With an admissible but inconsistent heuristic, A∗ requires some extra bookkeeping to ensure optimality.
96 Chapter 3. Solving Problems by Searching
(a) The initial state
(b) After expanding Arad
(c) After expanding Sibiu
Arad
Sibiu Timisoara
447=118+329
Zerind
449=75+374393=140+253
Arad
366=0+366
(d) After expanding Rimnicu Vilcea
(e) After expanding Fagaras
(f) After expanding Pitesti
Zerind
Arad
Sibiu
Arad
Timisoara
Rimnicu VilceaFagaras Oradea
447=118+329 449=75+374
646=280+366 413=220+193415=239+176 671=291+380
Zerind
Arad
Sibiu Timisoara
447=118+329 449=75+374
Rimnicu Vilcea
Craiova Pitesti Sibiu
526=366+160 553=300+253417=317+100
Zerind
Arad
Sibiu
Arad
Timisoara
Sibiu Bucharest
Fagaras Oradea
Craiova Pitesti Sibiu
447=118+329 449=75+374
646=280+366
591=338+253 450=450+0 526=366+160 553=300+253417=317+100
671=291+380
Zerind
Arad
Sibiu
Arad
Timisoara
Sibiu Bucharest
Oradea
Craiova Pitesti Sibiu
Bucharest Craiova Rimnicu Vilcea
418=418+0
447=118+329 449=75+374
646=280+366
591=338+253 450=450+0 526=366+160 553=300+253
615=455+160 607=414+193
671=291+380
Rimnicu Vilcea
Fagaras Rimnicu Vilcea
Arad Fagaras Oradea
646=280+366 415=239+176 671=291+380
Figure 3.24 Stages in an A∗ search for Bucharest. Nodes are labeled with f = g +h.T h e
h values are the straight-line distances to Bucharest taken from Figure 3.22.

Section 3.5. Informed (Heuristic) Search Strategies 97
O
Z
A
T
L
M
D
C
R
F
P
G
B
U
H
E
V
I
N
380
400
420
S
Figure 3.25 Map of Romania showing contours at f = 380, f = 400,a n df = 420, with
Arad as the start state. Nodes inside a given contour have f-costs less than or equal to the
contour value.
Figure 3.9; because f is nondecreasing along any path, n′ would have lower f-cost than n
and would have been selected ﬁrst.
From the two preceding observations, it follows that the sequence of nodes expanded
by A∗ using GRAPH -SEARCH is in nondecreasing order of f(n). Hence, the ﬁrst goal node
selected for expansion must be an optimal solution because f is the true cost for goal nodes
(which have h=0 ) and all later goal nodes will be at least as expensive.
The fact that f-costs are nondecreasing along any path also means that we can draw
contours in the state space, just like the contours in a topographic map. Figure 3.25 showsCONTOUR
an example. Inside the contour labeled 400, all nodes have f(n) less than or equal to 400,
and so on. Then, because A ∗ expands the frontier node of lowest f-cost, we can see that an
A∗ search fans out from the start node, adding nodes in concentric bands of increasingf-cost.
With uniform-cost search (A ∗ search using h(n)=0 ), the bands will be “circular”
around the start state. With more accurate heuristics, the bands will stretch toward the goal
state and become more narrowly focused around the optimal path. If C∗ is the cost of the
optimal solution path, then we can say the following:
•A∗ expands all nodes with f(n) <C ∗.
•A∗ might then expand some of the nodes right on the “goal contour” (wheref(n)= C∗)
before selecting a goal node.
Completeness requires that there be only ﬁnitely many nodes with cost less than or equal to
C∗, a condition that is true if all step costs exceed some ﬁnite ϵ and if b is ﬁnite.
Notice that A ∗ expands no nodes with f(n) >C ∗—for example, Timisoara is not
expanded in Figure 3.24 even though it is a child of the root. We say that the subtree below
98 Chapter 3. Solving Problems by Searching
Timisoara is pruned; becausehSLD is admissible, the algorithm can safely ignore this subtreePRUNING
while still guaranteeing optimality. The concept of pruning—eliminating possibilities from
consideration without having to examine them—is important for many areas of AI.
One ﬁnal observation is that among optimal algorithms of this type—algorithms that
extend search paths from the root and use the same heuristic information—A ∗ is optimally
efﬁcient for any given consistent heuristic. That is, no other optimal algorithm is guaran-OPTIMALL Y
EFFICIENT
teed to expand fewer nodes than A∗ (except possibly through tie-breaking among nodes with
f(n)= C∗). This is because any algorithm that does not expand all nodes with f(n) <C ∗
runs the risk of missing the optimal solution.
That A∗ search is complete, optimal, and optimally efﬁcient among all such algorithms
is rather satisfying. Unfortunately, it does not mean that A∗ is the answer to all our searching
needs. The catch is that, for most problems, the number of states within the goal contour
search space is still exponential in the length of the solution. The details of the analysis are
beyond the scope of this book, but the basic results are as follows. For problems with constant
step costs, the growth in run time as a function of the optimal solution depth d is analyzed in
terms of the the absolute error or the relative error of the heuristic. The absolute error is
ABSOLUTE ERROR
RELA TIVE ERROR deﬁned as Δ ≡h∗−h,w h e r eh∗ is the actual cost of getting from the root to the goal, and
the relative error is deﬁned as ϵ≡(h∗−h)/h∗.
The complexity results depend very strongly on the assumptions made about the state
space. The simplest model studied is a state space that has a single goal and is essentially a
tree with reversible actions. (The 8-puzzle satisﬁes the ﬁrst and third of these assumptions.)
In this case, the time complexity of A
∗ is exponential in the maximum absolute error, that is,
O(bΔ ). For constant step costs, we can write this as O(bϵd),w h e r ed is the solution depth.
For almost all heuristics in practical use, the absolute error is at least proportional to the path
cost h
∗,s o ϵ is constant or growing and the time complexity is exponential in d. We can
also see the effect of a more accurate heuristic:O(bϵd)= O((bϵ)d), so the effective branching
factor (deﬁned more formally in the next section) is bϵ.
When the state space has many goal states—particularly near-optimal goal states—the
search process can be led astray from the optimal path and there is an extra cost proportional
to the number of goals whose cost is within a factor ϵ of the optimal cost. Finally, in the
general case of a graph, the situation is even worse. There can be exponentially many states
with f(n) <C
∗ even if the absolute error is bounded by a constant. For example, consider
a version of the vacuum world where the agent can clean up any square for unit cost without
even having to visit it: in that case, squares can be cleaned in any order. WithN initially dirty
squares, there are 2
N states where some subset has been cleaned and all of them are on an
optimal solution path—and hence satisfy f(n) <C ∗—even if the heuristic has an error of 1.
The complexity of A∗ often makes it impractical to insist on ﬁnding an optimal solution.
One can use variants of A ∗ that ﬁnd suboptimal solutions quickly, or one can sometimes
design heuristics that are more accurate but not strictly admissible. In any case, the use of a
good heuristic still provides enormous savings compared to the use of an uninformed search.
In Section 3.6, we look at the question of designing good heuristics.
Computation time is not, however, A∗’s main drawback. Because it keeps all generated
nodes in memory (as do all G RAPH -SEARCH algorithms), A∗ usually runs out of space long
Section 3.5. Informed (Heuristic) Search Strategies 99
function RECURSIVE -BEST-FIRST -SEARCH (problem) returns a solution, or failure
return RBFS( problem,M AKE -NODE (problem.INITIAL -STATE),∞)
function RBFS( problem,node,f
 limit) returns a solution, or failure and a newf-cost limit
if problem.GOAL -TEST (node.STATE) then return SOLUTION (node)
successors←[]
for each action in problem.ACTIONS (node.STATE) do
add CHILD -NODE (problem,node,action)i n t osuccessors
if successors is empty then return failure,∞
for each s in successors do /* update f with value from previous search, if any */
s.f←max(s.g + s.h, node.f ))
loop do
best←the lowest f-value node in successors
if best.f > f
 limit then return failure, best.f
alternative←the second-lowest f-value among successors
result,best.f←RBFS( problem,best,min(f
 limit,alternative))
if result̸= failure then return result
Figure 3.26 The algorithm for recursive best-ﬁrst search.
before it runs out of time. For this reason, A ∗ is not practical for many large-scale prob-
lems. There are, however, algorithms that overcome the space problem without sacriﬁcing
optimality or completeness, at a small cost in execution time. We discuss these next.
3.5.3 Memory-bounded heuristic search
The simplest way to reduce memory requirements for A ∗ is to adapt the idea of iterative
deepening to the heuristic search context, resulting in the iterative-deepening A∗ (IDA∗)a l -
ITERA TIVE-
DEEPENING
A∗
gorithm. The main difference between IDA∗ and standard iterative deepening is that the cutoff
used is the f-cost (g +h) rather than the depth; at each iteration, the cutoff value is the small-
est f-cost of any node that exceeded the cutoff on the previous iteration. IDA ∗ is practical
for many problems with unit step costs and avoids the substantial overhead associated with
keeping a sorted queue of nodes. Unfortunately, it suffers from the same difﬁculties with real-
valued costs as does the iterative version of uniform-cost search described in Exercise 3.17.
This section brieﬂy examines two other memory-bounded algorithms, called RBFS and MA
∗.
Recursive best-ﬁrst search (RBFS) is a simple recursive algorithm that attempts toRECURSIVE
BEST -FIRST SEARCH
mimic the operation of standard best-ﬁrst search, but using only linear space. The algorithm
is shown in Figure 3.26. Its structure is similar to that of a recursive depth-ﬁrst search, but
rather than continuing indeﬁnitely down the current path, it uses the f
 limit variable to keep
track of the f-value of the best alternative path available from any ancestor of the current
node. If the current node exceeds this limit, the recursion unwinds back to the alternative
path. As the recursion unwinds, RBFS replaces the f-value of each node along the path
with a backed-up value—the best f-value of its children. In this way, RBFS remembers theBACKED-UP VALUE
f-value of the best leaf in the forgotten subtree and can therefore decide whether it’s worth
100 Chapter 3. Solving Problems by Searching
Zerind
Arad
Sibiu
Arad Fagaras Oradea
Craiova Sibiu
Bucharest Craiova Rimnicu Vilcea
Zerind
Arad
Sibiu
Arad
Sibiu Bucharest
Rimnicu VilceaOradea
Zerind
Arad
Sibiu
Arad
Timisoara
Timisoara
Timisoara
Fagaras Oradea Rimnicu Vilcea
Craiova Pitesti Sibiu
646 415 671
526 553
646 671
450591
646 671
526 553
418 615 607
447 449
447
447 449
449
366
393
366
393
413
413 417415
366
393
415 450 417
Rimnicu Vilcea
Fagaras
447
415
447
447
417
(a) After expanding Arad, Sibiu, 
      and Rimnicu Vilcea
(c) After switching back to Rimnicu Vilcea
      and expanding Pitesti
(b) After unwinding back to Sibiu 
      and expanding Fagaras
447
447
∞
∞
∞
417
417
Pitesti
Figure 3.27 Stages in an RBFS search for the shortest route to Bucharest. The f-limit
value for each recursive call is shown on top of each current node, and every node is labeled
with its f-cost. (a) The path via Rimnicu Vilcea is followed until the current best leaf (Pitesti)
has a value that is worse than the best alternative path (Fagaras). (b) The recursion unwinds
and the best leaf value of the forgotten subtree (417) is backed up to Rimnicu Vilcea; then
Fagaras is expanded, revealing a best leaf value of 450. (c) The recursion unwinds and the
best leaf value of the forgotten subtree (450) is backed up to Fagaras; then Rimnicu Vilcea is
expanded. This time, because the best alternative path (through Timisoara) costs at least 447,
the expansion continues to Bucharest.
reexpanding the subtree at some later time. Figure 3.27 shows how RBFS reaches Bucharest.
RBFS is somewhat more efﬁcient than IDA ∗, but still suffers from excessive node re-
generation. In the example in Figure 3.27, RBFS follows the path via Rimnicu Vilcea, then
Section 3.5. Informed (Heuristic) Search Strategies 101
“changes its mind” and tries Fagaras, and then changes its mind back again. These mind
changes occur because every time the current best path is extended, its f-value is likely to
increase—h is usually less optimistic for nodes closer to the goal. When this happens, the
second-best path might become the best path, so the search has to backtrack to follow it.
Each mind change corresponds to an iteration of IDA
∗ and could require many reexpansions
of forgotten nodes to recreate the best path and extend it one more node.
Like A∗ tree search, RBFS is an optimal algorithm if the heuristic function h(n) is
admissible. Its space complexity is linear in the depth of the deepest optimal solution, but
its time complexity is rather difﬁcult to characterize: it depends both on the accuracy of the
heuristic function and on how often the best path changes as nodes are expanded.
IDA
∗ and RBFS suffer from using too little memory. Between iterations, IDA∗ retains
only a single number: the current f-cost limit. RBFS retains more information in memory,
but it uses only linear space: even if more memory were available, RBFS has no way to make
use of it. Because they forget most of what they have done, both algorithms may end up reex-
panding the same states many times over. Furthermore, they suffer the potentially exponential
increase in complexity associated with redundant paths in graphs (see Section 3.3).
It seems sensible, therefore, to use all available memory. Two algorithms that do this
are MA
∗ (memory-bounded A∗)a n d SMA∗ (simpliﬁed MA∗). SMA ∗ is—well—simpler, soMA*
SMA* we will describe it. SMA∗ proceeds just like A∗, expanding the best leaf until memory is full.
At this point, it cannot add a new node to the search tree without dropping an old one. SMA∗
always drops the worst leaf node—the one with the highest f-value. Like RBFS, SMA ∗
then backs up the value of the forgotten node to its parent. In this way, the ancestor of a
forgotten subtree knows the quality of the best path in that subtree. With this information,
SMA
∗ regenerates the subtree only when all other paths have been shown to look worse than
the path it has forgotten. Another way of saying this is that, if all the descendants of a noden
are forgotten, then we will not know which way to go from n, but we will still have an idea
of how worthwhile it is to go anywhere from n.
The complete algorithm is too complicated to reproduce here,10 but there is one subtlety
worth mentioning. We said that SMA∗ expands the best leaf and deletes the worst leaf. What
if all the leaf nodes have the same f-value? To avoid selecting the same node for deletion
and expansion, SMA∗ expands the newest best leaf and deletes the oldest worst leaf. These
coincide when there is only one leaf, but in that case, the current search tree must be a single
path from root to leaf that ﬁlls all of memory. If the leaf is not a goal node, theneven if it is on
an optimal solution path, that solution is not reachable with the available memory. Therefore,
the node can be discarded exactly as if it had no successors.
SMA
∗ is complete if there is any reachable solution—that is, if d, the depth of the
shallowest goal node, is less than the memory size (expressed in nodes). It is optimal if any
optimal solution is reachable; otherwise, it returns the best reachable solution. In practical
terms, SMA
∗ is a fairly robust choice for ﬁnding optimal solutions, particularly when the state
space is a graph, step costs are not uniform, and node generation is expensive compared to
the overhead of maintaining the frontier and the explored set.
10 A rough sketch appeared in the ﬁrst edition of this book.
102 Chapter 3. Solving Problems by Searching
On very hard problems, however, it will often be the case that SMA∗ is forced to switch
back and forth continually among many candidate solution paths, only a small subset of which
can ﬁt in memory. (This resembles the problem of thrashing in disk paging systems.) Then
THRASHING
the extra time required for repeated regeneration of the same nodes means that problems
that would be practically solvable by A ∗, given unlimited memory, become intractable for
SMA∗. That is to say, memory limitations can make a problem intractable from the point
of view of computation time. Although no current theory explains the tradeoff between time
and memory, it seems that this is an inescapable problem. The only way out is to drop the
optimality requirement.
3.5.4 Learning to search better
We have presented several ﬁxed strategies—breadth-ﬁrst, greedy best-ﬁrst, and so on—that
have been designed by computer scientists. Could an agent learn how to search better? The
answer is yes, and the method rests on an important concept called themetalevel state space.METALEVEL STA TE
SP ACE
Each state in a metalevel state space captures the internal (computational) state of a program
that is searching in an object-level state space such as Romania. For example, the internalOBJECT -LEVEL STA TE
SP ACE
state of the A∗ algorithm consists of the current search tree. Each action in the metalevel state
space is a computation step that alters the internal state; for example, each computation step
in A
∗ expands a leaf node and adds its successors to the tree. Thus, Figure 3.24, which shows
a sequence of larger and larger search trees, can be seen as depicting a path in the metalevel
state space where each state on the path is an object-level search tree.
Now, the path in Figure 3.24 has ﬁve steps, including one step, the expansion of Fagaras,
that is not especially helpful. For harder problems, there will be many such missteps, and a
metalevel learning algorithm can learn from these experiences to avoid exploring unpromis-
METALEVEL
LEARNING
ing subtrees. The techniques used for this kind of learning are described in Chapter 21. The
goal of learning is to minimize the total cost of problem solving, trading off computational
expense and path cost.
3.6 H EURISTIC FUNCTIONS
In this section, we look at heuristics for the 8-puzzle, in order to shed light on the nature of
heuristics in general.
The 8-puzzle was one of the earliest heuristic search problems. As mentioned in Sec-
tion 3.2, the object of the puzzle is to slide the tiles horizontally or vertically into the empty
space until the conﬁguration matches the goal conﬁguration (Figure 3.28).
The average solution cost for a randomly generated 8-puzzle instance is about 22 steps.
The branching factor is about 3. (When the empty tile is in the middle, four moves are
possible; when it is in a corner, two; and when it is along an edge, three.) This means
that an exhaustive tree search to depth 22 would look at about 3
22 ≈3.1× 1010 states.
A graph search would cut this down by a factor of about 170,000 because only 9!/2=
181,440 distinct states are reachable. (See Exercise 3.4.) This is a manageable number, but
Section 3.6. Heuristic Functions 103
2
Start State Goal State
1
3 4
6 7
5
1
2
3
4
6
7
8
5
8
Figure 3.28 A typical instance of the 8-puzzle. The solution is 26 steps long.
the corresponding number for the 15-puzzle is roughly 1013, so the next order of business is
to ﬁnd a good heuristic function. If we want to ﬁnd the shortest solutions by using A ∗,w e
need a heuristic function that never overestimates the number of steps to the goal. There is a
long history of such heuristics for the 15-puzzle; here are two commonly used candidates:
•h
1 = the number of misplaced tiles. For Figure 3.28, all of the eight tiles are out of
position, so the start state would have h1 =8 . h1 is an admissible heuristic because it
is clear that any tile that is out of place must be moved at least once.
•h2 = the sum of the distances of the tiles from their goal positions. Because tiles
cannot move along diagonals, the distance we will count is the sum of the horizontal
and vertical distances. This is sometimes called the city block distance or Manhattan
distance. h
2 is also admissible because all any move can do is move one tile one stepMANHA TT AN
DIST ANCE
closer to the goal. Tiles 1 to 8 in the start state give a Manhattan distance of
h2 =3+1+2+2+2+3+3+2=1 8 .
As expected, neither of these overestimates the true solution cost, which is 26.
3.6.1 The effect of heuristic accuracy on performance
One way to characterize the quality of a heuristic is the effective branching factor b∗.I ft h eEFFECTIVE
BRANCHING FACTOR
total number of nodes generated by A∗ for a particular problem is N and the solution depth is
d,t h e nb∗ is the branching factor that a uniform tree of depth d w o u l dh a v et oh a v ei no r d e r
to contain N +1 nodes. Thus,
N +1=1+ b∗ +( b∗)2 +··· +( b∗)d .
For example, if A∗ ﬁnds a solution at depth 5 using 52 nodes, then the effective branching
factor is 1.92. The effective branching factor can vary across problem instances, but usually
it is fairly constant for sufﬁciently hard problems. (The existence of an effective branching
factor follows from the result, mentioned earlier, that the number of nodes expanded by A ∗
grows exponentially with solution depth.) Therefore, experimental measurements of b∗ on a
small set of problems can provide a good guide to the heuristic’s overall usefulness. A well-
designed heuristic would have a value of b∗ close to 1, allowing fairly large problems to be
solved at reasonable computational cost.
104 Chapter 3. Solving Problems by Searching
To test the heuristic functions h1 and h2, we generated 1200 random problems with
solution lengths from 2 to 24 (100 for each even number) and solved them with iterative
deepening search and with A
∗ tree search using bothh1 and h2. Figure 3.29 gives the average
number of nodes generated by each strategy and the effective branching factor. The results
suggest that h
2 is better than h1, and is far better than using iterative deepening search. Even
for small problems with d=1 2,A ∗ with h2 is 50,000 times more efﬁcient than uninformed
iterative deepening search.
Search Cost (nodes generated)
 Effective Branching Factor
d
 IDS
 A∗(h1)
 A∗(h2)
 IDS
 A∗(h1)
 A∗(h2)
2
 10
 6
 6
 2.45
 1.79
 1.79
4
 112
 13
 12
 2.87
 1.48
 1.45
6
 680
 20
 18
 2.73
 1.34
 1.30
8
 6384
 39
 25
 2.80
 1.33
 1.24
10
 47127
 93
 39
 2.79
 1.38
 1.22
12
 3644035
 227
 73
 2.78
 1.42
 1.24
14
 –
 539
 113
 –
 1.44
 1.23
16
 –
 1301
 211
 –
 1.45
 1.25
18
 –
 3056
 363
 –
 1.46
 1.26
20
 –
 7276
 676
 –
 1.47
 1.27
22
 –
 18094
 1219
 –
 1.48
 1.28
24
 –
 39135
 1641
 –
 1.48
 1.26
Figure 3.29 Comparison of the search costs and effective branching factors for the
ITERATIVE -DEEPENING -SEARCH and A∗ algorithms with h1, h2. Data are averaged over
100 instances of the 8-puzzle for each of various solution lengths d.
One might ask whether h2 is always better than h1. The answer is “Essentially, yes.” It
is easy to see from the deﬁnitions of the two heuristics that, for any node n, h2(n)≥h1(n).
We thus say that h2 dominates h1. Domination translates directly into efﬁciency: A ∗ usingDOMINA TION
h2 will never expand more nodes than A ∗ using h1 (except possibly for some nodes with
f(n)= C∗). The argument is simple. Recall the observation on page 97 that every node
with f(n) <C ∗ will surely be expanded. This is the same as saying that every node with
h(n) <C ∗−g(n) will surely be expanded. But because h2 is at least as big as h1 for all
nodes, every node that is surely expanded by A∗ search with h2 will also surely be expanded
with h1,a n d h1 might cause other nodes to be expanded as well. Hence, it is generally
better to use a heuristic function with higher values, provided it is consistent and that the
computation time for the heuristic is not too long.
3.6.2 Generating admissible heuristics from relaxed problems
We have seen that both h1 (misplaced tiles) and h2 (Manhattan distance) are fairly good
heuristics for the 8-puzzle and that h2 is better. How might one have come up with h2?I si t
possible for a computer to invent such a heuristic mechanically?
h1 and h2 are estimates of the remaining path length for the 8-puzzle, but they are also
perfectly accurate path lengths for simpliﬁed versions of the puzzle. If the rules of the puzzle
Section 3.6. Heuristic Functions 105
were changed so that a tile could move anywhere instead of just to the adjacent empty square,
then h1 would give the exact number of steps in the shortest solution. Similarly, if a tile could
move one square in any direction, even onto an occupied square, thenh2 would give the exact
number of steps in the shortest solution. A problem with fewer restrictions on the actions is
called a relaxed problem. The state-space graph of the relaxed problem is a supergraph of
RELAXED PROBLEM
the original state space because the removal of restrictions creates added edges in the graph.
Because the relaxed problem adds edges to the state space, any optimal solution in the
original problem is, by deﬁnition, also a solution in the relaxed problem; but the relaxed
problem may have better solutions if the added edges provide short cuts. Hence, the cost of
an optimal solution to a relaxed problem is an admissible heuristic for the original problem.
Furthermore, because the derived heuristic is an exact cost for the relaxed problem, it must
obey the triangle inequality and is therefore consistent (see page 95).
If a problem deﬁnition is written down in a formal language, it is possible to construct
relaxed problems automatically.
11 For example, if the 8-puzzle actions are described as
A tile can move from square A to square B if
A is horizontally or vertically adjacent to B and B is blank,
we can generate three relaxed problems by removing one or both of the conditions:
(a) A tile can move from square A to square B if A is adjacent to B.
(b) A tile can move from square A to square B if B is blank.
(c) A tile can move from square A to square B.
From (a), we can derive h
2 (Manhattan distance). The reasoning is that h2 would be the
proper score if we moved each tile in turn to its destination. The heuristic derived from (b) is
discussed in Exercise 3.31. From (c), we can derive h
1 (misplaced tiles) because it would be
the proper score if tiles could move to their intended destination in one step. Notice that it is
crucial that the relaxed problems generated by this technique can be solved essentiallywithout
search, because the relaxed rules allow the problem to be decomposed into eight independent
subproblems. If the relaxed problem is hard to solve, then the values of the corresponding
heuristic will be expensive to obtain.
12
A program called ABSOLVER can generate heuristics automatically from problem def-
initions, using the “relaxed problem” method and various other techniques (Prieditis, 1993).
ABSOLVER generated a new heuristic for the 8-puzzle that was better than any preexisting
heuristic and found the ﬁrst useful heuristic for the famous Rubik’s Cube puzzle.
One problem with generating new heuristic functions is that one often fails to get a
single “clearly best” heuristic. If a collection of admissible heuristics h1 ...h m is available
for a problem and none of them dominates any of the others, which should we choose? As it
turns out, we need not make a choice. We can have the best of all worlds, by deﬁning
h(n)=m a x{h
1(n),...,h m(n)} .
11 In Chapters 8 and 10, we describe formal languages suitable for this task; with formal descriptions that can be
manipulated, the construction of relaxed problems can be automated. For now, we use English.
12 Note that a perfect heuristic can be obtained simply by allowing h to run a full breadth-ﬁrst search “on the
sly.” Thus, there is a tradeoff between accuracy and computation time for heuristic functions.
106 Chapter 3. Solving Problems by Searching
Start State Goal State
1
2
3
4
6
8
5
21
3 6
7 8
54
Figure 3.30 A subproblem of the 8-puzzle instance given in Figure 3.28. The task is to
get tiles 1, 2, 3, and 4 into their correct positions, without worrying about what happens to
the other tiles.
This composite heuristic uses whichever function is most accurate on the node in question.
Because the component heuristics are admissible,h is admissible; it is also easy to prove that
h is consistent. Furthermore, h dominates all of its component heuristics.
3.6.3 Generating admissible heuristics from subproblems: Pattern databases
Admissible heuristics can also be derived from the solution cost of a subproblem of a givenSUBPROBLEM
problem. For example, Figure 3.30 shows a subproblem of the 8-puzzle instance in Fig-
ure 3.28. The subproblem involves getting tiles 1, 2, 3, 4 into their correct positions. Clearly,
the cost of the optimal solution of this subproblem is a lower bound on the cost of the com-
plete problem. It turns out to be more accurate than Manhattan distance in some cases.
The idea behind pattern databases is to store these exact solution costs for every pos-
P A TTERN DA TABASE
sible subproblem instance—in our example, every possible conﬁguration of the four tiles
and the blank. (The locations of the other four tiles are irrelevant for the purposes of solv-
ing the subproblem, but moves of those tiles do count toward the cost.) Then we compute
an admissible heuristic h
DB for each complete state encountered during a search simply by
looking up the corresponding subproblem conﬁguration in the database. The database itself is
constructed by searching back13 from the goal and recording the cost of each new pattern en-
countered; the expense of this search is amortized over many subsequent problem instances.
The choice of 1-2-3-4 is fairly arbitrary; we could also construct databases for 5-6-7-8,
for 2-4-6-8, and so on. Each database yields an admissible heuristic, and these heuristics can
be combined, as explained earlier, by taking the maximum value. A combined heuristic of
this kind is much more accurate than the Manhattan distance; the number of nodes generated
when solving random 15-puzzles can be reduced by a factor of 1000.
One might wonder whether the heuristics obtained from the 1-2-3-4 database and the
5-6-7-8 could be added, since the two subproblems seem not to overlap. Would this still give
an admissible heuristic? The answer is no, because the solutions of the 1-2-3-4 subproblem
and the 5-6-7-8 subproblem for a given state will almost certainly share some moves—it is
13 By working backward from the goal, the exact solution cost of every instance encountered is immediately
available. This is an example of dynamic programming, which we discuss further in Chapter 17.
Section 3.6. Heuristic Functions 107
unlikely that 1-2-3-4 can be moved into place without touching 5-6-7-8, and vice versa. But
what if we don’t count those moves? That is, we record not the total cost of solving the 1-2-
3-4 subproblem, but just the number of moves involving 1-2-3-4. Then it is easy to see that
the sum of the two costs is still a lower bound on the cost of solving the entire problem. This
is the idea behind disjoint pattern databases . With such databases, it is possible to solve
DISJOINT P A TTERN
DA TABASES
random 15-puzzles in a few milliseconds—the number of nodes generated is reduced by a
factor of 10,000 compared with the use of Manhattan distance. For 24-puzzles, a speedup of
roughly a factor of a million can be obtained.
Disjoint pattern databases work for sliding-tile puzzles because the problem can be
divided up in such a way that each move affects only one subproblem—because only one tile
is moved at a time. For a problem such as Rubik’s Cube, this kind of subdivision is difﬁcult
because each move affects 8 or 9 of the 26 cubies. More general ways of deﬁning additive,
admissible heuristics have been proposed that do apply to Rubik’s cube (Yang et al., 2008),
but they have not yielded a heuristic better than the best nonadditive heuristic for the problem.
3.6.4 Learning heuristics from experience
A heuristic function h(n) is supposed to estimate the cost of a solution beginning from the
state at node n. How could an agent construct such a function? One solution was given in
the preceding sections—namely, to devise relaxed problems for which an optimal solution
can be found easily. Another solution is to learn from experience. “Experience” here means
solving lots of 8-puzzles, for instance. Each optimal solution to an 8-puzzle problem provides
examples from which h(n) can be learned. Each example consists of a state from the solu-
tion path and the actual cost of the solution from that point. From these examples, a learning
algorithm can be used to construct a function h(n) that can (with luck) predict solution costs
for other states that arise during search. Techniques for doing just this using neural nets, de-
cision trees, and other methods are demonstrated in Chapter 18. (The reinforcement learning
methods described in Chapter 21 are also applicable.)
Inductive learning methods work best when supplied with features of a state that are
FEA TURE
relevant to predicting the state’s value, rather than with just the raw state description. For
example, the feature “number of misplaced tiles” might be helpful in predicting the actual
distance of a state from the goal. Let’s call this feature x
1(n). We could take 100 randomly
generated 8-puzzle conﬁgurations and gather statistics on their actual solution costs. We
might ﬁnd that when x
1(n) is 5, the average solution cost is around 14, and so on. Given
these data, the value ofx1 can be used to predicth(n). Of course, we can use several features.
A second featurex2(n) might be “number of pairs of adjacent tiles that are not adjacent in the
goal state.” How shouldx1(n) and x2(n) be combined to predicth(n)? A common approach
is to use a linear combination:
h(n)= c1x1(n)+ c2x2(n) .
The constants c1 and c2 are adjusted to give the best ﬁt to the actual data on solution costs.
One expects bothc1 and c2 to be positive because misplaced tiles and incorrect adjacent pairs
make the problem harder to solve. Notice that this heuristic does satisfy the condition that
h(n)=0 for goal states, but it is not necessarily admissible or consistent.
108 Chapter 3. Solving Problems by Searching
3.7 S UMMARY
This chapter has introduced methods that an agent can use to select actions in environments
that are deterministic, observable, static, and completely known. In such cases, the agent can
construct sequences of actions that achieve its goals; this process is called search.
•Before an agent can start searching for solutions, a goal must be identiﬁed and a well-
deﬁned problem must be formulated.
•A problem consists of ﬁve parts: the initial state, a set of actions,a transition model
describing the results of those actions, a goal test function, and a path cost function.
The environment of the problem is represented by a state space.A path through the
state space from the initial state to a goal state is a solution.
•Search algorithms treat states and actions as atomic: they do not consider any internal
structure they might possess.
•A general T
REE -SEARCH algorithm considers all possible paths to ﬁnd a solution,
whereas a GRAPH -SEARCH algorithm avoids consideration of redundant paths.
•Search algorithms are judged on the basis ofcompleteness, optimality, time complex-
ity,a n dspace complexity. Complexity depends on b, the branching factor in the state
space, and d, the depth of the shallowest solution.
•Uninformed search methods have access only to the problem deﬁnition. The basic
algorithms are as follows:
– Breadth-ﬁrst search expands the shallowest nodes ﬁrst; it is complete, optimal
for unit step costs, but has exponential space complexity.
– Uniform-cost search expands the node with lowest path cost,g(n), and is optimal
for general step costs.
– Depth-ﬁrst search expands the deepest unexpanded node ﬁrst. It is neither com-
plete nor optimal, but has linear space complexity. Depth-limited search adds a
depth bound.
– Iterative deepening search calls depth-ﬁrst search with increasing depth limits
until a goal is found. It is complete, optimal for unit step costs, has time complexity
comparable to breadth-ﬁrst search, and has linear space complexity.
– Bidirectional search can enormously reduce time complexity, but it is not always
applicable and may require too much space.
•Informed search methods may have access to a heuristic function h(n) that estimates
the cost of a solution from n.
– The generic best-ﬁrst search algorithm selects a node for expansion according to
an evaluation function.
– Greedy best-ﬁrst search expands nodes with minimal h(n). It is not optimal but
is often efﬁcient.
Bibliographical and Historical Notes 109
–A ∗ search expands nodes with minimal f(n)= g(n)+ h(n).A ∗ is complete and
optimal, provided that h(n) is admissible (for T REE -SEARCH ) or consistent (for
GRAPH -SEARCH ). The space complexity of A∗ is still prohibitive.
–R B F S (recursive best-ﬁrst search) and SMA∗ (simpliﬁed memory-bounded A ∗)
are robust, optimal search algorithms that use limited amounts of memory; given
enough time, they can solve problems that A
∗ cannot solve because it runs out of
memory.
•The performance of heuristic search algorithms depends on the quality of the heuristic
function. One can sometimes construct good heuristics by relaxing the problem deﬁ-
nition, by storing precomputed solution costs for subproblems in a pattern database, or
by learning from experience with the problem class.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
The topic of state-space search originated in more or less its current form in the early years of
AI. Newell and Simon’s work on the Logic Theorist (1957) and GPS (1961) led to the estab-
lishment of search algorithms as the primary weapons in the armory of 1960s AI researchers
and to the establishment of problem solving as the canonical AI task. Work in operations
research by Richard Bellman (1957) showed the importance of additive path costs in sim-
plifying optimization algorithms. The text on Automated Problem Solving by Nils Nilsson
(1971) established the area on a solid theoretical footing.
Most of the state-space search problems analyzed in this chapter have a long history
in the literature and are less trivial than they might seem. The missionaries and cannibals
problem used in Exercise 3.9 was analyzed in detail by Amarel (1968). It had been consid-
ered earlier—in AI by Simon and Newell (1961) and in operations research by Bellman and
Dreyfus (1962).
The 8-puzzle is a smaller cousin of the 15-puzzle, whose history is recounted at length
by Slocum and Sonneveld (2006). It was widely believed to have been invented by the fa-
mous American game designer Sam Loyd, based on his claims to that effect from 1891 on-
ward (Loyd, 1959). Actually it was invented by Noyes Chapman, a postmaster in Canastota,
New York, in the mid-1870s. (Chapman was unable to patent his invention, as a generic
patent covering sliding blocks with letters, numbers, or pictures was granted to Ernest Kinsey
in 1878.) It quickly attracted the attention of the public and of mathematicians (Johnson and
Story, 1879; Tait, 1880). The editors of the American Journal of Mathematics stated, “The
‘15’ puzzle for the last few weeks has been prominently before the American public, and may
safely be said to have engaged the attention of nine out of ten persons of both sexes and all
ages and conditions of the community.” Ratner and Warmuth (1986) showed that the general
n× n version of the 15-puzzle belongs to the class of NP-complete problems.
The 8-queens problem was ﬁrst published anonymously in the German chess maga-
zine Schach in 1848; it was later attributed to one Max Bezzel. It was republished in 1850
and at that time drew the attention of the eminent mathematician Carl Friedrich Gauss, who
110 Chapter 3. Solving Problems by Searching
attempted to enumerate all possible solutions; initially he found only 72, but eventually he
found the correct answer of 92, although Nauck published all 92 solutions ﬁrst, in 1850.
Netto (1901) generalized the problem to n queens, and Abramson and Yung (1989) found an
O(n) algorithm.
Each of the real-world search problems listed in the chapter has been the subject of a
good deal of research effort. Methods for selecting optimal airline ﬂights remain proprietary
for the most part, but Carl de Marcken (personal communication) has shown that airline ticket
pricing and restrictions have become so convoluted that the problem of selecting an optimal
ﬂight is formally undecidable. The traveling-salesperson problem is a standard combinato-
rial problem in theoretical computer science (Lawler et al., 1992). Karp (1972) proved the
TSP to be NP-hard, but effective heuristic approximation methods were developed (Lin and
Kernighan, 1973). Arora (1998) devised a fully polynomial approximation scheme for Eu-
clidean TSPs. VLSI layout methods are surveyed by Shahookar and Mazumder (1991), and
many layout optimization papers appear in VLSI journals. Robotic navigation and assembly
problems are discussed in Chapter 25.
Uninformed search algorithms for problem solving are a central topic of classical com-
puter science (Horowitz and Sahni, 1978) and operations research (Dreyfus, 1969). Breadth-
ﬁrst search was formulated for solving mazes by Moore (1959). The method of dynamic
programming (Bellman, 1957; Bellman and Dreyfus, 1962), which systematically records
solutions for all subproblems of increasing lengths, can be seen as a form of breadth-ﬁrst
search on graphs. The two-point shortest-path algorithm of Dijkstra (1959) is the origin
of uniform-cost search. These works also introduced the idea of explored and frontier sets
(closed and open lists).
A version of iterative deepening designed to make efﬁcient use of the chess clock was
ﬁrst used by Slate and Atkin (1977) in the C
HESS 4.5 game-playing program. Martelli’s
algorithm B (1977) includes an iterative deepening aspect and also dominates A∗’s worst-case
performance with admissible but inconsistent heuristics. The iterative deepening technique
came to the fore in work by Korf (1985a). Bidirectional search, which was introduced by
Pohl (1971), can also be effective in some cases.
The use of heuristic information in problem solving appears in an early paper by Simon
and Newell (1958), but the phrase “heuristic search” and the use of heuristic functions that
estimate the distance to the goal came somewhat later (Newell and Ernst, 1965; Lin, 1965).
Doran and Michie (1966) conducted extensive experimental studies of heuristic search. Al-
though they analyzed path length and “penetrance” (the ratio of path length to the total num-
ber of nodes examined so far), they appear to have ignored the information provided by the
path cost g(n).T h e A
∗ algorithm, incorporating the current path cost into heuristic search,
was developed by Hart, Nilsson, and Raphael (1968), with some later corrections (Hartet al.,
1972). Dechter and Pearl (1985) demonstrated the optimal efﬁciency of A∗.
The original A∗ paper introduced the consistency condition on heuristic functions. The
monotone condition was introduced by Pohl (1977) as a simpler replacement, but Pearl (1984)
showed that the two were equivalent.
Pohl (1977) pioneered the study of the relationship between the error in heuristic func-
tions and the time complexity of A∗. Basic results were obtained for tree search with unit step
Bibliographical and Historical Notes 111
costs and a single goal node (Pohl, 1977; Gaschnig, 1979; Huynet al., 1980; Pearl, 1984) and
with multiple goal nodes (Dinh et al., 2007). The “effective branching factor” was proposed
by Nilsson (1971) as an empirical measure of the efﬁciency; it is equivalent to assuming a
time cost ofO((b
∗)d). For tree search applied to a graph, Korfet al. (2001) argue that the time
cost is better modeled as O(bd− k),w h e r ek depends on the heuristic accuracy; this analysis
has elicited some controversy, however. For graph search, Helmert and R¨oger (2008) noted
that several well-known problems contained exponentially many nodes on optimal solution
paths, implying exponential time complexity for A
∗ even with constant absolute error in h.
There are many variations on the A∗ algorithm. Pohl (1973) proposed the use ofdynamic
weighting, which uses a weighted sum fw(n)= wgg(n)+ whh(n) of the current path length
and the heuristic function as an evaluation function, rather than the simple sumf(n)= g(n)+
h(n) used in A∗. The weights wg and wh are adjusted dynamically as the search progresses.
Pohl’s algorithm can be shown to beϵ-admissible—that is, guaranteed to ﬁnd solutions within
a factor 1+ ϵ of the optimal solution, where ϵ is a parameter supplied to the algorithm. The
same property is exhibited by the A∗
ϵ algorithm (Pearl, 1984), which can select any node from
the frontier provided itsf-cost is within a factor1+ ϵ of the lowest-f-cost frontier node. The
selection can be done so as to minimize search cost.
Bidirectional versions of A∗ have been investigated; a combination of bidirectional A∗
and known landmarks was used to efﬁciently ﬁnd driving routes for Microsoft’s online map
service (Goldberg et al., 2006). After caching a set of paths between landmarks, the algorithm
can ﬁnd an optimal path between any pair of points in a 24 million point graph of the United
States, searching less than 0.1% of the graph. Others approaches to bidirectional search
include a breadth-ﬁrst search backward from the goal up to a ﬁxed depth, followed by a
forward IDA
∗ search (Dillenburg and Nelson, 1994; Manzini, 1995).
A∗ and other state-space search algorithms are closely related to the branch-and-bound
techniques that are widely used in operations research (Lawler and Wood, 1966). The
relationships between state-space search and branch-and-bound have been investigated in
depth (Kumar and Kanal, 1983; Nau et al., 1984; Kumar et al., 1988). Martelli and Monta-
nari (1978) demonstrate a connection between dynamic programming (see Chapter 17) and
certain types of state-space search. Kumar and Kanal (1988) attempt a “grand uniﬁcation” of
heuristic search, dynamic programming, and branch-and-bound techniques under the name
of CDP—the “composite decision process.”
Because computers in the late 1950s and early 1960s had at most a few thousand words
of main memory, memory-bounded heuristic search was an early research topic. The Graph
Traverser (Doran and Michie, 1966), one of the earliest search programs, commits to an
operator after searching best-ﬁrst up to the memory limit. IDA
∗ (Korf, 1985a, 1985b) was the
ﬁrst widely used optimal, memory-bounded heuristic search algorithm, and a large number
of variants have been developed. An analysis of the efﬁciency of IDA
∗ and of its difﬁculties
with real-valued heuristics appears in Patrick et al. (1992).
RBFS (Korf, 1993) is actually somewhat more complicated than the algorithm shown
in Figure 3.26, which is closer to an independently developed algorithm called iterative ex-
pansion (Russell, 1992). RBFS uses a lower bound as well as the upper bound; the two al-ITERA TIVE
EXP ANSION
gorithms behave identically with admissible heuristics, but RBFS expands nodes in best-ﬁrst
112 Chapter 3. Solving Problems by Searching
order even with an inadmissible heuristic. The idea of keeping track of the best alternative
path appeared earlier in Bratko’s (1986) elegant Prolog implementation of A∗ and in the DTA∗
algorithm (Russell and Wefald, 1991). The latter work also discusses metalevel state spaces
and metalevel learning.
The MA
∗ algorithm appeared in Chakrabarti et al. (1989). SMA∗, or Simpliﬁed MA∗,
emerged from an attempt to implement MA∗ as a comparison algorithm for IE (Russell, 1992).
Kaindl and Khorsand (1994) have applied SMA∗ to produce a bidirectional search algorithm
that is substantially faster than previous algorithms. Korf and Zhang (2000) describe a divide-
and-conquer approach, and Zhou and Hansen (2002) introduce memory-bounded A
∗ graph
search and a strategy for switching to breadth-ﬁrst search to increase memory-efﬁciency
(Zhou and Hansen, 2006). Korf (1995) surveys memory-bounded search techniques.
The idea that admissible heuristics can be derived by problem relaxation appears in the
seminal paper by Held and Karp (1970), who used the minimum-spanning-tree heuristic to
solve the TSP. (See Exercise 3.30.)
The automation of the relaxation process was implemented successfully by Priedi-
tis (1993), building on earlier work with Mostow (Mostow and Prieditis, 1989). Holte and
Hernadvolgyi (2001) describe more recent steps towards automating the process. The use of
pattern databases to derive admissible heuristics is due to Gasser (1995) and Culberson and
Schaeffer (1996, 1998); disjoint pattern databases are described by Korf and Felner (2002);
a similar method using symbolic patterns is due to Edelkamp (2009). Felner et al. (2007)
show how to compress pattern databases to save space. The probabilistic interpretation of
heuristics was investigated in depth by Pearl (1984) and Hansson and Mayer (1989).
By far the most comprehensive source on heuristics and heuristic search algorithms
is Pearl’s (1984) Heuristics text. This book provides especially good coverage of the wide
variety of offshoots and variations of A
∗, including rigorous proofs of their formal properties.
Kanal and Kumar (1988) present an anthology of important articles on heuristic search, and
Rayward-Smith et al. (1996) cover approaches from Operations Research. Papers about new
search algorithms—which, remarkably, continue to be discovered—appear in journals such
as Artiﬁcial Intelligence and Journal of the ACM.
The topic of parallel search algorithms was not covered in the chapter, partly because
P ARALLEL SEARCH
it requires a lengthy discussion of parallel computer architectures. Parallel search became a
popular topic in the 1990s in both AI and theoretical computer science (Mahanti and Daniels,
1993; Grama and Kumar, 1995; Crauser et al., 1998) and is making a comeback in the era
of new multicore and cluster architectures (Ralphs et al. , 2004; Korf and Schultze, 2005).
Also of increasing importance are search algorithms for very large graphs that require disk
storage (Korf, 2008).
EXERCISES
3.1 Explain why problem formulation must follow goal formulation.
3.2 Your goal is to navigate a robot out of a maze. The robot starts in the center of the maze
Exercises 113
facing north. You can turn the robot to face north, east, south, or west. You can direct the
robot to move forward a certain distance, although it will stop before hitting a wall.
a. Formulate this problem. How large is the state space?
b. In navigating a maze, the only place we need to turn is at the intersection of two or
more corridors. Reformulate this problem using this observation. How large is the state
space now?
c. From each point in the maze, we can move in any of the four directions until we reach a
turning point, and this is the only action we need to do. Reformulate the problem using
these actions. Do we need to keep track of the robot’s orientation now?
d. In our initial description of the problem we already abstracted from the real world,
restricting actions and removing details. List three such simpliﬁcations we made.
3.3 Suppose two friends live in different cities on a map, such as the Romania map shown
in Figure 3.2. On every turn, we can simultaneously move each friend to a neighboring city
on the map. The amount of time needed to move from cityi to neighbor j is equal to the road
distance d(i, j) between the cities, but on each turn the friend that arrives ﬁrst must wait until
the other one arrives (and calls the ﬁrst on his/her cell phone) before the next turn can begin.
We want the two friends to meet as quickly as possible.
a. Write a detailed formulation for this search problem. (You will ﬁnd it helpful to deﬁne
some formal notation here.)
b.L e t D(i, j) be the straight-line distance between cities i and j. Which of the following
heuristic functions are admissible? (i) D(i, j); (ii) 2· D(i, j); (iii) D(i, j)/2.
c. Are there completely connected maps for which no solution exists?
d. Are there maps in which all solutions require one friend to visit the same city twice?
3.4 Show that the 8-puzzle states are divided into two disjoint sets, such that any state is
reachable from any other state in the same set, while no state is reachable from any state in
the other set. ( Hint: See Berlekamp et al. (1982).) Devise a procedure to decide which set a
given state is in, and explain why this is useful for generating random states.
3.5 Consider the n-queens problem using the “efﬁcient” incremental formulation given on
page 72. Explain why the state space has at least
3√
n! states and estimate the largest n for
which exhaustive exploration is feasible. (Hint: Derive a lower bound on the branching factor
by considering the maximum number of squares that a queen can attack in any column.)
3.6 Give a complete problem formulation for each of the following. Choose a formulation
that is precise enough to be implemented.
a. Using only four colors, you have to color a planar map in such a way that no two
adjacent regions have the same color.
b. A 3-foot-tall monkey is in a room where some bananas are suspended from the 8-foot
ceiling. He would like to get the bananas. The room contains two stackable, movable,
climbable 3-foot-high crates.
114 Chapter 3. Solving Problems by Searching
S
G
Figure 3.31 A scene with polygonal obstacles. S and G are the start and goal states.
c. You have a program that outputs the message “illegal input record” when fed a certain
ﬁle of input records. You know that processing of each record is independent of the
other records. You want to discover what record is illegal.
d. You have three jugs, measuring 12 gallons, 8 gallons, and 3 gallons, and a water faucet.
You can ﬁll the jugs up or empty them out from one to another or onto the ground. You
need to measure out exactly one gallon.
3.7 Consider the problem of ﬁnding the shortest path between two points on a plane that has
convex polygonal obstacles as shown in Figure 3.31. This is an idealization of the problem
that a robot has to solve to navigate in a crowded environment.
a. Suppose the state space consists of all positions (x, y) in the plane. How many states
are there? How many paths are there to the goal?
b. Explain brieﬂy why the shortest path from one polygon vertex to any other in the scene
must consist of straight-line segments joining some of the vertices of the polygons.
Deﬁne a good state space now. How large is this state space?
c. Deﬁne the necessary functions to implement the search problem, including an A
CTIONS
function that takes a vertex as input and returns a set of vectors, each of which maps the
current vertex to one of the vertices that can be reached in a straight line. (Do not forget
the neighbors on the same polygon.) Use the straight-line distance for the heuristic
function.
d. Apply one or more of the algorithms in this chapter to solve a range of problems in the
domain, and comment on their performance.
3.8 On page 68, we said that we would not consider problems with negative path costs. In
this exercise, we explore this decision in more depth.
a. Suppose that actions can have arbitrarily large negative costs; explain why this possi-
bility would force any optimal algorithm to explore the entire state space.
Exercises 115
b. Does it help if we insist that step costs must be greater than or equal to some negative
constant c? Consider both trees and graphs.
c. Suppose that a set of actions forms a loop in the state space such that executing the set in
some order results in no net change to the state. If all of these actions have negative cost,
what does this imply about the optimal behavior for an agent in such an environment?
d. One can easily imagine actions with high negative cost, even in domains such as route
ﬁnding. For example, some stretches of road might have such beautiful scenery as to
far outweigh the normal costs in terms of time and fuel. Explain, in precise terms,
within the context of state-space search, why humans do not drive around scenic loops
indeﬁnitely, and explain how to deﬁne the state space and actions for route ﬁnding so
that artiﬁcial agents can also avoid looping.
e. Can you think of a real domain in which step costs are such as to cause looping?
3.9 The missionaries and cannibals problem is usually stated as follows. Three mission-
aries and three cannibals are on one side of a river, along with a boat that can hold one or
two people. Find a way to get everyone to the other side without ever leaving a group of mis-
sionaries in one place outnumbered by the cannibals in that place. This problem is famous in
AI because it was the subject of the ﬁrst paper that approached problem formulation from an
analytical viewpoint (Amarel, 1968).
a. Formulate the problem precisely, making only those distinctions necessary to ensure a
valid solution. Draw a diagram of the complete state space.
b. Implement and solve the problem optimally using an appropriate search algorithm. Is it
a good idea to check for repeated states?
c. Why do you think people have a hard time solving this puzzle, given that the state space
is so simple?
3.10 Deﬁne in your own words the following terms: state, state space, search tree, search
node, goal, action, transition model, and branching factor.
3.11 What’s the difference between a world state, a state description, and a search node?
Why is this distinction useful?
3.12 An action such asGo(Sibiu) really consists of a long sequence of ﬁner-grained actions:
turn on the car, release the brake, accelerate forward, etc. Having composite actions of this
kind reduces the number of steps in a solution sequence, thereby reducing the search time.
Suppose we take this to the logical extreme, by making super-composite actions out of every
possible sequence of Go actions. Then every problem instance is solved by a single super-
composite action, such as Go(Sibiu)Go(Rimnicu Vilcea)Go(Pitesti)Go(Bucharest). Explain
how search would work in this formulation. Is this a practical approach for speeding up
problem solving?
3.13 Prove that G
RAPH -SEARCH satisﬁes the graph separation property illustrated in Fig-
ure 3.9. (Hint: Begin by showing that the property holds at the start, then show that if it holds
before an iteration of the algorithm, it holds afterwards.) Describe a search algorithm that
violates the property.
116 Chapter 3. Solving Problems by Searching
x 12
x 16
x 2 x 2
Figure 3.32 The track pieces in a wooden railway set; each is labeled with the number of
copies in the set. Note that curved pieces and “ fork” pieces (“switches” or “points”) can be
ﬂipped over so they can curve in either direction. Each curve subtends 45 degrees.
3.14 Which of the following are true and which are false? Explain your answers.
a. Depth-ﬁrst search always expands at least as many nodes as A∗ search with an admissi-
ble heuristic.
b. h(n)=0 is an admissible heuristic for the 8-puzzle.
c.A ∗ is of no use in robotics because percepts, states, and actions are continuous.
d. Breadth-ﬁrst search is complete even if zero step costs are allowed.
e. Assume that a rook can move on a chessboard any number of squares in a straight line,
vertically or horizontally, but cannot jump over other pieces. Manhattan distance is an
admissible heuristic for the problem of moving the rook from square A to square B in
the smallest number of moves.
3.15 Consider a state space where the start state is number 1 and each state k has two
successors: numbers 2k and 2k +1 .
a. Draw the portion of the state space for states 1 to 15.
b. Suppose the goal state is 11. List the order in which nodes will be visited for breadth-
ﬁrst search, depth-limited search with limit 3, and iterative deepening search.
c. How well would bidirectional search work on this problem? What is the branching
factor in each direction of the bidirectional search?
d. Does the answer to (c) suggest a reformulation of the problem that would allow you to
solve the problem of getting from state 1 to a given goal state with almost no search?
e. Call the action going from k to 2k Left, and the action going to 2k +1 Right. Can you
ﬁnd an algorithm that outputs the solution to this problem without any search at all?
3.16 A basic wooden railway set contains the pieces shown in Figure 3.32. The task is to
connect these pieces into a railway that has no overlapping tracks and no loose ends where a
train could run off onto the ﬂoor.
a. Suppose that the pieces ﬁt together exactly with no slack. Give a precise formulation of
the task as a search problem.
b. Identify a suitable uninformed search algorithm for this task and explain your choice.
c. Explain why removing any one of the “fork” pieces makes the problem unsolvable.
Exercises 117
d. Give an upper bound on the total size of the state space deﬁned by your formulation.
(Hint: think about the maximum branching factor for the construction process and the
maximum depth, ignoring the problem of overlapping pieces and loose ends. Begin by
pretending that every piece is unique.)
3.17 On page 90, we mentioned iterative lengthening search , an iterative analog of uni-
form cost search. The idea is to use increasing limits on path cost. If a node is generated
whose path cost exceeds the current limit, it is immediately discarded. For each new itera-
tion, the limit is set to the lowest path cost of any node discarded in the previous iteration.
a. Show that this algorithm is optimal for general path costs.
b. Consider a uniform tree with branching factor b, solution depth d, and unit step costs.
How many iterations will iterative lengthening require?
c. Now consider step costs drawn from the continuous range[ϵ,1],w h e r e0 <ϵ< 1.H o w
many iterations are required in the worst case?
d. Implement the algorithm and apply it to instances of the 8-puzzle and traveling sales-
person problems. Compare the algorithm’s performance to that of uniform-cost search,
and comment on your results.
3.18 Describe a state space in which iterative deepening search performs much worse than
depth-ﬁrst search (for example, O(n
2) vs. O(n)).
3.19 Write a program that will take as input two Web page URLs and ﬁnd a path of links
from one to the other. What is an appropriate search strategy? Is bidirectional search a good
idea? Could a search engine be used to implement a predecessor function?
3.20 Consider the vacuum-world problem deﬁned in Figure 2.2.
a. Which of the algorithms deﬁned in this chapter would be appropriate for this problem?
Should the algorithm use tree search or graph search?
b. Apply your chosen algorithm to compute an optimal sequence of actions for a 3× 3
world whose initial state has dirt in the three top squares and the agent in the center.
c. Construct a search agent for the vacuum world, and evaluate its performance in a set of
3× 3 worlds with probability 0.2 of dirt in each square. Include the search cost as well
as path cost in the performance measure, using a reasonable exchange rate.
d. Compare your best search agent with a simple randomized reﬂex agent that sucks if
there is dirt and otherwise moves randomly.
e. Consider what would happen if the world were enlarged to n× n. How does the per-
formance of the search agent and of the reﬂex agent vary with n?
3.21 Prove each of the following statements, or give a counterexample:
a. Breadth-ﬁrst search is a special case of uniform-cost search.
b. Depth-ﬁrst search is a special case of best-ﬁrst tree search.
c. Uniform-cost search is a special case of A∗ search.
118 Chapter 3. Solving Problems by Searching
3.22 Compare the performance of A∗ and RBFS on a set of randomly generated problems
in the 8-puzzle (with Manhattan distance) and TSP (with MST—see Exercise 3.30) domains.
Discuss your results. What happens to the performance of RBFS when a small random num-
ber is added to the heuristic values in the 8-puzzle domain?
3.23 Trace the operation of A
∗ search applied to the problem of getting to Bucharest from
Lugoj using the straight-line distance heuristic. That is, show the sequence of nodes that the
algorithm will consider and the f, g,a n dh score for each node.
3.24 Devise a state space in which A
∗ using GRAPH -SEARCH returns a suboptimal solution
with an h(n) function that is admissible but inconsistent.
3.25 The heuristic path algorithm (Pohl, 1977) is a best-ﬁrst search in which the evalu-HEURISTIC P A TH
ALGORITHM
ation function is f(n)=( 2 −w)g(n)+ wh(n). For what values of w is this complete?
For what values is it optimal, assuming that h is admissible? What kind of search does this
perform for w =0 , w =1 ,a n dw =2 ?
3.26 Consider the unbounded version of the regular 2D grid shown in Figure 3.9. The start
state is at the origin, (0,0), and the goal state is at (x, y).
a. What is the branching factor b in this state space?
b. How many distinct states are there at depth k (for k> 0)?
c. What is the maximum number of nodes expanded by breadth-ﬁrst tree search?
d. What is the maximum number of nodes expanded by breadth-ﬁrst graph search?
e.I s h =|u−x| +|v−y| an admissible heuristic for a state at (u, v)? Explain.
f. How many nodes are expanded by A∗ graph search using h?
g.D o e sh remain admissible if some links are removed?
h.D o e sh remain admissible if some links are added between nonadjacent states?
3.27 n vehicles occupy squares (1,1) through (n, 1) (i.e., the bottom row) of ann×n grid.
The vehicles must be moved to the top row but in reverse order; so the vehicle i that starts in
(i,1) must end up in (n−i +1 ,n ). On each time step, every one of the n vehicles can move
one square up, down, left, or right, or stay put; but if a vehicle stays put, one other adjacent
vehicle (but not more than one) can hop over it. Two vehicles cannot occupy the same square.
a. Calculate the size of the state space as a function of n.
b. Calculate the branching factor as a function of n.
c. Suppose that vehicle i is at (x
i,y i); write a nontrivial admissible heuristic hi for the
number of moves it will require to get to its goal location (n−i +1 ,n ), assuming no
other vehicles are on the grid.
d. Which of the following heuristics are admissible for the problem of moving all n vehi-
cles to their destinations? Explain.
(i) ∑n
i =1 hi.
(ii) max{h1,...,h n}.
(iii) min{h1,...,h n}.
Exercises 119
3.28 Invent a heuristic function for the 8-puzzle that sometimes overestimates, and show
how it can lead to a suboptimal solution on a particular problem. (You can use a computer to
help if you want.) Prove that if h never overestimates by more than c,A
∗ using h returns a
solution whose cost exceeds that of the optimal solution by no more than c.
3.29 Prove that if a heuristic is consistent, it must be admissible. Construct an admissible
heuristic that is not consistent.
3.30 The traveling salesperson problem (TSP) can be solved with the minimum-spanning-
tree (MST) heuristic, which estimates the cost of completing a tour, given that a partial tour
has already been constructed. The MST cost of a set of cities is the smallest sum of the link
costs of any tree that connects all the cities.
a. Show how this heuristic can be derived from a relaxed version of the TSP.
b. Show that the MST heuristic dominates straight-line distance.
c. Write a problem generator for instances of the TSP where cities are represented by
random points in the unit square.
d. Find an efﬁcient algorithm in the literature for constructing the MST, and use it with A
∗
graph search to solve instances of the TSP.
3.31 On page 105, we deﬁned the relaxation of the 8-puzzle in which a tile can move from
square A to square B if B is blank. The exact solution of this problem deﬁnes Gaschnig’s
heuristic (Gaschnig, 1979). Explain why Gaschnig’s heuristic is at least as accurate as h1
(misplaced tiles), and show cases where it is more accurate than both h1 and h2 (Manhattan
distance). Explain how to calculate Gaschnig’s heuristic efﬁciently.
3.32 We gave two simple heuristics for the 8-puzzle: Manhattan distance and misplaced
tiles. Several heuristics in the literature purport to improve on this—see, for example, Nils-
son (1971), Mostow and Prieditis (1989), and Hansson et al. (1992). Test these claims by
implementing the heuristics and comparing the performance of the resulting algorithms.


--- BOOK CHAPTER: 4_Beyond_Classical_Search ---

4
BEYOND CLASSICAL
SEARCH
In which we relax the simplifying assumptions of the previous chapter , thereby
getting closer to the real world.
Chapter 3 addressed a single category of problems: observable, deterministic, known envi-
ronments where the solution is a sequence of actions. In this chapter, we look at what happens
when these assumptions are relaxed. We begin with a fairly simple case: Sections 4.1 and 4.2
cover algorithms that perform purely local search in the state space, evaluating and modify-
ing one or more current states rather than systematically exploring paths from an initial state.
These algorithms are suitable for problems in which all that matters is the solution state, not
the path cost to reach it. The family of local search algorithms includes methods inspired by
statistical physics (simulated annealing) and evolutionary biology (genetic algorithms).
Then, in Sections 4.3–4.4, we examine what happens when we relax the assumptions
of determinism and observability. The key idea is that if an agent cannot predict exactly what
percept it will receive, then it will need to consider what to do under each contingency that
its percepts may reveal. With partial observability, the agent will also need to keep track of
the states it might be in.
Finally, Section 4.5 investigates online search, in which the agent is faced with a state
space that is initially unknown and must be explored.
4.1 L OCAL SEARCH ALGORITHMS AND OPTIMIZA TION PROBLEMS
The search algorithms that we have seen so far are designed to explore search spaces sys-
tematically. This systematicity is achieved by keeping one or more paths in memory and by
recording which alternatives have been explored at each point along the path. When a goal is
found, the path to that goal also constitutes asolution to the problem. In many problems, how-
ever, the path to the goal is irrelevant. For example, in the 8-queens problem (see page 71),
what matters is the ﬁnal conﬁguration of queens, not the order in which they are added. The
same general property holds for many important applications such as integrated-circuit de-
sign, factory-ﬂoor layout, job-shop scheduling, automatic programming, telecommunications
network optimization, vehicle routing, and portfolio management.
120
Section 4.1. Local Search Algorithms and Optimization Problems 121
If the path to the goal does not matter, we might consider a different class of algo-
rithms, ones that do not worry about paths at all. Local search algorithms operate usingLOCAL SEARCH
a single current node (rather than multiple paths) and generally move only to neighborsCURRENT NODE
of that node. Typically, the paths followed by the search are not retained. Although local
search algorithms are not systematic, they have two key advantages: (1) they use very little
memory—usually a constant amount; and (2) they can often ﬁnd reasonable solutions in large
or inﬁnite (continuous) state spaces for which systematic algorithms are unsuitable.
In addition to ﬁnding goals, local search algorithms are useful for solving pure op-
timization problems, in which the aim is to ﬁnd the best state according to an objectiveOPTIMIZA TION
PROBLEM
function. Many optimization problems do not ﬁt the “standard” search model introduced inOBJECTIVE
FUNCTION
Chapter 3. For example, nature provides an objective function—reproductive ﬁtness—that
Darwinian evolution could be seen as attempting to optimize, but there is no “goal test” and
no “path cost” for this problem.
To understand local search, we ﬁnd it useful to consider the state-space landscape (as
STA TE-SP ACE
LANDSCAPE
in Figure 4.1). A landscape has both “location” (deﬁned by the state) and “elevation” (deﬁned
by the value of the heuristic cost function or objective function). If elevation corresponds to
cost, then the aim is to ﬁnd the lowest valley—a global minimum; if elevation corresponds
GLOBAL MINIMUM
to an objective function, then the aim is to ﬁnd the highest peak—a global maximum.( Y o uGLOBAL MAXIMUM
can convert from one to the other just by inserting a minus sign.) Local search algorithms
explore this landscape. A complete local search algorithm always ﬁnds a goal if one exists;
an optimal algorithm always ﬁnds a global minimum/maximum.
current
state
objective function
state space
global maximum
local maximum
“flat” local maximum
shoulder
Figure 4.1 A one-dimensional state-space landscape in which elevation corresponds to the
objective function. The aim is to ﬁnd the global maximum. Hill-climbing search modiﬁes
the current state to try to improve it, as shown by the arrow. The various topographic features
are deﬁned in the text.

122 Chapter 4. Beyond Classical Search
function HILL -CLIMBING (problem) returns a state that is a local maximum
current←MAKE -NODE (problem.INITIAL -STATE)
loop do
neighbor←a highest-valued successor of current
if neighbor.VALUE ≤current.VALUE then return current.STATE
current←neighbor
Figure 4.2 The hill-climbing search algorithm, which is the most basic local search tech-
nique. At each step the current node is replaced by the best neighbor; in this version, that
means the neighbor with the highest V
ALUE , but if a heuristic cost estimate h is used, we
would ﬁnd the neighbor with the lowest h.
4.1.1 Hill-climbing search
The hill-climbing search algorithm ( steepest-ascent version) is shown in Figure 4.2. It isHILL CLIMBING
STEEPEST ASCENT simply a loop that continually moves in the direction of increasing value—that is, uphill. It
terminates when it reaches a “peak” where no neighbor has a higher value. The algorithm
does not maintain a search tree, so the data structure for the current node need only record
the state and the value of the objective function. Hill climbing does not look ahead beyond
the immediate neighbors of the current state. This resembles trying to ﬁnd the top of Mount
Everest in a thick fog while suffering from amnesia.
To illustrate hill climbing, we will use the 8-queens problem introduced on page 71.
Local search algorithms typically use a complete-state formulation, where each state has
8 queens on the board, one per column. The successors of a state are all possible states
generated by moving a single queen to another square in the same column (so each state has
8× 7=5 6 successors). The heuristic cost function h is the number of pairs of queens that
are attacking each other, either directly or indirectly. The global minimum of this function
is zero, which occurs only at perfect solutions. Figure 4.3(a) shows a state with h=1 7.T h e
ﬁgure also shows the values of all its successors, with the best successors having h=1 2.
Hill-climbing algorithms typically choose randomly among the set of best successors if there
is more than one.
Hill climbing is sometimes calledgreedy local search because it grabs a good neighbor
GREEDY LOCAL
SEARCH
state without thinking ahead about where to go next. Although greed is considered one of the
seven deadly sins, it turns out that greedy algorithms often perform quite well. Hill climbing
often makes rapid progress toward a solution because it is usually quite easy to improve a bad
state. For example, from the state in Figure 4.3(a), it takes just ﬁve steps to reach the state
in Figure 4.3(b), which has h=1 and is very nearly a solution. Unfortunately, hill climbing
often gets stuck for the following reasons:
•Local maxima: a local maximum is a peak that is higher than each of its neighboring
LOCAL MAXIMUM
states but lower than the global maximum. Hill-climbing algorithms that reach the
vicinity of a local maximum will be drawn upward toward the peak but will then be
stuck with nowhere else to go. Figure 4.1 illustrates the problem schematically. More
Section 4.1. Local Search Algorithms and Optimization Problems 123
14
18
17
15
14
18
14
14
14
14
14
12
16
12
13
16
17
14
18
13
14
17
15
18
15
13
15
13
12
15
15
13
15
12
13
14
14
14
16
12
14
12
12
15
16
13
14
12
14
18
16
16
16
14
16
14
(a) (b)
Figure 4.3 (a) An 8-queens state with heuristic cost estimateh =17 , showing the value of
h for each possible successor obtained by moving a queen within its column. The best moves
are marked. (b) A local minimum in the 8-queens state space; the state has h =1 but every
successor has a higher cost.
concretely, the state in Figure 4.3(b) is a local maximum (i.e., a local minimum for the
cost h); every move of a single queen makes the situation worse.
•Ridges: a ridge is shown in Figure 4.4. Ridges result in a sequence of local maximaRIDGE
that is very difﬁcult for greedy algorithms to navigate.
•Plateaux: a plateau is a ﬂat area of the state-space landscape. It can be a ﬂat localPLA TEAU
maximum, from which no uphill exit exists, or a shoulder, from which progress isSHOULDER
possible. (See Figure 4.1.) A hill-climbing search might get lost on the plateau.
In each case, the algorithm reaches a point at which no progress is being made. Starting from
a randomly generated 8-queens state, steepest-ascent hill climbing gets stuck 86% of the time,
solving only 14% of problem instances. It works quickly, taking just 4 steps on average when
it succeeds and 3 when it gets stuck—not bad for a state space with 8
8 ≈17 million states.
The algorithm in Figure 4.2 halts if it reaches a plateau where the best successor has
the same value as the current state. Might it not be a good idea to keep going—to allow a
sideways move in the hope that the plateau is really a shoulder, as shown in Figure 4.1? The
SIDEWAYS MOVE
answer is usually yes, but we must take care. If we always allow sideways moves when there
are no uphill moves, an inﬁnite loop will occur whenever the algorithm reaches a ﬂat local
maximum that is not a shoulder. One common solution is to put a limit on the number of con-
secutive sideways moves allowed. For example, we could allow up to, say, 100 consecutive
sideways moves in the 8-queens problem. This raises the percentage of problem instances
solved by hill climbing from 14% to 94%. Success comes at a cost: the algorithm averages
roughly 21 steps for each successful instance and 64 for each failure.
124 Chapter 4. Beyond Classical Search
Figure 4.4 Illustration of why ridges cause difﬁculties for hill climbing. The grid of states
(dark circles) is superimposed on a ridge rising from left to right, creating a sequence of local
maxima that are not directly connected to each other. From each local maximum, all the
available actions point downhill.
Many variants of hill climbing have been invented.Stochastic hill climbing chooses atSTOCHASTIC HILL
CLIMBING
random from among the uphill moves; the probability of selection can vary with the steepness
of the uphill move. This usually converges more slowly than steepest ascent, but in some
state landscapes, it ﬁnds better solutions. First-choice hill climbing implements stochastic
FIRST -CHOICE HILL
CLIMBING
hill climbing by generating successors randomly until one is generated that is better than the
current state. This is a good strategy when a state has many (e.g., thousands) of successors.
The hill-climbing algorithms described so far are incomplete—they often fail to ﬁnd
a goal when one exists because they can get stuck on local maxima. Random-restart hill
climbing adopts the well-known adage, “If at ﬁrst you don’t succeed, try, try again.” It con-RANDOM-REST ART
HILL CLIMBING
ducts a series of hill-climbing searches from randomly generated initial states, 1 until a goal
is found. It is trivially complete with probability approaching 1, because it will eventually
generate a goal state as the initial state. If each hill-climbing search has a probability p of
success, then the expected number of restarts required is 1/p. For 8-queens instances with
no sideways moves allowed, p≈0.14, so we need roughly 7 iterations to ﬁnd a goal (6 fail-
ures and 1 success). The expected number of steps is the cost of one successful iteration plus
(1−p)/p times the cost of failure, or roughly 22 steps in all. When we allow sideways moves,
1/0.94≈1.06 iterations are needed on average and(1× 21)+ (0.06/0.94)× 64≈25 steps.
For 8-queens, then, random-restart hill climbing is very effective indeed. Even for three mil-
lion queens, the approach can ﬁnd solutions in under a minute.
2
1 Generating a random state from an implicitly speciﬁed state space can be a hard problem in itself.
2 Luby et al. (1993) prove that it is best, in some cases, to restarta randomized search algorithm after a particular,
ﬁxed amount of time and that this can be much more efﬁcient than letting each search continue indeﬁnitely.
Disallowing or limiting the number of sideways moves is an example of this idea.
Section 4.1. Local Search Algorithms and Optimization Problems 125
The success of hill climbing depends very much on the shape of the state-space land-
scape: if there are few local maxima and plateaux, random-restart hill climbing will ﬁnd a
good solution very quickly. On the other hand, many real problems have a landscape that
looks more like a widely scattered family of balding porcupines on a ﬂat ﬂoor, with miniature
porcupines living on the tip of each porcupine needle, ad inﬁnitum. NP-hard problems typi-
cally have an exponential number of local maxima to get stuck on. Despite this, a reasonably
good local maximum can often be found after a small number of restarts.
4.1.2 Simulated annealing
A hill-climbing algorithm that never makes “downhill” moves toward states with lower value
(or higher cost) is guaranteed to be incomplete, because it can get stuck on a local maxi-
mum. In contrast, a purely random walk—that is, moving to a successor chosen uniformly
at random from the set of successors—is complete but extremely inefﬁcient. Therefore, it
seems reasonable to try to combine hill climbing with a random walk in some way that yields
both efﬁciency and completeness. Simulated annealing is such an algorithm. In metallurgy,
SIMULA TED
ANNEALING
annealing is the process used to temper or harden metals and glass by heating them to a
high temperature and then gradually cooling them, thus allowing the material to reach a low-
energy crystalline state. To explain simulated annealing, we switch our point of view from
hill climbing to gradient descent (i.e., minimizing cost) and imagine the task of getting a
GRADIENT DESCENT
ping-pong ball into the deepest crevice in a bumpy surface. If we just let the ball roll, it will
come to rest at a local minimum. If we shake the surface, we can bounce the ball out of the
local minimum. The trick is to shake just hard enough to bounce the ball out of local min-
ima but not hard enough to dislodge it from the global minimum. The simulated-annealing
solution is to start by shaking hard (i.e., at a high temperature) and then gradually reduce the
intensity of the shaking (i.e., lower the temperature).
The innermost loop of the simulated-annealing algorithm (Figure 4.5) is quite similar to
hill climbing. Instead of picking the best move, however, it picks arandom move. If the move
improves the situation, it is always accepted. Otherwise, the algorithm accepts the move with
some probability less than 1. The probability decreases exponentially with the “badness” of
the move—the amount Δ E by which the evaluation is worsened. The probability also de-
creases as the “temperature” T goes down: “bad” moves are more likely to be allowed at the
start when T is high, and they become more unlikely as T decreases. If the schedule lowers
T slowly enough, the algorithm will ﬁnd a global optimum with probability approaching 1.
Simulated annealing was ﬁrst used extensively to solve VLSI layout problems in the
early 1980s. It has been applied widely to factory scheduling and other large-scale optimiza-
tion tasks. In Exercise 4.4, you are asked to compare its performance to that of random-restart
hill climbing on the 8-queens puzzle.
4.1.3 Local beam search
Keeping just one node in memory might seem to be an extreme reaction to the problem of
memory limitations. The local beam search algorithm
3 keeps track of k states rather thanLOCAL BEAM
SEARCH
3 Local beam search is an adaptation of beam search, which is a path-based algorithm.
126 Chapter 4. Beyond Classical Search
function SIMULATED -ANNEALING (problem,schedule) returns a solution state
inputs: problem, a problem
schedule, a mapping from time to “temperature”
current←MAKE -NODE (problem.INITIAL -STATE)
for t =1 to∞do
T←schedule(t)
if T =0 then return current
next←a randomly selected successor of current
ΔE←next.VALUE – current.VALUE
if ΔE > 0 then current←next
else current←next only with probability eΔ E/T
Figure 4.5 The simulated annealing algorithm, a version of stochastic hill climbing where
some downhill moves are allowed. Downhill moves are accepted readily early in the anneal-
ing schedule and then less often as time goes on. The schedule input determines the value of
the temperature T as a function of time.
just one. It begins with k randomly generated states. At each step, all the successors of all k
states are generated. If any one is a goal, the algorithm halts. Otherwise, it selects the k best
successors from the complete list and repeats.
At ﬁrst sight, a local beam search with k states might seem to be nothing more than
running k random restarts in parallel instead of in sequence. In fact, the two algorithms
are quite different. In a random-restart search, each search process runs independently of
the others. In a local beam search, useful information is passed among the parallel search
threads. In effect, the states that generate the best successors say to the others, “Come over
here, the grass is greener!” The algorithm quickly abandons unfruitful searches and moves
its resources to where the most progress is being made.
In its simplest form, local beam search can suffer from a lack of diversity among the
k states—they can quickly become concentrated in a small region of the state space, making
the search little more than an expensive version of hill climbing. A variant called stochastic
beam search, analogous to stochastic hill climbing, helps alleviate this problem. Instead
STOCHASTIC BEAM
SEARCH
of choosing the best k from the the pool of candidate successors, stochastic beam search
chooses k successors at random, with the probability of choosing a given successor being
an increasing function of its value. Stochastic beam search bears some resemblance to the
process of natural selection, whereby the “successors” (offspring) of a “state” (organism)
populate the next generation according to its “value” (ﬁtness).
4.1.4 Genetic algorithms
A genetic algorithm (or GA) is a variant of stochastic beam search in which successor statesGENETIC
ALGORITHM
are generated by combining two parent states rather than by modifying a single state. The
analogy to natural selection is the same as in stochastic beam search, except that now we are
dealing with sexual rather than asexual reproduction.
Section 4.1. Local Search Algorithms and Optimization Problems 127
(a)
Initial Population
(b)
Fitness Function
(c)
Selection
(d)
Crossover
(e)
Mutation
24
23
20
11
29%
31%
26%
14%
32752411
24748552
32752411
24415124
32748552
24752411
32752124
24415411
32252124
24752411
32748152
24415417
24748552
32752411
24415124
32543213
Figure 4.6 The genetic algorithm, illustrated for digit strings representing 8-queens states.
The initial population in (a) is ranked by the ﬁ tness function in (b), resulting in pairs for
mating in (c). They produce offspring in (d), which are subject to mutation in (e).
+=
Figure 4.7 The 8-queens states corresponding to the ﬁrst two parents in Figure 4.6(c) and
the ﬁrst offspring in Figure 4.6(d). The shaded columns are lost in the crossover step and the
unshaded columns are retained.
Like beam searches, GAs begin with a set of k randomly generated states, called the
population. Each state, or individual, is represented as a string over a ﬁnite alphabet—mostPOPULA TION
INDIVIDUAL commonly, a string of 0s and 1s. For example, an 8-queens state must specify the positions of
8 queens, each in a column of 8 squares, and so requires 8× log2 8=2 4 bits. Alternatively,
the state could be represented as 8 digits, each in the range from 1 to 8. (We demonstrate later
that the two encodings behave differently.) Figure 4.6(a) shows a population of four 8-digit
strings representing 8-queens states.
The production of the next generation of states is shown in Figure 4.6(b)–(e). In (b),
each state is rated by the objective function, or (in GA terminology) the ﬁtness function.A
FITNESS FUNCTION
ﬁtness function should return higher values for better states, so, for the 8-queens problem
we use the number of nonattacking pairs of queens, which has a value of 28 for a solution.
The values of the four states are 24, 23, 20, and 11. In this particular variant of the genetic
algorithm, the probability of being chosen for reproducing is directly proportional to the
ﬁtness score, and the percentages are shown next to the raw scores.
In (c), two pairs are selected at random for reproduction, in accordance with the prob-
128 Chapter 4. Beyond Classical Search
abilities in (b). Notice that one individual is selected twice and one not at all. 4 For each
pair to be mated, a crossover point is chosen randomly from the positions in the string. InCROSSOVER
Figure 4.6, the crossover points are after the third digit in the ﬁrst pair and after the ﬁfth digit
in the second pair.5
In (d), the offspring themselves are created by crossing over the parent strings at the
crossover point. For example, the ﬁrst child of the ﬁrst pair gets the ﬁrst three digits from the
ﬁrst parent and the remaining digits from the second parent, whereas the second child gets
the ﬁrst three digits from the second parent and the rest from the ﬁrst parent. The 8-queens
states involved in this reproduction step are shown in Figure 4.7. The example shows that
when two parent states are quite different, the crossover operation can produce a state that is
a long way from either parent state. It is often the case that the population is quite diverse
early on in the process, so crossover (like simulated annealing) frequently takes large steps in
the state space early in the search process and smaller steps later on when most individuals
are quite similar.
Finally, in (e), each location is subject to random mutation with a small independent
MUTA TION
probability. One digit was mutated in the ﬁrst, third, and fourth offspring. In the 8-queens
problem, this corresponds to choosing a queen at random and moving it to a random square
in its column. Figure 4.8 describes an algorithm that implements all these steps.
Like stochastic beam search, genetic algorithms combine an uphill tendency with ran-
dom exploration and exchange of information among parallel search threads. The primary
advantage, if any, of genetic algorithms comes from the crossover operation. Yet it can be
shown mathematically that, if the positions of the genetic code are permuted initially in a
random order, crossover conveys no advantage. Intuitively, the advantage comes from the
ability of crossover to combine large blocks of letters that have evolved independently to per-
form useful functions, thus raising the level of granularity at which the search operates. For
example, it could be that putting the ﬁrst three queens in positions 2, 4, and 6 (where they do
not attack each other) constitutes a useful block that can be combined with other blocks to
construct a solution.
The theory of genetic algorithms explains how this works using the idea of a schema,
SCHEMA
which is a substring in which some of the positions can be left unspeciﬁed. For example,
the schema 246***** describes all 8-queens states in which the ﬁrst three queens are in
positions 2, 4, and 6, respectively. Strings that match the schema (such as 24613578) are
called instances of the schema. It can be shown that if the average ﬁtness of the instances of
INSTANCE
a schema is above the mean, then the number of instances of the schema within the population
will grow over time. Clearly, this effect is unlikely to be signiﬁcant if adjacent bits are totally
unrelated to each other, because then there will be few contiguous blocks that provide a
consistent beneﬁt. Genetic algorithms work best when schemata correspond to meaningful
components of a solution. For example, if the string is a representation of an antenna, then the
schemata may represent components of the antenna, such as reﬂectors and deﬂectors. A good
4 There are many variants of this selection rule. The method of culling, in which all individuals below a given
threshold are discarded, can be shown to converge faster than the random version (Baum et al., 1995).
5 It is here that the encoding matters. If a 24-bit encoding is used instead of 8 digits, then the crossover point
has a 2/3 chance of being in the middle of a digit, which results in an essentially arbitrary mutation of that digit.
Section 4.2. Local Search in Continuous Spaces 129
function GENETIC -ALGORITHM (population,F ITNESS -FN) returns an individual
inputs: population, a set of individuals
FITNESS -FN, a function that measures the ﬁtness of an individual
repeat
new
 population←empty set
for i =1 to SIZE (population) do
x←RANDOM -SELECTION (population,F ITNESS -FN)
y←RANDOM -SELECTION (population,F ITNESS -FN)
child←REPRODUCE (x ,y)
if (small random probability) then child←MUTATE(child)
add child to new
 population
population←new
 population
until some individual is ﬁt enough, or enough time has elapsed
return the best individual in population, according to FITNESS -FN
function REPRODUCE (x ,y) returns an individual
inputs: x ,y, parent individuals
n←LENGTH (x ); c←random number from 1 to n
return APPEND (SUBSTRING (x ,1 ,c), SUBSTRING (y,c +1 ,n))
Figure 4.8 A genetic algorithm. The algorithm is the same as the one diagrammed in
Figure 4.6, with one variation: in this more popular version, each mating of two parents
produces only one offspring, not two.
component is likely to be good in a variety of different designs. This suggests that successful
use of genetic algorithms requires careful engineering of the representation.
In practice, genetic algorithms have had a widespread impact on optimization problems,
such as circuit layout and job-shop scheduling. At present, it is not clear whether the appeal
of genetic algorithms arises from their performance or from their æsthetically pleasing origins
in the theory of evolution. Much work remains to be done to identify the conditions under
which genetic algorithms perform well.
4.2 L OCAL SEARCH IN CONTINUOUS SPACES
In Chapter 2, we explained the distinction between discrete and continuous environments,
pointing out that most real-world environments are continuous. Yet none of the algorithms
we have described (except for ﬁrst-choice hill climbing and simulated annealing) can handle
continuous state and action spaces, because they have inﬁnite branching factors. This section
provides a very brief introduction to some local search techniques for ﬁnding optimal solu-
tions in continuous spaces. The literature on this topic is vast; many of the basic techniques
130 Chapter 4. Beyond Classical Search
EVOLUTION AND SEARCH
The theory of evolution was developed in Charles Darwin’s On the Origin of
Species by Means of Natural Selection (1859) and independently by Alfred Russel
Wallace (1858). The central idea is simple: variations occur in reproduction and
will be preserved in successive generations approximately in proportion to their
effect on reproductive ﬁtness.
Darwin’s theory was developed with no knowledge of how the traits of organ-
isms can be inherited and modiﬁed. The probabilistic laws governing these pro-
cesses were ﬁrst identiﬁed by Gregor Mendel (1866), a monk who experimented
with sweet peas. Much later, Watson and Crick (1953) identiﬁed the structure of the
DNA molecule and its alphabet, AGTC (adenine, guanine, thymine, cytosine). In
the standard model, variation occurs both by point mutations in the letter sequence
and by “crossover” (in which the DNA of an offspring is generated by combining
long sections of DNA from each parent).
The analogy to local search algorithms has already been described; the princi-
pal difference between stochastic beam search and evolution is the use ofsexual re-
production, wherein successors are generated from multiple organisms rather than
just one. The actual mechanisms of evolution are, however, far richer than most
genetic algorithms allow. For example, mutations can involve reversals, duplica-
tions, and movement of large chunks of DNA; some viruses borrow DNA from one
organism and insert it in another; and there are transposable genes that do nothing
but copy themselves many thousands of times within the genome. There are even
genes that poison cells from potential mates that do not carry the gene, thereby in-
creasing their own chances of replication. Most important is the fact that the genes
themselves encode the mechanisms whereby the genome is reproduced and trans-
lated into an organism. In genetic algorithms, those mechanisms are a separate
program that is not represented within the strings being manipulated.
Darwinian evolution may appear inefﬁcient, having generated blindly some
10
45 or so organisms without improving its search heuristics one iota. Fifty
years before Darwin, however, the otherwise great French naturalist Jean Lamarck
(1809) proposed a theory of evolution whereby traits acquired by adaptation dur-
ing an organism’s lifetime would be passed on to its offspring. Such a process
would be effective but does not seem to occur in nature. Much later, James Bald-
win (1896) proposed a superﬁcially similar theory: that behavior learned during an
organism’s lifetime could accelerate the rate of evolution. Unlike Lamarck’s, Bald-
win’s theory is entirely consistent with Darwinian evolution because it relies on se-
lection pressures operating on individuals that have found local optima among the
set of possible behaviors allowed by their genetic makeup. Computer simulations
conﬁrm that the “Baldwin effect” is real, once “ordinary” evolution has created
organisms whose internal performance measure correlates with actual ﬁtness.

Section 4.2. Local Search in Continuous Spaces 131
originated in the 17th century, after the development of calculus by Newton and Leibniz.6 We
ﬁnd uses for these techniques at several places in the book, including the chapters on learning,
vision, and robotics.
We begin with an example. Suppose we want to place three new airports anywhere
in Romania, such that the sum of squared distances from each city on the map (Figure 3.2)
to its nearest airport is minimized. The state space is then deﬁned by the coordinates of
the airports: (x
1,y1), (x2,y2),a n d (x3,y3).T h i s i s a six-dimensional space; we also say
that states are deﬁned by six variables. (In general, states are deﬁned by an n-dimensionalVARIABLE
vector of variables, x.) Moving around in this space corresponds to moving one or more of
the airports on the map. The objective function f(x1,y1,x2,y2,x3,y3) is relatively easy to
compute for any particular state once we compute the closest cities. Let Ci be the set of
cities whose closest airport (in the current state) is airporti. Then, in the neighborhood of the
current state,w h e r et h eCis remain constant, we have
f(x1,y1,x2,y2,x3,y3)=
3∑
i=1
∑
c∈Ci
(xi−xc)2 +( yi−yc)2 . (4.1)
This expression is correct locally, but not globally because the sets Ci are (discontinuous)
functions of the state.
One way to avoid continuous problems is simply todiscretize the neighborhood of eachDISCRETIZA TION
state. For example, we can move only one airport at a time in either the x or y direction by
a ﬁxed amount ±δ. With 6 variables, this gives 12 possible successors for each state. We
can then apply any of the local search algorithms described previously. We could also ap-
ply stochastic hill climbing and simulated annealing directly, without discretizing the space.
These algorithms choose successors randomly, which can be done by generating random vec-
tors of length δ.
Many methods attempt to use the gradient of the landscape to ﬁnd a maximum. The
GRADIENT
gradient of the objective function is a vector∇f that gives the magnitude and direction of the
steepest slope. For our problem, we have
∇f =
⎞ ∂f
∂x1
, ∂f
∂y1
, ∂f
∂x2
, ∂f
∂y2
, ∂f
∂x3
, ∂f
∂y3
⎠
.
In some cases, we can ﬁnd a maximum by solving the equation∇f =0 . (This could be done,
for example, if we were placing just one airport; the solution is the arithmetic mean of all the
cities’ coordinates.) In many cases, however, this equation cannot be solved in closed form.
For example, with three airports, the expression for the gradient depends on what cities are
closest to each airport in the current state. This means we can compute the gradient locally
(but not globally); for example,
∂f
∂x1
=2
∑
c∈C1
(xi−xc) . (4.2)
Given a locally correct expression for the gradient, we can perform steepest-ascent hill climb-
6 A basic knowledge of multivariate calculus and vector arithmetic is useful for reading this section.
132 Chapter 4. Beyond Classical Search
ing by updating the current state according to the formula
x←x + α∇f(x) ,
where α is a small constant often called the step size. In other cases, the objective functionSTEP SIZE
might not be available in a differentiable form at all—for example, the value of a particular set
of airport locations might be determined by running some large-scale economic simulation
package. In those cases, we can calculate a so-called empirical gradient by evaluating the
EMPIRICAL
GRADIENT
response to small increments and decrements in each coordinate. Empirical gradient search
is the same as steepest-ascent hill climbing in a discretized version of the state space.
Hidden beneath the phrase “ α is a small constant” lies a huge variety of methods for
adjusting α. The basic problem is that, if α is too small, too many steps are needed; if α
is too large, the search could overshoot the maximum. The technique of line search tries toLINE SEARCH
overcome this dilemma by extending the current gradient direction—usually by repeatedly
doubling α—until f starts to decrease again. The point at which this occurs becomes the new
current state. There are several schools of thought about how the new direction should be
chosen at this point.
For many problems, the most effective algorithm is the venerable Newton–Raphson
NEWTON–RAPHSON
method. This is a general technique for ﬁnding roots of functions—that is, solving equations
of the form g(x)=0 . It works by computing a new estimate for the root x according to
Newton’s formula
x←x−g(x)/g′(x) .
To ﬁnd a maximum or minimum of f, we need to ﬁnd x such that the gradient is zero (i.e.,
∇f(x)= 0). Thus, g(x) in Newton’s formula becomes∇f(x), and the update equation can
be written in matrix–vector form as
x←x−H−1
f (x)∇f(x) ,
where Hf (x) is the Hessian matrix of second derivatives, whose elements Hij are givenHESSIAN
by ∂2f/∂xi∂xj . For our airport example, we can see from Equation (4.2) that Hf (x) is
particularly simple: the off-diagonal elements are zero and the diagonal elements for airport
i are just twice the number of cities in C
i. A moment’s calculation shows that one step of
the update moves airport i directly to the centroid of Ci, which is the minimum of the local
expression for f from Equation (4.1).7 For high-dimensional problems, however, computing
the n2 entries of the Hessian and inverting it may be expensive, so many approximate versions
of the Newton–Raphson method have been developed.
Local search methods suffer from local maxima, ridges, and plateaux in continuous
state spaces just as much as in discrete spaces. Random restarts and simulated annealing can
be used and are often helpful. High-dimensional continuous spaces are, however, big places
in which it is easy to get lost.
A ﬁnal topic with which a passing acquaintance is useful is constrained optimization.
CONSTRAINED
OPTIMIZA TION
An optimization problem is constrained if solutions must satisfy some hard constraints on the
values of the variables. For example, in our airport-siting problem, we might constrain sites
7 In general, the Newton–Raphson update can be seen as ﬁtting a quadratic surface to f at x and then moving
directly to the minimum of that surface—which is also the minimum of f if f is quadratic.
Section 4.3. Searching with Nondeterministic Actions 133
to be inside Romania and on dry land (rather than in the middle of lakes). The difﬁculty of
constrained optimization problems depends on the nature of the constraints and the objective
function. The best-known category is that of linear programming problems, in which con-
LINEAR
PROGRAMMING
straints must be linear inequalities forming a convex set 8 and the objective function is alsoCONVEX SET
linear. The time complexity of linear programming is polynomial in the number of variables.
Linear programming is probably the most widely studied and broadly useful class of
optimization problems. It is a special case of the more general problem of convex opti-
mization, which allows the constraint region to be any convex region and the objective toCONVEX
OPTIMIZA TION
be any function that is convex within the constraint region. Under certain conditions, convex
optimization problems are also polynomially solvable and may be feasible in practice with
thousands of variables. Several important problems in machine learning and control theory
can be formulated as convex optimization problems (see Chapter 20).
4.3 S EARCHING WITH NONDETERMINISTIC ACTIONS
In Chapter 3, we assumed that the environment is fully observable and deterministic and that
the agent knows what the effects of each action are. Therefore, the agent can calculate exactly
which state results from any sequence of actions and always knows which state it is in. Its
percepts provide no new information after each action, although of course they tell the agent
the initial state.
When the environment is either partially observable or nondeterministic (or both), per-
cepts become useful. In a partially observable environment, every percept helps narrow down
the set of possible states the agent might be in, thus making it easier for the agent to achieve
its goals. When the environment is nondeterministic, percepts tell the agent which of the pos-
sible outcomes of its actions has actually occurred. In both cases, the future percepts cannot
be determined in advance and the agent’s future actions will depend on those future percepts.
So the solution to a problem is not a sequence but acontingency plan (also known as a strat-
CONTINGENCY PLAN
egy) that speciﬁes what to do depending on what percepts are received. In this section, weSTRA TEGY
examine the case of nondeterminism, deferring partial observability to Section 4.4.
4.3.1 The erratic vacuum world
As an example, we use the vacuum world, ﬁrst introduced in Chapter 2 and deﬁned as a
search problem in Section 3.2.1. Recall that the state space has eight states, as shown in
Figure 4.9. There are three actions— Left, Right,a n d Suck—and the goal is to clean up all
the dirt (states 7 and 8). If the environment is observable, deterministic, and completely
known, then the problem is trivially solvable by any of the algorithms in Chapter 3 and the
solution is an action sequence. For example, if the initial state is 1, then the action sequence
[Suck,Right,Suck] will reach a goal state, 8.
8 A set of points S is convex if the line joining any two points inS is also contained in S.A convex function is
one for which the space “above” it forms a convex set; by deﬁnition, convex functions have no local (as opposed
to global) minima.
134 Chapter 4. Beyond Classical Search
12
87
56
34
Figure 4.9 The eight possible states of the vacuum world; states 7 and 8 are goal states.
Now suppose that we introduce nondeterminism in the form of a powerful but erratic
vacuum cleaner. In the erratic vacuum world,t h eSuck action works as follows:ERRA TIC VACUUM
WORLD
•When applied to a dirty square the action cleans the square and sometimes cleans up
dirt in an adjacent square, too.
•When applied to a clean square the action sometimes deposits dirt on the carpet.9
To provide a precise formulation of this problem, we need to generalize the notion of atran-
sition model from Chapter 3. Instead of deﬁning the transition model by a R ESULT function
that returns a single state, we use a R ESULTS function that returns a set of possible outcome
states. For example, in the erratic vacuum world, the Suck action in state 1 leads to a state in
the set{5,7}—the dirt in the right-hand square may or may not be vacuumed up.
We also need to generalize the notion of a solution to the problem. For example, if we
start in state 1, there is no single sequence of actions that solves the problem. Instead, we
need a contingency plan such as the following:
[Suck, if State =5 then [Right, Suck] else [] ]. (4.3)
Thus, solutions for nondeterministic problems can contain nested if–then–else statements;
this means that they are trees rather than sequences. This allows the selection of actions
based on contingencies arising during execution. Many problems in the real, physical world
are contingency problems because exact prediction is impossible. For this reason, many
people keep their eyes open while walking around or driving.
9 We assume that most readers face similar problems and can sympathize with our agent. We apologize to
owners of modern, efﬁcient home appliances who cannot take advantage of this pedagogical device.
Section 4.3. Searching with Nondeterministic Actions 135
4.3.2 AND –OR search trees
The next question is how to ﬁnd contingent solutions to nondeterministic problems. As in
Chapter 3, we begin by constructing search trees, but here the trees have a different character.
In a deterministic environment, the only branching is introduced by the agent’s own choices
in each state. We call these nodes
OR nodes. In the vacuum world, for example, at an OROR NODE
node the agent chooses Left or Right or Suck . In a nondeterministic environment, branching
is also introduced by the environment’s choice of outcome for each action. We call these
nodes AND nodes. For example, the Suck action in state 1 leads to a state in the set {5,7},AND NODE
so the agent would need to ﬁnd a plan for state 5 and for state 7. These two kinds of nodes
alternate, leading to an AND –OR tree as illustrated in Figure 4.10.AND–OR TREE
A solution for an AND –OR search problem is a subtree that (1) has a goal node at every
leaf, (2) speciﬁes one action at each of its OR nodes, and (3) includes every outcome branch
at each of its AND nodes. The solution is shown in bold lines in the ﬁgure; it corresponds
to the plan given in Equation (4.3). (The plan uses if–then–else notation to handle the AND
branches, but when there are more than two branches at a node, it might be better to use acase
LeftSuck
RightSuck
RightSuck
6 
GOAL
8 
GOAL
7 
1 
2 5 
1 
LOOP
5 
LOOP
5 
LOOP
Left Suck
1 
LOOP GOAL
8 4 
Figure 4.10 The ﬁrst two levels of the search tree for the erratic vacuum world. State
nodes are OR nodes where some action must be chosen. At the AND nodes, shown as circles,
every outcome must be handled, as indicated by the arc linking the outgoing branches. The
solution found is shown in bold lines.

136 Chapter 4. Beyond Classical Search
function AND-OR-GRAPH -SEARCH (problem) returns a conditional plan, or failure
OR-SEARCH (problem.INITIAL -STATE,problem,[] )
function OR-SEARCH (state,problem,path) returns a conditional plan, or failure
if problem.GOAL -TEST (state) then return the empty plan
if state is on path then return failure
for each action in problem.ACTIONS (state) do
plan←AND-SEARCH (RESULTS (state,action),problem,[state| path])
if plan̸= failure then return [action| plan]
return failure
function AND-SEARCH (states,problem,path) returns a conditional plan, or failure
for each si in states do
plani←OR-SEARCH (si,problem,path)
if plani = failure then return failure
return [if s1 then plan1 else if s2 then plan2 else ... if sn− 1 then plann− 1 else plann]
Figure 4.11 An algorithm for searching AND –OR graphs generated by nondeterministic
environments. It returns a conditional plan that reaches a goal state in all circumstances. (The
notation [x| l] refers to the list formed by adding object x to the front of list l.)
construct.) Modifying the basic problem-solving agent shown in Figure 3.1 to execute con-
tingent solutions of this kind is straightforward. One may also consider a somewhat different
agent design, in which the agent can act before it has found a guaranteed plan and deals with
some contingencies only as they arise during execution. This type of interleaving of search
INTERLEAVING
and execution is also useful for exploration problems (see Section 4.5) and for game playing
(see Chapter 5).
Figure 4.11 gives a recursive, depth-ﬁrst algorithm for AND –OR graph search. One
key aspect of the algorithm is the way in which it deals with cycles, which often arise in
nondeterministic problems (e.g., if an action sometimes has no effect or if an unintended
effect can be corrected). If the current state is identical to a state on the path from the root,
then it returns with failure. This doesn’t mean that there is no solution from the current state;
it simply means that if there is a noncyclic solution, it must be reachable from the earlier
incarnation of the current state, so the new incarnation can be discarded. With this check, we
ensure that the algorithm terminates in every ﬁnite state space, because every path must reach
a goal, a dead end, or a repeated state. Notice that the algorithm does not check whether the
current state is a repetition of a state on someother path from the root, which is important for
efﬁciency. Exercise 4.5 investigates this issue.
AND –OR graphs can also be explored by breadth-ﬁrst or best-ﬁrst methods. The concept
of a heuristic function must be modiﬁed to estimate the cost of a contingent solution rather
than a sequence, but the notion of admissibility carries over and there is an analog of the A ∗
algorithm for ﬁnding optimal solutions. Pointers are given in the bibliographical notes at the
end of the chapter.
Section 4.3. Searching with Nondeterministic Actions 137
Suck Right
6 
1 
2 5 
Right
Figure 4.12 Part of the search graph for the slippery vacuum world, where we have shown
(some) cycles explicitly. All solutions for this problem are cyclic plans because there is no
w a yt om o v er e l i a b l y .
4.3.3 Try, try again
Consider the slippery vacuum world, which is identical to the ordinary (non-erratic) vac-
uum world except that movement actions sometimes fail, leaving the agent in the same loca-
tion. For example, moving Right in state 1 leads to the state set {1,2}. Figure 4.12 shows
part of the search graph; clearly, there are no longer any acyclic solutions from state 1, and
A
ND-OR-GRAPH -SEARCH would return with failure. There is, however, a cyclic solution,CYCLIC SOLUTION
w h i c hi st ok e e pt r y i n gRight until it works. We can express this solution by adding alabel toLABEL
denote some portion of the plan and using that label later instead of repeating the plan itself.
Thus, our cyclic solution is
[Suck,L1 : Right, if State =5 then L1 else Suck] .
(A better syntax for the looping part of this plan would be “ while State =5 do Right.”)
In general a cyclic plan may be considered a solution provided that every leaf is a goal
state and that a leaf is reachable from every point in the plan. The modiﬁcations needed
to A
ND-OR-GRAPH -SEARCH are covered in Exercise 4.6. The key realization is that a loop
in the state space back to a state L translates to a loop in the plan back to the point where the
subplan for state L is executed.
Given the deﬁnition of a cyclic solution, an agent executing such a solution will eventu-
ally reach the goalprovided that each outcome of a nondeterministic action eventually occurs.
Is this condition reasonable? It depends on the reason for the nondeterminism. If the action
rolls a die, then it’s reasonable to suppose that eventually a six will be rolled. If the action is
to insert a hotel card key into the door lock, but it doesn’t work the ﬁrst time, then perhaps it
will eventually work, or perhaps one has the wrong key (or the wrong room!). After seven or
138 Chapter 4. Beyond Classical Search
eight tries, most people will assume the problem is with the key and will go back to the front
desk to get a new one. One way to understand this decision is to say that the initial problem
formulation (observable, nondeterministic) is abandoned in favor of a different formulation
(partially observable, deterministic) where the failure is attributed to an unobservable prop-
erty of the key. We have more to say on this issue in Chapter 13.
4.4 S EARCHING WITH PARTIAL OBSERV ATIONS
We now turn to the problem of partial observability, where the agent’s percepts do not suf-
ﬁce to pin down the exact state. As noted at the beginning of the previous section, if the
agent is in one of several possible states, then an action may lead to one of several possible
outcomes—even if the environment is deterministic . The key concept required for solving
partially observable problems is the belief state, representing the agent’s current belief about
BELIEF STA TE
the possible physical states it might be in, given the sequence of actions and percepts up to
that point. We begin with the simplest scenario for studying belief states, which is when the
agent has no sensors at all; then we add in partial sensing as well as nondeterministic actions.
4.4.1 Searching with no observation
When the agent’s percepts provide no information at all , we have what is called a sensor-
less problem or sometimes a conformant problem. At ﬁrst, one might think the sensorlessSENSORLESS
CONFORMANT agent has no hope of solving a problem if it has no idea what state it’s in; in fact, sensorless
problems are quite often solvable. Moreover, sensorless agents can be surprisingly useful,
primarily because they don’t rely on sensors working properly. In manufacturing systems,
for example, many ingenious methods have been developed for orienting parts correctly from
an unknown initial position by using a sequence of actions with no sensing at all. The high
cost of sensing is another reason to avoid it: for example, doctors often prescribe a broad-
spectrum antibiotic rather than using the contingent plan of doing an expensive blood test,
then waiting for the results to come back, and then prescribing a more speciﬁc antibiotic and
perhaps hospitalization because the infection has progressed too far.
We can make a sensorless version of the vacuum world. Assume that the agent knows
the geography of its world, but doesn’t know its location or the distribution of dirt. In that
case, its initial state could be any element of the set{1,2,3,4,5,6,7, 8}. Now, consider what
happens if it tries the actionRight. This will cause it to be in one of the states{2,4,6,8}—the
agent now has more information! Furthermore, the action sequence [Right,Suck] will always
end up in one of the states{4,8}. Finally, the sequence [ Right,Suck,Left,Suck] is guaranteed
to reach the goal state 7 no matter what the start state. We say that the agent can coerce the
COERCION
w o r l di n t os t a t e7 .
To solve sensorless problems, we search in the space of belief states rather than physical
states.10 Notice that in belief-state space, the problem is fully observable because the agent
10 In a fully observable environment, each belief state contains one physical state. Thus, we can view the algo-
rithms in Chapter 3 as searching in a belief-state space of singleton belief states.
Section 4.4. Searching with Partial Observations 139
always knows its own belief state. Furthermore, the solution (if any) is always a sequence of
actions. This is because, as in the ordinary problems of Chapter 3, the percepts received after
each action are completely predictable—they’re always empty! So there are no contingencies
to plan for. This is true even if the environment is nondeterminstic.
It is instructive to see how the belief-state search problem is constructed. Suppose
the underlying physical problem P is deﬁned by A
CTIONS P ,R ESULT P ,G OAL -TEST P ,a n d
STEP -COST P . Then we can deﬁne the corresponding sensorless problem as follows:
•Belief states: The entire belief-state space contains every possible set of physical states.
If P has N states, then the sensorless problem has up to 2N states, although many may
be unreachable from the initial state.
•Initial state: Typically the set of all states in P , although in some cases the agent will
have more knowledge than this.
•Actions: This is slightly tricky. Suppose the agent is in belief state b={s1,s2},b u t
ACTIONS P (s1)̸= ACTIONS P (s2); then the agent is unsure of which actions are legal.
If we assume that illegal actions have no effect on the environment, then it is safe to
take the union of all the actions in any of the physical states in the current belief stateb:
A
CTIONS (b)=
⋃
s∈b
ACTIONS P (s) .
On the other hand, if an illegal action might be the end of the world, it is safer to allow
only the intersection, that is, the set of actions legal in all the states. For the vacuum
world, every state has the same legal actions, so both methods give the same result.
•Transition model: The agent doesn’t know which state in the belief state is the right
one; so as far as it knows, it might get to any of the states resulting from applying the
action to one of the physical states in the belief state. For deterministic actions, the set
of states that might be reached is
b
′ = RESULT (b,a)= {s′ : s′ = RESULT P (s,a) and s∈b} . (4.4)
With deterministic actions, b′ is never larger than b. With nondeterminism, we have
b′ = RESULT (b,a)= {s′ : s′ ∈RESULTS P (s,a) and s∈b}
=
⋃
s∈b
RESULTS P (s,a) ,
which may be larger than b, as shown in Figure 4.13. The process of generating
the new belief state after the action is called the prediction step; the notation b′ =PREDICTION
PREDICT P (b, a) will come in handy.
•Goal test: The agent wants a plan that is sure to work, which means that a belief state
satisﬁes the goal only if all the physical states in it satisfy G OAL -TEST P . The agent
may accidentally achieve the goal earlier, but it won’t know that it has done so.
•Path cost: This is also tricky. If the same action can have different costs in different
states, then the cost of taking an action in a given belief state could be one of several
values. (This gives rise to a new class of problems, which we explore in Exercise 4.9.)
For now we assume that the cost of an action is the same in all states and so can be
transferred directly from the underlying physical problem.
140 Chapter 4. Beyond Classical Search
2 
4 
1 
3 
2 
4 
1 
3 
1 
3 
(b)(a)
Figure 4.13 (a) Predicting the next belief state for the sensorless vacuum world with a
deterministic action, Right. (b) Prediction for the same belief state and action in the slippery
version of the sensorless vacuum world.
Figure 4.14 shows the reachable belief-state space for the deterministic, sensorless vacuum
world. There are only 12 reachable belief states out of 28 = 256possible belief states.
The preceding deﬁnitions enable the automatic construction of the belief-state problem
formulation from the deﬁnition of the underlying physical problem. Once this is done, we
can apply any of the search algorithms of Chapter 3. In fact, we can do a little bit more
than that. In “ordinary” graph search, newly generated states are tested to see if they are
identical to existing states. This works for belief states, too; for example, in Figure 4.14, the
action sequence [ Suck,Left,Suck] starting at the initial state reaches the same belief state as
[Right,Left,Suck], namely,{5,7}. Now, consider the belief state reached by [ Left], namely,
{1,3,5,7}. Obviously, this is not identical to {5,7}, but it is a superset. It is easy to prove
(Exercise 4.8) that if an action sequence is a solution for a belief stateb, it is also a solution for
any subset of b. Hence, we can discard a path reaching{1,3,5,7} if{5,7} has already been
generated. Conversely, if {1,3,5,7} has already been generated and found to be solvable,
then any subset,s u c ha s{5,7}, is guaranteed to be solvable. This extra level of pruning may
dramatically improve the efﬁciency of sensorless problem solving.
Even with this improvement, however, sensorless problem-solving as we have described
it is seldom feasible in practice. The difﬁculty is not so much the vastness of the belief-state
space—even though it is exponentially larger than the underlying physical state space; in
most cases the branching factor and solution length in the belief-state space and physical
state space are not so different. The real difﬁculty lies with the size of each belief state. For
example, the initial belief state for the 10× 10 vacuum world contains 100× 2
100 or around
1032 physical states—far too many if we use the atomic representation, which is an explicit
list of states.
One solution is to represent the belief state by some more compact description. In
English, we could say the agent knows “Nothing” in the initial state; after moving Left,w e
could say, “Not in the rightmost column,” and so on. Chapter 7 explains how to do this in a
formal representation scheme. Another approach is to avoid the standard search algorithms,
which treat belief states as black boxes just like any other problem state. Instead, we can look
Section 4.4. Searching with Partial Observations 141
L
R
S
L
R
S
LR
S
LR
S
L
R
S
LR
SL
R
S
113
57
24
68
23
45 6
78
45
78
53
7
64
8
4
8
5
7
6
8
87
3
7
Figure 4.14 The reachable portion of the belief-state space for the deterministic, sensor-
less vacuum world. Each shaded box corresponds to a single belief state. At any given point,
the agent is in a particular belief state but does not know which physical state it is in. The
initial belief state (complete ignorance) is t he top center box. Actions are represented by
labeled links. Self-loops are omitted for clarity.
inside the belief states and develop incremental belief-state search algorithms that build up
INCREMENT AL
BELIEF-STA TE
SEARCH
the solution one physical state at a time. For example, in the sensorless vacuum world, the
initial belief state is{1,2,3,4,5,6,7, 8}, and we have to ﬁnd an action sequence that works
in all 8 states. We can do this by ﬁrst ﬁnding a solution that works for state 1; then we check
if it works for state 2; if not, go back and ﬁnd a different solution for state 1, and so on. Just
as an
AND –OR search has to ﬁnd a solution for every branch at an AND node, this algorithm
has to ﬁnd a solution for every state in the belief state; the difference is that AND –OR search
can ﬁnd a different solution for each branch, whereas an incremental belief-state search has
to ﬁnd one solution that works for all the states.
The main advantage of the incremental approach is that it is typically able to detect
failure quickly—when a belief state is unsolvable, it is usually the case that a small subset of
the belief state, consisting of the ﬁrst few states examined, is also unsolvable. In some cases,
142 Chapter 4. Beyond Classical Search
this leads to a speedup proportional to the size of the belief states, which may themselves be
as large as the physical state space itself.
Even the most efﬁcient solution algorithm is not of much use when no solutions exist.
Many things just cannot be done without sensing. For example, the sensorless 8-puzzle is
impossible. On the other hand, a little bit of sensing can go a long way. For example, every
8-puzzle instance is solvable if just one square is visible—the solution involves moving each
tile in turn into the visible square and then keeping track of its location.
4.4.2 Searching with observations
For a general partially observable problem, we have to specify how the environment generates
percepts for the agent. For example, we might deﬁne the local-sensing vacuum world to be
one in which the agent has a position sensor and a local dirt sensor but has no sensor capable
of detecting dirt in other squares. The formal problem speciﬁcation includes a P
ERCEPT (s)
function that returns the percept received in a given state. (If sensing is nondeterministic,
then we use a P
ERCEPTS function that returns a set of possible percepts.) For example, in the
local-sensing vacuum world, the PERCEPT in state 1 is[A,Dirty]. Fully observable problems
are a special case in which P ERCEPT (s)= s for every state s, while sensorless problems are
a special case in which PERCEPT (s)= null .
When observations are partial, it will usually be the case that several states could have
produced any given percept. For example, the percept [A,Dirty] is produced by state 3 as
well as by state 1. Hence, given this as the initial percept, the initial belief state for the
local-sensing vacuum world will be {1,3}.T h e A
CTIONS ,S TEP -COST ,a n dG OAL -TEST
are constructed from the underlying physical problem just as for sensorless problems, but the
transition model is a bit more complicated. We can think of transitions from one belief state
to the next for a particular action as occurring in three stages, as shown in Figure 4.15:
•The prediction stage is the same as for sensorless problems: given the actiona in belief
state b, the predicted belief state is ˆb= PREDICT (b, a).11
•The observation prediction stage determines the set of percepts o that could be ob-
served in the predicted belief state:
POSSIBLE -PERCEPTS (ˆb)= {o : o= PERCEPT (s) and s∈ˆb} .
•The update stage determines, for each possible percept, the belief state that would
result from the percept. The new belief state bo is just the set of states in ˆb that could
have produced the percept:
bo = UPDATE(ˆb,o )= {s : o= PERCEPT (s) and s∈ˆb} .
Notice that each updated belief statebo can be no larger than the predicted belief stateˆb;
observations can only help reduce uncertainty compared to the sensorless case. More-
over, for deterministic sensing, the belief states for the different possible percepts will
be disjoint, forming a partition of the original predicted belief state.
11 Here, and throughout the book, the “hat” in ˆb means an estimated or predicted value for b.
Section 4.4. Searching with Partial Observations 143
2 
4 
4 
1 
2 
4 
1 
3 
2 
1 
3 3 
(b)
(a)
4 
2 
1 
3 
Right
[A,Dirty]
[B,Dirty]
[B,Clean]
Right [B,Dirty]
[B,Clean]
Figure 4.15 Two example of transitions in local-sensing vacuum worlds. (a) In the de-
terministic world, Right is applied in the initial belief state, resulting in a new belief state
with two possible physical states; for those states, the possible percepts are [B, Dirty] and
[B, Clean], leading to two belief states, each of which is a singleton. (b) In the slippery
world, Right is applied in the initial belief state, giving a new belief state with four physi-
cal states; for those states, the possible percepts are [A,Dirty], [B, Dirty],a n d[B, Clean],
leading to three belief states as shown.
Putting these three stages together, we obtain the possible belief states resulting from a given
action and the subsequent possible percepts:
RESULTS (b, a)= {bo : bo = UPDATE(PREDICT (b, a),o) and
o ∈POSSIBLE -PERCEPTS (PREDICT (b, a))} . (4.5)
Again, the nondeterminism in the partially observable problem comes from the inability
to predict exactly which percept will be received after acting; underlying nondeterminism in
the physical environment may contribute to this inability by enlarging the belief state at the
prediction stage, leading to more percepts at the observation stage.
4.4.3 Solving partially observable problems
The preceding section showed how to derive the R ESULTS function for a nondeterministic
belief-state problem from an underlying physical problem and the PERCEPT function. Given
144 Chapter 4. Beyond Classical Search
7 
5 
1 
3 
4 2 
Suck
[B,Dirty] [B,Clean]
Right
[A,Clean]
Figure 4.16 The ﬁrst level of the AND –OR search tree for a problem in the local-sensing
vacuum world; Suck is the ﬁrst step of the solution.
such a formulation, the AND –OR search algorithm of Figure 4.11 can be applied directly to
derive a solution. Figure 4.16 shows part of the search tree for the local-sensing vacuum
world, assuming an initial percept [A,Dirty]. The solution is the conditional plan
[Suck, Right, if Bstate ={6} then Suck else [] ].
Notice that, because we supplied a belief-state problem to the
AND –OR search algorithm, it
returned a conditional plan that tests the belief state rather than the actual state. This is as it
should be: in a partially observable environment the agent won’t be able to execute a solution
that requires testing the actual state.
As in the case of standard search algorithms applied to sensorless problems, the
AND –
OR search algorithm treats belief states as black boxes, just like any other states. One can
improve on this by checking for previously generated belief states that are subsets or supersets
of the current state, just as for sensorless problems. One can also derive incremental search
algorithms, analogous to those described for sensorless problems, that provide substantial
speedups over the black-box approach.
4.4.4 An agent for partially observable environments
The design of a problem-solving agent for partially observable environments is quite similar
to the simple problem-solving agent in Figure 3.1: the agent formulates a problem, calls a
search algorithm (such as A
ND-OR-GRAPH -SEARCH ) to solve it, and executes the solution.
There are two main differences. First, the solution to a problem will be a conditional plan
rather than a sequence; if the ﬁrst step is an if–then–else expression, the agent will need to
test the condition in the if-part and execute the then-part or the else-part accordingly. Second,
the agent will need to maintain its belief state as it performs actions and receives percepts.
This process resembles the prediction–observation–update process in Equation (4.5) but is
actually simpler because the percept is given by the environment rather than calculated by the
Section 4.4. Searching with Partial Observations 145
7 
5 
6 
2 1 
3 
6 
4 
8 
2 [B,Dirty]Right[A,Clean]
7 
5 
Suck
Figure 4.17 Two prediction–update cycles of belief-state maintenance in the kindergarten
vacuum world with local sensing.
agent. Given an initial belief state b, an action a, and a percept o, the new belief state is:
b′ = UPDATE(PREDICT (b, a),o) . (4.6)
Figure 4.17 shows the belief state being maintained in the kindergarten vacuum world with
local sensing, wherein any square may become dirty at any time unless the agent is actively
cleaning it at that moment.
12
In partially observable environments—which include the vast majority of real-world
environments—maintaining one’s belief state is a core function of any intelligent system.
This function goes under various names, including monitoring, ﬁltering and state estima-
MONITORING
FILTERING tion. Equation (4.6) is called a recursive state estimator because it computes the new belief
STA TE ESTIMA TION
RECURSIVE
state from the previous one rather than by examining the entire percept sequence. If the agent
is not to “fall behind,” the computation has to happen as fast as percepts are coming in. As
the environment becomes more complex, the exact update computation becomes infeasible
and the agent will have to compute an approximate belief state, perhaps focusing on the im-
plications of the percept for the aspects of the environment that are of current interest. Most
work on this problem has been done for stochastic, continuous-state environments with the
tools of probability theory, as explained in Chapter 15. Here we will show an example in a
discrete environment with detrministic sensors and nondeterministic actions.
The example concerns a robot with the task of localization: working out where it is,
LOCALIZA TION
given a map of the world and a sequence of percepts and actions. Our robot is placed in the
maze-like environment of Figure 4.18. The robot is equipped with four sonar sensors that
tell whether there is an obstacle—the outer wall or a black square in the ﬁgure—in each of
the four compass directions. We assume that the sensors give perfectly correct data, and that
the robot has a correct map of the enviornment. But unfortunately the robot’s navigational
system is broken, so when it executes aMove action, it moves randomly to one of the adjacent
squares. The robot’s task is to determine its current location.
Suppose the robot has just been switched on, so it does not know where it is. Thus its
initial belief state b consists of the set of all locations. The the robot receives the percept
12 The usual apologies to those who are unfamiliar with the effect of small children on the environment.
146 Chapter 4. Beyond Classical Search
(a) Possible locations of robot after E1 =N S W
(b) Possible locations of robot After E1 =N S W , E2 =N S
Figure 4.18 Possible positions of the robot,⊙, (a) after one observation E1 =NSW and
(b) after a second observationE2 = NS . When sensors are noiseless and the transition model
is accurate, there are no other possible locations for the robot consistent with this sequence
of two observations.
NSW, meaning there are obstacles to the north, west, and south, and does an update using the
equation bo = UPDATE(b), yielding the 4 locations shown in Figure 4.18(a). You can inspect
the maze to see that those are the only four locations that yield the percept NWS .
Next the robot executes a Move action, but the result is nondeterministic. The new be-
lief state, ba = PREDICT (bo,Move), contains all the locations that are one step away from the
locations in bo. When the second percept, NS, arrives, the robot does U PDATE(ba,NS) and
ﬁnds that the belief state has collapsed down to the single location shown in Figure 4.18(b).
That’s the only location that could be the result of
U
PDATE(PREDICT (UPDATE(b,NSW ),Move),NS) .
With nondetermnistic actions the P REDICT step grows the belief state, but the U PDATE step
shrinks it back down—as long as the percepts provide some useful identifying information.
Sometimes the percepts don’t help much for localization: If there were one or more long
east-west corridors, then a robot could receive a long sequence of NS percepts, but never
know where in the corridor(s) it was.
Section 4.5. Online Search Agents and Unknown Environments 147
4.5 O NLINE SEARCH AGENTS AND UNKNOWN ENVIRONMENTS
So far we have concentrated on agents that use ofﬂine search algorithms. They computeOFFLINE SEARCH
a complete solution before setting foot in the real world and then execute the solution. In
contrast, an online search13 agent interleaves computation and action: ﬁrst it takes an action,ONLINE SEARCH
then it observes the environment and computes the next action. Online search is a good idea
in dynamic or semidynamic domains—domains where there is a penalty for sitting around
and computing too long. Online search is also helpful in nondeterministic domains because
it allows the agent to focus its computational efforts on the contingencies that actually arise
rather than those that might happen but probably won’t. Of course, there is a tradeoff: the
more an agent plans ahead, the less often it will ﬁnd itself up the creek without a paddle.
Online search is a necessary idea for unknown environments, where the agent does not
know what states exist or what its actions do. In this state of ignorance, the agent faces an
exploration problem and must use its actions as experiments in order to learn enough to
EXPLORA TION
PROBLEM
make deliberation worthwhile.
The canonical example of online search is a robot that is placed in a new building and
must explore it to build a map that it can use for getting from A to B. Methods for escaping
from labyrinths—required knowledge for aspiring heroes of antiquity—are also examples of
online search algorithms. Spatial exploration is not the only form of exploration, however.
Consider a newborn baby: it has many possible actions but knows the outcomes of none of
them, and it has experienced only a few of the possible states that it can reach. The baby’s
gradual discovery of how the world works is, in part, an online search process.
4.5.1 Online search problems
An online search problem must be solved by an agent executing actions, rather than by pure
computation. We assume a deterministic and fully observable environment (Chapter 17 re-
laxes these assumptions), but we stipulate that the agent knows only the following:
•A
CTIONS (s), which returns a list of actions allowed in state s;
•The step-cost function c(s, a, s′)—note that this cannot be used until the agent knows
that s′ is the outcome; and
•GOAL -TEST (s).
Note in particular that the agent cannot determine R ESULT (s,a) except by actually being
in s and doing a. For example, in the maze problem shown in Figure 4.19, the agent does
not know that going Up from (1,1) leads to (1,2); nor, having done that, does it know that
going Down will take it back to (1,1). This degree of ignorance can be reduced in some
applications—for example, a robot explorer might know how its movement actions work and
be ignorant only of the locations of obstacles.
13 The term “online” is commonly used in computer science to refer to algorithms that must process input data
as they are received rather than waiting for the entire input data set to become available.
148 Chapter 4. Beyond Classical Search
G
S1
2
3
123
Figure 4.19 A simple maze problem. The agent starts at S and must reach G but knows
nothing of the environment.
S
G
S
G
A
A
S G
(a) (b)
Figure 4.20 (a) Two state spaces that might lead an online search agent into a dead end.
Any given agent will fail in at least one of these spaces. (b) A two-dimensional environment
that can cause an online search agent to follow an arbitrarily inefﬁcient route to the goal.
Whichever choice the agent makes, the adversary blocks that route with another long, thin
wall, so that the path followed is much longer than the best possible path.
Finally, the agent might have access to an admissible heuristic function h(s) that es-
timates the distance from the current state to a goal state. For example, in Figure 4.19, the
agent might know the location of the goal and be able to use the Manhattan-distance heuristic.
Typically, the agent’s objective is to reach a goal state while minimizing cost. (Another
possible objective is simply to explore the entire environment.) The cost is the total path cost
of the path that the agent actually travels. It is common to compare this cost with the path
cost of the path the agent would follow if it knew the search space in advance —that is, the
actual shortest path (or shortest complete exploration). In the language of online algorithms,
this is called the competitive ratio; we would like it to be as small as possible.
COMPETITIVE RA TIO
Section 4.5. Online Search Agents and Unknown Environments 149
Although this sounds like a reasonable request, it is easy to see that the best achievable
competitive ratio is inﬁnite in some cases. For example, if some actions are irreversible—IRREVERSIBLE
i.e., they lead to a state from which no action leads back to the previous state—the online
search might accidentally reach a dead-end state from which no goal state is reachable. Per-DEAD END
haps the term “accidentally” is unconvincing—after all, there might be an algorithm that
happens not to take the dead-end path as it explores. Our claim, to be more precise, is thatno
algorithm can avoid dead ends in all state spaces. Consider the two dead-end state spaces in
Figure 4.20(a). To an online search algorithm that has visited states S and A, the two state
spaces look identical, so it must make the same decision in both. Therefore, it will fail in
one of them. This is an example of an adversary argument—we can imagine an adversaryADVERSARY
ARGUMENT
constructing the state space while the agent explores it and putting the goals and dead ends
wherever it chooses.
Dead ends are a real difﬁculty for robot exploration—staircases, ramps, cliffs, one-way
streets, and all kinds of natural terrain present opportunities for irreversible actions. To make
progress, we simply assume that the state space issafely explorable—that is, some goal state
SAFEL Y EXPLORABLE
is reachable from every reachable state. State spaces with reversible actions, such as mazes
and 8-puzzles, can be viewed as undirected graphs and are clearly safely explorable.
Even in safely explorable environments, no bounded competitive ratio can be guaran-
teed if there are paths of unbounded cost. This is easy to show in environments with irre-
versible actions, but in fact it remains true for the reversible case as well, as Figure 4.20(b)
shows. For this reason, it is common to describe the performance of online search algorithms
in terms of the size of the entire state space rather than just the depth of the shallowest goal.
4.5.2 Online search agents
After each action, an online agent receives a percept telling it what state it has reached; from
this information, it can augment its map of the environment. The current map is used to
decide where to go next. This interleaving of planning and action means that online search
algorithms are quite different from the ofﬂine search algorithms we have seen previously. For
example, ofﬂine algorithms such as A
∗ can expand a node in one part of the space and then
immediately expand a node in another part of the space, because node expansion involves
simulated rather than real actions. An online algorithm, on the other hand, can discover
successors only for a node that it physically occupies. To avoid traveling all the way across
the tree to expand the next node, it seems better to expand nodes in a local order. Depth-ﬁrst
search has exactly this property because (except when backtracking) the next node expanded
is a child of the previous node expanded.
An online depth-ﬁrst search agent is shown in Figure 4.21. This agent stores its map
in a table, R
ESULT [s,a], that records the state resulting from executing action a in state s.
Whenever an action from the current state has not been explored, the agent tries that action.
The difﬁculty comes when the agent has tried all the actions in a state. In ofﬂine depth-ﬁrst
search, the state is simply dropped from the queue; in an online search, the agent has to
backtrack physically. In depth-ﬁrst search, this means going back to the state from which the
agent most recently entered the current state. To achieve that, the algorithm keeps a table that
150 Chapter 4. Beyond Classical Search
function ONLINE -DFS-A GENT (s′) returns an action
inputs: s′, a percept that identiﬁes the current state
persistent: result, a table indexed by state and action, initially empty
untried, a table that lists, for each state, the actions not yet tried
unbacktracked, a table that lists, for each state, the backtracks not yet tried
s, a, the previous state and action, initially null
if GOAL -TEST (s′) then return stop
if s′ i san e ws t a t e( n o ti nuntried) then untried[s′]←ACTIONS (s′)
if s is not null then
result[s,a]←s′
add s to the front of unbacktracked[s′]
if untried[s′]i se m p t ythen
if unbacktracked[s′]i se m p t ythen return stop
else a←an action b such that result[s′,b]=P OP(unbacktracked[s′])
else a←POP(untried[s′])
s←s′
return a
Figure 4.21 An online search agent that uses depth-ﬁrst exploration. The agent is appli-
cable only in state spaces in which every action can be “undone” by some other action.
lists, for each state, the predecessor states to which the agent has not yet backtracked. If the
agent has run out of states to which it can backtrack, then its search is complete.
We recommend that the reader trace through the progress of O NLINE -DFS-A GENT
when applied to the maze given in Figure 4.19. It is fairly easy to see that the agent will, in
the worst case, end up traversing every link in the state space exactly twice. For exploration,
this is optimal; for ﬁnding a goal, on the other hand, the agent’s competitive ratio could be
arbitrarily bad if it goes off on a long excursion when there is a goal right next to the initial
state. An online variant of iterative deepening solves this problem; for an environment that is
a uniform tree, the competitive ratio of such an agent is a small constant.
Because of its method of backtracking, O
NLINE -DFS-A GENT works only in state
spaces where the actions are reversible. There are slightly more complex algorithms that
work in general state spaces, but no such algorithm has a bounded competitive ratio.
4.5.3 Online local search
Like depth-ﬁrst search, hill-climbing search has the property of locality in its node expan-
sions. In fact, because it keeps just one current state in memory, hill-climbing search is
already an online search algorithm! Unfortunately, it is not very useful in its simplest form
because it leaves the agent sitting at local maxima with nowhere to go. Moreover, random
restarts cannot be used, because the agent cannot transport itself to a new state.
Instead of random restarts, one might consider using a random walk to explore theRANDOM WALK
environment. A random walk simply selects at random one of the available actions from the
Section 4.5. Online Search Agents and Unknown Environments 151
S G
Figure 4.22 An environment in which a random walk will take exponentially many steps
to ﬁnd the goal.
current state; preference can be given to actions that have not yet been tried. It is easy to
prove that a random walk will eventually ﬁnd a goal or complete its exploration, provided
that the space is ﬁnite.14 On the other hand, the process can be very slow. Figure 4.22 shows
an environment in which a random walk will take exponentially many steps to ﬁnd the goal
because, at each step, backward progress is twice as likely as forward progress. The example
is contrived, of course, but there are many real-world state spaces whose topology causes
these kinds of “traps” for random walks.
Augmenting hill climbing with memory rather than randomness turns out to be a more
effective approach. The basic idea is to store a “current best estimate” H(s) of the cost to
reach the goal from each state that has been visited. H(s) starts out being just the heuristic
estimate h(s) and is updated as the agent gains experience in the state space. Figure 4.23
shows a simple example in a one-dimensional state space. In (a), the agent seems to be
stuck in a ﬂat local minimum at the shaded state. Rather than staying where it is, the agent
should follow what seems to be the best path to the goal given the current cost estimates for
its neighbors. The estimated cost to reach the goal through a neighbor s
′ is the cost to get
to s′ plus the estimated cost to get to a goal from there—that is, c(s, a, s′)+ H(s′).I n t h e
example, there are two actions, with estimated costs1+9 and 1+2 , so it seems best to move
right. Now, it is clear that the cost estimate of 2 for the shaded state was overly optimistic.
Since the best move cost 1 and led to a state that is at least 2 steps from a goal, the shaded
state must be at least 3 steps from a goal, so its H should be updated accordingly, as shown
in Figure 4.23(b). Continuing this process, the agent will move back and forth twice more,
updating H each time and “ﬂattening out” the local minimum until it escapes to the right.
An agent implementing this scheme, which is called learning real-time A
∗ (LRTA∗), isLRT A*
shown in Figure 4.24. Like O NLINE -DFS-A GENT , it builds a map of the environment in
the result table. It updates the cost estimate for the state it has just left and then chooses the
“apparently best” move according to its current cost estimates. One important detail is that
actions that have not yet been tried in a states are always assumed to lead immediately to the
goal with the least possible cost, namelyh(s).T h i soptimism under uncertainty encouragesOPTIMISM UNDER
UNCERT AINTY
the agent to explore new, possibly promising paths.
An LRTA∗ agent is guaranteed to ﬁnd a goal in any ﬁnite, safely explorable environment.
Unlike A∗, however, it is not complete for inﬁnite state spaces—there are cases where it can be
led inﬁnitely astray. It can explore an environment ofn states inO(n2) steps in the worst case,
14 Random walks are complete on inﬁnite one-dimensional and two-dimensional grids. On a three-dimensional
grid, the probability that the walk ever returns to the starting point is only about 0.3405 (Hughes, 1995).
152 Chapter 4. Beyond Classical Search
1
2
1 11 1 11
1 1 11 1 11
1 1 11 1 11
2
2
3
4
4
4
3
3
3
1 1 11 1 11
3
1 1 11 1 11
5
3
5
5
4
(a)
(b)
(c)
(d)
(e)
8 9
8
9
89
8
9
89
44
34
Figure 4.23 Five iterations of LRTA∗ on a one-dimensional state space. Each state is
labeled with H(s), the current cost estimate to reach a goal, and each link is labeled with its
step cost. The shaded state marks the location of the agent, and the updated cost estimates at
each iteration are circled.
function LRTA*-A GENT (s′) returns an action
inputs: s′, a percept that identiﬁes the current state
persistent: result, a table, indexed by state and action, initially empty
H , a table of cost estimates indexed by state, initially empty
s, a, the previous state and action, initially null
if GOAL -TEST (s′) then return stop
if s′ i san e ws t a t e( n o ti nH ) then H [s′]←h(s′)
if s is not null
result[s,a]←s′
H [s]← min
b ∈ ACTIONS (s)
LRTA*-C OST (s,b,result[s,b],H )
a←an action b in ACTIONS (s′) that minimizes LRTA*-C OST (s′,b,result[s′,b],H )
s←s′
return a
function LRTA*-C OST (s,a,s′,H ) returns a cost estimate
if s′ is undeﬁned then return h(s)
else return c(s, a, s′)+ H[s′]
Figure 4.24 LRTA*-A GENT selects an action according to the values of neighboring
states, which are updated as the agent moves about the state space.

Section 4.6. Summary 153
but often does much better. The LRTA∗ agent is just one of a large family of online agents that
one can deﬁne by specifying the action selection rule and the update rule in different ways.
We discuss this family, developed originally for stochastic environments, in Chapter 21.
4.5.4 Learning in online search
The initial ignorance of online search agents provides several opportunities for learning. First,
the agents learn a “map” of the environment—more precisely, the outcome of each action in
each state—simply by recording each of their experiences. (Notice that the assumption of
deterministic environments means that one experience is enough for each action.) Second,
the local search agents acquire more accurate estimates of the cost of each state by using local
updating rules, as in LRTA
∗. In Chapter 21, we show that these updates eventually converge
to exact values for every state, provided that the agent explores the state space in the right
way. Once exact values are known, optimal decisions can be taken simply by moving to the
lowest-cost successor—that is, pure hill climbing is then an optimal strategy.
If you followed our suggestion to trace the behavior of O
NLINE -DFS-A GENT in the
environment of Figure 4.19, you will have noticed that the agent is not very bright. For
example, after it has seen that the Up action goes from (1,1) to (1,2), the agent still has no
idea that the Down action goes back to (1,1) or that the Up action also goes from (2,1) to
(2,2), from (2,2) to (2,3), and so on. In general, we would like the agent to learn that Up
increases the y-coordinate unless there is a wall in the way, that Down reduces it, and so on.
For this to happen, we need two things. First, we need a formal and explicitly manipulable
representation for these kinds of general rules; so far, we have hidden the information inside
the black box called the R
ESULT function. Part III is devoted to this issue. Second, we need
algorithms that can construct suitable general rules from the speciﬁc observations made by
the agent. These are covered in Chapter 18.
4.6 S UMMARY
This chapter has examined search algorithms for problems beyond the “classical” case of
ﬁnding the shortest path to a goal in an observable, deterministic, discrete environment.
•Local search methods such as hill climbing operate on complete-state formulations,
keeping only a small number of nodes in memory. Several stochastic algorithms have
been developed, including simulated annealing, which returns optimal solutions when
given an appropriate cooling schedule.
•Many local search methods apply also to problems in continuous spaces. Linear pro-
gramming and convex optimization problems obey certain restrictions on the shape
of the state space and the nature of the objective function, and admit polynomial-time
algorithms that are often extremely efﬁcient in practice.
•A genetic algorithm is a stochastic hill-climbing search in which a large population of
states is maintained. New states are generated by mutation and by crossover,w h i c h
combines pairs of states from the population.
154 Chapter 4. Beyond Classical Search
•In nondeterministic environments, agents can apply AND –OR search to generate con-
tingent plans that reach the goal regardless of which outcomes occur during execution.
•When the environment is partially observable, the belief state represents the set of
possible states that the agent might be in.
•Standard search algorithms can be applied directly to belief-state space to solvesensor-
less problems, and belief-state AND –OR search can solve general partially observable
problems. Incremental algorithms that construct solutions state-by-state within a belief
state are often more efﬁcient.
•Exploration problems arise when the agent has no idea about the states and actions of
its environment. For safely explorable environments, online search agents can build a
map and ﬁnd a goal if one exists. Updating heuristic estimates from experience provides
an effective method to escape from local minima.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Local search techniques have a long history in mathematics and computer science. Indeed,
the Newton–Raphson method (Newton, 1671; Raphson, 1690) can be seen as a very efﬁ-
cient local search method for continuous spaces in which gradient information is available.
Brent (1973) is a classic reference for optimization algorithms that do not require such in-
formation. Beam search, which we have presented as a local search algorithm, originated
as a bounded-width variant of dynamic programming for speech recognition in the H
ARPY
system (Lowerre, 1976). A related algorithm is analyzed in depth by Pearl (1984, Ch. 5).
The topic of local search was reinvigorated in the early 1990s by surprisingly good re-
sults for large constraint-satisfaction problems such as n-queens (Minton et al. , 1992) and
logical reasoning (Selman et al. , 1992) and by the incorporation of randomness, multiple
simultaneous searches, and other improvements. This renaissance of what Christos Papadim-
itriou has called “New Age” algorithms also sparked increased interest among theoretical
computer scientists (Koutsoupias and Papadimitriou, 1992; Aldous and Vazirani, 1994). In
the ﬁeld of operations research, a variant of hill climbing calledtabu search has gained popu-
TABU SEARCH
larity (Glover and Laguna, 1997). This algorithm maintains a tabu list ofk previously visited
states that cannot be revisited; as well as improving efﬁciency when searching graphs, this list
can allow the algorithm to escape from some local minima. Another useful improvement on
hill climbing is the S TAGE algorithm (Boyan and Moore, 1998). The idea is to use the local
maxima found by random-restart hill climbing to get an idea of the overall shape of the land-
scape. The algorithm ﬁts a smooth surface to the set of local maxima and then calculates the
global maximum of that surface analytically. This becomes the new restart point. The algo-
rithm has been shown to work in practice on hard problems. Gomeset al. (1998) showed that
the run times of systematic backtracking algorithms often have a heavy-tailed distribution,
HEAVY -T AILED
DISTRIBUTION
which means that the probability of a very long run time is more than would be predicted if
the run times were exponentially distributed. When the run time distribution is heavy-tailed,
random restarts ﬁnd a solution faster, on average, than a single run to completion.
Bibliographical and Historical Notes 155
Simulated annealing was ﬁrst described by Kirkpatrick et al. (1983), who borrowed
directly from the Metropolis algorithm (which is used to simulate complex systems in
physics (Metropolis et al., 1953) and was supposedly invented at a Los Alamos dinner party).
Simulated annealing is now a ﬁeld in itself, with hundreds of papers published every year.
Finding optimal solutions in continuous spaces is the subject matter of several ﬁelds,
including optimization theory, optimal control theory,a n dt h ecalculus of variations.T h e
basic techniques are explained well by Bishop (1995); Press et al. (2007) cover a wide range
of algorithms and provide working software.
As Andrew Moore points out, researchers have taken inspiration for search and opti-
mization algorithms from a wide variety of ﬁelds of study: metallurgy (simulated annealing),
biology (genetic algorithms), economics (market-based algorithms), entomology (ant colony
optimization), neurology (neural networks), animal behavior (reinforcement learning), moun-
taineering (hill climbing), and others.
Linear programming (LP) was ﬁrst studied systematically by the Russian mathemati-
cian Leonid Kantorovich (1939). It was one of the ﬁrst applications of computers; the sim-
plex algorithm (Dantzig, 1949) is still used despite worst-case exponential complexity. Kar-
markar (1984) developed the far more efﬁcient family of interior-point methods, which was
shown to have polynomial complexity for the more general class of convex optimization prob-
lems by Nesterov and Nemirovski (1994). Excellent introductions to convex optimization are
provided by Ben-Tal and Nemirovski (2001) and Boyd and Vandenberghe (2004).
Work by Sewall Wright (1931) on the concept of a ﬁtness landscape was an impor-
tant precursor to the development of genetic algorithms. In the 1950s, several statisticians,
including Box (1957) and Friedman (1959), used evolutionary techniques for optimization
problems, but it wasn’t until Rechenberg (1965) introduced evolution strategies to solve op-
EVOLUTION
STRA TEGY
timization problems for airfoils that the approach gained popularity. In the 1960s and 1970s,
John Holland (1975) championed genetic algorithms, both as a useful tool and as a method
to expand our understanding of adaptation, biological or otherwise (Holland, 1995). The ar-
tiﬁcial life movement (Langton, 1995) takes this idea one step further, viewing the products
ARTIFICIAL LIFE
of genetic algorithms as organisms rather than solutions to problems. Work in this ﬁeld by
Hinton and Nowlan (1987) and Ackley and Littman (1991) has done much to clarify the im-
plications of the Baldwin effect. For general background on evolution, we recommend Smith
and Szathm´ary (1999), Ridley (2004), and Carroll (2007).
Most comparisons of genetic algorithms to other approaches (especially stochastic hill
climbing) have found that the genetic algorithms are slower to converge (O’Reilly and Op-
pacher, 1994; Mitchell et al., 1996; Juels and Wattenberg, 1996; Baluja, 1997). Such ﬁndings
are not universally popular within the GA community, but recent attempts within that com-
munity to understand population-based search as an approximate form of Bayesian learning
(see Chapter 20) might help close the gap between the ﬁeld and its critics (Pelikan et al. ,
1999). The theory of quadratic dynamical systems may also explain the performance of
GAs (Rabani et al., 1998). See Lohn et al. (2001) for an example of GAs applied to antenna
design, and Renner and Ekart (2003) for an application to computer-aided design.
The ﬁeld of genetic programming is closely related to genetic algorithms. The princi-
GENETIC
PROGRAMMING
pal difference is that the representations that are mutated and combined are programs rather
156 Chapter 4. Beyond Classical Search
than bit strings. The programs are represented in the form of expression trees; the expressions
can be in a standard language such as Lisp or can be specially designed to represent circuits,
robot controllers, and so on. Crossover involves splicing together subtrees rather than sub-
strings. This form of mutation guarantees that the offspring are well-formed expressions,
which would not be the case if programs were manipulated as strings.
Interest in genetic programming was spurred by John Koza’s work (Koza, 1992, 1994),
but it goes back at least to early experiments with machine code by Friedberg (1958) and
with ﬁnite-state automata by Fogel et al. (1966). As with genetic algorithms, there is debate
about the effectiveness of the technique. Koza et al. (1999) describe experiments in the use
of genetic programming to design circuit devices.
The journals Evolutionary Computation and IEEE Transactions on Evolutionary Com-
putation cover genetic algorithms and genetic programming; articles are also found in Com-
plex Systems , Adaptive Behavior ,a n d Artiﬁcial Life . The main conference is the Genetic
and Evolutionary Computation Conference (GECCO). Good overview texts on genetic algo-
rithms are given by Mitchell (1996), Fogel (2000), and Langdon and Poli (2002), and by the
free online book by Poli et al. (2008).
The unpredictability and partial observability of real environments were recognized
early on in robotics projects that used planning techniques, including Shakey (Fikes et al. ,
1972) and F
REDDY (Michie, 1974). The problems received more attention after the publica-
tion of McDermott’s (1978a) inﬂuential article, Planning and Acting.
The ﬁrst work to make explicit use ofAND –OR trees seems to have been Slagle’s SAINT
program for symbolic integration, mentioned in Chapter 1. Amarel (1967) applied the idea
to propositional theorem proving, a topic discussed in Chapter 7, and introduced a search
algorithm similar to A
ND-OR-GRAPH -SEARCH . The algorithm was further developed and
formalized by Nilsson (1971), who also described AO ∗—which, as its name suggests, ﬁnds
optimal solutions given an admissible heuristic. AO∗ was analyzed and improved by Martelli
and Montanari (1973). AO ∗ is a top-down algorithm; a bottom-up generalization of A ∗ is
A∗LD, for A∗ Lightest Derivation (Felzenszwalb and McAllester, 2007). Interest in AND –OR
search has undergone a revival in recent years, with new algorithms for ﬁnding cyclic solu-
tions (Jimenez and Torras, 2000; Hansen and Zilberstein, 2001) and new techniques inspired
by dynamic programming (Bonet and Geffner, 2005).
The idea of transforming partially observable problems into belief-state problems orig-
inated with Astrom (1965) for the much more complex case of probabilistic uncertainty (see
Chapter 17). Erdmann and Mason (1988) studied the problem of robotic manipulation with-
out sensors, using a continuous form of belief-state search. They showed that it was possible
to orient a part on a table from an arbitrary initial position by a well-designed sequence of tilt-
ing actions. More practical methods, based on a series of precisely oriented diagonal barriers
across a conveyor belt, use the same algorithmic insights (Wiegley et al., 1996).
The belief-state approach was reinvented in the context of sensorless and partially ob-
servable search problems by Genesereth and Nourbakhsh (1993). Additional work was done
on sensorless problems in the logic-based planning community (Goldman and Boddy, 1996;
Smith and Weld, 1998). This work has emphasized concise representations for belief states,
as explained in Chapter 11. Bonet and Geffner (2000) introduced the ﬁrst effective heuristics
Exercises 157
for belief-state search; these were reﬁned by Bryce et al. (2006). The incremental approach
to belief-state search, in which solutions are constructed incrementally for subsets of states
within each belief state, was studied in the planning literature by Kurienet al. (2002); several
new incremental algorithms were introduced for nondeterministic, partially observable prob-
lems by Russell and Wolfe (2005). Additional references for planning in stochastic, partially
observable environments appear in Chapter 17.
Algorithms for exploring unknown state spaces have been of interest for many centuries.
Depth-ﬁrst search in a maze can be implemented by keeping one’s left hand on the wall; loops
can be avoided by marking each junction. Depth-ﬁrst search fails with irreversible actions;
the more general problem of exploring Eulerian graphs (i.e., graphs in which each node has
EULERIAN GRAPH
equal numbers of incoming and outgoing edges) was solved by an algorithm due to Hierholzer
(1873). The ﬁrst thorough algorithmic study of the exploration problem for arbitrary graphs
was carried out by Deng and Papadimitriou (1990), who developed a completely general
algorithm but showed that no bounded competitive ratio is possible for exploring a general
graph. Papadimitriou and Yannakakis (1991) examined the question of ﬁnding paths to a goal
in geometric path-planning environments (where all actions are reversible). They showed that
a small competitive ratio is achievable with square obstacles, but with general rectangular
obstacles no bounded ratio can be achieved. (See Figure 4.20.)
The LRTA
∗ algorithm was developed by Korf (1990) as part of an investigation into
real-time search for environments in which the agent must act after searching for only aREAL-TIME SEARCH
ﬁxed amount of time (a common situation in two-player games). LRTA ∗ is in fact a special
case of reinforcement learning algorithms for stochastic environments (Bartoet al., 1995). Its
policy of optimism under uncertainty—always head for the closest unvisited state—can result
in an exploration pattern that is less efﬁcient in the uninformed case than simple depth-ﬁrst
search (Koenig, 2000). Dasgupta et al. (1994) show that online iterative deepening search is
optimally efﬁcient for ﬁnding a goal in a uniform tree with no heuristic information. Sev-
eral informed variants on the LRTA
∗ theme have been developed with different methods for
searching and updating within the known portion of the graph (Pemberton and Korf, 1992).
As yet, there is no good understanding of how to ﬁnd goals with optimal efﬁciency when
using heuristic information.
EXERCISES
4.1 Give the name of the algorithm that results from each of the following special cases:
a. Local beam search with k =1 .
b. Local beam search with one initial state and no limit on the number of states retained.
c. Simulated annealing with T =0 at all times (and omitting the termination test).
d. Simulated annealing with T =∞at all times.
e. Genetic algorithm with population size N =1 .
158 Chapter 4. Beyond Classical Search
4.2 Exercise 3.16 considers the problem of building railway tracks under the assumption
that pieces ﬁt exactly with no slack. Now consider the real problem, in which pieces don’t
ﬁt exactly but allow for up to 10 degrees of rotation to either side of the “proper” alignment.
Explain how to formulate the problem so it could be solved by simulated annealing.
4.3 In this exercise, we explore the use of local search methods to solve TSPs of the type
deﬁned in Exercise 3.30.
a. Implement and test a hill-climbing method to solve TSPs. Compare the results with op-
timal solutions obtained from the A∗ algorithm with the MST heuristic (Exercise 3.30).
b. Repeat part (a) using a genetic algorithm instead of hill climbing. You may want to
consult Larra˜naga et al. (1999) for some suggestions for representations.
4.4 Generate a large number of 8-puzzle and 8-queens instances and solve them (where pos-
sible) by hill climbing (steepest-ascent and ﬁrst-choice variants), hill climbing with random
restart, and simulated annealing. Measure the search cost and percentage of solved problems
and graph these against the optimal solution cost. Comment on your results.
4.5 The A
ND-OR-GRAPH -SEARCH algorithm in Figure 4.11 checks for repeated states
only on the path from the root to the current state. Suppose that, in addition, the algorithm
were to store every visited state and check against that list. (See B
READTH -FIRST-SEARCH
in Figure 3.11 for an example.) Determine the information that should be stored and how the
algorithm should use that information when a repeated state is found. (Hint: You will need to
distinguish at least between states for which a successful subplan was constructed previously
and states for which no subplan could be found.) Explain how to use labels, as deﬁned in
Section 4.3.3, to avoid having multiple copies of subplans.
4.6 Explain precisely how to modify the AND-OR-GRAPH -SEARCH algorithm to generate
a cyclic plan if no acyclic plan exists. You will need to deal with three issues: labeling the plan
steps so that a cyclic plan can point back to an earlier part of the plan, modifying OR-SEARCH
so that it continues to look for acyclic plans after ﬁnding a cyclic plan, and augmenting the
plan representation to indicate whether a plan is cyclic. Show how your algorithm works on
(a) the slippery vacuum world, and (b) the slippery, erratic vacuum world. You might wish to
use a computer implementation to check your results.
4.7 In Section 4.4.1 we introduced belief states to solve sensorless search problems. A
sequence of actions solves a sensorless problem if it maps every physical state in the initial
belief state b to a goal state. Suppose the agent knows h
∗(s), the true optimal cost of solving
the physical state s in the fully observable problem, for every states in b. Find an admissible
heuristic h(b) for the sensorless problem in terms of these costs, and prove its admissibilty.
Comment on the accuracy of this heuristic on the sensorless vacuum problem of Figure 4.14.
How well does A
∗ perform?
4.8 This exercise explores subset–superset relations between belief states in sensorless or
partially observable environments.
a. Prove that if an action sequence is a solution for a belief state b, it is also a solution for
any subset of b. Can anything be said about supersets of b?
Exercises 159
b. Explain in detail how to modify graph search for sensorless problems to take advantage
of your answers in (a).
c. Explain in detail how to modify AND –OR search for partially observable problems,
beyond the modiﬁcations you describe in (b).
4.9 On page 139 it was assumed that a given action would have the same cost when ex-
ecuted in any physical state within a given belief state. (This leads to a belief-state search
problem with well-deﬁned step costs.) Now consider what happens when the assumption
does not hold. Does the notion of optimality still make sense in this context, or does it require
modiﬁcation? Consider also various possible deﬁnitions of the “cost” of executing an action
in a belief state; for example, we could use the minimum of the physical costs; or the maxi-
mum; or a cost interval with the lower bound being the minimum cost and the upper bound
being the maximum; or just keep the set of all possible costs for that action. For each of these,
explore whether A
∗ (with modiﬁcations if necessary) can return optimal solutions.
4.10 Consider the sensorless version of the erratic vacuum world. Draw the belief-state
space reachable from the initial belief state{1,2,3,4,5,6,7, 8}, and explain why the problem
is unsolvable.
4.11 We can turn the navigation problem in Exercise 3.7 into an environment as follows:
•The percept will be a list of the positions, relative to the agent , of the visible vertices.
The percept does not include the position of the robot! The robot must learn its own po-
sition from the map; for now, you can assume that each location has a different “view.”
•Each action will be a vector describing a straight-line path to follow. If the path is
unobstructed, the action succeeds; otherwise, the robot stops at the point where its
path ﬁrst intersects an obstacle. If the agent returns a zero motion vector and is at the
goal (which is ﬁxed and known), then the environment teleports the agent to a random
location (not inside an obstacle).
•The performance measure charges the agent 1 point for each unit of distance traversed
and awards 1000 points each time the goal is reached.
a. Implement this environment and a problem-solving agent for it. After each teleporta-
tion, the agent will need to formulate a new problem, which will involve discovering its
current location.
b. Document your agent’s performance (by having the agent generate suitable commentary
as it moves around) and report its performance over 100 episodes.
c. Modify the environment so that 30% of the time the agent ends up at an unintended
destination (chosen randomly from the other visible vertices if any; otherwise, no move
at all). This is a crude model of the motion errors of a real robot. Modify the agent
so that when such an error is detected, it ﬁnds out where it is and then constructs a
plan to get back to where it was and resume the old plan. Remember that sometimes
getting back to where it was might also fail! Show an example of the agent successfully
overcoming two successive motion errors and still reaching the goal.
160 Chapter 4. Beyond Classical Search
d. Now try two different recovery schemes after an error: (1) head for the closest vertex on
the original route; and (2) replan a route to the goal from the new location. Compare the
performance of the three recovery schemes. Would the inclusion of search costs affect
the comparison?
e. Now suppose that there are locations from which the view is identical. (For example,
suppose the world is a grid with square obstacles.) What kind of problem does the agent
now face? What do solutions look like?
4.12 Suppose that an agent is in a 3× 3 maze environment like the one shown in Fig-
ure 4.19. The agent knows that its initial location is (1,1), that the goal is at (3,3), and that the
actions Up, Down, Left, Right have their usual effects unless blocked by a wall. The agent
does not know where the internal walls are. In any given state, the agent perceives the set of
legal actions; it can also tell whether the state is one it has visited before.
a. Explain how this online search problem can be viewed as an ofﬂine search in belief-state
space, where the initial belief state includes all possible environment conﬁgurations.
How large is the initial belief state? How large is the space of belief states?
b. How many distinct percepts are possible in the initial state?
c. Describe the ﬁrst few branches of a contingency plan for this problem. How large
(roughly) is the complete plan?
Notice that this contingency plan is a solution forevery possible environment ﬁtting the given
description. Therefore, interleaving of search and execution is not strictly necessary even in
unknown environments.
4.13 In this exercise, we examine hill climbing in the context of robot navigation, using the
environment in Figure 3.31 as an example.
a. Repeat Exercise 4.11 using hill climbing. Does your agent ever get stuck in a local
minimum? Is it possible for it to get stuck with convex obstacles?
b. Construct a nonconvex polygonal environment in which the agent gets stuck.
c. Modify the hill-climbing algorithm so that, instead of doing a depth-1 search to decide
where to go next, it does a depth- k search. It should ﬁnd the best k-step path and do
one step along it, and then repeat the process.
d.I s t h e r e s o m ek for which the new algorithm is guaranteed to escape from local minima?
e. Explain how LRTA∗ enables the agent to escape from local minima in this case.
4.14 Like DFS, online DFS is incomplete for reversible state spaces with inﬁnite paths. For
example, suppose that states are points on the inﬁnite two-dimensional grid and actions are
unit vectors (1,0), (0,1), (−1,0), (0,−1), tried in that order. Show that online DFS starting
at (0,0) will not reach (1,−1). Suppose the agent can observe, in addition to its current
state, all successor states and the actions that would lead to them. Write an algorithm that
is complete even for bidirected state spaces with inﬁnite paths. What states does it visit in
reaching (1,−1)?


END_INSTRUCTION
