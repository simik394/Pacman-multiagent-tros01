
START_INSTRUCTION
You are an expert AI tutor creating a comprehensive study guide.
Your task is to rewrite and expand the "Lesson Notes" provided below into a cohesive, readable study text.

1. **Structure:** Follow the exact structure and order of the topics in the "Lesson Notes". Do not reorder them.
2. **Enrichment:** Use the "Textbook Context" provided to expand on every bullet point in the notes.
    - If the notes mention a concept (e.g., "Rationality"), define it precisely using the textbook.
    - If the notes list algorithms (e.g., "A*", "Simulated Annealing"), explain how they work, their complexity, and pros/cons using the textbook details.
    - Add examples from the textbook where appropriate.
3. **Tone:** Academic but accessible study material. Not just bullet points—write paragraphs where necessary to explain complex ideas.
4. **Language:** The output must be in **Czech** (since the lesson notes are in Czech), but you can keep standard English AI terminology (like "Overfitting") in parentheses.

--- LESSON NOTES (Skeleton) ---
Strojové učení

Metody učení:
• učení se zapamatováním
• učení se z instrukcí
• učení se z analogie
• učení se na základě vysvětlení
• učení se z příkladů
• učení se pozorováním a objevováním
Zpětná vazba v procesu učení
• příklady jsou klasifikované (učení s učitelem)
• malý počet příkladů klasifikovaných, zbytek neklasifikovaný (částečné učení s učitelem)
• algoritmus se "ptá" učitele na zařazení příkladů (aktivní učení)
• nepřímé náznaky odvozené s chování učitele
• žádné (učení bez učitele)
Úloha empirického učení z dat
•
• Cílem je nalézt takové znalosti, aby se minimalizovala chyba klasifikace
Pak očekáváme, že tyto znalosti budou dobře korespondovat i s dalšími příklady, které nebyly 
použity v procesu učení.
○ předpoklad stacionarity konceptu
•
Učení na základě podobnosti
• Objekty, patřící do téže třídy mají podobné charakteristiky
• nebezpečí garbage-in, garbage-out
Induktivní učení
• Hledáme hypotézu h, která bude konsistentní s (neznámou) funkcí f na trénovacích datech
Bias vyjadřuje kvalitu hypotézy, variance vyjadřuje robustnost hypotézy
Overfitting
situace, kdy se model v průběhu učení soustředí na trénovací data a nemá dostatečnou 
schopnost generalizovat
•
• chyba na trénovacích datech klesé
• → chyba na testovacích datech začíná růst
• složité algoritmy
Underfitting
situace, ve které se model nedokáže naučit tak dobře souvislost mezi daty, a tak je začne 
predikovat špatně
•
• "jednoduché" algoritmy
Učení jako prohledávání
• hledáme struktury i parametry modelu
• hledání optimální hypotézy v prostoru možných modelů
• př. rozhodovací stromy
Učení jako aproximace
• hledáme "jen" parametry modelu
• na základě konečného počtu bodů [xi ,yi] se snažíme určit parametry předpokládané hypotézy 
y = h(x)
• aproximace neznámé cílové funkce f(x), která mapuje vstup x na odpovídající výstupy y
• př. neuronové sítě
Neuronová síť
• představuje výpočetní systém inspirovaný biologickými neuronovými sítěmi
n-tice (U, W, A, O, net, ex)
○ U je konečná množina výpočetních jednotek (neuronů),
○ W je struktura sítě, tedy zobrazení z U X U do R (vazby mezi neurony),
○ A je zobrazení definující aktivační funkci
Au : R → R pro každou jednotku u,
○ O je zobrazení definující výstupní funkci 
Ou : R → R pro každou jednotku u, 
○ net je zobrazení, definující vstupní funkci (přenosovou funkci) 
Iu : (R X R)U → R pro každou jednotku u,
ex je externí vstupní funkce ex: U → R , která každé jednotce u přiřadí externí vstup 
jakožto reálnou hodnotu exU = ex(u) R.
○
•
•
PAC teorie (Probably Aproximately Correct)
slouží k analýze, zda se model strojového učení dokáže efektivně naučit generalizovat z 
trénovacích dat na neviděná data.
•
Poskytuje formální záruky, že model s dostatečným počtem dat dokáže dosáhnout chyby menší 
než zvolená hodnota ϵ pravděpodobností alespoň 1−δ. 
•
PAC teorie také odhaduje minimální počet trénovacích vzorků potřebných k dosažení těchto 
záruk a zavádí pojmy, jako je VC dimenze, která měří kapacitu modelu. 
•
Díky tomu pomáhá vyhnout se přeučení a nalézt rovnováhu mezi komplexitou modelu a 
požadavky na data.
•
No free lunch
• Pro libovolné dva algoritmy A a B platí, že je-li algoritmus A lepší (ve smyslu chyby) než
Pro libovolné dva algoritmy A a B platí, že je-li algoritmus A lepší (ve smyslu chyby) než
algoritmus B na nějakých k datových souborech (úlohách), pak existuje jiných k datových 
souborů (úloh), na kterých je algoritmus B lepší než algoritmus A.
•
• neexistuje univerzálně nejlepší algoritmus
Ošklivé kačátko
• Počet atributů, ve kterých se popisy libovolných dvou příkladů shodují, je konstantní.
• Klasifikace předpokládá nějaký bias

--- TEXTBOOK CONTEXT (Source Material) ---


--- BOOK CHAPTER: 18_Learning_from_Examples ---

18
LEARNING FROM
EXAMPLES
In which we describe agents that can improve their behavior through diligent
study of their own experiences.
An agent is learning if it improves its performance on future tasks after making observationsLEARNING
about the world. Learning can range from the trivial, as exhibited by jotting down a phone
number, to the profound, as exhibited by Albert Einstein, who inferred a new theory of the
universe. In this chapter we will concentrate on one class of learning problem, which seems
restricted but actually has vast applicability: from a collection of input–output pairs, learn a
function that predicts the output for new inputs.
Why would we want an agent to learn? If the design of the agent can be improved,
why wouldn’t the designers just program in that improvement to begin with? There are three
main reasons. First, the designers cannot anticipate all possible situations that the agent
might ﬁnd itself in. For example, a robot designed to navigate mazes must learn the layout
of each new maze it encounters. Second, the designers cannot anticipate all changes over
time; a program designed to predict tomorrow’s stock market prices must learn to adapt when
conditions change from boom to bust. Third, sometimes human programmers have no idea
how to program a solution themselves. For example, most people are good at recognizing the
faces of family members, but even the best programmers are unable to program a computer
to accomplish that task, except by using learning algorithms. This chapter ﬁrst gives an
overview of the various forms of learning, then describes one popular approach, decision-
tree learning, in Section 18.3, followed by a theoretical analysis of learning in Sections 18.4
and 18.5. We look at various learning systems used in practice: linear models, nonlinear
models (in particular, neural networks), nonparametric models, and support vector machines.
Finally we show how ensembles of models can outperform a single model.
18.1 F ORMS OF LEARNING
Any component of an agent can be improved by learning from data. The improvements, and
the techniques used to make them, depend on four major factors:
•Which component is to be improved.
693
694 Chapter 18. Learning from Examples
•What prior knowledge the agent already has.
•What representation is used for the data and the component.
•What feedback is available to learn from.
Components to be learned
Chapter 2 described several agent designs. The components of these agents include:
1. A direct mapping from conditions on the current state to actions.
2. A means to infer relevant properties of the world from the percept sequence.
3. Information about the way the world evolves and about the results of possible actions
the agent can take.
4. Utility information indicating the desirability of world states.
5. Action-value information indicating the desirability of actions.
6. Goals that describe classes of states whose achievement maximizes the agent’s utility.
Each of these components can be learned. Consider, for example, an agent training to become
a taxi driver. Every time the instructor shouts “Brake!” the agent might learn a condition–
action rule for when to brake (component 1); the agent also learns every time the instructor
does not shout. By seeing many camera images that it is told contain buses, it can learn
to recognize them (2). By trying actions and observing the results—for example, braking
hard on a wet road—it can learn the effects of its actions (3). Then, when it receives no tip
from passengers who have been thoroughly shaken up during the trip, it can learn a useful
component of its overall utility function (4).
Representation and prior knowledge
We have seen several examples of representations for agent components: propositional and
ﬁrst-order logical sentences for the components in a logical agent; Bayesian networks for
the inferential components of a decision-theoretic agent, and so on. Effective learning algo-
rithms have been devised for all of these representations. This chapter (and most of current
machine learning research) covers inputs that form a factored representation—a vector of
attribute values—and outputs that can be either a continuous numerical value or a discrete
value. Chapter 19 covers functions and prior knowledge composed of ﬁrst-order logic sen-
tences, and Chapter 20 concentrates on Bayesian networks.
There is another way to look at the various types of learning. We say that learning
a (possibly incorrect) general function or rule from speciﬁc input–output pairs is called in-
ductive learning. We will see in Chapter 19 that we can also do analytical or deductive
INDUCTIVE
LEARNING
learning: going from a known general rule to a new rule that is logically entailed, but isDEDUCTIVE
LEARNING
useful because it allows more efﬁcient processing.
Feedback to learn from
There are three types of feedback that determine the three main types of learning:
In unsupervised learning the agent learns patterns in the input even though no explicitUNSUPERVISED
LEARNING
feedback is supplied. The most common unsupervised learning task is clustering: detectingCLUSTERING
Section 18.2. Supervised Learning 695
potentially useful clusters of input examples. For example, a taxi agent might gradually
develop a concept of “good trafﬁc days” and “bad trafﬁc days” without ever being given
labeled examples of each by a teacher.
In reinforcement learning the agent learns from a series of reinforcements—rewards
REINFORCEMENT
LEARNING
or punishments. For example, the lack of a tip at the end of the journey gives the taxi agent an
indication that it did something wrong. The two points for a win at the end of a chess game
tells the agent it did something right. It is up to the agent to decide which of the actions prior
to the reinforcement were most responsible for it.
In supervised learning the agent observes some example input–output pairs and learns
SUPERVISED
LEARNING
a function that maps from input to output. In component 1 above, the inputs are percepts and
the output are provided by a teacher who says “Brake!” or “Turn left.” In component 2, the
inputs are camera images and the outputs again come from a teacher who says “that’s a bus.”
In 3, the theory of braking is a function from states and braking actions to stopping distance
in feet. In this case the output value is available directly from the agent’s percepts (after the
fact); the environment is the teacher.
In practice, these distinction are not always so crisp. In semi-supervised learning we
SEMI-SUPERVISED
LEARNING
are given a few labeled examples and must make what we can of a large collection of un-
labeled examples. Even the labels themselves may not be the oracular truths that we hope
for. Imagine that you are trying to build a system to guess a person’s age from a photo. You
gather some labeled examples by snapping pictures of people and asking their age. That’s
supervised learning. But in reality some of the people lied about their age. It’s not just
that there is random noise in the data; rather the inaccuracies are systematic, and to uncover
them is an unsupervised learning problem involving images, self-reported ages, and true (un-
known) ages. Thus, both noise and lack of labels create a continuum between supervised and
unsupervised learning.
18.2 S UPERVISED LEARNING
The task of supervised learning is this:
Given a training set of N example input–output pairsTRAINING SET
(x1,y1),(x2,y2),... (xN,y N ) ,
where each yj was generated by an unknown function y = f(x),
discover a function h that approximates the true function f.
Here x and y can be any value; they need not be numbers. The function h is a hypothesis.1HYPOTHESIS
Learning is a search through the space of possible hypotheses for one that will perform well,
even on new examples beyond the training set. To measure the accuracy of a hypothesis we
give it a test set of examples that are distinct from the training set. We say a hypothesis
TEST SET
1 A note on notation: except where noted, we will usej to index theN examples; xj will always be the input and
yj the output. In cases where the input is speciﬁcally a vector of attribute values (beginning with Section 18.3),
we will use xj for the jth example and we will use i to index the n attributes of each example. The elements of
xj are written xj,1,x j,2,...,x j,n.
696 Chapter 18. Learning from Examples
(c)(a) (b) (d)
x x x x
f(x) f(x) f(x) f(x)
Figure 18.1 (a) Example (x, f(x)) pairs and a consistent, linear hypothesis. (b) A con-
sistent, degree-7 polynomial hypothesis for the same data set. (c) A different data set, which
admits an exact degree-6 polynomial ﬁt or an approximate linear ﬁt. (d) A simple, exact
sinusoidal ﬁt to the same data set.
generalizes well if it correctly predicts the value of y for novel examples. Sometimes theGENERALIZA TION
function f is stochastic—it is not strictly a function of x, and what we have to learn is a
conditional probability distribution, P(Y| x).
When the output y is one of a ﬁnite set of values (such as sunny, cloudy or rainy),
the learning problem is called classiﬁcation, and is called Boolean or binary classiﬁcationCLASSIFICA TION
if there are only two values. When y is a number (such as tomorrow’s temperature), the
learning problem is called regression. (Technically, solving a regression problem is ﬁndingREGRESSION
a conditional expectation or average value of y, because the probability that we have found
exactly the right real-valued number for y is 0.)
Figure 18.1 shows a familiar example: ﬁtting a function of a single variable to some data
points. The examples are points in the (x, y) plane, where y = f(x). We don’t know what f
is, but we will approximate it with a function h selected from a hypothesis space,H,w h i c hHYPOTHESIS SP ACE
for this example we will take to be the set of polynomials, such asx5+3x2+2. Figure 18.1(a)
shows some data with an exact ﬁt by a straight line (the polynomial 0.4x +3 ). The line is
called a consistent hypothesis because it agrees with all the data. Figure 18.1(b) shows a high-CONSISTENT
degree polynomial that is also consistent with the same data. This illustrates a fundamental
problem in inductive learning:how do we choose from among multiple consistent hypotheses?
One answer is to prefer the simplest hypothesis consistent with the data. This principle is
called Ockham’s razor, after the 14th-century English philosopher William of Ockham, whoOCKHAM’S RAZOR
used it to argue sharply against all sorts of complications. Deﬁning simplicity is not easy, but
it seems clear that a degree-1 polynomial is simpler than a degree-7 polynomial, and thus (a)
should be preferred to (b). We will make this intuition more precise in Section 18.4.3.
Figure 18.1(c) shows a second data set. There is no consistent straight line for this
data set; in fact, it requires a degree-6 polynomial for an exact ﬁt. There are just 7 data
points, so a polynomial with 7 parameters does not seem to be ﬁnding any pattern in the
data and we do not expect it to generalize well. A straight line that is not consistent with
any of the data points, but might generalize fairly well for unseen values of x,i sa l s os h o w n
in (c). In general, there is a tradeoff between complex hypotheses that ﬁt the training data
well and simpler hypotheses that may generalize better . In Figure 18.1(d) we expand the
Section 18.3. Learning Decision Trees 697
hypothesis space H to allow polynomials over both x and sin(x), and ﬁnd that the data in
(c) can be ﬁtted exactly by a simple function of the form ax + b + csin(x). This shows the
importance of the choice of hypothesis space. We say that a learning problem isrealizable ifREALIZABLE
the hypothesis space contains the true function. Unfortunately, we cannot always tell whether
a given learning problem is realizable, because the true function is not known.
In some cases, an analyst looking at a problem is willing to make more ﬁne-grained
distinctions about the hypothesis space, to say—even before seeing any data—not just that a
hypothesis is possible or impossible, but rather how probable it is. Supervised learning can
be done by choosing the hypothesis h
∗ that is most probable given the data:
h∗ =a r g m a x
h∈H
P(h|data) .
By Bayes’ rule this is equivalent to
h∗ =a r g m a x
h∈H
P(data|h)P(h) .
Then we can say that the prior probability P(h) is high for a degree-1 or -2 polynomial,
lower for a degree-7 polynomial, and especially low for degree-7 polynomials with large,
sharp spikes as in Figure 18.1(b). We allow unusual-looking functions when the data say we
really need them, but we discourage them by giving them a low prior probability.
Why not letH be the class of all Java programs, or Turing machines? After all, every
computable function can be represented by some Turing machine, and that is the best we
can do. One problem with this idea is that it does not take into account the computational
complexity of learning. There is a tradeoff between the expressiveness of a hypothesis space
and the complexity of ﬁnding a good hypothesis within that space. For example, ﬁtting a
straight line to data is an easy computation; ﬁtting high-degree polynomials is somewhat
harder; and ﬁtting Turing machines is in general undecidable. A second reason to prefer
simple hypothesis spaces is that presumably we will want to use h after we have learned it,
and computing h(x) when h is a linear function is guaranteed to be fast, while computing
an arbitrary Turing machine program is not even guaranteed to terminate. For these reasons,
most work on learning has focused on simple representations.
We will see that the expressiveness–complexity tradeoff is not as simple as it ﬁrst seems:
it is often the case, as we saw with ﬁrst-order logic in Chapter 8, that an expressive language
makes it possible for asimple hypothesis to ﬁt the data, whereas restricting the expressiveness
of the language means that any consistent hypothesis must be very complex. For example,
the rules of chess can be written in a page or two of ﬁrst-order logic, but require thousands of
pages when written in propositional logic.
18.3 L EARNING DECISION TREES
Decision tree induction is one of the simplest and yet most successful forms of machine
learning. We ﬁrst describe the representation—the hypothesis space—and then show how to
learn a good hypothesis.
698 Chapter 18. Learning from Examples
18.3.1 The decision tree representation
A decision tree represents a function that takes as input a vector of attribute values andDECISION TREE
returns a “decision”—a single output value. The input and output values can be discrete or
continuous. For now we will concentrate on problems where the inputs have discrete values
and the output has exactly two possible values; this is Boolean classiﬁcation, where each
example input will be classiﬁed as true (a positive example) or false (a negative example).
POSITIVE
NEGA TIVE A decision tree reaches its decision by performing a sequence of tests. Each internal
node in the tree corresponds to a test of the value of one of the input attributes, Ai,a n d
the branches from the node are labeled with the possible values of the attribute, Ai =vik.
Each leaf node in the tree speciﬁes a value to be returned by the function. The decision tree
representation is natural for humans; indeed, many “How To” manuals (e.g., for car repair)
are written entirely as a single decision tree stretching over hundreds of pages.
As an example, we will build a decision tree to decide whether to wait for a table at a
restaurant. The aim here is to learn a deﬁnition for the goal predicate WillWait .F i r s t w e
GOAL PREDICA TE
list the attributes that we will consider as part of the input:
1. Alternate: whether there is a suitable alternative restaurant nearby.
2. Bar: whether the restaurant has a comfortable bar area to wait in.
3. Fri/Sat: true on Fridays and Saturdays.
4. Hungry: whether we are hungry.
5. Patrons: how many people are in the restaurant (values are None, Some,a n dFull).
6. Price: the restaurant’s price range ($, $$, $$$).
7. Raining: whether it is raining outside.
8. Reservation: whether we made a reservation.
9. Type: the kind of restaurant (French, Italian, Thai, or burger).
10. WaitEstimate: the wait estimated by the host (0–10 minutes, 10–30, 30–60, or >60).
Note that every variable has a small set of possible values; the value of WaitEstimate,f o r
example, is not an integer, rather it is one of the four discrete values 0–10, 10–30, 30–60, or
>60. The decision tree usually used by one of us (SR) for this domain is shown in Figure 18.2.
Notice that the tree ignores thePrice and Type attributes. Examples are processed by the tree
starting at the root and following the appropriate branch until a leaf is reached. For instance,
an example with Patrons =Full and WaitEstimate =0 –10 will be classiﬁed as positive
(i.e., yes, we will wait for a table).
18.3.2 Expressiveness of decision trees
A Boolean decision tree is logically equivalent to the assertion that the goal attribute is true
if and only if the input attributes satisfy one of the paths leading to a leaf with value true.
Writing this out in propositional logic, we have
Goal ⇔(Path
1∨Path2∨···) ,
where each Path is a conjunction of attribute-value tests required to follow that path. Thus,
the whole expression is equivalent to disjunctive normal form (see page 283), which means
Section 18.3. Learning Decision Trees 699
that any function in propositional logic can be expressed as a decision tree. As an example,
the rightmost path in Figure 18.2 is
Path =( Patrons =Full∧WaitEstimate = 0–10) .
For a wide variety of problems, the decision tree format yields a nice, concise result. But
some functions cannot be represented concisely. For example, the majority function, which
returns true if and only if more than half of the inputs are true, requires an exponentially
large decision tree. In other words, decision trees are good for some kinds of functions and
bad for others. Is there any kind of representation that is efﬁcient for all kinds of functions?
Unfortunately, the answer is no. We can show this in a general way. Consider the set of all
Boolean functions on n attributes. How many different functions are in this set? This is just
the number of different truth tables that we can write down, because the function is deﬁned
by its truth table. A truth table over n attributes has 2
n rows, one for each combination of
values of the attributes. We can consider the “answer” column of the table as a2n-bit number
that deﬁnes the function. That means there are22n
different functions (and there will be more
than that number of trees, since more than one tree can compute the same function). This is
a scary number. For example, with just the ten Boolean attributes of our restaurant problem
there are 2
1024 or about 10308 different functions to choose from, and for 20 attributes there
are over 10300,000. We will need some ingenious algorithms to ﬁnd good hypotheses in such
a large space.
18.3.3 Inducing decision trees from examples
An example for a Boolean decision tree consists of an(x,y ) pair, where x is a vector of values
for the input attributes, and y is a single Boolean output value. A training set of 12 examples
No  Ye s
No  Ye s
No  Ye s
No  Ye s
None Some Full
>60 30-60 10-30 0-10
No  Ye s
Alternate?
Hungry?
Reservation?
Bar? Raining?
Alternate?
Patrons?
Fri/Sat?
No Yes
No Yes
Yes
Yes
No  Yes
No Yes
YesNoYes
No  Ye s
YesNo
WaitEstimate?
Figure 18.2 A decision tree for deciding whether to wait for a table.

700 Chapter 18. Learning from Examples
Example
 Input Attributes
 Goal
Alt
 Bar
 Fri
 Hun
 Pat
 Price
 Rain
 Res
 Type
 Est
 WillW ait
x1
 Ye s
 No
 No
 Ye s
 Some
 $$$
 No
 Ye s
 French
 0–10
 y1 = Ye s
x2
 Ye s
 No
 No
 Ye s
 Full
 $
 No
 No
 Thai
 30–60
 y2 = No
x3
 No
 Ye s
 No
 No
 Some
 $
 No
 No
 Burger
 0–10
 y3 = Ye s
x4
 Ye s
 No
 Ye s
 Ye s
 Full
 $
 Ye s
 No
 Thai
 10–30
 y4 = Ye s
x5
 Ye s
 No
 Ye s
 No
 Full
 $$$
 No
 Ye s
 French
 >60
 y5 = No
x6
 No
 Ye s
 No
 Ye s
 Some
 $$
 Ye s
 Ye s
 Italian
 0–10
 y6 = Ye s
x7
 No
 Ye s
 No
 No
 None
 $
 Ye s
 No
 Burger
 0–10
 y7 = No
x8
 No
 No
 No
 Ye s
 Some
 $$
 Ye s
 Ye s
 Thai
 0–10
 y8 = Ye s
x9
 No
 Ye s
 Ye s
 No
 Full
 $
 Ye s
 No
 Burger
 >60
 y9 = No
x10
 Ye s
 Ye s
 Ye s
 Ye s
 Full
 $$$
 No
 Ye s
 Italian
 10–30
 y10 = No
x11
 No
 No
 No
 No
 None
 $
 No
 No
 Thai
 0–10
 y11 = No
x12
 Ye s
 Ye s
 Ye s
 Ye s
 Full
 $
 No
 No
 Burger
 30–60
 y12 = Ye s
Figure 18.3 Examples for the restaurant domain.
is shown in Figure 18.3. The positive examples are the ones in which the goal WillWait is
true (x1, x3,... ); the negative examples are the ones in which it is false (x2, x5,... ).
We want a tree that is consistent with the examples and is as small as possible. Un-
fortunately, no matter how we measure size, it is an intractable problem to ﬁnd the smallest
consistent tree; there is no way to efﬁciently search through the 2
2n
trees. With some simple
heuristics, however, we can ﬁnd a good approximate solution: a small (but not smallest) con-
sistent tree. The D
ECISION -TREE -LEARNING algorithm adopts a greedy divide-and-conquer
strategy: always test the most important attribute ﬁrst. This test divides the problem up into
smaller subproblems that can then be solved recursively. By “most important attribute,” we
mean the one that makes the most difference to the classiﬁcation of an example. That way, we
hope to get to the correct classiﬁcation with a small number of tests, meaning that all paths in
the tree will be short and the tree as a whole will be shallow.
Figure 18.4(a) shows thatType is a poor attribute, because it leaves us with four possible
outcomes, each of which has the same number of positive as negative examples. On the other
hand, in (b) we see thatPatrons is a fairly important attribute, because if the value isNone or
Some, then we are left with example sets for which we can answer deﬁnitively (No and Yes,
respectively). If the value is Full, we are left with a mixed set of examples. In general, after
the ﬁrst attribute test splits up the examples, each outcome is a new decision tree learning
problem in itself, with fewer examples and one less attribute. There are four cases to consider
for these recursive problems:
1. If the remaining examples are all positive (or all negative), then we are done: we can
answer Yes or No. Figure 18.4(b) shows examples of this happening in the None and
Some branches.
2. If there are some positive and some negative examples, then choose the best attribute to
split them. Figure 18.4(b) shows Hungry being used to split the remaining examples.
3. If there are no examples left, it means that no example has been observed for this com-
Section 18.3. Learning Decision Trees 701
(a)
None Some Full
Patrons?
YesNo Hungry?
(b)
No Yes
121 3 4 6 8
2 5 7 9 10 11
French Italian Thai Burger
Type?
121 3 4 6 8
2 5 7 9 10 11
1
5
6
10
4 8
2 11
123
7 9 7 11
1 3 6 8 124
2 5 9 10
124
2 105 9
Figure 18.4 Splitting the examples by testing on attributes. At each node we show the
positive (light boxes) and negative (dark boxes) examples remaining. (a) Splitting on Type
brings us no nearer to distingui shing between positive and neg ative examples. (b) Splitting
on Patrons does a good job of separating positive and n egative examples. After splitting on
Patrons, Hungry is a fairly good second test.
bination of attribute values, and we return a default value calculated from the plurality
classiﬁcation of all the examples that were used in constructing the node’s parent. These
are passed along in the variable parent
 examples.
4. If there are no attributes left, but both positive and negative examples, it means that
these examples have exactly the same description, but different classiﬁcations. This can
happen because there is an error or noise in the data; because the domain is nondeter-
NOISE
ministic; or because we can’t observe an attribute that would distinguish the examples.
The best we can do is return the plurality classiﬁcation of the remaining examples.
The D ECISION -TREE -LEARNING algorithm is shown in Figure 18.5. Note that the set of
examples is crucial for constructing the tree, but nowhere do the examples appear in the tree
itself. A tree consists of just tests on attributes in the interior nodes, values of attributes on
the branches, and output values on the leaf nodes. The details of the I
MPORTANCE function
are given in Section 18.3.4. The output of the learning algorithm on our sample training
set is shown in Figure 18.6. The tree is clearly different from the original tree shown in
Figure 18.2. One might conclude that the learning algorithm is not doing a very good job
of learning the correct function. This would be the wrong conclusion to draw, however. The
learning algorithm looks at theexamples, not at the correct function, and in fact, its hypothesis
(see Figure 18.6) not only is consistent with all the examples, but is considerably simpler
than the original tree! The learning algorithm has no reason to include tests for Raining and
Reservation, because it can classify all the examples without them. It has also detected an
interesting and previously unsuspected pattern: the ﬁrst author will wait for Thai food on
weekends. It is also bound to make some mistakes for cases where it has seen no examples.
For example, it has never seen a case where the wait is 0–10 minutes but the restaurant is full.
702 Chapter 18. Learning from Examples
function DECISION -TREE -LEARNING (examples,attributes,parent
 examples) returns
at r e e
if examples is empty then return PLURALITY -VALUE (parent
 examples)
else if all examples have the same classiﬁcation then return the classiﬁcation
else if attributes is empty then return PLURALITY -VALUE (examples)
else
A←argmaxa ∈ attributes IMPORTANCE (a,examples)
tree←a new decision tree with root test A
for each value vk of A do
exs←{e : e∈examples and e.A = vk}
subtree←DECISION -TREE -LEARNING (exs,attributes−A,examples)
add a branch to tree with label (A = vk) and subtree subtree
return tree
Figure 18.5 The decision-tree learning algorithm. The function I MPORTANCE is de-
scribed in Section 18.3.4. The function PLURALITY -VALUE selects the most common output
value among a set of examples, breaking ties randomly.
None Some Full
Patrons?
No Yes
No  Yes
Hungry?
No
No  Yes
Fri/Sat?
YesNo
Yes
Type?
French Italian Thai Burger
Yes No
Figure 18.6 The decision tree induced from the 12-example training set.
In that case it says not to wait when Hungry is false, but I (SR) would certainly wait. With
more training examples the learning program could correct this mistake.
We note there is a danger of over-interpreting the tree that the algorithm selects. When
there are several variables of similar importance, the choice between them is somewhat arbi-
trary: with slightly different input examples, a different variable would be chosen to split on
ﬁrst, and the whole tree would look completely different. The function computed by the tree
would still be similar, but the structure of the tree can vary widely.
We can evaluate the accuracy of a learning algorithm with a learning curve,a ss h o w n
LEARNING CURVE
in Figure 18.7. We have 100 examples at our disposal, which we split into a training set and
Section 18.3. Learning Decision Trees 703
0.4
0.5
0.6
0.7
0.8
0.9
1
0 20 40 60 80 100
Proportion correct on test set
Training set size
Figure 18.7 A learning curve for the decision tree learning algorithm on 100 randomly
generated examples in the restaurant domain. Each data point is the average of 20 trials.
a test set. We learn a hypothesish with the training set and measure its accuracy with the test
set. We do this starting with a training set of size 1 and increasing one at a time up to size
99. For each size we actually repeat the process of randomly splitting 20 times, and average
the results of the 20 trials. The curve shows that as the training set size grows, the accuracy
increases. (For this reason, learning curves are also called happy graphs.) In this graph we
reach 95% accuracy, and it looks like the curve might continue to increase with more data.
18.3.4 Choosing attribute tests
The greedy search used in decision tree learning is designed to approximately minimize the
depth of the ﬁnal tree. The idea is to pick the attribute that goes as far as possible toward
providing an exact classiﬁcation of the examples. A perfect attribute divides the examples
into sets, each of which are all positive or all negative and thus will be leaves of the tree. The
Patrons attribute is not perfect, but it is fairly good. A really useless attribute, such asType,
leaves the example sets with roughly the same proportion of positive and negative examples
as the original set.
All we need, then, is a formal measure of “fairly good” and “really useless” and we can
implement the I
MPORTANCE function of Figure 18.5. We will use the notion of information
gain, which is deﬁned in terms of entropy, the fundamental quantity in information theoryENTROPY
(Shannon and Weaver, 1949).
Entropy is a measure of the uncertainty of a random variable; acquisition of information
corresponds to a reduction in entropy. A random variable with only one value—a coin that
always comes up heads—has no uncertainty and thus its entropy is deﬁned as zero; thus, we
gain no information by observing its value. A ﬂip of a fair coin is equally likely to come up
heads or tails, 0 or 1, and we will soon show that this counts as “1 bit” of entropy. The roll
of a fair four-sided die has 2 bits of entropy, because it takes two bits to describe one of four
equally probable choices. Now consider an unfair coin that comes up heads 99% of the time.
Intuitively, this coin has less uncertainty than the fair coin—if we guess heads we’ll be wrong
only 1% of the time—so we would like it to have an entropy measure that is close to zero, but
704 Chapter 18. Learning from Examples
positive. In general, the entropy of a random variableV with values vk, each with probability
P(vk),i sd e ﬁ n e da s
Entropy: H(V )=
∑
k
P(vk)l o g2
1
P(vk) =−
∑
k
P(vk)l o g2 P(vk) .
We can check that the entropy of a fair coin ﬂip is indeed 1 bit:
H(Fair)= −(0.5l o g2 0.5+0 .5l o g2 0.5) = 1 .
If the coin is loaded to give 99% heads, we get
H(Loaded)= −(0.99 log2 0.99 + 0.01 log2 0.01)≈0.08 bits.
It will help to deﬁne B(q) as the entropy of a Boolean random variable that is true with
probability q:
B(q)=−(q log2 q +( 1−q)l o g2(1−q)) .
Thus, H(Loaded)= B(0.99) ≈0.08. Now let’s get back to decision tree learning. If a
training set contains p positive examples and n negative examples, then the entropy of the
goal attribute on the whole set is
H(Goal)= B
⎞ p
p + n
⎠
.
The restaurant training set in Figure 18.3 has p = n =6 , so the corresponding entropy is
B(0.5) or exactly 1 bit. A test on a single attributeA might give us only part of this 1 bit. We
can measure exactly how much by looking at the entropy remaining after the attribute test.
An attribute A with d distinct values divides the training setE into subsets E1,...,E d.
Each subset Ek has pk positive examples and nk negative examples, so if we go along that
branch, we will need an additional B(pk/(pk + nk)) bits of information to answer the ques-
tion. A randomly chosen example from the training set has thekth value for the attribute with
probability (pk + nk)/(p + n), so the expected entropy remaining after testing attribute A is
Remainder(A)=
d∑
k=1
pk+nk
p+n B( pk
pk+nk
) .
The information gain from the attribute test on A is the expected reduction in entropy:INFORMA TION GAIN
Gain(A)= B( p
p+n)−Remainder(A) .
In fact Gain(A) is just what we need to implement the IMPORTANCE function. Returning to
the attributes considered in Figure 18.4, we have
Gain(Patrons)=1 −
[ 2
12B(0
2)+ 4
12B(4
4)+ 6
12B(2
6)
]
≈0.541 bits,
Gain(Type)=1 −
[ 2
12B(1
2)+ 2
12B(1
2)+ 4
12B(2
4)+ 4
12B(2
4)
]
=0 bits,
conﬁrming our intuition that Patrons is a better attribute to split on. In fact, Patrons has
the maximum gain of any of the attributes and would be chosen by the decision-tree learning
algorithm as the root.
Section 18.3. Learning Decision Trees 705
18.3.5 Generalization and overﬁtting
On some problems, the D ECISION -TREE -LEARNING algorithm will generate a large tree
when there is actually no pattern to be found. Consider the problem of trying to predict
whether the roll of a die will come up as 6 or not. Suppose that experiments are carried out
with various dice and that the attributes describing each training example include the color
of the die, its weight, the time when the roll was done, and whether the experimenters had
their ﬁngers crossed. If the dice are fair, the right thing to learn is a tree with a single node
that says “no,” But the D
ECISION -TREE -LEARNING algorithm will seize on any pattern it
can ﬁnd in the input. If it turns out that there are 2 rolls of a 7-gram blue die with ﬁngers
crossed and they both come out 6, then the algorithm may construct a path that predicts 6 in
that case. This problem is called overﬁtting. A general phenomenon, overﬁtting occurs with
OVERFITTING
all types of learners, even when the target function is not at all random. In Figure 18.1(b) and
(c), we saw polynomial functions overﬁtting the data. Overﬁtting becomes more likely as the
hypothesis space and the number of input attributes grows, and less likely as we increase the
number of training examples.
For decision trees, a technique called decision tree pruning combats overﬁtting. Prun-
DECISION TREE
PRUNING
ing works by eliminating nodes that are not clearly relevant. We start with a full tree, as
generated by D ECISION -TREE -LEARNING . We then look at a test node that has only leaf
nodes as descendants. If the test appears to be irrelevant—detecting only noise in the data—
then we eliminate the test, replacing it with a leaf node. We repeat this process, considering
each test with only leaf descendants, until each one has either been pruned or accepted as is.
The question is, how do we detect that a node is testing an irrelevant attribute? Suppose
we are at a node consisting ofp positive andn negative examples. If the attribute is irrelevant,
we would expect that it would split the examples into subsets that each have roughly the same
proportion of positive examples as the whole set,p/(p + n), and so the information gain will
be close to zero.
2 Thus, the information gain is a good clue to irrelevance. Now the question
is, how large a gain should we require in order to split on a particular attribute?
We can answer this question by using a statistical signiﬁcance test. Such a test beginsSIGNIFICANCE TEST
by assuming that there is no underlying pattern (the so-called null hypothesis). Then the ac-NULL HYPOTHESIS
tual data are analyzed to calculate the extent to which they deviate from a perfect absence of
pattern. If the degree of deviation is statistically unlikely (usually taken to mean a 5% prob-
ability or less), then that is considered to be good evidence for the presence of a signiﬁcant
pattern in the data. The probabilities are calculated from standard distributions of the amount
of deviation one would expect to see in random sampling.
In this case, the null hypothesis is that the attribute is irrelevant and, hence, that the
information gain for an inﬁnitely large sample would be zero. We need to calculate the
probability that, under the null hypothesis, a sample of size v = n + p would exhibit the
observed deviation from the expected distribution of positive and negative examples. We can
measure the deviation by comparing the actual numbers of positive and negative examples in
2 The gain will be strictly positive except for the unlikely case where all the proportions are exactly the same.
(See Exercise 18.5.)
706 Chapter 18. Learning from Examples
each subset, pk and nk, with the expected numbers, ˆpk and ˆnk, assuming true irrelevance:
ˆpk = p× pk + nk
p + n ˆnk = n× pk + nk
p + n .
A convenient measure of the total deviation is given by
Δ=
d∑
k=1
(pk −ˆpk)2
ˆpk
+ (nk −ˆnk)2
ˆnk
.
Under the null hypothesis, the value of Δ is distributed according to the χ2 (chi-squared)
distribution with v−1 degrees of freedom. We can use a χ2 table or a standard statistical
library routine to see if a particular Δ value conﬁrms or rejects the null hypothesis. For
example, consider the restaurant type attribute, with four values and thus three degrees of
freedom. A value of Δ=7 .82 or more would reject the null hypothesis at the 5% level (and a
value ofΔ=1 1.35 or more would reject at the 1% level). Exercise 18.8 asks you to extend the
D
ECISION -TREE -LEARNING algorithm to implement this form of pruning, which is known
as χ2 pruning.χ2 PRUNING
With pruning, noise in the examples can be tolerated. Errors in the example’s label (e.g.,
an example(x,Yes) that should be(x,No)) give a linear increase in prediction error, whereas
errors in the descriptions of examples (e.g., Price =$ when it was actually Price =$ $)h a v e
an asymptotic effect that gets worse as the tree shrinks down to smaller sets. Pruned trees
perform signiﬁcantly better than unpruned trees when the data contain a large amount of
noise. Also, the pruned trees are often much smaller and hence easier to understand.
One ﬁnal warning: You might think that χ
2 pruning and information gain look similar,
so why not combine them using an approach called early stopping—have the decision treeEARL Y STOPPING
algorithm stop generating nodes when there is no good attribute to split on, rather than going
to all the trouble of generating nodes and then pruning them away. The problem with early
stopping is that it stops us from recognizing situations where there is no one good attribute,
but there are combinations of attributes that are informative. For example, consider the XOR
function of two binary attributes. If there are roughly equal number of examples for all four
combinations of input values, then neither attribute will be informative, yet the correct thing
to do is to split on one of the attributes (it doesn’t matter which one), and then at the second
level we will get splits that are informative. Early stopping would miss this, but generate-
and-then-prune handles it correctly.
18.3.6 Broadening the applicability of decision trees
In order to extend decision tree induction to a wider variety of problems, a number of issues
must be addressed. We will brieﬂy mention several, suggesting that a full understanding is
best obtained by doing the associated exercises:
•Missing data: In many domains, not all the attribute values will be known for every
example. The values might have gone unrecorded, or they might be too expensive to
obtain. This gives rise to two problems: First, given a complete decision tree, how
should one classify an example that is missing one of the test attributes? Second, how
Section 18.3. Learning Decision Trees 707
should one modify the information-gain formula when some examples have unknown
values for the attribute? These questions are addressed in Exercise 18.9.
•Multivalued attributes: When an attribute has many possible values, the information
gain measure gives an inappropriate indication of the attribute’s usefulness. In the ex-
treme case, an attribute such as ExactTime has a different value for every example,
which means each subset of examples is a singleton with a unique classiﬁcation, and
the information gain measure would have its highest value for this attribute. But choos-
ing this split ﬁrst is unlikely to yield the best tree. One solution is to use the gain ratio
GAIN RA TIO
(Exercise 18.10). Another possibility is to allow a Boolean test of the formA= vk,t h a t
is, picking out just one of the possible values for an attribute, leaving the remaining
values to possibly be tested later in the tree.
•Continuous and integer-valued input attributes : Continuous or integer-valued at-
tributes such asHeight and Weight, have an inﬁnite set of possible values. Rather than
generate inﬁnitely many branches, decision-tree learning algorithms typically ﬁnd the
split point that gives the highest information gain. For example, at a given node in
SPLIT POINT
the tree, it might be the case that testing on Weight > 160 gives the most informa-
tion. Efﬁcient methods exist for ﬁnding good split points: start by sorting the values
of the attribute, and then consider only split points that are between two examples in
sorted order that have different classiﬁcations, while keeping track of the running totals
of positive and negative examples on each side of the split point. Splitting is the most
expensive part of real-world decision tree learning applications.
•Continuous-valued output attributes : If we are trying to predict a numerical output
value, such as the price of an apartment, then we need a regression tree rather than a
REGRESSION TREE
classiﬁcation tree. A regression tree has at each leaf a linear function of some subset
of numerical attributes, rather than a single value. For example, the branch for two-
bedroom apartments might end with a linear function of square footage, number of
bathrooms, and average income for the neighborhood. The learning algorithm must
decide when to stop splitting and begin applying linear regression (see Section 18.6)
over the attributes.
A decision-tree learning system for real-world applications must be able to handle all of
these problems. Handling continuous-valued variables is especially important, because both
physical and ﬁnancial processes provide numerical data. Several commercial packages have
been built that meet these criteria, and they have been used to develop thousands of ﬁelded
systems. In many areas of industry and commerce, decision trees are usually the ﬁrst method
tried when a classiﬁcation method is to be extracted from a data set. One important property
of decision trees is that it is possible for a human to understand the reason for the output of the
learning algorithm. (Indeed, this is a legal requirement for ﬁnancial decisions that are subject
to anti-discrimination laws.) This is a property not shared by some other representations,
such as neural networks.
708 Chapter 18. Learning from Examples
18.4 E V ALUATING AND CHOOSING THE BEST HYPOTHESIS
We want to learn a hypothesis that ﬁts the future data best. To make that precise we need
to deﬁne “future data” and “best.” We make the stationarity assumption : that there is aST A TIONARITY
ASSUMPTION
probability distribution over examples that remains stationary over time. Each example data
point (before we see it) is a random variableEj whose observed valueej =(xj,y j) is sampled
from that distribution, and is independent of the previous examples:
P(Ej|Ej−1,Ej−2,... )= P(Ej) ,
and each example has an identical prior probability distribution:
P(Ej)= P(Ej−1)= P(Ej−2)= ··· .
Examples that satisfy these assumptions are called independent and identically distributed or
i.i.d.. An i.i.d. assumption connects the past to the future; without some such connection, allI.I.D.
bets are off—the future could be anything. (We will see later that learning can still occur if
there are slow changes in the distribution.)
The next step is to deﬁne “best ﬁt.” We deﬁne the error rate of a hypothesis as theERROR RA TE
proportion of mistakes it makes—the proportion of times thath(x)̸= y for an(x, y) example.
Now, just because a hypothesis h has a low error rate on the training set does not mean that
it will generalize well. A professor knows that an exam will not accurately evaluate students
if they have already seen the exam questions. Similarly, to get an accurate evaluation of a
hypothesis, we need to test it on a set of examples it has not seen yet. The simplest approach is
the one we have seen already: randomly split the available data into a training set from which
the learning algorithm producesh and a test set on which the accuracy ofh is evaluated. This
method, sometimes called holdout cross-validation, has the disadvantage that it fails to useHOLDOUT
CROSS-VALIDA TION
all the available data; if we use half the data for the test set, then we are only training on half
the data, and we may get a poor hypothesis. On the other hand, if we reserve only 10% of
the data for the test set, then we may, by statistical chance, get a poor estimate of the actual
accuracy.
We can squeeze more out of the data and still get an accurate estimate using a technique
called k-fold cross-validation. The idea is that each example serves double duty—as training
K-FOLD
CROSS-VALIDA TION
data and test data. First we split the data into k equal subsets. We then perform k rounds of
learning; on each round 1/k of the data is held out as a test set and the remaining examples
are used as training data. The average test set score of the k rounds should then be a better
estimate than a single score. Popular values for k are 5 and 10—enough to give an estimate
that is statistically likely to be accurate, at a cost of 5 to 10 times longer computation time.
The extreme is k = n, also known as leave-one-out cross-validation or LOOCV.
LEAVE-ONE-OUT
CROSS-VALIDA TION
LOOCV Despite the best efforts of statistical methodologists, users frequently invalidate their
results by inadvertently peeking at the test data. Peeking can happen like this: A learningPEEKING
algorithm has various “knobs” that can be twiddled to tune its behavior—for example, various
different criteria for choosing the next attribute in decision tree learning. The researcher
generates hypotheses for various different settings of the knobs, measures their error rates on
the test set, and reports the error rate of the best hypothesis. Alas, peeking has occurred! The
Section 18.4. Evaluating and Choosing the Best Hypothesis 709
reason is that the hypothesis was selected on the basis of its test set error rate , so information
about the test set has leaked into the learning algorithm.
Peeking is a consequence of using test-set performance to bothchoose a hypothesis and
evaluate i t . T h ew a yt oa v o i dt h i si st oreally hold the test set out—lock it away until you
are completely done with learning and simply wish to obtain an independent evaluation of
the ﬁnal hypothesis. (And then, if you don’t like the results ... you have to obtain, and lock
away, a completely new test set if you want to go back and ﬁnd a better hypothesis.) If the
test set is locked away, but you still want to measure performance on unseen data as a way of
selecting a good hypothesis, then divide the available data (without the test set) into a training
set and a validation set. The next section shows how to use validation sets to ﬁnd a good
VALIDA TION SET
tradeoff between hypothesis complexity and goodness of ﬁt.
18.4.1 Model selection: Complexity versus goodness of ﬁt
In Figure 18.1 (page 696) we showed that higher-degree polynomials can ﬁt the training data
better, but when the degree is too high they will overﬁt, and perform poorly on validation data.
Choosing the degree of the polynomial is an instance of the problem ofmodel selection.Y o uMODEL SELECTION
can think of the task of ﬁnding the best hypothesis as two tasks: model selection deﬁnes the
hypothesis space and then optimization ﬁnds the best hypothesis within that space.OPTIMIZA TION
In this section we explain how to select among models that are parameterized by size.
For example, with polynomials we havesize =1 for linear functions, size =2 for quadratics,
and so on. For decision trees, the size could be the number of nodes in the tree. In all cases
we want to ﬁnd the value of the size parameter that best balances underﬁtting and overﬁtting
to give the best test set accuracy.
An algorithm to perform model selection and optimization is shown in Figure 18.8. It
is a wrapper that takes a learning algorithm as an argument (D
ECISION -TREE -LEARNING ,WRAPPER
for example). The wrapper enumerates models according to a parameter, size. For each size,
it uses cross validation on Learner to compute the average error rate on the training and
test sets. We start with the smallest, simplest models (which probably underﬁt the data), and
iterate, considering more complex models at each step, until the models start to overﬁt. In
Figure 18.9 we see typical curves: the training set error decreases monotonically (although
there may in general be slight random variation), while the validation set error decreases at
ﬁrst, and then increases when the model begins to overﬁt. The cross-validation procedure
picks the value ofsize with the lowest validation set error; the bottom of the U-shaped curve.
We then generate a hypothesis of that size, using all the data (without holding out any of it).
Finally, of course, we should evaluate the returned hypothesis on a separate test set.
This approach requires that the learning algorithm accept a parameter,size, and deliver
a hypothesis of that size. As we said, for decision tree learning, the size can be the number of
nodes. We can modify D
ECISION -TREE -LEARNER so that it takes the number of nodes as
an input, builds the tree breadth-ﬁrst rather than depth-ﬁrst (but at each level it still chooses
the highest gain attribute ﬁrst), and stops when it reaches the desired number of nodes.
710 Chapter 18. Learning from Examples
function CROSS -VALIDATION -WRAPPER (Learner,k,examples) returns a hypothesis
local variables: errT, an array, indexed bysize, storing training-set error rates
errV , an array, indexed bysize, storing validation-set error rates
for size =1 to∞do
errT[size],errV [size]←CROSS -VALIDATION (Learner,size,k,examples)
if errT has converged then do
best
 size←the value of size with minimum errV [size]
return Learner(best
 size,examples)
function CROSS -VALIDATION (Learner,size,k,examples) returns two values:
average training set error rate, average validation set error rate
fold
 errT←0; fold
 errV←0
for fold =1 to k do
training
 set,validation
 set←PARTITION (examples,fold,k)
h←Learner(size,training
 set)
fold
 errT←fold
 errT +E RROR -RATE(h,training
 set)
fold
 errV←fold
 errV +ERROR -RATE(h,validation
 set)
return fold
 errT/k, fold
 errV /k
Figure 18.8 An algorithm to select the model that has the lowest error rate on validation
data by building models of increasing complexity, and choosing the one with best empir-
ical error rate on validation data. Here errT means error rate on the training data, and
errV means error rate on the validation data. Learner(size,examples) returns a hypoth-
esis whose complexity is set by the parameter size, and which is trained on the examples.
PARTITION (examples, fold, k) splits examples into two subsets: a validation set of size N/k
and a training set with all the other examples. The split is different for each value of fold.
18.4.2 From error rates to loss
So far, we have been trying to minimize error rate. This is clearly better than maximizing
error rate, but it is not the full story. Consider the problem of classifying email messages
as spam or non-spam. It is worse to classify non-spam as spam (and thus potentially miss
an important message) then to classify spam as non-spam (and thus suffer a few seconds of
annoyance). So a classiﬁer with a 1% error rate, where almost all the errors were classifying
spam as non-spam, would be better than a classiﬁer with only a 0.5% error rate, if most of
those errors were classifying non-spam as spam. We saw in Chapter 16 that decision-makers
should maximize expected utility, and utility is what learners should maximize as well. In
machine learning it is traditional to express utilities by means of a loss function.T h e l o s s
LOSS FUNCTION
function L(x, y,ˆy) is deﬁned as the amount of utility lost by predicting h(x)=ˆy when the
correct answer is f(x)= y:
L(x, y,ˆy)= Utility(result of using y given an input x)
−Utility(result of using ˆy given an input x)
Section 18.4. Evaluating and Choosing the Best Hypothesis 711
 0
 10
 20
 30
 40
 50
 60
 1  2  3  4  5  6  7  8  9  10
Error rate
Tree size
Validation Set Error
Training Set Error
Figure 18.9 Error rates on training data (lower, dashed line) and validation data (upper,
solid line) for different size decision trees. We stop when the training set error rate asymp-
totes, and then choose the tree with minimal error on the validation set; in this case the tree
of size 7 nodes.
This is the most general formulation of the loss function. Often a simpliﬁed version is used,
L(y, ˆy), that is independent of x. We will use the simpliﬁed version for the rest of this
chapter, which means we can’t say that it is worse to misclassify a letter from Mom than it
is to misclassify a letter from our annoying cousin, but we can say it is 10 times worse to
classify non-spam as spam than vice-versa:
L(spam,nospam)=1 ,L (nospam,spam)=1 0 .
Note that L(y,y ) is always zero; by deﬁnition there is no loss when you guess exactly right.
For functions with discrete outputs, we can enumerate a loss value for each possible mis-
classiﬁcation, but we can’t enumerate all the possibilities for real-valued data. If f(x) is
137.035999, we would be fairly happy with h(x) = 137.036, but just how happy should we
be? In general small errors are better than large ones; two functions that implement that idea
are the absolute value of the difference (called the L
1 loss), and the square of the difference
(called the L2 loss). If we are content with the idea of minimizing error rate, we can use
the L0/1 loss function, which has a loss of 1 for an incorrect answer and is appropriate for
discrete-valued outputs:
Absolute value loss: L1(y, ˆy)= |y−ˆy|
Squared error loss: L2(y, ˆy)=( y−ˆy)2
0/1 loss: L0/1(y, ˆy)=0 if y =ˆy, else 1
The learning agent can theoretically maximize its expected utility by choosing the hypoth-
esis that minimizes expected loss over all input–output pairs it will see. It is meaningless
to talk about this expectation without deﬁning a prior probability distribution, P(X,Y ) over
examples. Let E be the set of all possible input–output examples. Then the expected gener-
alization loss for a hypothesis h (with respect to loss function L)i sGENERALIZA TION
LOSS
712 Chapter 18. Learning from Examples
GenLossL(h)=
∑
(x,y)∈E
L(y,h (x))P(x, y) ,
and the best hypothesis, h∗, is the one with the minimum expected generalization loss:
h∗ =a r g m i n
h∈H
GenLossL(h) .
Because P(x, y) is not known, the learning agent can only estimate generalization loss with
empirical loss on a set of examples, E:EMPIRICAL LOSS
EmpLossL,E(h)= 1
N
∑
(x,y)∈E
L(y,h (x)) .
The estimated best hypothesis ˆh∗ is then the one with minimum empirical loss:
ˆh∗ =a r g m i n
h∈H
EmpLossL,E(h) .
There are four reasons why ˆh∗ may differ from the true function,f: unrealizability, variance,
noise, and computational complexity. First, f may not be realizable—may not be in H—or
may be present in such a way that other hypotheses are preferred. Second, a learning algo-
rithm will return different hypotheses for different sets of examples, even if those sets are
drawn from the same true function f, and those hypotheses will make different predictions
on new examples. The higher the variance among the predictions, the higher the probability
of signiﬁcant error. Note that even when the problem is realizable, there will still be random
variance, but that variance decreases towards zero as the number of training examples in-
creases. Third, f may be nondeterministic or noisy—it may return different values for f(x)
NOISE
each time x occurs. By deﬁnition, noise cannot be predicted; in many cases, it arises because
the observed labelsy are the result of attributes of the environment not listed inx. And ﬁnally,
whenH is complex, it can be computationally intractable to systematically search the whole
hypothesis space. The best we can do is a local search (hill climbing or greedy search) that
explores only part of the space. That gives us an approximation error. Combining the sources
of error, we’re left with an estimation of an approximation of the true functionf.
Traditional methods in statistics and the early years of machine learning concentrated
on small-scale learning, where the number of training examples ranged from dozens to the
SMALL-SCALE
LEARNING
low thousands. Here the generalization error mostly comes from the approximation error of
not having the truef in the hypothesis space, and from estimation error of not having enough
training examples to limit variance. In recent years there has been more emphasis on large-
scale learning, often with millions of examples. Here the generalization error is dominatedLARGE-SCALE
LEARNING
by limits of computation: there is enough data and a rich enough model that we could ﬁnd an
h that is very close to the true f, but the computation to ﬁnd it is too complex, so we settle
for a sub-optimal approximation.
18.4.3 Regularization
In Section 18.4.1, we saw how to do model selection with cross-validation on model size. An
alternative approach is to search for a hypothesis that directly minimizes the weighted sum of
Section 18.5. The Theory of Learning 713
empirical loss and the complexity of the hypothesis, which we will call the total cost:
Cost(h)= EmpLoss(h)+ λComplexity(h)
ˆh∗ =a r g m i n
h∈H
Cost(h) .
Here λ is a parameter, a positive number that serves as a conversion rate between loss and
hypothesis complexity (which after all are not measured on the same scale). This approach
combines loss and complexity into one metric, allowing us to ﬁnd the best hypothesis all at
once. Unfortunately we still need to do a cross-validation search to ﬁnd the hypothesis that
generalizes best, but this time it is with different values of λ rather than size. We select the
value of λ that gives us the best validation set score.
This process of explicitly penalizing complex hypotheses is called regularization (be-
REGULARIZA TION
cause it looks for a function that is more regular, or less complex). Note that the cost function
requires us to make two choices: the loss function and the complexity measure, which is
called a regularization function. The choice of regularization function depends on the hy-
pothesis space. For example, a good regularization function for polynomials is the sum of
the squares of the coefﬁcients—keeping the sum small would guide us away from the wiggly
polynomials in Figure 18.1(b) and (c). We will show an example of this type of regularization
in Section 18.6.
Another way to simplify models is to reduce the dimensions that the models work with.
A process of feature selection can be performed to discard attributes that appear to be irrel-
FEA TURE SELECTION
evant. χ2 pruning is a kind of feature selection.
It is in fact possible to have the empirical loss and the complexity measured on the
same scale, without the conversion factor λ: they can both be measured in bits. First encode
the hypothesis as a Turing machine program, and count the number of bits. Then count
the number of bits required to encode the data, where a correctly predicted example costs
zero bits and the cost of an incorrectly predicted example depends on how large the error is.
The minimum description length or MDL hypothesis minimizes the total number of bits
MINIMUM
DESCRIPTION
LENGTH
required. This works well in the limit, but for smaller problems there is a difﬁculty in that
the choice of encoding for the program—for example, how best to encode a decision tree
as a bit string—affects the outcome. In Chapter 20 (page 805), we describe a probabilistic
interpretation of the MDL approach.
18.5 T HE THEORY OF LEARNING
The main unanswered question in learning is this: How can we be sure that our learning
algorithm has produced a hypothesis that will predict the correct value for previously unseen
inputs? In formal terms, how do we know that the hypothesis h is close to the target function
f if we don’t know what f is? These questions have been pondered for several centuries.
In more recent decades, other questions have emerged: how many examples do we need
to get a good h? What hypothesis space should we use? If the hypothesis space is very
complex, can we even ﬁnd the best h, or do we have to settle for a local maximum in the
714 Chapter 18. Learning from Examples
space of hypotheses? How complex should h be? How do we avoid overﬁtting? This section
examines these questions.
We’ll start with the question of how many examples are needed for learning. We saw
from the learning curve for decision tree learning on the restaurant problem (Figure 18.7 on
page 703) that improves with more training data. Learning curves are useful, but they are
speciﬁc to a particular learning algorithm on a particular problem. Are there some more gen-
eral principles governing the number of examples needed in general? Questions like this are
addressed by computational learning theory, which lies at the intersection of AI, statistics,
COMPUT A TIONAL
LEARNING THEORY
and theoretical computer science. The underlying principle is that any hypothesis that is seri-
ously wrong will almost certainly be “found out” with high probability after a small number
of examples, because it will make an incorrect prediction. Thus, any hypothesis that is consis-
tent with a sufﬁciently large set of training examples is unlikely to be seriously wrong: that is,
it must be probably approximately correct . Any learning algorithm that returns hypotheses
PROBABL Y
APPROXIMA TEL Y
CORRECT
that are probably approximately correct is called a PAC learning algorithm; we can use thisP AC LEARNING
approach to provide bounds on the performance of various learning algorithms.
PAC-learning theorems, like all theorems, are logical consequences of axioms. When
a theorem (as opposed to, say, a political pundit) states something about the future based on
the past, the axioms have to provide the “juice” to make that connection. For PAC learning,
the juice is provided by the stationarity assumption introduced on page 708, which says that
future examples are going to be drawn from the same ﬁxed distribution P(E)= P(X,Y )
as past examples. (Note that we do not have to know what distribution that is, just that it
doesn’t change.) In addition, to keep things simple, we will assume that the true function f
is deterministic and is a member of the hypothesis classH that is being considered.
The simplest PAC theorems deal with Boolean functions, for which the 0/1 loss is ap-
propriate. The error rate of a hypothesis h, deﬁned informally earlier, is deﬁned formally
here as the expected generalization error for examples drawn from the stationary distribution:
error(h)= GenLoss
L0/1 (h)=
∑
x,y
L0/1(y,h (x))P(x, y) .
In other words, error (h) is the probability that h misclassiﬁes a new example. This is the
same quantity being measured experimentally by the learning curves shown earlier.
A hypothesis h is called approximately correct if error(h) ≤ϵ,w h e r eϵ is a small
constant. We will show that we can ﬁnd an N such that, after seeing N examples, with high
probability, all consistent hypotheses will be approximately correct. One can think of an
approximately correct hypothesis as being “close” to the true function in hypothesis space: it
lies inside what is called the ϵ-ball around the true function f. The hypothesis space outsideϵ
-BALL
this ball is calledHbad.
We can calculate the probability that a “seriously wrong” hypothesis hb ∈Hbad is
consistent with the ﬁrst N examples as follows. We know that error (hb) >ϵ . Thus, the
probability that it agrees with a given example is at most 1−ϵ. Since the examples are
independent, the bound for N examples is
P(hb agrees with N examples)≤(1−ϵ)N .
Section 18.5. The Theory of Learning 715
The probability thatHbad contains at least one consistent hypothesis is bounded by the sum
of the individual probabilities:
P(Hbad contains a consistent hypothesis)≤|Hbad|(1−ϵ)N ≤|H|(1−ϵ)N ,
where we have used the fact that |Hbad|≤| H|. We would like to reduce the probability of
this event below some small number δ:
|H|(1−ϵ)N ≤δ.
Given that 1−ϵ≤e− ϵ, we can achieve this if we allow the algorithm to see
N≥1
ϵ
⎞
ln 1
δ+l n|H|
⎠
(18.1)
examples. Thus, if a learning algorithm returns a hypothesis that is consistent with this many
examples, then with probability at least 1−δ,i th a se r r o ra tm o s tϵ. In other words, it is
probably approximately correct. The number of required examples, as a function of ϵ and δ,
is called the sample complexity of the hypothesis space.SAMPLE
COMPLEXITY
As we saw earlier, if H is the set of all Boolean functions on n attributes, then|H| =
22n
. Thus, the sample complexity of the space grows as 2n. Because the number of possible
examples is also 2n, this suggests that PAC-learning in the class of all Boolean functions
requires seeing all, or nearly all, of the possible examples. A moment’s thought reveals the
reason for this: H contains enough hypotheses to classify any given set of examples in all
possible ways. In particular, for any set of N examples, the set of hypotheses consistent with
those examples contains equal numbers of hypotheses that predict x
N+1 to be positive and
hypotheses that predict xN+1 to be negative.
To obtain real generalization to unseen examples, then, it seems we need to restrict
the hypothesis space H in some way; but of course, if we do restrict the space, we might
eliminate the true function altogether. There are three ways to escape this dilemma. The ﬁrst,
which we will cover in Chapter 19, is to bring prior knowledge to bear on the problem. The
second, which we introduced in Section 18.4.3, is to insist that the algorithm return not just
any consistent hypothesis, but preferably a simple one (as is done in decision tree learning). In
cases where ﬁnding simple consistent hypotheses is tractable, the sample complexity results
are generally better than for analyses based only on consistency. The third escape, which
we pursue next, is to focus on learnable subsets of the entire hypothesis space of Boolean
functions. This approach relies on the assumption that the restricted language contains a
hypothesis h that is close enough to the true function f; the beneﬁts are that the restricted
hypothesis space allows for effective generalization and is typically easier to search. We now
examine one such restricted language in more detail.
18.5.1 PAC learning example: Learning decision lists
We now show how to apply PAC learning to a new hypothesis space: decision lists .ADECISION LISTS
decision list consists of a series of tests, each of which is a conjunction of literals. If a
test succeeds when applied to an example description, the decision list speciﬁes the value
to be returned. If the test fails, processing continues with the next test in the list. Decision
lists resemble decision trees, but their overall structure is simpler: they branch only in one
716 Chapter 18. Learning from Examples
Patrons(x, Some)
No
Yes Yes
No
Patrons(x, Full)   Fri/Sat(x)
Yes
No
Yes
^
Figure 18.10 A decision list for the restaurant problem.
direction. In contrast, the individual tests are more complex. Figure 18.10 shows a decision
list that represents the following hypothesis:
WillWait ⇔(Patrons = Some)∨(Patrons = Full∧Fri/Sat) .
If we allow tests of arbitrary size, then decision lists can represent any Boolean function
(Exercise 18.14). On the other hand, if we restrict the size of each test to at most k literals,
then it is possible for the learning algorithm to generalize successfully from a small number
of examples. We call this languagek
-DL. The example in Figure 18.10 is in 2-DL. It is easy tok-DL
show (Exercise 18.14) that k-DL includes as a subset the language k-DT, the set of all decisionk-DT
trees of depth at most k. It is important to remember that the particular language referred to
by k-DL depends on the attributes used to describe the examples. We will use the notation
k-DL(n) to denote a k-DL language using n Boolean attributes.
The ﬁrst task is to show that k-DL is learnable—that is, that any function in k-DL can
be approximated accurately after training on a reasonable number of examples. To do this,
we need to calculate the number of hypotheses in the language. Let the language of tests—
conjunctions of at most k literals using n attributes—be Conj(n,k ). Because a decision list
is constructed of tests, and because each test can be attached to either aYes or a No outcome
or can be absent from the decision list, there are at most3
|Conj(n,k)| distinct sets of component
tests. Each of these sets of tests can be in any order, so
|k-DL(n)|≤3|Conj(n,k)||Conj(n,k )|! .
The number of conjunctions of k literals from n attributes is given by
|Conj(n,k )| =
k∑
i=0
⎞2n
i
⎠
= O(nk) .
Hence, after some work, we obtain
|k-DL(n)| =2 O(nk log2(nk)) .
We can plug this into Equation (18.1) to show that the number of examples needed for PAC-
learning a k-DL function is polynomial in n:
N≥1
ϵ
⎞
ln 1
δ+ O(nk log2(nk))
⎠
.
Therefore, any algorithm that returns a consistent decision list will PAC-learn ak-DL function
in a reasonable number of examples, for small k.
The next task is to ﬁnd an efﬁcient algorithm that returns a consistent decision list.
We will use a greedy algorithm called D ECISION -LIST-LEARNING that repeatedly ﬁnds a
Section 18.6. Regression and Classiﬁcation with Linear Models 717
function DECISION -LIST-LEARNING (examples) returns a decision list, or failure
if examples is empty then return the trivial decision list No
t←a test that matches a nonempty subset examplest of examples
such that the members of examplest are all positive or all negative
if there is no such t then return failure
if the examples in examplest are positive then o←Yes else o←No
return a decision list with initial test t and outcome o and remaining tests given by
DECISION -LIST-LEARNING (examples −examplest)
Figure 18.11 An algorithm for learning decision lists.
0.4
0.5
0.6
0.7
0.8
0.9
1
0 20 40 60 80 100
Proportion correct on test set
Training set size
Decision tree
Decision list
Figure 18.12 Learning curve for DECISION -LIST-LEARNING algorithm on the restaurant
data. The curve for DECISION -TREE -LEARNING is shown for comparison.
test that agrees exactly with some subset of the training set. Once it ﬁnds such a test, it
adds it to the decision list under construction and removes the corresponding examples. It
then constructs the remainder of the decision list, using just the remaining examples. This is
repeated until there are no examples left. The algorithm is shown in Figure 18.11.
This algorithm does not specify the method for selecting the next test to add to the
decision list. Although the formal results given earlier do not depend on the selection method,
it would seem reasonable to prefer small tests that match large sets of uniformly classiﬁed
examples, so that the overall decision list will be as compact as possible. The simplest strategy
is to ﬁnd the smallest testt that matches any uniformly classiﬁed subset, regardless of the size
of the subset. Even this approach works quite well, as Figure 18.12 suggests.
18.6 R EGRESSION AND CLASSIFICATION WITH LINEAR MODELS
Now it is time to move on from decision trees and lists to a different hypothesis space, one
that has been used for hundred of years: the class of linear functions of continuous-valued
LINEAR FUNCTION
718 Chapter 18. Learning from Examples
 300
 400
 500
 600
 700
 800
 900
 1000
 500  1000  1500  2000  2500  3000  3500
House price in $1000
House size in square feet
w0
w1
Loss
(a) (b)
Figure 18.13 (a) Data points of price versus ﬂoor space of houses for sale in Berkeley,
CA, in July 2009, along with the linear function hypothesis that minimizes squared error
loss: y =0 .232x + 246. (b) Plot of the loss function ∑
j(w1xj + w0 −yj)2 for various
values of w0,w1. Note that the loss function is convex, with a single global minimum.
inputs. We’ll start with the simplest case: regression with a univariate linear function, oth-
erwise known as “ﬁtting a straight line.” Section 18.6.2 covers the multivariate case. Sec-
tions 18.6.3 and 18.6.4 show how to turn linear functions into classiﬁers by applying hard
and soft thresholds.
18.6.1 Univariate linear regression
A univariate linear function (a straight line) with inputx and output y has the formy = w1x+
w0,w h e r ew0 and w1 are real-valued coefﬁcients to be learned. We use the letter w because
we think of the coefﬁcients as weights;t h ev a l u eo fy is changed by changing the relativeWEIGHT
weight of one term or another. We’ll deﬁne w to be the vector [w0,w1], and deﬁne
hw(x)= w1x + w0 .
Figure 18.13(a) shows an example of a training set of n points in the x, y plane, each point
representing the size in square feet and the price of a house offered for sale. The task of
ﬁnding the h
w that best ﬁts these data is called linear regression. To ﬁt a line to the data, allLINEAR REGRESSION
we have to do is ﬁnd the values of the weights[w0,w1] that minimize the empirical loss. It is
traditional (going back to Gauss3) to use the squared loss function, L2, summed over all the
training examples:
Loss(hw)=
N∑
j =1
L2(yj,h w(xj)) =
N∑
j =1
(yj −hw(xj))2 =
N∑
j =1
(yj −(w1xj + w0))2 .
3 Gauss showed that if the yj values have normally distributed noise, then the most likely values of w1 and w0
are obtained by minimizing the sum of the squares of the errors.
Section 18.6. Regression and Classiﬁcation with Linear Models 719
We would like to ﬁnd w∗ =a r g m i nw Loss(hw).T h e s u m∑N
j =1 (yj −(w1xj + w0))2 is
minimized when its partial derivatives with respect tow0 and w1 are zero:
∂
∂w0
N∑
j =1
(yj −(w1xj + w0))2 =0 and ∂
∂w1
N∑
j =1
(yj −(w1xj + w0))2 =0 . (18.2)
These equations have a unique solution:
w1 = N(∑xjyj)−(∑xj)(∑yj)
N(∑x2
j )−(∑xj)2 ; w0 =(
∑
yj −w1(
∑
xj))/N . (18.3)
For the example in Figure 18.13(a), the solution is w1 =0.232, w0 = 246, and the line with
those weights is shown as a dashed line in the ﬁgure.
Many forms of learning involve adjusting weights to minimize a loss, so it helps to
have a mental picture of what’s going on in weight space—the space deﬁned by all possibleWEIGHT SP ACE
settings of the weights. For univariate linear regression, the weight space deﬁned by w0 and
w1 is two-dimensional, so we can graph the loss as a function of w0 and w1 in a 3D plot (see
Figure 18.13(b)). We see that the loss function is convex, as deﬁned on page 133; this is true
for every linear regression problem with an L2 loss function, and implies that there are no
local minima. In some sense that’s the end of the story for linear models; if we need to ﬁt
lines to data, we apply Equation (18.3).
4
To go beyond linear models, we will need to face the fact that the equations deﬁning
minimum loss (as in Equation (18.2)) will often have no closed-form solution. Instead, we
will face a general optimization search problem in a continuous weight space. As indicated
in Section 4.2 (page 129), such problems can be addressed by a hill-climbing algorithm that
follows the gradient of the function to be optimized. In this case, because we are trying to
minimize the loss, we will use gradient descent . We choose any starting point in weight
GRADIENT DESCENT
space—here, a point in the ( w0, w1) plane—and then move to a neighboring point that is
downhill, repeating until we converge on the minimum possible loss:
w ←any point in the parameter space
loop until convergence do
for each wi in w do
wi ←wi−α ∂
∂wi
Loss(w) (18.4)
The parameter α, which we called the step size in Section 4.2, is usually called the learning
rate when we are trying to minimize loss in a learning problem. It can be a ﬁxed constant, orLEARNING RA TE
it can decay over time as the learning process proceeds.
For univariate regression, the loss function is a quadratic function, so the partial deriva-
tive will be a linear function. (The only calculus you need to know is that ∂
∂xx2 =2x and
∂
∂xx=1 .) Let’s ﬁrst work out the partial derivatives—the slopes—in the simpliﬁed case of
4 With some caveats: the L2 loss function is appropriate when there is normally-distributed noise that is inde-
pendent of x; all results rely on the stationarity assumption; etc.
720 Chapter 18. Learning from Examples
only one training example, (x, y):
∂
∂wi
Loss(w)= ∂
∂wi
(y−hw(x))2
=2 ( y−hw(x))× ∂
∂wi
(y−hw(x))
=2 ( y−hw(x))× ∂
∂wi
(y−(w1x + w0)) , (18.5)
applying this to both w0 and w1 we get:
∂
∂w0
Loss(w)= −2(y−hw(x)) ; ∂
∂w1
Loss(w)= −2(y−hw(x))× x
Then, plugging this back into Equation (18.4), and folding the 2 into the unspeciﬁed learning
rate α, we get the following learning rule for the weights:
w0 ←w0 + α(y−hw(x)) ; w1 ←w1 + α(y−hw(x))× x
These updates make intuitive sense: if hw(x) >y , i.e., the output of the hypothesis is too
large, reduce w0 a bit, and reduce w1 if x was a positive input but increase w1 if x was a
negative input.
The preceding equations cover one training example. ForN training examples, we want
to minimize the sum of the individual losses for each example. The derivative of a sum is the
sum of the derivatives, so we have:
w
0 ←w0 + α
∑
j
(yj −hw(xj)) ; w1 ←w1 + α
∑
j
(yj −hw(xj))× xj .
These updates constitute the batch gradient descent learning rule for univariate linear re-BA TCH GRADIENT
DESCENT
gression. Convergence to the unique global minimum is guaranteed (as long as we pick α
small enough) but may be very slow: we have to cycle through all the training data for every
step, and there may be many steps.
There is another possibility, called stochastic gradient descent , where we considerSTOCHASTIC
GRADIENT DESCENT
only a single training point at a time, taking a step after each one using Equation (18.5).
Stochastic gradient descent can be used in an online setting, where new data are coming in
one at a time, or ofﬂine, where we cycle through the same data as many times as is neces-
sary, taking a step after considering each single example. It is often faster than batch gradient
descent. With a ﬁxed learning rate α, however, it does not guarantee convergence; it can os-
cillate around the minimum without settling down. In some cases, as we see later, a schedule
of decreasing learning rates (as in simulated annealing) does guarantee convergence.
18.6.2 Multivariate linear regression
We can easily extend to multivariate linear regression problems, in which each example xj
MULTIVARIA TE
LINEAR REGRESSION
is an n-element vector.5 Our hypothesis space is the set of functions of the form
hsw(xj)= w0 + w1xj,1 +··· + wnxj,n = w0 +
∑
i
wixj,i .
5 The reader may wish to consult Appendix A for a brief summary of linear algebra.
Section 18.6. Regression and Classiﬁcation with Linear Models 721
The w0 term, the intercept, stands out as different from the others. We can ﬁx that by inventing
a dummy input attribute, xj,0, which is deﬁned as always equal to 1. Then h is simply the
dot product of the weights and the input vector (or equivalently, the matrix product of the
transpose of the weights and the input vector):
h
sw(xj)= w· xj = w⊤xj =
∑
i
wixj,i .
The best vector of weights, w∗, minimizes squared-error loss over the examples:
w∗ =a r g m i n
w
∑
j
L2(yj, w· xj).
Multivariate linear regression is actually not much more complicated than the univariate case
we just covered. Gradient descent will reach the (unique) minimum of the loss function; the
update equation for each weight w
i is
wi ←wi + α
∑
j
xj,i(yj −hw(xj)) . (18.6)
It is also possible to solve analytically for the w that minimizes loss. Let y be the vector of
outputs for the training examples, and X be the data matrix, i.e., the matrix of inputs withDATA MATRIX
one n-dimensional example per row. Then the solution
w∗ =( X⊤X)−1X⊤y
minimizes the squared error.
With univariate linear regression we didn’t have to worry about overﬁtting. But with
multivariate linear regression in high-dimensional spaces it is possible that some dimension
that is actually irrelevant appears by chance to be useful, resulting in overﬁtting.
Thus, it is common to useregularization on multivariate linear functions to avoid over-
ﬁtting. Recall that with regularization we minimize the total cost of a hypothesis, counting
both the empirical loss and the complexity of the hypothesis:
Cost(h)= EmpLoss(h)+ λComplexity(h) .
For linear functions the complexity can be speciﬁed as a function of the weights. We can
consider a family of regularization functions:
Complexity(h
w)= Lq(w)=
∑
i
|wi|q .
As with loss functions,6 with q =1 we have L1 regularization, which minimizes the sum of
the absolute values; with q =2 , L2 regularization minimizes the sum of squares. Which reg-
ularization function should you pick? That depends on the speciﬁc problem, but L1 regular-
ization has an important advantage: it tends to produce a sparse model. That is, it often setsSP ARSE MODEL
many weights to zero, effectively declaring the corresponding attributes to be irrelevant—just
as DECISION -TREE -LEARNING does (although by a different mechanism). Hypotheses that
discard attributes can be easier for a human to understand, and may be less likely to overﬁt.
6 It is perhaps confusing thatL1 and L2 are used for both loss functions and regularization functions. They need
not be used in pairs: you could use L2 loss with L1 regularization, or vice versa.
722 Chapter 18. Learning from Examples
w1
w2
w*
w1
w2
w*
Figure 18.14 Why L1 regularization tends to produce a sparse model. (a) With L1 regu-
larization (box), the minimal achievable loss (concentric contours) often occurs on an axis,
meaning a weight of zero. (b) With L2 regularization (circle), the minimal loss is likely to
o c c u ra n y w h e r eo nt h ec i r c l e ,g i v i n gn op r e f e r e n c et oz e r ow e i g h t s .
Figure 18.14 gives an intuitive explanation of whyL1 regularization leads to weights of
zero, while L2 regularization does not. Note that minimizing Loss(w)+ λComplexity(w)
is equivalent to minimizing Loss(w) subject to the constraint that Complexity(w)≤c,f o r
some constant c that is related to λ. Now, in Figure 18.14(a) the diamond-shaped box repre-
sents the set of points w in two-dimensional weight space that have L1 complexity less than
c; our solution will have to be somewhere inside this box. The concentric ovals represent
contours of the loss function, with the minimum loss at the center. We want to ﬁnd the point
in the box that is closest to the minimum; you can see from the diagram that, for an arbitrary
position of the minimum and its contours, it will be common for the corner of the box to ﬁnd
its way closest to the minimum, just because the corners are pointy. And of course the corners
are the points that have a value of zero in some dimension. In Figure 18.14(b), we’ve done
the same for the L
2 complexity measure, which represents a circle rather than a diamond.
Here you can see that, in general, there is no reason for the intersection to appear on one of
the axes; thus L
2 regularization does not tend to produce zero weights. The result is that the
number of examples required to ﬁnd a goodh is linear in the number of irrelevant features for
L2 regularization, but only logarithmic with L1 regularization. Empirical evidence on many
problems supports this analysis.
Another way to look at it is that L1 regularization takes the dimensional axes seriously,
while L2 treats them as arbitrary. The L2 function is spherical, which makes it rotationally
invariant: Imagine a set of points in a plane, measured by their x and y coordinates. Now
imagine rotating the axes by 45o. You’d get a different set of (x′,y ′) values representing
the same points. If you apply L2 regularization before and after rotating, you get exactly
the same point as the answer (although the point would be described with the new (x′,y ′)
coordinates). That is appropriate when the choice of axes really is arbitrary—when it doesn’t
matter whether your two dimensions are distances north and east; or distances north-east and
Section 18.6. Regression and Classiﬁcation with Linear Models 723
south-east. With L1 regularization you’d get a different answer, because theL1 function is not
rotationally invariant. That is appropriate when the axes are not interchangeable; it doesn’t
make sense to rotate “number of bathrooms” 45
o towards “lot size.”
18.6.3 Linear classiﬁers with a hard threshold
Linear functions can be used to do classiﬁcation as well as regression. For example, Fig-
ure 18.15(a) shows data points of two classes: earthquakes (which are of interest to seismolo-
gists) and underground explosions (which are of interest to arms control experts). Each point
is deﬁned by two input values, x
1 and x2, that refer to body and surface wave magnitudes
computed from the seismic signal. Given these training data, the task of classiﬁcation is to
learn a hypothesis h that will take new (x
1,x2) points and return either 0 for earthquakes or
1 for explosions.
 2.5
 3
 3.5
 4
 4.5
 5
 5.5
 6
 6.5
 7
 7.5
 4.5  5  5.5  6  6.5  7
x2
x1
 2.5
 3
 3.5
 4
 4.5
 5
 5.5
 6
 6.5
 7
 7.5
 4.5  5  5.5  6  6.5  7
x2
x1
(a) (b)
Figure 18.15 (a) Plot of two seismic data parameters, body wave magnitude x1 and sur-
face wave magnitude x2, for earthquakes (white circles) and nuclear explosions (black cir-
cles) occurring between 1982 and 1990 in Asia and the Middle East (Kebeasy et al., 1998).
Also shown is a decision boundary between the classes. (b) The same domain with more data
points. The earthquakes and explosions are no longer linearly separable.
A decision boundary is a line (or a surface, in higher dimensions) that separates theDECISION
BOUNDARY
two classes. In Figure 18.15(a), the decision boundary is a straight line. A linear decision
boundary is called a linear separator and data that admit such a separator are calledlinearlyLINEAR SEP ARA TOR
separable. The linear separator in this case is deﬁned byLINEAR
SEP ARABILITY
x2 =1 .7x1−4.9 or −4.9+1 .7x1−x2 =0 .
The explosions, which we want to classify with value 1, are to the right of this line with higher
values of x1 and lower values of x2, so they are points for which −4.9+1 .7x1 −x2 > 0,
while earthquakes have −4.9+1 .7x1 −x2 < 0. Using the convention of a dummy input
x0 =1 , we can write the classiﬁcation hypothesis as
hw(x)=1 if w· x≥0 and 0 otherwise.
724 Chapter 18. Learning from Examples
Alternatively, we can think of h as the result of passing the linear function w· x through a
threshold function:THRESHOLD
FUNCTION
hw(x)= Threshold(w· x) where Threshold(z)=1 if z≥0 and 0 otherwise.
The threshold function is shown in Figure 18.17(a).
Now that the hypothesis hw(x) has a well-deﬁned mathematical form, we can think
about choosing the weights w to minimize the loss. In Sections 18.6.1 and 18.6.2, we did
this both in closed form (by setting the gradient to zero and solving for the weights) and
by gradient descent in weight space. Here, we cannot do either of those things because the
gradient is zero almost everywhere in weight space except at those points where w· x =0 ,
and at those points the gradient is undeﬁned.
There is, however, a simple weight update rule that converges to a solution—that is, a
linear separator that classiﬁes the data perfectly–provided the data are linearly separable. For
a single example (x,y ),w eh a v e
w
i ←wi + α(y−hw(x))× xi (18.7)
which is essentially identical to the Equation (18.6), the update rule for linear regression! This
rule is called theperceptron learning rule, for reasons that will become clear in Section 18.7.PERCEPTRON
LEARNING RULE
Because we are considering a 0/1 classiﬁcation problem, however, the behavior is somewhat
different. Both the true value y and the hypothesis output hw(x) are either 0 or 1, so there are
three possibilities:
•If the output is correct, i.e., y = hw(x), then the weights are not changed.
•If y is 1 but hw(x) is 0, then wi is increased when the corresponding input xi is positive
and decreased when xi is negative. This makes sense, because we want to make w· x
bigger so that hw(x) outputs a 1.
•If y is 0 buthw(x) is 1, then wi is decreased when the corresponding inputxi is positive
and increased when xi is negative. This makes sense, because we want to make w· x
smaller so that hw(x) outputs a 0.
Typically the learning rule is applied one example at a time, choosing examples at random
(as in stochastic gradient descent). Figure 18.16(a) shows a training curve for this learningTRAINING CURVE
rule applied to the earthquake/explosion data shown in Figure 18.15(a). A training curve
measures the classiﬁer performance on a ﬁxed training set as the learning process proceeds
on that same training set. The curve shows the update rule converging to a zero-error linear
separator. The “convergence” process isn’t exactly pretty, but it always works. This particular
run takes 657 steps to converge, for a data set with 63 examples, so each example is presented
roughly 10 times on average. Typically, the variation across runs is very large.
We have said that the perceptron learning rule converges to a perfect linear separator
when the data points are linearly separable, but what if they are not? This situation is all
too common in the real world. For example, Figure 18.15(b) adds back in the data points
left out by Kebeasy et al. (1998) when they plotted the data shown in Figure 18.15(a). In
Figure 18.16(b), we show the perceptron learning rule failing to converge even after 10,000
steps: even though it hits the minimum-error solution (three errors) many times, the algo-
rithm keeps changing the weights. In general, the perceptron rule may not converge to a
Section 18.6. Regression and Classiﬁcation with Linear Models 725
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  100  200  300  400  500  600  700
Proportion correct
Number of weight updates
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  20000  40000  60000  80000  100000
Proportion correct
Number of weight updates
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  20000  40000  60000  80000  100000
Proportion correct
Number of weight updates
(a) (b) (c)
Figure 18.16 (a) Plot of total training-set accuracy vs. number of iterations through the
training set for the perceptron learning rule, given the earthquake/explosion data in Fig-
ure 18.15(a). (b) The same plot for the noisy, non-separable data in Figure 18.15(b); note
the change in scale of the x-axis. (c) The same plot as in (b), with a learning rate schedule
α(t) = 1000/(1000 +t).
stable solution for ﬁxed learning rate α,b u ti f α decays as O(1/t) where t is the iteration
number, then the rule can be shown to converge to a minimum-error solution when examples
are presented in a random sequence.
7 It can also be shown that ﬁnding the minimum-error
solution is NP-hard, so one expects that many presentations of the examples will be required
for convergence to be achieved. Figure 18.16(b) shows the training process with a learning
rate schedule α(t) = 1000/(1000 + t): convergence is not perfect after 100,000 iterations,
but it is much better than the ﬁxed-α case.
18.6.4 Linear classiﬁcation with logistic regression
We have seen that passing the output of a linear function through the threshold function
creates a linear classiﬁer; yet the hard nature of the threshold causes some problems: the
hypothesis hw(x) is not differentiable and is in fact a discontinuous function of its inputs and
its weights; this makes learning with the perceptron rule a very unpredictable adventure. Fur-
thermore, the linear classiﬁer always announces a completely conﬁdent prediction of 1 or 0,
even for examples that are very close to the boundary; in many situations, we really need
more gradated predictions.
All of these issues can be resolved to a large extent by softening the threshold function—
approximating the hard threshold with a continuous, differentiable function. In Chapter 14
(page 522), we saw two functions that look like soft thresholds: the integral of the standard
normal distribution (used for the probit model) and the logistic function (used for the logit
model). Although the two functions are very similar in shape, the logistic function
Logistic(z)= 1
1+ e− z
7 Technically, we require that P∞
t =1 α(t)= ∞ and P∞
t =1 α2(t) < ∞ . The decay α(t)= O(1/t) satisﬁes
these conditions.
726 Chapter 18. Learning from Examples
 0
 0.5
 1
-8 -6 -4 -2  0  2  4  6  8
 0
 0.5
 1
-6 -4 -2  0  2  4  6
-2  0  2  4  6
-4-2 0 2 4 6 8 10
 0
 0.2
 0.4
 0.6
 0.8
 1
x1
x2
(a) (b) (c)
Figure 18.17 (a) The hard threshold function Threshold(z) with 0/1 output. Note
that the function is nondifferentiable at z =0 . (b) The logistic function, Logistic(z)=
1
1+e− z , also known as the sigmoid function. (c) Plot of a logistic regression hypothesis
hw(x)= Logistic(w· x) for the data shown in Figure 18.15(b).
has more convenient mathematical properties. The function is shown in Figure 18.17(b).
With the logistic function replacing the threshold function, we now have
hw(x)= Logistic(w· x)= 1
1+ e− w·x .
An example of such a hypothesis for the two-input earthquake/explosion problem is shown in
Figure 18.17(c). Notice that the output, being a number between 0 and 1, can be interpreted
as a probability of belonging to the class labeled 1. The hypothesis forms a soft boundary
in the input space and gives a probability of 0.5 for any input at the center of the boundary
region, and approaches 0 or 1 as we move away from the boundary.
The process of ﬁtting the weights of this model to minimize loss on a data set is called
logistic regression. There is no easy closed-form solution to ﬁnd the optimal value ofw with
LOGISTIC
REGRESSION
this model, but the gradient descent computation is straightforward. Because our hypotheses
no longer output just 0 or 1, we will use the L2 loss function; also, to keep the formulas
readable, we’ll use g to stand for the logistic function, with g′ its derivative.
For a single example (x,y ), the derivation of the gradient is the same as for linear
regression (Equation (18.5)) up to the point where the actual form of h is inserted. (For this
derivation, we will need the chain rule: ∂g(f(x))/∂x= g′(f(x))∂f(x)/∂x.) We haveCHAIN RULE
∂
∂wi
Loss(w)= ∂
∂wi
(y−hw(x))2
=2 ( y−hw(x))× ∂
∂wi
(y−hw(x))
= −2(y−hw(x))× g′(w· x)× ∂
∂wi
w· x
= −2(y−hw(x))× g′(w· x)× xi .
Section 18.7. Artiﬁcial Neural Networks 727
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  1000  2000  3000  4000  5000
Squared error per example
Number of weight updates
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  20000  40000  60000  80000  100000
Squared error per example
Number of weight updates
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  20000  40000  60000  80000  100000
Squared error per example
Number of weight updates
(a) (b) (c)
Figure 18.18 Repeat of the experiments in Figure 18.16 using logistic regression and
squared error. The plot in (a) covers 5000 iterations rather than 1000, while (b) and (c) use
t h es a m es c a l e .
The derivativeg′ of the logistic function satisﬁes g′(z)= g(z)(1−g(z)),s ow eh a v e
g′(w· x)= g(w· x)(1−g(w· x)) = hw(x)(1−hw(x))
so the weight update for minimizing the loss is
wi ←wi + α(y−hw(x))× hw(x)(1−hw(x))× xi . (18.8)
Repeating the experiments of Figure 18.16 with logistic regression instead of the linear
threshold classiﬁer, we obtain the results shown in Figure 18.18. In (a), the linearly sep-
arable case, logistic regression is somewhat slower to converge, but behaves much more
predictably. In (b) and (c), where the data are noisy and nonseparable, logistic regression
converges far more quickly and reliably. These advantages tend to carry over into real-world
applications and logistic regression has become one of the most popular classiﬁcation tech-
niques for problems in medicine, marketing and survey analysis, credit scoring, public health,
and other applications.
18.7 A RTIFICIAL NEURAL NETWORKS
We turn now to what seems to be a somewhat unrelated topic: the brain. In fact, as we
will see, the technical ideas we have discussed so far in this chapter turn out to be useful in
building mathematical models of the brain’s activity; conversely, thinking about the brain has
helped in extending the scope of the technical ideas.
Chapter 1 touched brieﬂy on the basic ﬁndings of neuroscience—in particular, the hy-
pothesis that mental activity consists primarily of electrochemical activity in networks of
brain cells called neurons. (Figure 1.2 on page 11 showed a schematic diagram of a typical
neuron.) Inspired by this hypothesis, some of the earliest AI work aimed to create artiﬁcial
neural networks. (Other names for the ﬁeld include connectionism, parallel distributed
NEURAL NETWORK
processing,a n d neural computation .) Figure 18.19 shows a simple mathematical model
of the neuron devised by McCulloch and Pitts (1943). Roughly speaking, it “ﬁres” when a
linear combination of its inputs exceeds some (hard or soft) threshold—that is, it implements
728 Chapter 18. Learning from Examples
Output
Σ
Input
Links
Activation
Function
Input
Function
Output
Links
a0 = 1 aj = g(inj)
aj
ginjwi,j
w0,j
Bias Weight
ai
Figure 18.19 A simple mathematical model for a neuron. The unit’s output activation is
aj = g(∑n
i =0 wi,jai),w h e r eai is the output activation of uniti and wi,j is the weight on the
link from unit i to this unit.
a linear classiﬁer of the kind described in the preceding section. A neural network is just a
collection of units connected together; the properties of the network are determined by its
topology and the properties of the “neurons.”
Since 1943, much more detailed and realistic models have been developed, both for
neurons and for larger systems in the brain, leading to the modern ﬁeld of computational
neuroscience. On the other hand, researchers in AI and statistics became interested in theCOMPUT A TIONAL
NEUROSCIENCE
more abstract properties of neural networks, such as their ability to perform distributed com-
putation, to tolerate noisy inputs, and to learn. Although we understand now that other kinds
of systems—including Bayesian networks—have these properties, neural networks remain
one of the most popular and effective forms of learning system and are worthy of study in
their own right.
18.7.1 Neural network structures
Neural networks are composed of nodes or units (see Figure 18.19) connected by directedUNIT
links. A link from unit i to unit j serves to propagate the activation ai from i to j.8 Each linkLINK
ACTIVA TION also has a numeric weight wi,j associated with it, which determines the strength and sign of
WEIGHT the connection. Just as in linear regression models, each unit has a dummy input a0 =1 with
an associated weight w0,j . Each unit j ﬁrst computes a weighted sum of its inputs:
inj =
n∑
i =0
wi,jai .
Then it applies an activation function g to this sum to derive the output:ACTIVA TION
FUNCTION
aj = g(inj)= g
⎞ n∑
i =0
wi,jai
⎠
. (18.9)
8 A note on notation: for this section, we are forced to suspend our usual conventions. Input attributes are still
indexed by i , so that an “external” activation ai is given by input xi; but index j will refer to internal units
rather than examples. Throughout this section, the mathematical derivations concern a single generic example x,
omitting the usual summations over examples to obtain results for the whole data set.
Section 18.7. Artiﬁcial Neural Networks 729
The activation function g is typically either a hard threshold (Figure 18.17(a)), in which case
the unit is called aperceptron, or a logistic function (Figure 18.17(b)), in which case the termPERCEPTRON
sigmoid perceptron is sometimes used. Both of these nonlinear activation function ensureSIGMOID
PERCEPTRON
the important property that the entire network of units can represent a nonlinear function (see
Exercise 18.22). As mentioned in the discussion of logistic regression (page 725), the logistic
activation function has the added advantage of being differentiable.
Having decided on the mathematical model for individual “neurons,” the next task is
to connect them together to form a network. There are two fundamentally distinct ways to
do this. A feed-forward network has connections only in one direction—that is, it forms a
FEED-FORWARD
NETWORK
directed acyclic graph. Every node receives input from “upstream” nodes and delivers output
to “downstream” nodes; there are no loops. A feed-forward network represents a function of
its current input; thus, it has no internal state other than the weights themselves. A recurrent
network, on the other hand, feeds its outputs back into its own inputs. This means thatRECURRENT
NETWORK
the activation levels of the network form a dynamical system that may reach a stable state or
exhibit oscillations or even chaotic behavior. Moreover, the response of the network to a given
input depends on its initial state, which may depend on previous inputs. Hence, recurrent
networks (unlike feed-forward networks) can support short-term memory. This makes them
more interesting as models of the brain, but also more difﬁcult to understand. This section
will concentrate on feed-forward networks; some pointers for further reading on recurrent
networks are given at the end of the chapter.
Feed-forward networks are usually arranged inlayers, such that each unit receives input
LAYERS
only from units in the immediately preceding layer. In the next two subsections, we will look
at single-layer networks, in which every unit connects directly from the network’s inputs to
its outputs, and multilayer networks, which have one or more layers of hidden units that areHIDDEN UNIT
not connected to the outputs of the network. So far in this chapter, we have considered only
learning problems with a single output variabley, but neural networks are often used in cases
where multiple outputs are appropriate. For example, if we want to train a network to add
two input bits, each a 0 or a 1, we will need one output for the sum bit and one for the carry
bit. Also, when the learning problem involves classiﬁcation into more than two classes—for
example, when learning to categorize images of handwritten digits—it is common to use one
output unit for each class.
18.7.2 Single-layer feed-forward neural networks (perceptrons)
A network with all the inputs connected directly to the outputs is called asingle-layer neural
network,o ra perceptron network . Figure 18.20 shows a simple two-input, two-outputPERCEPTRON
NETWORK
perceptron network. With such a network, we might hope to learn the two-bit adder function,
for example. Here are all the training data we will need:
x1
 x2
 y3 (carry)
 y4 (sum)
0
 0
 0
 0
0
 1
 0
 1
1
 0
 0
 1
1
 1
 1
 0

730 Chapter 18. Learning from Examples
The ﬁrst thing to notice is that a perceptron network withm outputs is reallym separate
networks, because each weight affects only one of the outputs. Thus, there will be m sepa-
rate training processes. Furthermore, depending on the type of activation function used, the
training processes will be either the perceptron learning rule (Equation (18.7) on page 724)
or gradient descent rule for the logistic regression (Equation (18.8) on page 727).
If you try either method on the two-bit-adder data, something interesting happens. Unit
3 learns the carry function easily, but unit 4 completely fails to learn the sum function. No,
unit 4 is not defective! The problem is with the sum function itself. We saw in Section 18.6
that linear classiﬁers (whether hard or soft) can represent linear decision boundaries in the in-
put space. This works ﬁne for the carry function, which is a logical
AND (see Figure 18.21(a)).
The sum function, however, is an XOR (exclusive OR) of the two inputs. As Figure 18.21(c)
illustrates, this function is not linearly separable so the perceptron cannot learn it.
The linearly separable functions constitute just a small fraction of all Boolean func-
tions; Exercise 18.20 asks you to quantify this fraction. The inability of perceptrons to learn
even such simple functions as
XOR was a signiﬁcant setback to the nascent neural network
w3,5
3,6w
4,5w
4,6w
5
6
w1,3
1,4w
2,3w
2,4w
1
2
3
4
w1,3
1,4w
2,3w
2,4w
1
2
3
4
(b)(a)
Figure 18.20 (a) A perceptron network with two inputs and two output units. (b) A neural
network with two inputs, one hidden layer of two units, and one output unit. Not shown are
the dummy inputs and their associated weights.
(a)x1 and x2
1
0
01
x1
x2
(b)x1 or x2
01
1
0
x1
x2
(c)x1 xor x2
?
01
1
0
x1
x2
Figure 18.21 Linear separability in threshold perceptrons. Black dots indicate a point in
the input space where the value of the function is 1, and white dots indicate a point where the
value is 0. The perceptron returns 1 on the region on the non-shaded side of the line. In (c),
no such line exists that correctly classiﬁes the inputs.

Section 18.7. Artiﬁcial Neural Networks 731
0.4
0.5
0.6
0.7
0.8
0.9
1
0 10 20 30 40 50 60 70 80 90 100
Proportion correct on test set
Training set size
Perceptron
Decision tree
0.4
0.5
0.6
0.7
0.8
0.9
1
0 10 20 30 40 50 60 70 80 90 100
Proportion correct on test set
Training set size
Perceptron
Decision tree
(a) (b)
Figure 18.22 Comparing the performance of perceptrons and decision trees. (a) Percep-
trons are better at learning the majority function of 11 inputs. (b) Decision trees are better at
learning the WillWait predicate in the restaurant example.
community in the 1960s. Perceptrons are far from useless, however. Section 18.6.4 noted
that logistic regression (i.e., training a sigmoid perceptron) is even today a very popular and
effective tool. Moreover, a perceptron can represent some quite “complex” Boolean func-
tions very compactly. For example, the majority function, which outputs a 1 only if more
than half of its n inputs are 1, can be represented by a perceptron with each w
i =1 and with
w0 =−n/2. A decision tree would need exponentially many nodes to represent this function.
Figure 18.22 shows the learning curve for a perceptron on two different problems. On
the left, we show the curve for learning the majority function with 11 Boolean inputs (i.e.,
the function outputs a 1 if 6 or more inputs are 1). As we would expect, the perceptron learns
the function quite quickly, because the majority function is linearly separable. On the other
hand, the decision-tree learner makes no progress, because the majority function is very hard
(although not impossible) to represent as a decision tree. On the right, we have the restaurant
example. The solution problem is easily represented as a decision tree, but is not linearly
separable. The best plane through the data correctly classiﬁes only 65%.
18.7.3 Multilayer feed-forward neural networks
(McCulloch and Pitts, 1943) were well aware that a single threshold unit would not solve all
their problems. In fact, their paper proves that such a unit can represent the basic Boolean
functions
AND , OR,a n dNOT and then goes on to argue that any desired functionality can be
obtained by connecting large numbers of units into (possibly recurrent) networks of arbitrary
depth. The problem was that nobody knew how to train such networks.
This turns out to be an easy problem if we think of a network the right way: as a
function h
w(x) parameterized by the weights w. Consider the simple network shown in Fig-
ure 18.20(b), which has two input units, two hidden units, and two output unit. (In addition,
each unit has a dummy input ﬁxed at 1.) Given an input vector x =(x
1,x2), the activations
732 Chapter 18. Learning from Examples
-4 -2 0 2 4x1
-4 -2 0 2 4
x2
0
0.2
0.4
0.6
0.8
1
hW(x1, x2)
-4 -2 0 2 4x1
-4 -2 0 2 4
x2
0
0.2
0.4
0.6
0.8
1
hW(x1, x2)
(a) (b)
Figure 18.23 (a) The result of combining two opposite-facing soft threshold functions to
produce a ridge. (b) The result of combining two ridges to produce a bump.
of the input units are set to (a1,a2)=( x1,x2). The output at unit 5 is given by
a5 = g(w0,5,+w3,5 a3 + w4,5 a4)
= g(w0,5,+w3,5 g(w0,3 + w1,3 a1 + w2,3 a2)+ w4,5 g(w04+ w1,4 a1 + w2,4 a2))
= g(w0,5,+w3,5 g(w0,3 + w1,3 x1 + w2,3 x2)+ w4,5 g(w04+ w1,4 x1 + w2,4 x2)).
Thus, we have the output expressed as a function of the inputs and the weights. A similar
expression holds for unit 6. As long as we can calculate the derivatives of such expressions
with respect to the weights, we can use the gradient-descent loss-minimization method to
train the network. Section 18.7.4 shows exactly how to do this. And because the function
represented by a network can be highly nonlinear—composed, as it is, of nested nonlinear soft
threshold functions—we can see neural networks as a tool for doing nonlinear regression.
NONLINEAR
REGRESSION
Before delving into learning rules, let us look at the ways in which networks generate
complicated functions. First, remember that each unit in a sigmoid network represents a soft
threshold in its input space, as shown in Figure 18.17(c) (page 726). With one hidden layer
and one output layer, as in Figure 18.20(b), each output unit computes a soft-thresholded
linear combination of several such functions. For example, by adding two opposite-facing
soft threshold functions and thresholding the result, we can obtain a “ridge” function as shown
in Figure 18.23(a). Combining two such ridges at right angles to each other (i.e., combining
the outputs from four hidden units), we obtain a “bump” as shown in Figure 18.23(b).
With more hidden units, we can produce more bumps of different sizes in more places.
In fact, with a single, sufﬁciently large hidden layer, it is possible to represent any continuous
function of the inputs with arbitrary accuracy; with two layers, even discontinuous functions
can be represented.
9 Unfortunately, for any particular network structure, it is harder to char-
acterize exactly which functions can be represented and which ones cannot.
9 The proof is complex, but the main point is that the required number of hidden units grows exponentially with
the number of inputs. For example, 2n/n hidden units are needed to encode all Boolean functions of n inputs.
Section 18.7. Artiﬁcial Neural Networks 733
18.7.4 Learning in multilayer networks
First, let us dispense with one minor complication arising in multilayer networks: interactions
among the learning problems when the network has multiple outputs. In such cases, we
should think of the network as implementing a vector functionh
w rather than a scalar function
hw; for example, the network in Figure 18.20(b) returns a vector [a5,a6]. Similarly, the
target output will be a vector y. Whereas a perceptron network decomposes into m separate
learning problems for anm-output problem, this decomposition fails in a multilayer network.
For example, both a5 and a6 in Figure 18.20(b) depend on all of the input-layer weights, so
updates to those weights will depend on errors in botha5 and a6. Fortunately, this dependency
is very simple in the case of any loss function that is additive across the components of the
error vector y−hw(x).F o rt h eL2 loss, we have, for any weight w,
∂
∂wLoss(w)= ∂
∂w|y−hw(x)|2 = ∂
∂w
∑
k
(yk −ak)2 =
∑
k
∂
∂w(yk −ak)2 (18.10)
where the index k ranges over nodes in the output layer. Each term in the ﬁnal summation
is just the gradient of the loss for the kth output, computed as if the other outputs did not
exist. Hence, we can decompose an m-output learning problem into m learning problems,
provided we remember to add up the gradient contributions from each of them when updating
the weights.
The major complication comes from the addition of hidden layers to the network.
Whereas the error y−h
w at the output layer is clear, the error at the hidden layers seems
mysterious because the training data do not say what value the hidden nodes should have.
Fortunately, it turns out that we can back-propagate the error from the output layer to the
BACK-PROP AGA TION
hidden layers. The back-propagation process emerges directly from a derivation of the overall
error gradient. First, we will describe the process with an intuitive justiﬁcation; then, we will
show the derivation.
At the output layer, the weight-update rule is identical to Equation (18.8). We have
multiple output units, so let Errk be the kth component of the error vector y−hw. We will
also ﬁnd it convenient to deﬁne a modiﬁed error Δ k =Errk × g′(ink), so that the weight-
update rule becomes
wj,k ←wj,k + α× aj × Δ k . (18.11)
To update the connections between the input units and the hidden units, we need to deﬁne a
quantity analogous to the error term for output nodes. Here is where we do the error back-
propagation. The idea is that hidden nodej is “responsible” for some fraction of the errorΔ
k
in each of the output nodes to which it connects. Thus, the Δ k values are divided according
to the strength of the connection between the hidden node and the output node and are prop-
agated back to provide the Δ
j values for the hidden layer. The propagation rule for the Δ
values is the following:
Δ j = g′(inj)
∑
k
wj,kΔ k . (18.12)
734 Chapter 18. Learning from Examples
function BACK -PROP -LEARNING (examples,network) returns a neural network
inputs: examples, a set of examples, each with input vector x and output vector y
network, a multilayer network with L layers, weights wi,j , activation function g
local variables: Δ , a vector of errors, indexed by network node
repeat
for each weight wi,j in network do
wi,j←a small random number
for each example (x, y) in examples do
/* Propagate the inputs forward to compute the outputs */
for each node i in the input layer do
ai←xi
for ℓ =2 to L do
for each node j in layer ℓ do
inj←∑
i wi,j ai
aj←g(inj)
/* Propagate deltas backward from output layer to input layer */
for each node j in the output layer do
Δ[j]←g′(inj) × (yj −aj)
for ℓ = L−1 to 1 do
for each node i in layer ℓ do
Δ[i]←g′(ini) ∑
j wi,j Δ[j]
/* Update every weight in network using deltas */
for each weight wi,j in network do
wi,j←wi,j + α × ai × Δ[j]
until some stopping criterion is satisﬁed
return network
Figure 18.24 The back-propagation algorithm for learning in multilayer networks.
Now the weight-update rule for the weights between the inputs and the hidden layer is essen-
tially identical to the update rule for the output layer:
wi,j ←wi,j + α× ai× Δ j .
The back-propagation process can be summarized as follows:
•Compute the Δ values for the output units, using the observed error.
•Starting with output layer, repeat the following for each layer in the network, until the
earliest hidden layer is reached:
– Propagate the Δ values back to the previous layer.
– Update the weights between the two layers.
The detailed algorithm is shown in Figure 18.24.
For the mathematically inclined, we will now derive the back-propagation equations
from ﬁrst principles. The derivation is quite similar to the gradient calculation for logistic
Section 18.7. Artiﬁcial Neural Networks 735
regression (leading up to Equation (18.8) on page 727), except that we have to use the chain
rule more than once.
Following Equation (18.10), we compute just the gradient for Lossk =( yk −ak)2 at
the kth output. The gradient of this loss with respect to weights connecting the hidden layer
to the output layer will be zero except for weights wj,k that connect to the kth output unit.
For those weights, we have
∂Lossk
∂wj,k
= −2(yk −ak) ∂ak
∂wj,k
=−2(yk −ak)∂g(ink)
∂wj,k
= −2(yk −ak)g′(ink) ∂ink
∂wj,k
=−2(yk −ak)g′(ink) ∂
∂wj,k
⎛
⎝ ∑
j
wj,kaj
⎞
⎠
= −2(yk −ak)g′(ink)aj =−ajΔ k ,
with Δ k deﬁned as before. To obtain the gradient with respect to thewi,j weights connecting
the input layer to the hidden ¡layer, we have to expand out the activations aj and reapply the
chain rule. We will show the derivation in gory detail because it is interesting to see how the
derivative operator propagates back through the network:
∂Loss
k
∂wi,j
= −2(yk −ak) ∂ak
∂wi,j
=−2(yk −ak)∂g(ink)
∂wi,j
= −2(yk −ak)g′(ink) ∂ink
∂wi,j
=−2Δk
∂
∂wi,j
⎛
⎝ ∑
j
wj,kaj
⎞
⎠
= −2Δkwj,k
∂aj
∂wi,j
=−2Δkwj,k
∂g(inj)
∂wi,j
= −2Δkwj,kg′(inj) ∂inj
∂wi,j
= −2Δkwj,kg′(inj) ∂
∂wi,j
⎞∑
i
wi,jai
⎠
= −2Δkwj,kg′(inj)ai =−aiΔ j ,
where Δ j is deﬁned as before. Thus, we obtain the update rules obtained earlier from intuitive
considerations. It is also clear that the process can be continued for networks with more than
one hidden layer, which justiﬁes the general algorithm given in Figure 18.24.
Having made it through (or skipped over) all the mathematics, let’s see how a single-
hidden-layer network performs on the restaurant problem. First, we need to determine the
structure of the network. We have 10 attributes describing each example, so we will need
10 input units. Should we have one hidden layer or two? How many nodes in each layer?
Should they be fully connected? There is no good theory that will tell us the answer. (See the
next section.) As always, we can use cross-validation: try several different structures and see
which one works best. It turns out that a network with one hidden layer containing four nodes
is about right for this problem. In Figure 18.25, we show two curves. The ﬁrst is a training
curve showing the mean squared error on a given training set of 100 restaurant examples
736 Chapter 18. Learning from Examples
0
2
4
6
8
10
12
14
0 50 100 150 200 250 300 350 400
Total error on training set
Number of epochs
0.4
0.5
0.6
0.7
0.8
0.9
1
0 10 20 30 40 50 60 70 80 90 100
Proportion correct on test set
Training set size
Decision tree
Multilayer network
(a) (b)
Figure 18.25 (a) Training curve showing the gradual reduction in error as weights are
modiﬁed over several epochs, for a given set of examples in the restaurant domain. (b)
Comparative learning curves showing that decision-tree learning does slightly better on the
restaurant problem than back-propagation in a multilayer network.
during the weight-updating process. This demonstrates that the network does indeed converge
to a perfect ﬁt to the training data. The second curve is the standard learning curve for the
restaurant data. The neural network does learn well, although not quite as fast as decision-
tree learning; this is perhaps not surprising, because the data were generated from a simple
decision tree in the ﬁrst place.
Neural networks are capable of far more complex learning tasks of course, although it
must be said that a certain amount of twiddling is needed to get the network structure right
and to achieve convergence to something close to the global optimum in weight space. There
are literally tens of thousands of published applications of neural networks. Section 18.11.1
looks at one such application in more depth.
18.7.5 Learning neural network structures
So far, we have considered the problem of learning weights, given a ﬁxed network structure;
just as with Bayesian networks, we also need to understand how to ﬁnd the best network
structure. If we choose a network that is too big, it will be able to memorize all the examples
by forming a large lookup table, but will not necessarily generalize well to inputs that have
not been seen before.
10 In other words, like all statistical models, neural networks are subject
to overﬁtting when there are too many parameters in the model. We saw this in Figure 18.1
(page 696), where the high-parameter models in (b) and (c) ﬁt all the data, but might not
generalize as well as the low-parameter models in (a) and (d).
If we stick to fully connected networks, the only choices to be made concern the number
10 It has been observed that very large networks do generalize well as long as the weights are kept small .T h i s
restriction keeps the activation values in the linear region of the sigmoid function g(x) where x is close to zero.
This, in turn, means that the network behaves like a linear function (Exercise 18.22) with far fewer parameters.
Section 18.8. Nonparametric Models 737
of hidden layers and their sizes. The usual approach is to try several and keep the best. The
cross-validation techniques of Chapter 18 are needed if we are to avoid peeking at the test
set. That is, we choose the network architecture that gives the highest prediction accuracy on
the validation sets.
If we want to consider networks that are not fully connected, then we need to ﬁnd
some effective search method through the very large space of possible connection topologies.
The optimal brain damage algorithm begins with a fully connected network and removes
OPTIMAL BRAIN
DAMAGE
connections from it. After the network is trained for the ﬁrst time, an information-theoretic
approach identiﬁes an optimal selection of connections that can be dropped. The network
is then retrained, and if its performance has not decreased then the process is repeated. In
addition to removing connections, it is also possible to remove units that are not contributing
much to the result.
Several algorithms have been proposed for growing a larger network from a smaller one.
One, the tiling algorithm, resembles decision-list learning. The idea is to start with a single
TILING
unit that does its best to produce the correct output on as many of the training examples as
possible. Subsequent units are added to take care of the examples that the ﬁrst unit got wrong.
The algorithm adds only as many units as are needed to cover all the examples.
18.8 N ONPARAMETRIC MODELS
Linear regression and neural networks use the training data to estimate a ﬁxed set of param-
eters w. That deﬁnes our hypothesis h
w(x), and at that point we can throw away the training
data, because they are all summarized by w. A learning model that summarizes data with a
set of parameters of ﬁxed size (independent of the number of training examples) is called a
parametric model.
P ARAMETRIC MODEL
No matter how much data you throw at a parametric model, it won’t change its mind
about how many parameters it needs. When data sets are small, it makes sense to have a strong
restriction on the allowable hypotheses, to avoid overﬁtting. But when there are thousands or
millions or billions of examples to learn from, it seems like a better idea to let the data speak
for themselves rather than forcing them to speak through a tiny vector of parameters. If the
data say that the correct answer is a very wiggly function, we shouldn’t restrict ourselves to
linear or slightly wiggly functions.
A nonparametric model is one that cannot be characterized by a bounded set of param-
NONP ARAMETRIC
MODEL
eters. For example, suppose that each hypothesis we generate simply retains within itself all
of the training examples and uses all of them to predict the next example. Such a hypothesis
family would be nonparametric because the effective number of parameters is unbounded—
it grows with the number of examples. This approach is called instance-based learning or
INSTANCE-BASED
LEARNING
memory-based learning. The simplest instance-based learning method istable lookup:t a k eTABLE LOOKUP
all the training examples, put them in a lookup table, and then when asked forh(x), see if x is
in the table; if it is, return the corresponding y. The problem with this method is that it does
not generalize well: when x is not in the table all it can do is return some default value.
738 Chapter 18. Learning from Examples
 2.5
 3
 3.5
 4
 4.5
 5
 5.5
 6
 6.5
 7
 7.5
 4.5  5  5.5  6  6.5  7
x1
x2
 2.5
 3
 3.5
 4
 4.5
 5
 5.5
 6
 6.5
 7
 7.5
 4.5  5  5.5  6  6.5  7
x1
x2
(k =1 )( k =5 )
Figure 18.26 (a) Ak-nearest-neighbor model showing the extent of the explosion class for
the data in Figure 18.15, with k =1 . Overﬁtting is apparent. (b) With k =5 , the overﬁtting
problem goes away for this data set.
18.8.1 Nearest neighbor models
We can improve on table lookup with a slight variation: given a queryxq,ﬁ n dt h ek examples
that are nearest to xq. This is called k-nearest neighbors lookup. We’ll use the notationNEAREST
NEIGHBORS
NN(k, xq) to denote the set of k nearest neighbors.
To do classiﬁcation, ﬁrst ﬁnd NN(k, xq), then take the plurality vote of the neighbors
(which is the majority vote in the case of binary classiﬁcation). To avoid ties, k is always
chosen to be an odd number. To do regression, we can take the mean or median of the k
neighbors, or we can solve a linear regression problem on the neighbors.
In Figure 18.26, we show the decision boundary of k-nearest-neighbors classiﬁcation
for k = 1 and 5 on the earthquake data set from Figure 18.15. Nonparametric methods are
still subject to underﬁtting and overﬁtting, just like parametric methods. In this case 1-nearest
neighbors is overﬁtting; it reacts too much to the black outlier in the upper right and the white
outlier at (5.4, 3.7). The 5-nearest-neighbors decision boundary is good; higher k would
underﬁt. As usual, cross-validation can be used to select the best value of k.
The very word “nearest” implies a distance metric. How do we measure the distance
from a query point x
q to an example point xj? Typically, distances are measured with a
Minkowski distance or Lp norm, deﬁned asMINKOWSKI
DIST ANCE
Lp(xj, xq)=(
∑
i
|xj,i−xq,i|p)1/p .
With p =2 this is Euclidean distance and with p =1 it is Manhattan distance. With Boolean
attribute values, the number of attributes on which the two points differ is called the Ham-
ming distance. Often p =2 is used if the dimensions are measuring similar properties, suchHAMMING DIST ANCE
as the width, height and depth of parts on a conveyor belt, and Manhattan distance is used if
they are dissimilar, such as age, weight, and gender of a patient. Note that if we use the raw
numbers from each dimension then the total distance will be affected by a change in scale
in any dimension. That is, if we change dimension i from measurements in centimeters to
Section 18.8. Nonparametric Models 739
miles while keeping the other dimensions the same, we’ll get different nearest neighbors. To
avoid this, it is common to applynormalization to the measurements in each dimension. OneNORMALIZA TION
simple approach is to compute the mean μi and standard deviation σi of the values in each
dimension, and rescale them so that xj,i becomes (xj,i −μi)/σi. A more complex metric
known as the Mahalanobis distance takes into account the covariance between dimensions.MAHALANOBIS
DIST ANCE
In low-dimensional spaces with plenty of data, nearest neighbors works very well: we
are likely to have enough nearby data points to get a good answer. But as the number of
dimensions rises we encounter a problem: the nearest neighbors in high-dimensional spaces
are usually not very near! Consider k-nearest-neighbors on a data set of N points uniformly
distributed throughout the interior of an n-dimensional unit hypercube. We’ll deﬁne the k-
neighborhood of a point as the smallest hypercube that contains the k-nearest neighbors. Let
ℓ be the average side length of a neighborhood. Then the volume of the neighborhood (which
contains k points) is ℓ
n and the volume of the full cube (which contains N points) is 1. So,
on average, ℓn =k/N.T a k i n gnth roots of both sides we get ℓ =( k/N)1/n.
To be concrete, let k =1 0 and N =1,000,000. In two dimensions ( n =2 ; a unit
square), the average neighborhood has ℓ=0.003, a small fraction of the unit square, and
in 3 dimensions ℓ is just 2% of the edge length of the unit cube. But by the time we get to 17
dimensions, ℓ is half the edge length of the unit hypercube, and in 200 dimensions it is 94%.
This problem has been called the curse of dimensionality.CURSE OF
DIMENSIONALITY
Another way to look at it: consider the points that fall within a thin shell making up the
outer 1% of the unit hypercube. These are outliers; in general it will be hard to ﬁnd a good
value for them because we will be extrapolating rather than interpolating. In one dimension,
these outliers are only 2% of the points on the unit line (those points where x<. 01 or
x>. 99), but in 200 dimensions, over 98% of the points fall within this thin shell—almost
all the points are outliers. You can see an example of a poor nearest-neighbors ﬁt on outliers
if you look ahead to Figure 18.28(b).
The NN(k, x
q) function is conceptually trivial: given a set of N examples and a query
xq, iterate through the examples, measure the distance to xq from each one, and keep the best
k. If we are satisﬁed with an implementation that takes O(N) execution time, then that is the
end of the story. But instance-based methods are designed for large data sets, so we would
like an algorithm with sublinear run time. Elementary analysis of algorithms tells us that
exact table lookup is O(N) with a sequential table, O(log N) with a binary tree, and O(1)
with a hash table. We will now see that binary trees and hash tables are also applicable for
ﬁnding nearest neighbors.
18.8.2 Finding nearest neighbors with k-d trees
A balanced binary tree over data with an arbitrary number of dimensions is called ak-d tree,K-D TREE
for k-dimensional tree. (In our notation, the number of dimensions is n, so they would be
n-d trees. The construction of a k-d tree is similar to the construction of a one-dimensional
balanced binary tree. We start with a set of examples and at the root node we split them along
the ith dimension by testing whether x
i ≤m. We chose the value m to be the median of the
examples along the ith dimension; thus half the examples will be in the left branch of the tree
740 Chapter 18. Learning from Examples
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 25  50  75  100  125  150  175  200
Edge length of neighborhood
Number of dimensions
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 25  50  75  100  125  150  175  200
Proportion of points in exterior shell
Number of dimensions
(a) (b)
Figure 18.27 The curse of dimensionality: (a) The length of the average neighborhood for
10-nearest-neighbors in a unit hypercube with 1,000,000 points, as a function of the number
of dimensions. (b) The proportion of points that fall within a thin shell consisting of the
outer 1% of the hypercube, as a function of the number of dimensions. Sampled from 10,000
randomly distributed points.
and half in the right. We then recursively make a tree for the left and right sets of examples,
stopping when there are fewer than two examples left. To choose a dimension to split on at
each node of the tree, one can simply select dimension i mod n at level i of the tree. (Note
that we may need to split on any given dimension several times as we proceed down the tree.)
Another strategy is to split on the dimension that has the widest spread of values.
Exact lookup from a k-d tree is just like lookup from a binary tree (with the slight
complication that you need to pay attention to which dimension you are testing at each node).
But nearest neighbor lookup is more complicated. As we go down the branches, splitting
the examples in half, in some cases we can discard the other half of the examples. But not
always. Sometimes the point we are querying for falls very close to the dividing boundary.
The query point itself might be on the left hand side of the boundary, but one or more of
the k nearest neighbors might actually be on the right-hand side. We have to test for this
possibility by computing the distance of the query point to the dividing boundary, and then
searching both sides if we can’t ﬁnd k examples on the left that are closer than this distance.
Because of this problem, k-d trees are appropriate only when there are many more examples
than dimensions, preferably at least 2
n examples. Thus, k-d trees work well with up to 10
dimensions with thousands of examples or up to 20 dimensions with millions of examples. If
we don’t have enough examples, lookup is no faster than a linear scan of the entire data set.
18.8.3 Locality-sensitive hashing
Hash tables have the potential to provide even faster lookup than binary trees. But how can
we ﬁnd nearest neighbors using a hash table, when hash codes rely on an exact match? Hash
codes randomly distribute values among the bins, but we want to have near points grouped
together in the same bin; we want a locality-sensitive hash (LSH).
LOCALITY -SENSITIVE
HASH
Section 18.8. Nonparametric Models 741
We can’t use hashes to solve NN(k, xq) exactly, but with a clever use of randomized
algorithms, we can ﬁnd an approximate solution. First we deﬁne the approximate near-
neighbors problem: given a data set of example points and a query point xq, ﬁnd, with highAPPROXIMA TE
NEAR-NEIGHBORS
probability, an example point (or points) that is near xq. To be more precise, we require that
if there is a point xj that is within a radius r of xq, then with high probability the algorithm
will ﬁnd a point xj′ that is within distancecr of q. If there is no point within radiusr then the
algorithm is allowed to report failure. The values of c and “high probability” are parameters
of the algorithm.
To solve approximate near neighbors, we will need a hash function g(x) that has the
property that, for any two points xj and xj′ , the probability that they have the same hash code
is small if their distance is more than cr , and is high if their distance is less than r.F o r
simplicity we will treat each point as a bit string. (Any features that are not Boolean can be
encoded into a set of Boolean features.)
The intuition we rely on is that if two points are close together in an n-dimensional
space, then they will necessarily be close when projected down onto a one-dimensional space
(a line). In fact, we can discretize the line into bins—hash buckets—so that, with high prob-
ability, near points project down to exactly the same bin. Points that are far away from each
other will tend to project down into different bins for most projections, but there will always
be a few projections that coincidentally project far-apart points into the same bin. Thus, the
bin for point x
q contains many (but not all) points that are near to xq, as well as some points
that are far away.
The trick of LSH is to createmultiple random projections and combine them. A random
projection is just a random subset of the bit-string representation. We choose ℓ different
random projections and create ℓ hash tables, g1(x),...,g ℓ(x). We then enter all the examples
into each hash table. Then when given a query pointxq, we fetch the set of points in bingk(q)
for each k, and union these sets together into a set of candidate points, C. Then we compute
the actual distance to xq for each of the points in C and return the k closest points. With high
probability, each of the points that are near to xq will show up in at least one of the bins, and
although some far-away points will show up as well, we can ignore those. With large real-
world problems, such as ﬁnding the near neighbors in a data set of 13 million Web images
using 512 dimensions (Torralbaet al., 2008), locality-sensitive hashing needs to examine only
a few thousand images out of 13 million to ﬁnd nearest neighbors; a thousand-fold speedup
over exhaustive or k-d tree approaches.
18.8.4 Nonparametric regression
Now we’ll look at nonparametric approaches to regression rather than classiﬁcation. Fig-
ure 18.28 shows an example of some different models. In (a), we have perhaps the simplest
method of all, known informally as “connect-the-dots,” and superciliously as “piecewise-
linear nonparametric regression.” This model creates a function h(x) that, when given a
query x
q, solves the ordinary linear regression problem with just two points: the training
examples immediately to the left and right of xq. When noise is low, this trivial method is
actually not too bad, which is why it is a standard feature of charting software in spreadsheets.
742 Chapter 18. Learning from Examples
 0
 1
 2
 3
 4
 5
 6
 7
 8
 0  2  4  6  8  10  12  14
 0
 1
 2
 3
 4
 5
 6
 7
 8
 0  2  4  6  8  10  12  14
(a) (b)
 0
 1
 2
 3
 4
 5
 6
 7
 8
 0  2  4  6  8  10  12  14
 0
 1
 2
 3
 4
 5
 6
 7
 8
 0  2  4  6  8  10  12  14
(c) (d)
Figure 18.28 Nonparametric regression models: (a) connect the dots, (b) 3-nearest neigh-
bors average, (c) 3-nearest-neighbors linear regression, (d) locally weighted regression with
a quadratic kernel of width k =10 .
But when the data are noisy, the resulting function is spiky, and does not generalize well.
k-nearest-neighbors regression (Figure 18.28(b)) improves on connect-the-dots. In-
NEAREST -
NEIGHBORS
REGRESSION
stead of using just the two examples to the left and right of a query point xq,w eu s et h e
k nearest neighbors (here 3). A larger value of k tends to smooth out the magnitude of
the spikes, although the resulting function has discontinuities. In (b), we have the k-nearest-
neighbors average: h(x) is the mean value of thek points, ∑yj/k. Notice that at the outlying
points, near x=0 and x=1 4, the estimates are poor because all the evidence comes from one
side (the interior), and ignores the trend. In (c), we havek-nearest-neighbor linear regression,
which ﬁnds the best line through thek examples. This does a better job of capturing trends at
the outliers, but is still discontinuous. In both (b) and (c), we’re left with the question of how
to choose a good value for k. The answer, as usual, is cross-validation.
Locally weighted regression(Figure 18.28(d)) gives us the advantages of nearest neigh-LOCALL Y WEIGHTED
REGRESSION
bors, without the discontinuities. To avoid discontinuities inh(x), we need to avoid disconti-
Section 18.8. Nonparametric Models 743
 0
 0.5
 1
-10 -5  0  5  10
Figure 18.29 A quadratic kernel, K(d)= m a x ( 0, 1 −(2|x|/k)2), with kernel width
k =1 0, centered on the query point x=0 .
nuities in the set of examples we use to estimateh(x). The idea of locally weighted regression
is that at each query point xq, the examples that are close to xq are weighted heavily, and the
examples that are farther away are weighted less heavily or not at all. The decrease in weight
over distance is always gradual, not sudden.
We decide how much to weight each example with a function known as a kernel.A
KERNEL
kernel function looks like a bump; in Figure 18.29 we see the speciﬁc kernel used to generate
Figure 18.28(d). We can see that the weight provided by this kernel is highest in the center
and reaches zero at a distance of±5. Can we choose just any function for a kernel? No. First,
note that we invoke a kernel functionK withK(Distance(xj, xq)),w h e r exq is a query point
that is a given distance from xj , and we want to know how much to weight that distance.
SoK should be symmetric around 0 and have a maximum at 0. The area under the kernel
must remain bounded as we go to±∞. Other shapes, such as Gaussians, have been used for
kernels, but the latest research suggests that the choice of shape doesn’t matter much. We
do have to be careful about the width of the kernel. Again, this is a parameter of the model
that is best chosen by cross-validation. Just as in choosing the k for nearest neighbors, if the
kernels are too wide we’ll get underﬁtting and if they are too narrow we’ll get overﬁtting. In
Figure 18.29(d), the value of k =1 0gives a smooth curve that looks about right—but maybe
it does not pay enough attention to the outlier at x=6 ; a narrower kernel width would be
more responsive to individual points.
Doing locally weighted regression with kernels is now straightforward. For a given
query point x
q we solve the following weighted regression problem using gradient descent:
w∗ =a r g m i n
w
∑
j
K(Distance(xq, xj)) (yj −w· xj)2 ,
where Distance is any of the distance metrics discussed for nearest neighbors. Then the
answer is h(xq)= w∗· xq.
Note that we need to solve a new regression problem forevery query point—that’s what
it means to be local. (In ordinary linear regression, we solved the regression problem once,
globally, and then used the same hw for any query point.) Mitigating against this extra work
744 Chapter 18. Learning from Examples
is the fact that each regression problem will be easier to solve, because it involves only the
examples with nonzero weight—the examples whose kernels overlap the query point. When
kernel widths are small, this may be just a few points.
Most nonparametric models have the advantage that it is easy to do leave-one-out cross-
validation without having to recompute everything. With a k-nearest-neighbors model, for
instance, when given a test example(x,y ) we retrieve the k nearest neighbors once, compute
the per-example loss L(y,h (x)) from them, and record that as the leave-one-out result for
every example that is not one of the neighbors. Then we retrieve the k +1 nearest neighbors
and record distinct results for leaving out each of the k neighbors. With N examples the
whole process is O(k), not O(kN).
18.9 S UPPORT VECTOR MACHINES
The support vector machine or SVM framework is currently the most popular approach forSUPPORT VECTOR
MACHINE
“off-the-shelf” supervised learning: if you don’t have any specialized prior knowledge about
a domain, then the SVM is an excellent method to try ﬁrst. There are three properties that
make SVMs attractive:
1. SVMs construct a maximum margin separator—a decision boundary with the largest
possible distance to example points. This helps them generalize well.
2. SVMs create a linear separating hyperplane, but they have the ability to embed the
data into a higher-dimensional space, using the so-called kernel trick. Often, data that
are not linearly separable in the original input space are easily separable in the higher-
dimensional space. The high-dimensional linear separator is actually nonlinear in the
original space. This means the hypothesis space is greatly expanded over methods that
use strictly linear representations.
3. SVMs are a nonparametric method—they retain training examples and potentially need
to store them all. On the other hand, in practice they often end up retaining only a
small fraction of the number of examples—sometimes as few as a small constant times
the number of dimensions. Thus SVMs combine the advantages of nonparametric and
parametric models: they have the ﬂexibility to represent complex functions, but they
are resistant to overﬁtting.
You could say that SVMs are successful because of one key insight and one neat trick. We
will cover each in turn. In Figure 18.30(a), we have a binary classiﬁcation problem with three
candidate decision boundaries, each a linear separator. Each of them is consistent with all
the examples, so from the point of view of 0/1 loss, each would be equally good. Logistic
regression would ﬁnd some separating line; the exact location of the line depends on all the
example points. The key insight of SVMs is that some examples are more important than
others, and that paying attention to them can lead to better generalization.
Consider the lowest of the three separating lines in (a). It comes very close to 5 of the
black examples. Although it classiﬁes all the examples correctly, and thus minimizes loss, it
Section 18.9. Support Vector Machines 745
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
(a) (b)
Figure 18.30 Support vector machine classiﬁcation: (a) Two classes of points (black and
white circles) and three candidate linear separators. (b) The maximum margin separator
(heavy line), is at the midpoint of the margin (area between dashed lines). The support
vectors (points with large circles) are the examples closest to the separator.
should make you nervous that so many examples are close to the line; it may be that other
black examples will turn out to fall on the other side of the line.
SVMs address this issue: Instead of minimizing expected empirical loss on the training
data, SVMs attempt to minimize expected generalization loss. We don’t know where the
as-yet-unseen points may fall, but under the probabilistic assumption that they are drawn
from the same distribution as the previously seen examples, there are some arguments from
computational learning theory (Section 18.5) suggesting that we minimize generalization loss
by choosing the separator that is farthest away from the examples we have seen so far. We
call this separator, shown in Figure 18.30(b) the maximum margin separator.T h e margin
MAXIMUM MARGIN
SEP ARA TOR
MARGIN is the width of the area bounded by dashed lines in the ﬁgure—twice the distance from the
separator to the nearest example point.
Now, how do we ﬁnd this separator? Before showing the equations, some notation:
Traditionally SVMs use the convention that class labels are +1 and -1, instead of the +1 and
0 we have been using so far. Also, where we put the intercept into the weight vector w (and
a corresponding dummy 1 value into x
j,0), SVMs do not do that; they keep the intercept
as a separate parameter, b. With that in mind, the separator is deﬁned as the set of points
{x : w· x + b=0}. We could search the space of w and b with gradient descent to ﬁnd the
parameters that maximize the margin while correctly classifying all the examples.
However, it turns out there is another approach to solving this problem. We won’t
show the details, but will just say that there is an alternative representation called the dual
746 Chapter 18. Learning from Examples
representation, in which the optimal solution is found by solving
argmax
α
∑
j
αj −1
2
∑
j,k
αjαkyjyk(xj · xk) (18.13)
subject to the constraints αj ≥0 and ∑
j αjyj =0 . T h i si sa quadratic programmingQUADRA TIC
PROGRAMMING
optimization problem, for which there are good software packages. Once we have found the
vector α we can get back to w with the equation w = ∑
j αj xj, or we can stay in the dual
representation. There are three important properties of Equation (18.13). First, the expression
is convex; it has a single global maximum that can be found efﬁciently. Second,the data enter
the expression only in the form of dot products of pairs of points. This second property is also
true of the equation for the separator itself; once the optimal αj have been calculated, it is
h(x)= sign
⎛
⎝ ∑
j
αjyj(x· xj)−b
⎞
⎠ . (18.14)
A ﬁnal important property is that the weights αj associated with each data point are zero ex-
cept for the support vectors—the points closest to the separator. (They are called “support”SUPPORT VECTOR
vectors because they “hold up” the separating plane.) Because there are usually many fewer
support vectors than examples, SVMs gain some of the advantages of parametric models.
What if the examples are not linearly separable? Figure 18.31(a) shows an input space
deﬁned by attributes x =(x1,x2), with positive examples (y =+ 1 ) inside a circular region
and negative examples (y =−1) outside. Clearly, there is no linear separator for this problem.
Now, suppose we re-express the input data—i.e., we map each input vectorx to a new vector
of feature values, F(x). In particular, let us use the three features
f1 = x2
1 ,f 2 = x2
2 ,f 3 =
√
2x1x2 . (18.15)
We will see shortly where these came from, but for now, just look at what happens. Fig-
ure 18.31(b) shows the data in the new, three-dimensional space deﬁned by the three features;
t h ed a t aa r elinearly separable in this space! This phenomenon is actually fairly general: if
data are mapped into a space of sufﬁciently high dimension, then they will almost always be
linearly separable—if you look at a set of points from enough directions, you’ll ﬁnd a way to
make them line up. Here, we used only three dimensions;
11 Exercise 18.16 asks you to show
that four dimensions sufﬁce for linearly separating a circle anywhere in the plane (not just at
the origin), and ﬁve dimensions sufﬁce to linearly separate any ellipse. In general (with some
special cases excepted) if we haveN data points then they will always be separable in spaces
of N−1 dimensions or more (Exercise 18.25).
Now, we would not usually expect to ﬁnd a linear separator in the input space x,b u t
we can ﬁnd linear separators in the high-dimensional feature spaceF(x) simply by replacing
xj·xk in Equation (18.13) withF(xj)·F(xk). This by itself is not remarkable—replacingx by
F(x) in any learning algorithm has the required effect—but the dot product has some special
properties. It turns out that F(xj)· F(xk) can often be computed without ﬁrst computing F
11 The reader may notice that we could have used just f1 and f2, but the 3D mapping illustrates the idea better.
Section 18.9. Support Vector Machines 747
-1.5
-1
-0.5
0
0.5
1
1.5
-1.5 -1 -0.5 0 0.5 1 1.5
x2
x1
0
0.5
1
1.5
2x1
2 0.5
1
1.5
2
2.5
x2
2
-3
-2
-1
0
1
2
3
√2x1x2
(a) (b)
Figure 18.31 (a) A two-dimensional training set with positive examples as black cir-
cles and negative examples as white circles. The true decision boundary, x2
1 + x2
2 ≤1,
is also shown. (b) The same data after mapping into a three-dimensional input space
(x2
1,x2
2
,
√
2x1x2). The circular decision boundary in (a) becomes a linear decision boundary
in three dimensions. Figure 18.30(b) gives a closeup of the separator in (b).
for each point. In our three-dimensional feature space deﬁned by Equation (18.15), a little bit
of algebra shows that
F(xj)· F(xk)=( xj · xk)2 .
(That’s why the
√
2 is in f3.) The expression (xj · xk)2 is called a kernel function,12 andKERNEL FUNCTION
is usually written as K(xj, xk). The kernel function can be applied to pairs of input data to
evaluate dot products in some corresponding feature space. So, we can ﬁnd linear separators
in the higher-dimensional feature space F(x) simply by replacing xj· xk in Equation (18.13)
with a kernel functionK(xj, xk). Thus, we can learn in the higher-dimensional space, but we
compute only kernel functions rather than the full list of features for each data point.
The next step is to see that there’s nothing special about the kernelK(xj, xk)=( xj·xk)2.
It corresponds to a particular higher-dimensional feature space, but other kernel functions
correspond to other feature spaces. A venerable result in mathematics, Mercer’s theo-
rem (1909), tells us that any “reasonable” 13 kernel function corresponds to some featureMERCER’S THEOREM
space. These feature spaces can be very large, even for innocuous-looking kernels. For ex-
ample, the polynomial kernel, K(xj, xk)=( 1+ xj · xk)d, corresponds to a feature spacePOL YNOMIAL
KERNEL
whose dimension is exponential in d.
12 This usage of “kernel function” is slightly different from the kernels in locally weighted regression. Some
SVM kernels are distance metrics, but not all are.
13 Here, “reasonable” means that the matrix Kjk = K(xj, xk) is positive deﬁnite.
748 Chapter 18. Learning from Examples
This then is the clever kernel trick : Plugging these kernels into Equation (18.13),KERNEL TRICK
optimal linear separators can be found efﬁciently in feature spaces with billions of (or , in
some cases, inﬁnitely many) dimensions. The resulting linear separators, when mapped back
to the original input space, can correspond to arbitrarily wiggly, nonlinear decision bound-
aries between the positive and negative examples.
In the case of inherently noisy data, we may not want a linear separator in some high-
dimensional space. Rather, we’d like a decision surface in a lower-dimensional space that
does not cleanly separate the classes, but reﬂects the reality of the noisy data. That is pos-
sible with the soft margin classiﬁer, which allows examples to fall on the wrong side of the
SOFT MARGIN
decision boundary, but assigns them a penalty proportional to the distance required to move
them back on the correct side.
The kernel method can be applied not only with learning algorithms that ﬁnd optimal
linear separators, but also with any other algorithm that can be reformulated to work only
with dot products of pairs of data points, as in Equations 18.13 and 18.14. Once this is
done, the dot product is replaced by a kernel function and we have a kernelized version
KERNELIZA TION
of the algorithm. This can be done easily for k-nearest-neighbors and perceptron learning
(Section 18.7.2), among others.
18.10 E NSEMBLE LEARNING
So far we have looked at learning methods in which a single hypothesis, chosen from a
hypothesis space, is used to make predictions. The idea of ensemble learning methods is
ENSEMBLE
LEARNING
to select a collection, or ensemble, of hypotheses from the hypothesis space and combine
their predictions. For example, during cross-validation we might generate twenty different
decision trees, and have them vote on the best classiﬁcation for a new example.
The motivation for ensemble learning is simple. Consider an ensemble of K =5 hy-
potheses and suppose that we combine their predictions using simple majority voting. For the
ensemble to misclassify a new example, at least three of the ﬁve hypotheses have to misclas-
sify it. The hope is that this is much less likely than a misclassiﬁcation by a single hypothesis.
Suppose we assume that each hypothesis h
k in the ensemble has an error of p—that is, the
probability that a randomly chosen example is misclassiﬁed byhk is p. Furthermore, suppose
we assume that the errors made by each hypothesis areindependent. In that case, if p is small,
then the probability of a large number of misclassiﬁcations occurring is minuscule. For ex-
ample, a simple calculation (Exercise 18.18) shows that using an ensemble of ﬁve hypotheses
reduces an error rate of 1 in 10 down to an error rate of less than 1 in 100. Now, obviously
the assumption of independence is unreasonable, because hypotheses are likely to be misled
in the same way by any misleading aspects of the training data. But if the hypotheses are at
least a little bit different, thereby reducing the correlation between their errors, then ensemble
learning can be very useful.
Another way to think about the ensemble idea is as a generic way of enlarging the
hypothesis space. That is, think of the ensemble itself as a hypothesis and the new hypothesis
Section 18.10. Ensemble Learning 749
+
+ +
+
+
+++++
+ +
+
+
–––
–
–
–
– –
–
–
–
–
–
–
–
–
–
–– ––
– –
––
– –
– ––
–
–
–– –
–
–– –
–
–
Figure 18.32 Illustration of the increased expressive power obtained by ensemble learn-
ing. We take three linear threshold hypotheses, each of which classiﬁes positively on the
unshaded side, and classify as positive any example classiﬁed positively by all three. The
resulting triangular region is a hypothesis not expressible in the original hypothesis space.
space as the set of all possible ensembles constructable from hypotheses in the original space.
Figure 18.32 shows how this can result in a more expressive hypothesis space. If the original
hypothesis space allows for a simple and efﬁcient learning algorithm, then the ensemble
method provides a way to learn a much more expressive class of hypotheses without incurring
much additional computational or algorithmic complexity.
The most widely used ensemble method is calledboosting. To understand how it works,
BOOSTING
we need ﬁrst to explain the idea of a weighted training set . In such a training set, eachWEIGHTED TRAINING
SET
example has an associated weight wj ≥0. The higher the weight of an example, the higher
is the importance attached to it during the learning of a hypothesis. It is straightforward to
modify the learning algorithms we have seen so far to operate with weighted training sets.14
Boosting starts with wj =1 for all the examples (i.e., a normal training set). From this
set, it generates the ﬁrst hypothesis,h1. This hypothesis will classify some of the training ex-
amples correctly and some incorrectly. We would like the next hypothesis to do better on the
misclassiﬁed examples, so we increase their weights while decreasing the weights of the cor-
rectly classiﬁed examples. From this new weighted training set, we generate hypothesis h
2.
The process continues in this way until we have generatedK hypotheses, where K is an input
to the boosting algorithm. The ﬁnal ensemble hypothesis is a weighted-majority combination
of all theK hypotheses, each weighted according to how well it performed on the training set.
Figure 18.33 shows how the algorithm works conceptually. There are many variants of the ba-
sic boosting idea, with different ways of adjusting the weights and combining the hypotheses.
One speciﬁc algorithm, called A
DABOOST , is shown in Figure 18.34. ADABOOST has a very
important property: if the input learning algorithm L is a weak learning algorithm—whichWEAK LEARNING
14 For learning algorithms in which this is not possible, one can instead create a replicated training set where
the jth example appears wj times, using randomization to handle fractional weights.
750 Chapter 18. Learning from Examples
h
h1 =h 2 =h 3 =h 4 =
Figure 18.33 How the boosting algorithm works. Each shaded rectangle corresponds to
an example; the height of the rectangle corresponds to the weight. The checks and crosses
indicate whether the example was classiﬁed correctly by the current hypothesis. The size of
the decision tree indicates the weight of that hypothesis in the ﬁnal ensemble.
means that L always returns a hypothesis with accuracy on the training set that is slightly
better than random guessing (i.e., 50%+ϵ for Boolean classiﬁcation)—then ADABOOST will
return a hypothesis that classiﬁes the training data perfectly for large enough K. Thus, the
algorithm boosts the accuracy of the original learning algorithm on the training data. This
result holds no matter how inexpressive the original hypothesis space and no matter how
complex the function being learned.
Let us see how well boosting does on the restaurant data. We will choose as our original
hypothesis space the class of decision stumps, which are decision trees with just one test, at
DECISION STUMP
the root. The lower curve in Figure 18.35(a) shows that unboosted decision stumps are not
very effective for this data set, reaching a prediction performance of only 81% on 100 training
examples. When boosting is applied (with K =5 ), the performance is better, reaching 93%
after 100 examples.
An interesting thing happens as the ensemble size K increases. Figure 18.35(b) shows
the training set performance (on 100 examples) as a function of K. Notice that the error
reaches zero when K is 20; that is, a weighted-majority combination of 20 decision stumps
sufﬁces to ﬁt the 100 examples exactly. As more stumps are added to the ensemble, the error
remains at zero. The graph also shows that the test set performance continues to increase
long after the training set error has reached zero. At K =2 0, the test performance is 0.95
(or 0.05 error), and the performance increases to 0.98 as late as K = 137, before gradually
dropping to 0.95.
This ﬁnding, which is quite robust across data sets and hypothesis spaces, came as quite
a surprise when it was ﬁrst noticed. Ockham’s razor tells us not to make hypotheses more
Section 18.10. Ensemble Learning 751
function ADABOOST (examples,L,K ) returns a weighted-majority hypothesis
inputs: examples,s e to fN labeled examples (x1,y1),..., (xN ,y N )
L, a learning algorithm
K , the number of hypotheses in the ensemble
local variables: w, a vector of N example weights, initially 1/N
h, a vector of K hypotheses
z, a vector of K hypothesis weights
for k =1 to K do
h[k]←L(examples, w)
error←0
for j =1 to N do
if h[k](xj)̸= yj then error←error + w[j]
for j =1 to N do
if h[k](xj)= yj then w[j]←w[j]· error/(1−error)
w←NORMALIZE (w)
z[k]←log (1−error)/error
return WEIGHTED -MAJORITY (h, z)
Figure 18.34 The ADABOOST variant of the boosting method for ensemble learning. The
algorithm generates hypotheses by successively reweighting the training examples. The func-
tion W EIGHTED -MAJORITY generates a hypothesis that returns the output value with the
highest vote from the hypotheses in h, with votes weighted by z.
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0 20 40 60 80 100
Proportion correct on test set
Training set size
Boosted decision stumps
Decision stump
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0  50  100  150  200
Training/test accuracy
Number of hypotheses K
Training error
Test error
(a) (b)
Figure 18.35 (a) Graph showing the performance of boosted decision stumps with K =5
versus unboosted decision stumps on the restaurant data. (b) The proportion correct on the
training set and the test set as a function of K, the number of hypotheses in the ensemble.
Notice that the test set accuracy improves slightly even after the training accuracy reaches 1,
i.e., after the ensemble ﬁts the data exactly.

752 Chapter 18. Learning from Examples
complex than necessary, but the graph tells us that the predictions improve as the ensemble
hypothesis gets more complex! Various explanations have been proposed for this. One view
is that boosting approximates Bayesian learning (see Chapter 20), which can be shown to
be an optimal learning algorithm, and the approximation improves as more hypotheses are
added. Another possible explanation is that the addition of further hypotheses enables the
ensemble to be more deﬁnite in its distinction between positive and negative examples, which
helps it when it comes to classifying new examples.
18.10.1 Online Learning
So far, everything we have done in this chapter has relied on the assumption that the data are
i.i.d. (independent and identically distributed). On the one hand, that is a sensible assumption:
if the future bears no resemblance to the past, then how can we predict anything? On the other
hand, it is too strong an assumption: it is rare that our inputs have captured all the information
that would make the future truly independent of the past.
In this section we examine what to do when the data are not i.i.d.; when they can change
over time. In this case, it matterswhen we make a prediction, so we will adopt the perspective
called online learning: an agent receives an input x
j from nature, predicts the correspondingONLINE LEARNING
yj, and then is told the correct answer. Then the process repeats with xj+1, and so on. One
might think this task is hopeless—if nature is adversarial, all the predictions may be wrong.
It turns out that there are some guarantees we can make.
Let us consider the situation where our input consists of predictions from a panel of
experts. For example, each day a set of K pundits predicts whether the stock market will go
up or down, and our task is to pool those predictions and make our own. One way to do this
is to keep track of how well each expert performs, and choose to believe them in proportion
to their past performance. This is called the randomized weighted majority algorithm.W e
RANDOMIZED
WEIGHTED
MAJORITY
ALGORITHM
can described it more formally:
1. Initialize a set of weights {w1,...,w K} all to 1.
2. Receive the predictions {ˆy1,..., ˆyK} from the experts.
3. Randomly choose an expert k∗, in proportion to its weight: P(k)= wk/(∑
k′ wk′).
4. Predict ˆyk∗.
5. Receive the correct answer y.
6. For each expert k such that ˆyk ̸= y, update wk←βwk
Here β is a number, 0 <β< 1, that tells how much to penalize an expert for each mistake.
We measure the success of this algorithm in terms of regret, which is deﬁned as theREGRET
number of additional mistakes we make compared to the expert who, in hindsight, had the
best prediction record. Let M ∗ be the number of mistakes made by the best expert. Then the
number of mistakes, M, made by the random weighted majority algorithm, is bounded by15
M< M ∗ln(1/β)+l n K
1−β .
15 See (Blum, 1996) for the proof.
Section 18.11. Practical Machine Learning 753
This bound holds for any sequence of examples, even ones chosen by adversaries trying to
do their worst. To be speciﬁc, when there are K =1 0 experts, if we choose β=1/2 then
our number of mistakes is bounded by 1.39M ∗ +4 .6,a n di fβ=3/4 by 1.15M ∗ +9 .2.I n
general, if β is close to 1 then we are responsive to change over the long run; if the best expert
changes, we will pick up on it before too long. However, we pay a penalty at the beginning,
when we start with all experts trusted equally; we may accept the advice of the bad experts
for too long. When β is closer to 0, these two factors are reversed. Note that we can chooseβ
to get asymptotically close to M ∗ in the long run; this is called no-regret learning (becauseNO-REGRET
LEARNING
the average amount of regret per trial tends to 0 as the number of trials increases).
Online learning is helpful when the data may be changing rapidly over time. It is also
useful for applications that involve a large collection of data that is constantly growing, even
if changes are gradual. For example, with a database of millions of Web images, you wouldn’t
want to train, say, a linear regression model on all the data, and then retrain from scratch every
time a new image is added. It would be more practical to have an online algorithm that allows
images to be added incrementally. For most learning algorithms based on minimizing loss,
there is an online version based on minimizing regret. It is a bonus that many of these online
algorithms come with guaranteed bounds on regret.
To some observers, it is surprising that there are such tight bounds on how well we can
do compared to a panel of experts. To others, the really surprising thing is that when panels
of human experts congregate—predicting stock market prices, sports outcomes, or political
contests—the viewing public is so willing to listen to them pontiﬁcate and so unwilling to
quantify their error rates.
18.11 P RACTICAL MACHINE LEARNING
We have introduced a wide range of machine learning techniques, each illustrated with simple
learning tasks. In this section, we consider two aspects of practical machine learning. The ﬁrst
involves ﬁnding algorithms capable of learning to recognize handwritten digits and squeezing
every last drop of predictive performance out of them. The second involves anything but—
pointing out that obtaining, cleaning, and representing the data can be at least as important as
algorithm engineering.
18.11.1 Case study: Handwritten digit recognition
Recognizing handwritten digits is an important problem with many applications, including
automated sorting of mail by postal code, automated reading of checks and tax returns, and
data entry for hand-held computers. It is an area where rapid progress has been made, in part
because of better learning algorithms and in part because of the availability of better training
sets. The United States National Institute of Science and Technology ( NIST) has archived a
database of 60,000 labeled digits, each 20×20 = 400pixels with 8-bit grayscale values. It
has become one of the standard benchmark problems for comparing new learning algorithms.
Some example digits are shown in Figure 18.36.
754 Chapter 18. Learning from Examples
Figure 18.36 Examples from the NIST database of handwritten digits. Top row: examples
of digits 0–9 that are easy to identify. Bottom row: more difﬁcult examples of the same digits.
Many different learning approaches have been tried. One of the ﬁrst, and probably the
simplest, is the 3-nearest-neighbor classiﬁer, which also has the advantage of requiring no
training time. As a memory-based algorithm, however, it must store all 60,000 images, and
its run time performance is slow. It achieved a test error rate of 2.4%.
A single-hidden-layer neural network was designed for this problem with 400 input
units (one per pixel) and 10 output units (one per class). Using cross-validation, it was found
that roughly 300 hidden units gave the best performance. With full interconnections between
layers, there were a total of 123,300 weights. This network achieved a 1.6% error rate.
A series of specialized neural networks called LeNet were devised to take advantage
of the structure of the problem—that the input consists of pixels in a two–dimensional array,
and that small changes in the position or slant of an image are unimportant. Each network
had an input layer of 32× 32 units, onto which the 20× 20 pixels were centered so that each
input unit is presented with a local neighborhood of pixels. This was followed by three layers
of hidden units. Each layer consisted of several planes of n×n arrays, where n is smaller
than the previous layer so that the network is down-sampling the input, and where the weights
of every unit in a plane are constrained to be identical, so that the plane is acting as a feature
detector: it can pick out a feature such as a long vertical line or a short semi-circular arc. The
output layer had 10 units. Many versions of this architecture were tried; a representative one
had hidden layers with 768, 192, and 30 units, respectively. The training set was augmented
by applying afﬁne transformations to the actual inputs: shifting, slightly rotating, and scaling
the images. (Of course, the transformations have to be small, or else a 6 will be transformed
into a 9!) The best error rate achieved by LeNet was 0.9%.
A boosted neural network combined three copies of the LeNet architecture, with the
second one trained on a mix of patterns that the ﬁrst one got 50% wrong, and the third one
trained on patterns for which the ﬁrst two disagreed. During testing, the three nets voted with
the majority ruling. The test error rate was 0.7%.
A support vector machine (see Section 18.9) with 25,000 support vectors achieved an
error rate of 1.1%. This is remarkable because the SVM technique, like the simple nearest-
neighbor approach, required almost no thought or iterated experimentation on the part of the
developer, yet it still came close to the performance of LeNet, which had had years of devel-
opment. Indeed, the support vector machine makes no use of the structure of the problem,
and would perform just as well if the pixels were presented in a permuted order.
Section 18.11. Practical Machine Learning 755
A virtual support vector machine starts with a regular SVM and then improves itVIRTUAL SUPPORT
VECTOR MACHINE
with a technique that is designed to take advantage of the structure of the problem. Instead of
allowing products of all pixel pairs, this approach concentrates on kernels formed from pairs
of nearby pixels. It also augments the training set with transformations of the examples, just
as LeNet did. A virtual SVM achieved the best error rate recorded to date, 0.56%.
Shape matching is a technique from computer vision used to align corresponding parts
of two different images of objects (Belongie et al. , 2002). The idea is to pick out a set
of points in each of the two images, and then compute, for each point in the ﬁrst image,
which point in the second image it corresponds to. From this alignment, we then compute a
transformation between the images. The transformation gives us a measure of the distance
between the images. This distance measure is better motivated than just counting the number
of differing pixels, and it turns out that a 3–nearest neighbor algorithm using this distance
measure performs very well. Training on only 20,000 of the 60,000 digits, and using 100
sample points per image extracted from a Canny edge detector, a shape matching classiﬁer
achieved 0.63% test error.
Humans are estimated to have an error rate of about 0.2% on this problem. This ﬁgure
is somewhat suspect because humans have not been tested as extensively as have machine
learning algorithms. On a similar data set of digits from the United States Postal Service,
human errors were at 2.5%.
The following ﬁgure summarizes the error rates, run time performance, memory re-
quirements, and amount of training time for the seven algorithms we have discussed. It also
adds another measure, the percentage of digits that must be rejected to achieve 0.5% error.
For example, if the SVM is allowed to reject 1.8% of the inputs—that is, pass them on for
someone else to make the ﬁnal judgment—then its error rate on the remaining 98.2% of the
inputs is reduced from 1.1% to 0.5%.
The following table summarizes the error rate and some of the other characteristics of
the seven techniques we have discussed.
3
 300
 Boosted
 Virtual
 Shape
NN
 Hidden
 LeNet
 LeNet
 SVM
 SVM
 Match
Error rate (pct.)
 2.4
 1.6
 0.9
 0.7
 1.1
 0.56
 0.63
Run time (millisec/digit)
 1000
 10
 30
 50
 2000
 200
Memory requirements (Mbyte)
 12
 .49
 .012
 .21
 11
Training time (days)
 0
 7
 14
 30
 10
% rejected to reach 0.5% error
 8.1
 3.2
 1.8
 0.5
 1.8
18.11.2 Case study: Word senses and house prices
In a textbook we need to deal with simple, toy data to get the ideas across: a small data set,
usually in two dimensions. But in practical applications of machine learning, the data set
is usually large, multidimensional, and messy. The data are not handed to the analyst in a
prepackaged set of (x,y ) values; rather the analyst needs to go out and acquire the right data.
There is a task to be accomplished, and most of the engineering problem is deciding what
data are necessary to accomplish the task; a smaller part is choosing and implementing an
756 Chapter 18. Learning from Examples
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 1  10  100  1000
Proportion correct on test set
Training set size (millions of words)
Figure 18.37 Learning curves for ﬁve learning algorithms on a common task. Note that
there appears to be more room for improvement in the horizontal direction (more training
data) than in the vertical direction (different machine learning algorithm). Adapted from
Banko and Brill (2001).
appropriate machine learning method to process the data. Figure 18.37 shows a typical real-
world example, comparing ﬁve learning algorithms on the task of word-sense classiﬁcation
(given a sentence such as “The bank folded,” classify the word “bank” as “money-bank” or
“river-bank”). The point is that machine learning researchers have focused mainly on the
vertical direction: Can I invent a new learning algorithm that performs better than previously
published algorithms on a standard training set of 1 million words? But the graph shows
there is more room for improvement in the horizontal direction: instead of inventing a new
algorithm, all I need to do is gather 10 million words of training data; even theworst algorithm
at 10 million words is performing better than the best algorithm at 1 million. As we gather
even more data, the curves continue to rise, dwarﬁng the differences between algorithms.
Consider another problem: the task of estimating the true value of houses that are for
sale. In Figure 18.13 we showed a toy version of this problem, doing linear regression of
house size to asking price. You probably noticed many limitations of this model. First, it is
measuring the wrong thing: we want to estimate the selling price of a house, not the asking
price. To solve this task we’ll need data on actual sales. But that doesn’t mean we should
throw away the data about asking price—we can use it as one of the input features. Besides
the size of the house, we’ll need more information: the number of rooms, bedrooms and
bathrooms; whether the kitchen and bathrooms have been recently remodeled; the age of
the house; we’ll also need information about the lot, and the neighborhood. But how do
we deﬁne neighborhood? By zip code? What if part of one zip code is on the “wrong”
side of the highway or train tracks, and the other part is desirable? What about the school
district? Should the name of the school district be a feature, or the average test scores?I n
addition to deciding what features to include, we will have to deal with missing data; different
areas have different customs on what data are reported, and individual cases will always be
missing some data. If the data you want are not available, perhaps you can set up a social
networking site to encourage people to share and correct data. In the end, this process of
Section 18.12. Summary 757
deciding what features to use, and how to use them, is just as important as choosing between
linear regression, decision trees, or some other form of learning.
That said, one does have to pick a method (or methods) for a problem. There is no
guaranteed way to pick the best method, but there are some rough guidelines. Decision
trees are good when there are a lot of discrete features and you believe that many of them
may be irrelevant. Nonparametric methods are good when you have a lot of data and no prior
knowledge, and when you don’t want to worry too much about choosing just the right features
(as long as there are fewer than 20 or so). However, nonparametric methods usually give you
a function h that is more expensive to run. Support vector machines are often considered the
best method to try ﬁrst, provided the data set is not too large.
18.12 S UMMARY
This chapter has concentrated on inductive learning of functions from examples. The main
points were as follows:
•Learning takes many forms, depending on the nature of the agent, the component to be
improved, and the available feedback.
•If the available feedback provides the correct answer for example inputs, then the learn-
ing problem is called supervised learning. The task is to learn a function y = h(x).
Learning a discrete-valued function is calledclassiﬁcation; learning a continuous func-
tion is called regression.
•Inductive learning involves ﬁnding a hypothesis that agrees well with the examples.
Ockham’s razor suggests choosing the simplest consistent hypothesis. The difﬁculty
of this task depends on the chosen representation.
•Decision trees can represent all Boolean functions. The information-gain heuristic
provides an efﬁcient method for ﬁnding a simple, consistent decision tree.
•The performance of a learning algorithm is measured by the learning curve,w h i c h
shows the prediction accuracy on the test set as a function of the training-set size.
•When there are multiple models to choose from, cross-validation can be used to select
a model that will generalize well.
•Sometimes not all errors are equal. A loss function tells us how bad each error is; the
goal is then to minimize loss over a validation set.
•Computational learning theory analyzes the sample complexity and computational
complexity of inductive learning. There is a tradeoff between the expressiveness of the
hypothesis language and the ease of learning.
•Linear regression is a widely used model. The optimal parameters of a linear regres-
sion model can be found by gradient descent search, or computed exactly.
•A linear classiﬁer with a hard threshold—also known as a perceptron—can be trained
by a simple weight update rule to ﬁt data that are linearly separable. In other cases,
the rule fails to converge.
758 Chapter 18. Learning from Examples
•Logistic regression replaces the perceptron’s hard threshold with a soft threshold de-
ﬁned by a logistic function. Gradient descent works well even for noisy data that are
not linearly separable.
•Neural networks represent complex nonlinear functions with a network of linear-
threshold units. termMultilayer feed-forward neural networks can represent any func-
tion, given enough units. The back-propagation algorithm implements a gradient de-
scent in parameter space to minimize the output error.
•Nonparametric models use all the data to make each prediction, rather than trying to
summarize the data ﬁrst with a few parameters. Examples include nearest neighbors
and locally weighted regression.
•Support vector machines ﬁnd linear separators with maximum margin to improve
the generalization performance of the classiﬁer. Kernel methods implicitly transform
the input data into a high-dimensional space where a linear separator may exist, even if
the original data are non-separable.
•Ensemble methods such as boosting often perform better than individual methods. In
online learning we can aggregate the opinions of experts to come arbitrarily close to the
best expert’s performance, even when the distribution of the data is constantly shifting.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Chapter 1 outlined the history of philosophical investigations into inductive learning. William
of Ockham
16 (1280–1349), the most inﬂuential philosopher of his century and a major con-
tributor to medieval epistemology, logic, and metaphysics, is credited with a statement called
“Ockham’s Razor”—in Latin, Entia non sunt multiplicanda praeter necessitatem ,a n di nE n -
glish, “Entities are not to be multiplied beyond necessity.” Unfortunately, this laudable piece
of advice is nowhere to be found in his writings in precisely these words (although he did
say “Pluralitas non est ponenda sine necessitate,” or “plurality shouldn’t be posited without
necessity”). A similar sentiment was expressed by Aristotle in 350
B.C.i n Physics book I,
chapter VI: “For the more limited, if adequate, is always preferable.”
The ﬁrst notable use of decision trees was in EPAM, the “Elementary Perceiver And
Memorizer” (Feigenbaum, 1961), which was a simulation of human concept learning. ID3
(Quinlan, 1979) added the crucial idea of choosing the attribute with maximum entropy; it is
the basis for the decision tree algorithm in this chapter. Information theory was developed by
Claude Shannon to aid in the study of communication (Shannon and Weaver, 1949). (Shan-
non also contributed one of the earliest examples of machine learning, a mechanical mouse
named Theseus that learned to navigate through a maze by trial and error.) The χ
2 method
of tree pruning was described by Quinlan (1986). C4.5, an industrial-strength decision tree
package, can be found in Quinlan (1993). An independent tradition of decision tree learning
exists in the statistical literature. Classiﬁcation and Regression Trees (Breiman et al., 1984),
known as the “CART book,” is the principal reference.
16 The name is often misspelled as “Occam,” perhaps from the French rendering, “Guillaume d’Occam.”
Bibliographical and Historical Notes 759
Cross-validation was ﬁrst introduced by Larson (1931), and in a form close to what
we show by Stone (1974) and Golub et al. (1979). The regularization procedure is due to
Tikhonov (1963). Guyon and Elisseeff (2003) introduce a journal issue devoted to the prob-
lem of feature selection. Banko and Brill (2001) and Halevy et al. (2009) discuss the advan-
tages of using large amounts of data. It was Robert Mercer, a speech researcher who said
in 1985 “There is no data like more data.” (Lyman and Varian, 2003) estimate that about 5
exabytes (5× 10
18 bytes) of data was produced in 2002, and that the rate of production is
doubling every 3 years.
Theoretical analysis of learning algorithms began with the work of Gold (1967) on
identiﬁcation in the limit . This approach was motivated in part by models of scientiﬁc
discovery from the philosophy of science (Popper, 1962), but has been applied mainly to the
problem of learning grammars from example sentences (Osherson et al., 1986).
Whereas the identiﬁcation-in-the-limit approach concentrates on eventual convergence,
the study of Kolmogorov complexity or algorithmic complexity, developed independently
KOLMOGOROV
COMPLEXITY
by Solomonoff (1964, 2009) and Kolmogorov (1965), attempts to provide a formal deﬁnition
for the notion of simplicity used in Ockham’s razor. To escape the problem that simplicity
depends on the way in which information is represented, it is proposed that simplicity be
measured by the length of the shortest program for a universal Turing machine that correctly
reproduces the observed data. Although there are many possible universal Turing machines,
and hence many possible “shortest” programs, these programs differ in length by at most a
constant that is independent of the amount of data. This beautiful insight, which essentially
shows that any initial representation bias will eventually be overcome by the data itself, is
marred only by the undecidability of computing the length of the shortest program. Approx-
imate measures such as the minimum description length , or MDL (Rissanen, 1984, 2007)
MINIMUM
DESCRIPTION
LENGTH
can be used instead and have produced excellent results in practice. The text by Li and Vi-
tanyi (1993) is the best source for Kolmogorov complexity.
The theory of PAC-learning was inaugurated by Leslie Valiant (1984). His work stressed
the importance of computational and sample complexity. With Michael Kearns (1990), Valiant
showed that several concept classes cannot be PAC-learned tractably, even though sufﬁcient
information is available in the examples. Some positive results were obtained for classes such
as decision lists (Rivest, 1987).
An independent tradition of sample-complexity analysis has existed in statistics, begin-
ning with the work on uniform convergence theory (Vapnik and Chervonenkis, 1971). The
UNIFORM
CONVERGENCE
THEORY
so-called VC dimension provides a measure roughly analogous to, but more general than, theVC DIMENSION
ln|H| measure obtained from PAC analysis. The VC dimension can be applied to continuous
function classes, to which standard PAC analysis does not apply. PAC-learning theory and
VC theory were ﬁrst connected by the “four Germans” (none of whom actually is German):
Blumer, Ehrenfeucht, Haussler, and Warmuth (1989).
Linear regression with squared error loss goes back to Legendre (1805) and Gauss
(1809), who were both working on predicting orbits around the sun. The modern use of
multivariate regression for machine learning is covered in texts such as Bishop (2007). Ng
(2004) analyzed the differences between L
1 and L2 regularization.
760 Chapter 18. Learning from Examples
The term logistic function comes from Pierre-Franc¸ois Verhulst (1804–1849), a statis-
tician who used the curve to model population growth with limited resources, a more realis-
tic model than the unconstrained geometric growth proposed by Thomas Malthus. Verhulst
called it the courbe logistique, because of its relation to the logarithmic curve. The term re-
gression is due to Francis Galton, nineteenth century statistician, cousin of Charles Darwin,
and initiator of the ﬁelds of meteorology, ﬁngerprint analysis, and statistical correlation, who
used it in the sense of regression to the mean. The term curse of dimensionality comes from
Richard Bellman (1961).
Logistic regression can be solved with gradient descent, or with the Newton-Raphson
method (Newton, 1671; Raphson, 1690). A variant of the Newton method called L-BFGS is
sometimes used for large-dimensional problems; the L stands for “limited memory,” meaning
that it avoids creating the full matrices all at once, and instead creates parts of them on the
ﬂy. BFGS are authors’ initials (Byrd et al., 1995).
Nearest-neighbors models date back at least to Fix and Hodges (1951) and have been a
standard tool in statistics and pattern recognition ever since. Within AI, they were popularized
by Stanﬁll and Waltz (1986), who investigated methods for adapting the distance metric to the
data. Hastie and Tibshirani (1996) developed a way to localize the metric to each point in the
space, depending on the distribution of data around that point. Gioniset al. (1999) introduced
locality-sensitive hashing, which has revolutionized the retrieval of similar objects in high-
dimensional spaces, particularly in computer vision. Andoni and Indyk (2006) provide a
recent survey of LSH and related methods.
The ideas behind kernel machines come from Aizerman et al. (1964) (who also in-
troduced the kernel trick), but the full development of the theory is due to Vapnik and his
colleagues (Boser et al. , 1992). SVMs were made practical with the introduction of the
soft-margin classiﬁer for handling noisy data in a paper that won the 2008 ACM Theory
and Practice Award (Cortes and Vapnik, 1995), and of the Sequential Minimal Optimization
(SMO) algorithm for efﬁciently solving SVM problems using quadratic programming (Platt,
1999). SVMs have proven to be very popular and effective for tasks such as text categoriza-
tion (Joachims, 2001), computational genomics (Cristianini and Hahn, 2007), and natural lan-
guage processing, such as the handwritten digit recognition of DeCoste and Sch¨olkopf (2002).
As part of this process, many new kernels have been designed that work with strings, trees,
and other nonnumerical data types. A related technique that also uses the kernel trick to im-
plicitly represent an exponential feature space is the voted perceptron (Freund and Schapire,
1999; Collins and Duffy, 2002). Textbooks on SVMs include Cristianini and Shawe-Taylor
(2000) and Sch¨olkopf and Smola (2002). A friendlier exposition appears in the AI Magazine
article by Cristianini and Sch¨olkopf (2002). Bengio and LeCun (2007) show some of the
limitations of SVMs and other local, nonparametric methods for learning functions that have
a global structure but do not have local smoothness.
Ensemble learning is an increasingly popular technique for improving the performance
of learning algorithms. Bagging (Breiman, 1996), the ﬁrst effective method, combines hy-
BAGGING
potheses learned from multiple bootstrap data sets, each generated by subsampling the orig-
inal data set. The boosting method described in this chapter originated with theoretical work
by Schapire (1990). The A DABOOST algorithm was developed by Freund and Schapire
Bibliographical and Historical Notes 761
(1996) and analyzed theoretically by Schapire (2003). Friedman et al. (2000) explain boost-
ing from a statistician’s viewpoint. Online learning is covered in a survey by Blum (1996)
and a book by Cesa-Bianchi and Lugosi (2006). Dredze et al. (2008) introduce the idea of
conﬁdence-weighted online learning for classiﬁcation: in addition to keeping a weight for
each parameter, they also maintain a measure of conﬁdence, so that a new example can have
a large effect on features that were rarely seen before (and thus had low conﬁdence) and a
small effect on common features that have already been well-estimated.
The literature on neural networks is rather too large (approximately 150,000 papers to
date) to cover in detail. Cowan and Sharp (1988b, 1988a) survey the early history, beginning
with the work of McCulloch and Pitts (1943). (As mentioned in Chapter 1, John McCarthy
has pointed to the work of Nicolas Rashevsky (1936, 1938) as the earliest mathematical model
of neural learning.) Norbert Wiener, a pioneer of cybernetics and control theory (Wiener,
1948), worked with McCulloch and Pitts and inﬂuenced a number of young researchers in-
cluding Marvin Minsky, who may have been the ﬁrst to develop a working neural network in
hardware in 1951 (see Minsky and Papert, 1988, pp. ix–x). Turing (1948) wrote a research
report titled Intelligent Machinery that begins with the sentence “I propose to investigate the
question as to whether it is possible for machinery to show intelligent behaviour” and goes on
to describe a recurrent neural network architecture he called “B-type unorganized machines”
and an approach to training them. Unfortunately, the report went unpublished until 1969, and
was all but ignored until recently.
Frank Rosenblatt (1957) invented the modern “perceptron” and proved the percep-
tron convergence theorem (1960), although it had been foreshadowed by purely mathemat-
ical work outside the context of neural networks (Agmon, 1954; Motzkin and Schoenberg,
1954). Some early work was also done on multilayer networks, including Gamba percep-
trons (Gamba et al. , 1961) and madalines (Widrow, 1962). Learning Machines (Nilsson,
1965) covers much of this early work and more. The subsequent demise of early perceptron
research efforts was hastened (or, the authors later claimed, merely explained) by the book
Perceptrons (Minsky and Papert, 1969), which lamented the ﬁeld’s lack of mathematical
rigor. The book pointed out that single-layer perceptrons could represent only linearly sepa-
rable concepts and noted the lack of effective learning algorithms for multilayer networks.
The papers in (Hinton and Anderson, 1981), based on a conference in San Diego in
1979, can be regarded as marking a renaissance of connectionism. The two-volume “PDP”
(Parallel Distributed Processing) anthology (Rumelhart et al. , 1986a) and a short article in
Nature (Rumelhart et al., 1986b) attracted a great deal of attention—indeed, the number of
papers on “neural networks” multiplied by a factor of 200 between 1980–84 and 1990–94.
The analysis of neural networks using the physical theory of magnetic spin glasses (Amit
et al., 1985) tightened the links between statistical mechanics and neural network theory—
providing not only useful mathematical insights but alsorespectability. The back-propagation
technique had been invented quite early (Bryson and Ho, 1969) but it was rediscovered several
times (Werbos, 1974; Parker, 1985).
The probabilistic interpretation of neural networks has several sources, including Baum
and Wilczek (1988) and Bridle (1990). The role of the sigmoid function is discussed by
Jordan (1995). Bayesian parameter learning for neural networks was proposed by MacKay
762 Chapter 18. Learning from Examples
(1992) and is explored further by Neal (1996). The capacity of neural networks to represent
functions was investigated by Cybenko (1988, 1989), who showed that two hidden layers are
enough to represent any function and a single layer is enough to represent any continuous
function. The “optimal brain damage” method for removing useless connections is by LeCun
et al. (1989), and Sietsma and Dow (1988) show how to remove useless units. The tiling
algorithm for growing larger structures is due to M´ ezard and Nadal (1989). LeCun et al.
(1995) survey a number of algorithms for handwritten digit recognition. Improved error rates
since then were reported by Belongie et al. (2002) for shape matching and DeCoste and
Sch¨olkopf (2002) for virtual support vectors. At the time of writing, the best test error rate
reported is 0.39% by Ranzato et al. (2007) using a convolutional neural network.
The complexity of neural network learning has been investigated by researchers in com-
putational learning theory. Early computational results were obtained by Judd (1990), who
showed that the general problem of ﬁnding a set of weights consistent with a set of examples
is NP-complete, even under very restrictive assumptions. Some of the ﬁrst sample complexity
results were obtained by Baum and Haussler (1989), who showed that the number of exam-
ples required for effective learning grows as roughly W log W ,w h e r eW is the number of
weights.
17 Since then, a much more sophisticated theory has been developed (Anthony and
Bartlett, 1999), including the important result that the representational capacity of a network
depends on the size of the weights as well as on their number, a result that should not be
surprising in the light of our discussion of regularization.
The most popular kind of neural network that we did not cover is the radial basis
function, or RBF, network. A radial basis function combines a weighted collection of kernelsRADIAL BASIS
FUNCTION
(usually Gaussians, of course) to do function approximation. RBF networks can be trained in
two phases: ﬁrst, an unsupervised clustering approach is used to train the parameters of the
Gaussians—the means and variances—are trained, as in Section 20.3.1. In the second phase,
the relative weights of the Gaussians are determined. This is a system of linear equations,
which we know how to solve directly. Thus, both phases of RBF training have a nice beneﬁt:
the ﬁrst phase is unsupervised, and thus does not require labeled training data, and the second
phase, although supervised, is efﬁcient. See Bishop (1995) for more details.
Recurrent networks, in which units are linked in cycles, were mentioned in the chap-
ter but not explored in depth. Hopﬁeld networks (Hopﬁeld, 1982) are probably the best-
HOPFIELD NETWORK
understood class of recurrent networks. They use bidirectional connections with symmetric
weights (i.e., wi,j = wj,i), all of the units are both input and output units, the activation
function g is the sign function, and the activation levels can only be±1. A Hopﬁeld network
functions as an associative memory: after the network trains on a set of examples, a newASSOCIA TIVE
MEMORY
stimulus will cause it to settle into an activation pattern corresponding to the example in the
training set that most closely resembles the new stimulus. For example, if the training set con-
sists of a set of photographs, and the new stimulus is a small piece of one of the photographs,
then the network activation levels will reproduce the photograph from which the piece was
taken. Notice that the original photographs are not stored separately in the network; each
17 This approximately conﬁrmed “Uncle Bernie’s rule.” The rule was named after Bernie Widrow, who recom-
mended using roughly ten times as many examples as weights.
Exercises 763
weight is a partial encoding of all the photographs. One of the most interesting theoretical
results is that Hopﬁeld networks can reliably store up to0.138N training examples, where N
is the number of units in the network.
Boltzmann machines (Hinton and Sejnowski, 1983, 1986) also use symmetric weights,BOLTZMANN
MACHINE
but include hidden units. In addition, they use a stochastic activation function, such that
the probability of the output being 1 is some function of the total weighted input. Boltz-
mann machines therefore undergo state transitions that resemble a simulated annealing search
(see Chapter 4) for the conﬁguration that best approximates the training set. It turns out that
Boltzmann machines are very closely related to a special case of Bayesian networks evaluated
with a stochastic simulation algorithm. (See Section 14.5.)
For neural nets, Bishop (1995), Ripley (1996), and Haykin (2008) are the leading texts.
The ﬁeld of computational neuroscience is covered by Dayan and Abbott (2001).
The approach taken in this chapter was inﬂuenced by the excellent course notes of David
Cohn, Tom Mitchell, Andrew Moore, and Andrew Ng. There are several top-notch textbooks
in Machine Learning (Mitchell, 1997; Bishop, 2007) and in the closely allied and overlapping
ﬁelds of pattern recognition (Ripley, 1996; Duda et al., 2001), statistics (Wasserman, 2004;
Hastie et al., 2001), data mining (Hand et al., 2001; Witten and Frank, 2005), computational
learning theory (Kearns and Vazirani, 1994; Vapnik, 1998) and information theory (Shannon
and Weaver, 1949; MacKay, 2002; Cover and Thomas, 2006). Other books concentrate on
implementations (Segaran, 2007; Marsland, 2009) and comparisons of algorithms (Michie
et al. , 1994). Current research in machine learning is published in the annual proceedings
of the International Conference on Machine Learning (ICML) and the conference on Neural
Information Processing Systems (NIPS), in Machine Learning and the Journal of Machine
Learning Research, and in mainstream AI journals.
EXERCISES
18.1 Consider the problem faced by an infant learning to speak and understand a language.
Explain how this process ﬁts into the general learning model. Describe the percepts and
actions of the infant, and the types of learning the infant must do. Describe the subfunctions
the infant is trying to learn in terms of inputs and outputs, and available example data.
18.2 Repeat Exercise 18.1 for the case of learning to play tennis (or some other sport with
which you are familiar). Is this supervised learning or reinforcement learning?
18.3 Suppose we generate a training set from a decision tree and then apply decision-tree
learning to that training set. Is it the case that the learning algorithm will eventually return
the correct tree as the training-set size goes to inﬁnity? Why or why not?
18.4 In the recursive construction of decision trees, it sometimes happens that a mixed set
of positive and negative examples remains at a leaf node, even after all the attributes have
been used. Suppose that we have p positive examples and n negative examples.
764 Chapter 18. Learning from Examples
a. Show that the solution used by D ECISION -TREE -LEARNING , which picks the majority
classiﬁcation, minimizes the absolute error over the set of examples at the leaf.
b. Show that the class probability p/(p + n) minimizes the sum of squared errors.CLASS PROBABILITY
18.5 Suppose that an attribute splits the set of examples E into subsets Ek and that each
subset has pk positive examples andnk negative examples. Show that the attribute has strictly
positive information gain unless the ratio pk/(pk + nk) is the same for all k.
18.6 Consider the following data set comprised of three binary input attributes (A1,A2,a n d
A3) and one binary output:
Example
 A1
 A2
 A3
 Output y
x1
 1
 0
 0
 0
x2
 1
 0
 1
 0
x3
 0
 1
 0
 0
x4
 1
 1
 1
 1
x5
 1
 1
 0
 1
Use the algorithm in Figure 18.5 (page 702) to learn a decision tree for these data. Show the
computations made to determine the attribute to split at each node.
18.7 A decision graph is a generalization of a decision tree that allows nodes (i.e., attributes
used for splits) to have multiple parents, rather than just a single parent. The resulting graph
must still be acyclic. Now, consider the XOR function of three binary input attributes, which
produces the value 1 if and only if an odd number of the three input attributes has value 1.
a. Draw a minimal-sized decision tree for the three-input XOR function.
b. Draw a minimal-sized decision graph for the three-input XOR function.
18.8 This exercise considers χ
2 pruning of decision trees (Section 18.3.5).
a. Create a data set with two input attributes, such that the information gain at the root of
the tree for both attributes is zero, but there is a decision tree of depth 2 that is consistent
with all the data. What would χ2 pruning do on this data set if applied bottom up? If
applied top down?
b. Modify D ECISION -TREE -LEARNING to include χ2-pruning. You might wish to con-
sult Quinlan (1986) or Kearns and Mansour (1998) for details.
18.9 The standard D ECISION -TREE -LEARNING algorithm described in the chapter does
not handle cases in which some examples have missing attribute values.
a. First, we need to ﬁnd a way to classify such examples, given a decision tree that includes
tests on the attributes for which values can be missing. Suppose that an example x has
a missing value for attribute A and that the decision tree tests for A at a node that x
reaches. One way to handle this case is to pretend that the example has all possible
values for the attribute, but to weight each value according to its frequency among all
of the examples that reach that node in the decision tree. The classiﬁcation algorithm
should follow all branches at any node for which a value is missing and should multiply
Exercises 765
the weights along each path. Write a modiﬁed classiﬁcation algorithm for decision trees
that has this behavior.
b. Now modify the information-gain calculation so that in any given collection of exam-
ples C at a given node in the tree during the construction process, the examples with
missing values for any of the remaining attributes are given “as-if” values according to
the frequencies of those values in the set C.
18.10 In Section 18.3.6, we noted that attributes with many different possible values can
cause problems with the gain measure. Such attributes tend to split the examples into numer-
ous small classes or even singleton classes, thereby appearing to be highly relevant according
to the gain measure. The gain-ratio criterion selects attributes according to the ratio between
their gain and their intrinsic information content—that is, the amount of information con-
tained in the answer to the question, “What is the value of this attribute?” The gain-ratio crite-
rion therefore tries to measure how efﬁciently an attribute provides information on the correct
classiﬁcation of an example. Write a mathematical expression for the information content of
an attribute, and implement the gain ratio criterion in D
ECISION -TREE -LEARNING .
18.11 Suppose you are running a learning experiment on a new algorithm for Boolean clas-
siﬁcation. You have a data set consisting of 100 positive and 100 negative examples. You
plan to use leave-one-out cross-validation and compare your algorithm to a baseline function,
a simple majority classiﬁer. (A majority classiﬁer is given a set of training data and then
always outputs the class that is in the majority in the training set, regardless of the input.)
You expect the majority classiﬁer to score about 50% on leave-one-out cross-validation, but
to your surprise, it scores zero every time. Can you explain why?
18.12 Construct a decision list to classify the data below. Select tests to be as small as
possible (in terms of attributes), breaking ties among tests with the same number of attributes
by selecting the one that classiﬁes the greatest number of examples correctly. If multiple tests
have the same number of attributes and classify the same number of examples, then break the
tie using attributes with lower index numbers (e.g., select A
1 over A2).
Example
 A1
 A2
 A3
 A4
 y
x1
 1
 0
 0
 0
 1
x2
 1
 0
 1
 1
 1
x3
 0
 1
 0
 0
 1
x4
 0
 1
 1
 0
 0
x5
 1
 1
 0
 1
 1
x6
 0
 1
 0
 1
 0
x7
 0
 0
 1
 1
 1
x8
 0
 0
 1
 0
 0
18.13 Prove that a decision list can represent the same function as a decision tree while
using at most as many rules as there are leaves in the decision tree for that function. Give an
example of a function represented by a decision list using strictly fewer rules than the number
of leaves in a minimal-sized decision tree for that same function.
766 Chapter 18. Learning from Examples
18.14 This exercise concerns the expressiveness of decision lists (Section 18.5).
a. Show that decision lists can represent any Boolean function, if the size of the tests is
not limited.
b. Show that if the tests can contain at mostk literals each, then decision lists can represent
any function that can be represented by a decision tree of depth k.
18.15 Suppose a 7-nearest-neighbors regression search returns{7,6,8,4,7,11,100} as the
7 nearest y values for a given x value. What is the value of ˆy that minimizes the L1 loss
function on this data? There is a common name in statistics for this value as a function of the
y values; what is it? Answer the same two questions for the L
2 loss function.
18.16 Figure 18.31 showed how a circle at the origin can be linearly separated by mapping
from the features (x1,x2) to the two dimensions(x2
1,x2
2
). But what if the circle is not located
at the origin? What if it is an ellipse, not a circle? The general equation for a circle (and
hence the decision boundary) is (x
1−a)2 +( x2−b)2−r2 =0 , and the general equation for
an ellipse is c(x1−a)2 + d(x2−b)2−1=0 .
a. Expand out the equation for the circle and show what the weights wi would be for the
decision boundary in the four-dimensional feature space (x1,x2,x2
1,x2
2
). Explain why
this means that any circle is linearly separable in this space.
b. Do the same for ellipses in the ﬁve-dimensional feature space (x1,x2,x2
1,x2
2
,x1x2).
18.17 Construct a support vector machine that computes the XOR function. Use values of
+1 and –1 (instead of 1 and 0) for both inputs and outputs, so that an example looks like
([−1,1],1) or ([−1,−1],−1). Map the input [x1,x2] into a space consisting ofx1 and x1 x2.
Draw the four input points in this space, and the maximal margin separator. What is the
margin? Now draw the separating line back in the original Euclidean input space.
18.18 Consider an ensemble learning algorithm that uses simple majority voting among
K learned hypotheses. Suppose that each hypothesis has error ϵ and that the errors made
by each hypothesis are independent of the others’. Calculate a formula for the error of the
ensemble algorithm in terms of K and ϵ, and evaluate it for the cases where K =5 , 10, and
20 and ϵ=0.1, 0.2, and 0.4. If the independence assumption is removed, is it possible for the
ensemble error to be worse than ϵ?
18.19 Construct by hand a neural network that computes the
XOR function of two inputs.
Make sure to specify what sort of units you are using.
18.20 Recall from Chapter 18 that there are22n
distinct Boolean functions ofn inputs. How
many of these are representable by a threshold perceptron?
18.21 Section 18.6.4 (page 725) noted that the output of the logistic function could be in-
terpreted as a probability p assigned by the model to the proposition that f(x)=1 ; the prob-
ability that f(x)=0 is therefore 1−p. Write down the probability p as a function of x
and calculate the derivative of log p with respect to each weight wi. Repeat the process for
log(1−p). These calculations give a learning rule for minimizing the negative-log-likelihood
Exercises 767
loss function for a probabilistic hypothesis. Comment on any resemblance to other learning
rules in the chapter.
18.22 Suppose you had a neural network with linear activation functions. That is, for each
unit the output is some constant c times the weighted sum of the inputs.
a. Assume that the network has one hidden layer. For a given assignment to the weights
w, write down equations for the value of the units in the output layer as a function of
w and the input layer x, without any explicit mention of the output of the hidden layer.
Show that there is a network with no hidden units that computes the same function.
b. Repeat the calculation in part (a), but this time do it for a network with any number of
hidden layers.
c. Suppose a network with one hidden layer and linear activation functions has n input
and output nodes and h hidden nodes. What effect does the transformation in part (a)
to a network with no hidden layers have on the total number of weights? Discuss in
particular the case h≪ n.
18.23 Suppose that a training set contains only a single example, repeated 100 times. In
80 of the 100 cases, the single output value is 1; in the other 20, it is 0. What will a back-
propagation network predict for this example, assuming that it has been trained and reaches
a global optimum? (Hint: to ﬁnd the global optimum, differentiate the error function and set
it to zero.)
18.24 The neural network whose learning performance is measured in Figure 18.25 has four
hidden nodes. This number was chosen somewhat arbitrarily. Use a cross-validation method
to ﬁnd the best number of hidden nodes.
18.25 Consider the problem of separatingN data points into positive and negative examples
using a linear separator. Clearly, this can always be done for N =2 points on a line of
dimension d=1 , regardless of how the points are labeled or where they are located (unless
the points are in the same place).
a. Show that it can always be done for N =3 points on a plane of dimensiond=2 , unless
they are collinear.
b. Show that it cannot always be done for N =4 points on a plane of dimension d=2 .
c. Show that it can always be done for N =4 points in a space of dimension d=3 , unless
they are coplanar.
d. Show that it cannot always be done for N =5 points in a space of dimension d=3 .
e. The ambitious student may wish to prove that N points in general position (but not
N +1 ) are linearly separable in a space of dimension N−1.


END_INSTRUCTION
