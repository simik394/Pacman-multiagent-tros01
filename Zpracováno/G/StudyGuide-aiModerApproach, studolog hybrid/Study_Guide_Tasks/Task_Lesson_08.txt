
START_INSTRUCTION
You are an expert AI tutor creating a comprehensive study guide.
Your task is to rewrite and expand the "Lesson Notes" provided below into a cohesive, readable study text.

1. **Structure:** Follow the exact structure and order of the topics in the "Lesson Notes". Do not reorder them.
2. **Enrichment:** Use the "Textbook Context" provided to expand on every bullet point in the notes.
    - If the notes mention a concept (e.g., "Rationality"), define it precisely using the textbook.
    - If the notes list algorithms (e.g., "A*", "Simulated Annealing"), explain how they work, their complexity, and pros/cons using the textbook details.
    - Add examples from the textbook where appropriate.
3. **Tone:** Academic but accessible study material. Not just bullet points—write paragraphs where necessary to explain complex ideas.
4. **Language:** The output must be in **Czech** (since the lesson notes are in Czech), but you can keep standard English AI terminology (like "Overfitting") in parentheses.

--- LESSON NOTES (Skeleton) ---
Použití znalostí v učení

Klasifikace znalosti
Podle formalizovatelnosti
○ explicitní - formalizované, artikulované
○ implicitní - primárně skryté (v datech)
○ tacitní - nevědomé a nesdělitelné znalosti skryté v myslích expertů
•
podle obsahu
○ deklarativní - zachycují co platí
○ procedurální - zachycují jak postupovat
•
Požadavky na znalosti
• transparentnost
• modulárnost
• modifikovatelnost
• užitečnost
Reprezentace znalosti v AI
Predikátová logika
○ rozšíření výrokové logiky
○ predikáty, funkce, logické spojky, kvantifikátory
•
Sémantické sítě
○ popisuje realitu jako objekty (uzly), které jsou v nějaké relaci (hrany)
•
rámce
○ datová struktura
obsahuje:
▪ data
▪ meta-data
▪ procedury
○
○ staly se inspirací pro OOP
•
pravidla
○ IF-THEN
○ použito v expertních systémech (rule-based)
•
Případy
○ mají podobu vyřešených problémů z dané aplikační oblasti
○ použití v systémech případového usuzování

--- TEXTBOOK CONTEXT (Source Material) ---


--- BOOK CHAPTER: 19_Knowledge_in_Learning ---

19
KNOWLEDGE IN
LEARNING
In which we examine the problem of learning when you know something already.
In all of the approaches to learning described in the previous chapter, the idea is to construct
a function that has the input–output behavior observed in the data. In each case, the learning
methods can be understood as searching a hypothesis space to ﬁnd a suitable function, starting
from only a very basic assumption about the form of the function, such as “second-degree
polynomial” or “decision tree” and perhaps a preference for simpler hypotheses. Doing this
amounts to saying that before you can learn something new, you must ﬁrst forget (almost)
everything you know. In this chapter, we study learning methods that can take advantage
of prior knowledge about the world. In most cases, the prior knowledge is represented
PRIOR KNOWLEDGE
as general ﬁrst-order logical theories; thus for the ﬁrst time we bring together the work on
knowledge representation and learning.
19.1 A L OGICAL FORMULATION OF LEARNING
Chapter 18 deﬁned pure inductive learning as a process of ﬁnding a hypothesis that agrees
with the observed examples. Here, we specialize this deﬁnition to the case where the hypoth-
esis is represented by a set of logical sentences. Example descriptions and classiﬁcations will
also be logical sentences, and a new example can be classiﬁed by inferring a classiﬁcation
sentence from the hypothesis and the example description. This approach allows for incre-
mental construction of hypotheses, one sentence at a time. It also allows for prior knowledge,
because sentences that are already known can assist in the classiﬁcation of new examples.
The logical formulation of learning may seem like a lot of extra work at ﬁrst, but it turns out
to clarify many of the issues in learning. It enables us to go well beyond the simple learning
methods of Chapter 18 by using the full power of logical inference in the service of learning.
19.1.1 Examples and hypotheses
Recall from Chapter 18 the restaurant learning problem: learning a rule for deciding whether
to wait for a table. Examples were described byattributes such as Alternate, Bar, Fri/Sat,
768
Section 19.1. A Logical Formulation of Learning 769
and so on. In a logical setting, an example is described by a logical sentence; the attributes
become unary predicates. Let us generically call the ith example Xi. For instance, the ﬁrst
example from Figure 18.3 (page 700) is described by the sentences
Alternate(X1)∧¬Bar(X1)∧¬Fri/Sat(X1)∧Hungry(X1)∧...
We will use the notationDi(Xi) to refer to the description ofXi,w h e r eDi can be any logical
expression taking a single argument. The classiﬁcation of the example is given by a literal
using the goal predicate, in this case
WillWait(X1) or ¬WillWait(X1) .
The complete training set can thus be expressed as the conjunction of all the example descrip-
tions and goal literals.
The aim of inductive learning in general is to ﬁnd a hypothesis that classiﬁes the ex-
amples well and generalizes well to new examples. Here we are concerned with hypotheses
expressed in logic; each hypothesis h
j will have the form
∀x Goal(x) ⇔Cj(x) ,
where Cj(x) is a candidate deﬁnition—some expression involving the attribute predicates.
For example, a decision tree can be interpreted as a logical expression of this form. Thus, the
tree in Figure 18.6 (page 702) expresses the following logical deﬁnition (which we will call
h
r for future reference):
∀r WillWait(r) ⇔Patrons(r,Some)
∨Patrons(r,Full)∧Hungry(r)∧Type(r,French)
∨Patrons(r,Full)∧Hungry(r)∧Type(r,Thai)
∧Fri/Sat(r)
∨Patrons(r,Full)∧Hungry(r)∧Type(r,Burger) .
(19.1)
Each hypothesis predicts that a certain set of examples—namely, those that satisfy its candi-
date deﬁnition—will be examples of the goal predicate. This set is called the extension ofEXTENSION
the predicate. Two hypotheses with different extensions are therefore logically inconsistent
with each other, because they disagree on their predictions for at least one example. If they
have the same extension, they are logically equivalent.
The hypothesis spaceH is the set of all hypotheses{h
1,...,h n} that the learning algo-
rithm is designed to entertain. For example, the D ECISION -TREE -LEARNING algorithm can
entertain any decision tree hypothesis deﬁned in terms of the attributes provided; its hypoth-
esis space therefore consists of all these decision trees. Presumably, the learning algorithm
believes that one of the hypotheses is correct; that is, it believes the sentence
h1∨h2∨h3∨... ∨hn . (19.2)
As the examples arrive, hypotheses that are not consistent with the examples can be ruled
out. Let us examine this notion of consistency more carefully. Obviously, if hypothesis hj is
consistent with the entire training set, it has to be consistent with each example in the training
set. What would it mean for it to be inconsistent with an example? There are two possible
ways that this can happen:
770 Chapter 19. Knowledge in Learning
•An example can be a false negative for the hypothesis, if the hypothesis says it shouldFALSE NEGATIVE
be negative but in fact it is positive. For instance, the new exampleX13 described by
Patrons(X13,Full)∧¬Hungry(X13)∧... ∧WillWait(X13)
would be a false negative for the hypothesis hr given earlier. From hr and the example
description, we can deduce both WillWait(X13), which is what the example says,
and¬WillWait(X13), which is what the hypothesis predicts. The hypothesis and the
example are therefore logically inconsistent.
•An example can be a false positive for the hypothesis, if the hypothesis says it shouldFALSE POSITIVE
be positive but in fact it is negative.1
If an example is a false positive or false negative for a hypothesis, then the example and the
hypothesis are logically inconsistent with each other. Assuming that the example is a correct
observation of fact, then the hypothesis can be ruled out. Logically, this is exactly analogous
to the resolution rule of inference (see Chapter 9), where the disjunction of hypotheses cor-
responds to a clause and the example corresponds to a literal that resolves against one of the
literals in the clause. An ordinary logical inference system therefore could, in principle, learn
from the example by eliminating one or more hypotheses. Suppose, for example, that the
example is denoted by the sentenceI
1, and the hypothesis space ish1∨h2∨h3∨h4.T h e ni f
I1 is inconsistent with h2 and h3, the logical inference system can deduce the new hypothesis
space h1∨h4.
We therefore can characterize inductive learning in a logical setting as a process of
gradually eliminating hypotheses that are inconsistent with the examples, narrowing down
the possibilities. Because the hypothesis space is usually vast (or even inﬁnite in the case of
ﬁrst-order logic), we do not recommend trying to build a learning system using resolution-
based theorem proving and a complete enumeration of the hypothesis space. Instead, we will
describe two approaches that ﬁnd logically consistent hypotheses with much less effort.
19.1.2 Current-best-hypothesis search
The idea behind current-best-hypothesis search is to maintain a single hypothesis, and toCURRENT -BEST -
HYPOTHESIS
adjust it as new examples arrive in order to maintain consistency. The basic algorithm was
described by John Stuart Mill (1843), and may well have appeared even earlier.
Suppose we have some hypothesis such as hr, of which we have grown quite fond.
As long as each new example is consistent, we need do nothing. Then along comes a false
negative example, X13. What do we do? Figure 19.1(a) shows hr schematically as a region:
everything inside the rectangle is part of the extension ofhr. The examples that have actually
been seen so far are shown as “+” or “–”, and we see that hr correctly categorizes all the
examples as positive or negative examples of WillWait . In Figure 19.1(b), a new example
(circled) is a false negative: the hypothesis says it should be negative but it is actually positive.
The extension of the hypothesis must be increased to include it. This is calledgeneralization;
GENERALIZA TION
one possible generalization is shown in Figure 19.1(c). Then in Figure 19.1(d), we see a false
positive: the hypothesis says the new example (circled) should be positive, but it actually is
1 The terms “false positive” and “false negative” are used in medicine to describe erroneous results from lab
tests. A result is a false positive if it indicates that the patient has the disease when in fact no disease is present.
Section 19.1. A Logical Formulation of Learning 771
(a) (b) (c) (d) (e)
+ ++
++
++
– –
–
–
––
–
– –
–
+ ++
++
++
– –
–
–
––
–
– –
–
+
+ ++
++
++
– –
–
–
––
–
– –
–
+
+ ++
++
++
– –
–
–
––
–
–
–
+–
+ ++
++
++
– –
–
–
––
–
–
–
+
–
–
Figure 19.1 (a) A consistent hypothesis. (b) A false negative. (c) The hypothesis is gen-
eralized. (d) A false positive. (e) The hypothesis is specialized.
function CURRENT -BEST-LEARNING (examples,h) returns a hypothesis or fail
if examples is empty then
return h
e←FIRST (examples)
if e is consistent with h then
return CURRENT -BEST-LEARNING (REST (examples), h)
else if e is a false positive for h then
for each h′ in specializations of h consistent with examples seen so far do
h′′←CURRENT -BEST-LEARNING (REST (examples), h′)
if h′′ ̸= fail then return h′′
else if e is a false negative for h then
for each h′ in generalizations of h consistent with examples seen so far do
h′′←CURRENT -BEST-LEARNING (REST (examples), h′)
if h′′ ̸= fail then return h′′
return fail
Figure 19.2 The current-best-hypothesis learning algorithm. It searches for a consis-
tent hypothesis that ﬁts all the examples and backtracks when no consistent specializa-
tion/generalization can be found. To start the algorithm, any hypothesis can be passed in;
it will be specialized or gneralized as needed.
negative. The extension of the hypothesis must be decreased to exclude the example. This is
called specialization; in Figure 19.1(e) we see one possible specialization of the hypothesis.SPECIALIZA TION
The “more general than” and “more speciﬁc than” relations between hypotheses provide the
logical structure on the hypothesis space that makes efﬁcient search possible.
We can now specify the CURRENT -BEST-LEARNING algorithm, shown in Figure 19.2.
Notice that each time we consider generalizing or specializing the hypothesis, we must check
for consistency with the other examples, because an arbitrary increase/decrease in the exten-
sion might include/exclude previously seen negative/positive examples.
772 Chapter 19. Knowledge in Learning
We have deﬁned generalization and specialization as operations that change the exten-
sion of a hypothesis. Now we need to determine exactly how they can be implemented as
syntactic operations that change the candidate deﬁnition associated with the hypothesis, so
that a program can carry them out. This is done by ﬁrst noting that generalization and special-
ization are also logical relationships between hypotheses. If hypothesis h
1, with deﬁnition
C1, is a generalization of hypothesis h2 with deﬁnition C2, then we must have
∀xC 2(x) ⇒C1(x) .
Therefore in order to construct a generalization of h2, we simply need to ﬁnd a deﬁni-
tion C1 that is logically implied by C2. This is easily done. For example, if C2(x) is
Alternate(x)∧Patrons(x,Some), then one possible generalization is given by C1(x) ≡
Patrons(x,Some). This is called dropping conditions. Intuitively, it generates a weakerDROPPING
CONDITIONS
deﬁnition and therefore allows a larger set of positive examples. There are a number of other
generalization operations, depending on the language being operated on. Similarly, we can
specialize a hypothesis by adding extra conditions to its candidate deﬁnition or by removing
disjuncts from a disjunctive deﬁnition. Let us see how this works on the restaurant example,
using the data in Figure 18.3.
•The ﬁrst example, X
1, is positive. The attribute Alternate(X1) is true, so let the initial
hypothesis be
h1 : ∀x WillWait(x) ⇔Alternate(x) .
•The second example,X2, is negative.h1 predicts it to be positive, so it is a false positive.
Therefore, we need to specializeh1. This can be done by adding an extra condition that
will rule out X2, while continuing to classify X1 as positive. One possibility is
h2 : ∀x WillWait(x) ⇔Alternate(x)∧Patrons(x,Some) .
•The third example, X3, is positive. h2 predicts it to be negative, so it is a false negative.
Therefore, we need to generalize h2.W ed r o pt h eAlternate condition, yielding
h3 : ∀x WillWait(x) ⇔Patrons(x,Some) .
•The fourth example,X4, is positive.h3 predicts it to be negative, so it is a false negative.
We therefore need to generalize h3. We cannot drop the Patrons condition, because
that would yield an all-inclusive hypothesis that would be inconsistent with X2.O n e
possibility is to add a disjunct:
h4 : ∀x WillWait(x) ⇔Patrons(x,Some)
∨(Patrons(x,Full)∧Fri/Sat(x)) .
Already, the hypothesis is starting to look reasonable. Obviously, there are other possibilities
consistent with the ﬁrst four examples; here are two of them:
h′
4 : ∀x WillWait(x) ⇔¬WaitEstimate(x, 30-60) .
h′′
4 : ∀x WillWait(x) ⇔Patrons(x,Some)
∨(Patrons(x,Full)∧WaitEstimate(x, 10-30)) .
The CURRENT -BEST-LEARNING algorithm is described nondeterministically, because at any
point, there may be several possible specializations or generalizations that can be applied. The
Section 19.1. A Logical Formulation of Learning 773
function VERSION -SPACE-LEARNING (examples) returns a version space
local variables: V , the version space: the set of all hypotheses
V←the set of all hypotheses
for each example e in examples do
if V is not empty then V←VERSION -SPACE-UPDATE (V ,e)
return V
function VERSION -SPACE-UPDATE (V ,e) returns an updated version space
V←{h∈V : h is consistent with e}
Figure 19.3 The version space learning algorithm. It ﬁnds a subset ofV that is consistent
with all the examples.
choices that are made will not necessarily lead to the simplest hypothesis, and may lead to an
unrecoverable situation where no simple modiﬁcation of the hypothesis is consistent with all
of the data. In such cases, the program must backtrack to a previous choice point.
The C
URRENT -BEST-LEARNING algorithm and its variants have been used in many
machine learning systems, starting with Patrick Winston’s (1970) “arch-learning” program.
With a large number of examples and a large space, however, some difﬁculties arise:
1. Checking all the previous examples over again for each modiﬁcation is very expensive.
2. The search process may involve a great deal of backtracking. As we saw in Chapter 18,
hypothesis space can be a doubly exponentially large place.
19.1.3 Least-commitment search
Backtracking arises because the current-best-hypothesis approach has to choose a particular
hypothesis as its best guess even though it does not have enough data yet to be sure of the
choice. What we can do instead is to keep around all and only those hypotheses that are
consistent with all the data so far. Each new example will either have no effect or will get
rid of some of the hypotheses. Recall that the original hypothesis space can be viewed as a
disjunctive sentence
h
1∨h2∨h3 ... ∨hn .
As various hypotheses are found to be inconsistent with the examples, this disjunction shrinks,
retaining only those hypotheses not ruled out. Assuming that the original hypothesis space
does in fact contain the right answer, the reduced disjunction must still contain the right an-
swer because only incorrect hypotheses have been removed. The set of hypotheses remaining
is called the version space, and the learning algorithm (sketched in Figure 19.3) is called the
VERSION SP ACE
version space learning algorithm (also the candidate elimination algorithm).CANDIDA TE
ELIMINA TION
One important property of this approach is that it is incremental: one never has to
go back and reexamine the old examples. All remaining hypotheses are guaranteed to be
consistent with them already. But there is an obvious problem. We already said that the
774 Chapter 19. Knowledge in Learning
This region all inconsistent
This region all inconsistent
More general
More specific
S1
G1
S2
G2 G3  . . . Gm
 . . . Sn
Figure 19.4 The version space contains all hypotheses consistent with the examples.
hypothesis space is enormous, so how can we possibly write down this enormous disjunction?
The following simple analogy is very helpful. How do you represent all the real num-
bers between 1 and 2? After all, there are an inﬁnite number of them! The answer is to use
an interval representation that just speciﬁes the boundaries of the set: [1,2]. It works because
we have an ordering on the real numbers.
We also have an ordering on the hypothesis space, namely, generalization/specialization.
This is a partial ordering, which means that each boundary will not be a point but rather a
set of hypotheses called a boundary set. The great thing is that we can represent the entire
BOUNDARY SET
version space using just two boundary sets: a most general boundary (the G-set) and a mostG-SET
speciﬁc boundary (the S-set). Everything in between is guaranteed to be consistent with theS-SET
examples. Before we prove this, let us recap:
•The current version space is the set of hypotheses consistent with all the examples so
far. It is represented by the S-set and G-set, each of which is a set of hypotheses.
•Every member of the S-set is consistent with all observations so far, and there are no
consistent hypotheses that are more speciﬁc.
•Every member of the G-set is consistent with all observations so far, and there are no
consistent hypotheses that are more general.
We want the initial version space (before any examples have been seen) to represent all possi-
ble hypotheses. We do this by setting the G-set to contain True (the hypothesis that contains
everything), and the S-set to contain False (the hypothesis whose extension is empty).
Figure 19.4 shows the general structure of the boundary-set representation of the version
space. To show that the representation is sufﬁcient, we need the following two properties:
Section 19.1. A Logical Formulation of Learning 775
1. Every consistent hypothesis (other than those in the boundary sets) is more speciﬁc than
some member of the G-set, and more general than some member of the S-set. (That is,
there are no “stragglers” left outside.) This follows directly from the deﬁnitions of S
and G. If there were a straggler h, then it would have to be no more speciﬁc than any
member of G, in which case it belongs in G; or no more general than any member of
S, in which case it belongs in S.
2. Every hypothesis more speciﬁc than some member of the G-set and more general than
some member of the S-set is a consistent hypothesis. (That is, there are no “holes” be-
tween the boundaries.) Any h between S and G must reject all the negative examples
rejected by each member ofG (because it is more speciﬁc), and must accept all the pos-
itive examples accepted by any member ofS (because it is more general). Thus, h must
agree with all the examples, and therefore cannot be inconsistent. Figure 19.5 shows
the situation: there are no known examples outside S but inside G, so any hypothesis
in the gap must be consistent.
We have therefore shown that if S and G are maintained according to their deﬁnitions, then
they provide a satisfactory representation of the version space. The only remaining problem
is how to update S and G for a new example (the job of the V
ERSION -SPACE-UPDATE
function). This may appear rather complicated at ﬁrst, but from the deﬁnitions and with the
help of Figure 19.4, it is not too hard to reconstruct the algorithm.
+ +
+ +
+
+
++
+ +
–
–
–
–
–
–
–
–
– – –
––
–
S1
G1
G2
Figure 19.5 The extensions of the members of G and S. No known examples lie in
between the two sets of boundaries.
We need to worry about the membersSi and Gi of the S- and G-sets. For each one, the
new example may be a false positive or a false negative.
1. False positive for Si: This means Si is too general, but there are no consistent special-
izations of Si (by deﬁnition), so we throw it out of the S-set.
2. False negative for Si: This means Si is too speciﬁc, so we replace it by all its immediate
generalizations, provided they are more speciﬁc than some member of G.
3. False positive for Gi: This means Gi is too general, so we replace it by all its immediate
specializations, provided they are more general than some member of S.
776 Chapter 19. Knowledge in Learning
4. False negative for Gi: This means Gi is too speciﬁc, but there are no consistent gener-
alizations of Gi (by deﬁnition) so we throw it out of the G-set.
We continue these operations for each new example until one of three things happens:
1. We have exactly one hypothesis left in the version space, in which case we return it as
the unique hypothesis.
2. The version space collapses—either S or G becomes empty, indicating that there are
no consistent hypotheses for the training set. This is the same case as the failure of the
simple version of the decision tree algorithm.
3. We run out of examples and have several hypotheses remaining in the version space.
This means the version space represents a disjunction of hypotheses. For any new
example, if all the disjuncts agree, then we can return their classiﬁcation of the example.
If they disagree, one possibility is to take the majority vote.
We leave as an exercise the application of the V
ERSION -SPACE-LEARNING algorithm to the
restaurant data.
There are two principal drawbacks to the version-space approach:
•If the domain contains noise or insufﬁcient attributes for exact classiﬁcation, the version
space will always collapse.
•If we allow unlimited disjunction in the hypothesis space, the S-set will always contain
a single most-speciﬁc hypothesis, namely, the disjunction of the descriptions of the
positive examples seen to date. Similarly, the G-set will contain just the negation of the
disjunction of the descriptions of the negative examples.
•For some hypothesis spaces, the number of elements in the S-set or G-set may grow
exponentially in the number of attributes, even though efﬁcient learning algorithms exist
for those hypothesis spaces.
To date, no completely successful solution has been found for the problem of noise. The
problem of disjunction can be addressed by allowing only limited forms of disjunction or by
including a generalization hierarchy of more general predicates. For example, instead ofGENERALIZA TION
HIERARCHY
using the disjunction WaitEstimate(x, 30-60)∨WaitEstimate(x, >60), we might use the
single literal LongWait(x). The set of generalization and specialization operations can be
easily extended to handle this.
The pure version space algorithm was ﬁrst applied in the Meta-D ENDRAL system,
which was designed to learn rules for predicting how molecules would break into pieces in
a mass spectrometer (Buchanan and Mitchell, 1978). Meta-D
ENDRAL was able to generate
rules that were sufﬁciently novel to warrant publication in a journal of analytical chemistry—
the ﬁrst real scientiﬁc knowledge generated by a computer program. It was also used in the
elegant L
EX system (Mitchell et al., 1983), which was able to learn to solve symbolic integra-
tion problems by studying its own successes and failures. Although version space methods
are probably not practical in most real-world learning problems, mainly because of noise,
they provide a good deal of insight into the logical structure of hypothesis space.
Section 19.2. Knowledge in Learning 777
Observations PredictionsHypotheses
Prior 
knowledge
Knowledge-based
inductive learning
Figure 19.6 A cumulative learning process uses, and adds to, its stock of background
knowledge over time.
19.2 K NOWLEDGE IN LEARNING
The preceding section described the simplest setting for inductive learning. To understand the
role of prior knowledge, we need to talk about the logical relationships among hypotheses,
example descriptions, and classiﬁcations. Let Descriptions denote the conjunction of all the
example descriptions in the training set, and let Classiﬁcationsdenote the conjunction of all
the example classiﬁcations. Then a Hypothesis that “explains the observations” must satisfy
the following property (recall that|= means “logically entails”):
Hypothesis∧Descriptions|= Classiﬁcations. (19.3)
We call this kind of relationship an entailment constraint,i nw h i c hHypothesis is the “un-ENTAILMENT
CONSTRAINT
known.” Pure inductive learning means solving this constraint, where Hypothesis is drawn
from some predeﬁned hypothesis space. For example, if we consider a decision tree as a
logical formula (see Equation (19.1) on page 769), then a decision tree that is consistent with
all the examples will satisfy Equation (19.3). If we place no restrictions on the logical form
of the hypothesis, of course, then Hypothesis = Classiﬁcations also satisﬁes the constraint.
Ockham’s razor tells us to prefer small, consistent hypotheses, so we try to do better than
simply memorizing the examples.
This simple knowledge-free picture of inductive learning persisted until the early 1980s.
The modern approach is to design agents that already know something and are trying to learn
some more. This may not sound like a terriﬁcally deep insight, but it makes quite a difference
to the way we design agents. It might also have some relevance to our theories about how
science itself works. The general idea is shown schematically in Figure 19.6.
An autonomous learning agent that uses background knowledge must somehow obtain
the background knowledge in the ﬁrst place, in order for it to be used in the new learning
episodes. This method must itself be a learning process. The agent’s life history will there-
fore be characterized by cumulative,o r incremental, development. Presumably, the agent
could start out with nothing, performing inductions in vacuo like a good little pure induc-
tion program. But once it has eaten from the Tree of Knowledge, it can no longer pursue
such naive speculations and should use its background knowledge to learn more and more
effectively. The question is then how to actually do this.
778 Chapter 19. Knowledge in Learning
19.2.1 Some simple examples
Let us consider some commonsense examples of learning with background knowledge. Many
apparently rational cases of inferential behavior in the face of observations clearly do not
follow the simple principles of pure induction.
•Sometimes one leaps to general conclusions after only one observation. Gary Larson
once drew a cartoon in which a bespectacled caveman, Zog, is roasting his lizard on
the end of a pointed stick. He is watched by an amazed crowd of his less intellectual
contemporaries, who have been using their bare hands to hold their victuals over the ﬁre.
This enlightening experience is enough to convince the watchers of a general principle
of painless cooking.
•Or consider the case of the traveler to Brazil meeting her ﬁrst Brazilian. On hearing him
speak Portuguese, she immediately concludes that Brazilians speak Portuguese, yet on
discovering that his name is Fernando, she does not conclude that all Brazilians are
called Fernando. Similar examples appear in science. For example, when a freshman
physics student measures the density and conductance of a sample of copper at a par-
ticular temperature, she is quite conﬁdent in generalizing those values to all pieces of
copper. Yet when she measures its mass, she does not even consider the hypothesis that
all pieces of copper have that mass. On the other hand, it would be quite reasonable to
make such a generalization over all pennies.
•Finally, consider the case of a pharmacologically ignorant but diagnostically sophisti-
cated medical student observing a consulting session between a patient and an expert
internist. After a series of questions and answers, the expert tells the patient to take a
course of a particular antibiotic. The medical student infers the general rule that that
particular antibiotic is effective for a particular type of infection.
These are all cases in which the use of background knowledge allows much faster learning
than one might expect from a pure induction program.
19.2.2 Some general schemes
In each of the preceding examples, one can appeal to prior knowledge to try to justify the
generalizations chosen. We will now look at what kinds of entailment constraints are operat-
ing in each case. The constraints will involve the Background knowledge, in addition to the
Hypothesis and the observed Descriptions and Classiﬁcations.
In the case of lizard toasting, the cavemen generalize by explaining the success of the
pointed stick: it supports the lizard while keeping the hand away from the ﬁre. From this
explanation, they can infer a general rule: that any long, rigid, sharp object can be used to toast
small, soft-bodied edibles. This kind of generalization process has been called explanation-
based learning,o r EBL. Notice that the general rule follows logically from the background
EXPLANA TION-
BASED
LEARNING
knowledge possessed by the cavemen. Hence, the entailment constraints satisﬁed by EBL are
the following:
Hypothesis∧Descriptions|= Classiﬁcations
Background|= Hypothesis .
Section 19.2. Knowledge in Learning 779
Because EBL uses Equation (19.3), it was initially thought to be a way to learn from ex-
amples. But because it requires that the background knowledge be sufﬁcient to explain the
Hypothesis, which in turn explains the observations, the agent does not actually learn any-
thing factually new from the example. The agent could have derived the example from what
it already knew, although that might have required an unreasonable amount of computation.
EBL is now viewed as a method for converting ﬁrst-principles theories into useful, special-
purpose knowledge. We describe algorithms for EBL in Section 19.3.
The situation of our traveler in Brazil is quite different, for she cannot necessarily ex-
plain why Fernando speaks the way he does, unless she knows her papal bulls. Moreover,
the same generalization would be forthcoming from a traveler entirely ignorant of colonial
history. The relevant prior knowledge in this case is that, within any given country, most
people tend to speak the same language; on the other hand, Fernando is not assumed to be
the name of all Brazilians because this kind of regularity does not hold for names. Similarly,
the freshman physics student also would be hard put to explain the particular values that she
discovers for the conductance and density of copper. She does know, however, that the mate-
rial of which an object is composed and its temperature together determine its conductance.
In each case, the prior knowledge Background concerns the relevance of a set of features to
RELEVANCE
the goal predicate. This knowledge, together with the observations, allows the agent to infer
a new, general rule that explains the observations:
Hypothesis∧Descriptions|= Classiﬁcations,
Background∧Descriptions∧Classiﬁcations|= Hypothesis . (19.4)
We call this kind of generalization relevance-based learning,o r RBL (although the name isRELEVANCE-BASED
LEARNING
not standard). Notice that whereas RBL does make use of the content of the observations, it
does not produce hypotheses that go beyond the logical content of the background knowledge
and the observations. It is a deductive form of learning and cannot by itself account for the
creation of new knowledge starting from scratch.
In the case of the medical student watching the expert, we assume that the student’s
prior knowledge is sufﬁcient to infer the patient’s disease D from the symptoms. This is
not, however, enough to explain the fact that the doctor prescribes a particular medicine M.
The student needs to propose another rule, namely, that M generally is effective against D.
Given this rule and the student’s prior knowledge, the student can now explain why the expert
prescribes M in this particular case. We can generalize this example to come up with the
entailment constraint
Background∧Hypothesis∧Descriptions|= Classiﬁcations. (19.5)
That is, the background knowledge and the new hypothesis combine to explain the examples.
As with pure inductive learning, the learning algorithm should propose hypotheses that are as
simple as possible, consistent with this constraint. Algorithms that satisfy constraint (19.5)
are called knowledge-based inductive learning,o r KBIL, algorithms.
KNOWLEDGE-BASED
INDUCTIVE
LEARNING
KBIL algorithms, which are described in detail in Section 19.5, have been studied
mainly in the ﬁeld of inductive logic programming ,o r ILP. In ILP systems, prior knowl-INDUCTIVE LOGIC
PROGRAMMING
edge plays two key roles in reducing the complexity of learning:
780 Chapter 19. Knowledge in Learning
1. Because any hypothesis generated must be consistent with the prior knowledge as well
as with the new observations, the effective hypothesis space size is reduced to include
only those theories that are consistent with what is already known.
2. For any given set of observations, the size of the hypothesis required to construct an
explanation for the observations can be much reduced, because the prior knowledge
will be available to help out the new rules in explaining the observations. The smaller
the hypothesis, the easier it is to ﬁnd.
In addition to allowing the use of prior knowledge in induction, ILP systems can formulate
hypotheses in general ﬁrst-order logic, rather than in the restricted attribute-based language
of Chapter 18. This means that they can learn in environments that cannot be understood by
simpler systems.
19.3 E XPLANATION -BASED LEARNING
Explanation-based learning is a method for extracting general rules from individual obser-
vations. As an example, consider the problem of differentiating and simplifying algebraic
expressions (Exercise 9.17). If we differentiate an expression such as X
2 with respect to
X, we obtain 2X. (We use a capital letter for the arithmetic unknown X, to distinguish it
from the logical variable x.) In a logical reasoning system, the goal might be expressed as
ASK(Derivative(X2,X )= d, KB), with solution d =2 X.
Anyone who knows differential calculus can see this solution “by inspection” as a result
of practice in solving such problems. A student encountering such problems for the ﬁrst time,
or a program with no experience, will have a much more difﬁcult job. Application of the
standard rules of differentiation eventually yields the expression 1× (2× (X
(2−1))),a n d
eventually this simpliﬁes to 2X. In the authors’ logic programming implementation, this
takes 136 proof steps, of which 99 are on dead-end branches in the proof. After such an
experience, we would like the program to solve the same problem much more quickly the
next time it arises.
The technique of memoization has long been used in computer science to speed up
MEMOIZA TION
programs by saving the results of computation. The basic idea of memo functions is to
accumulate a database of input–output pairs; when the function is called, it ﬁrst checks the
database to see whether it can avoid solving the problem from scratch. Explanation-based
learning takes this a good deal further, by creating general rules that cover an entire class
of cases. In the case of differentiation, memoization would remember that the derivative of
X
2 with respect to X is 2X, but would leave the agent to calculate the derivative ofZ2 with
respect to Z from scratch. We would like to be able to extract the general rule that for any
arithmetic unknown u, the derivative of u2 with respect to u is 2u. (An even more general
rule for un can also be produced, but the current example sufﬁces to make the point.) In
logical terms, this is expressed by the rule
ArithmeticUnknown(u) ⇒Derivative(u2,u)=2 u.
Section 19.3. Explanation-Based Learning 781
If the knowledge base contains such a rule, then any new case that is an instance of this rule
can be solved immediately.
This is, of course, merely a trivial example of a very general phenomenon. Once some-
thing is understood, it can be generalized and reused in other circumstances. It becomes an
“obvious” step and can then be used as a building block in solving problems still more com-
plex. Alfred North Whitehead (1911), co-author with Bertrand Russell of Principia Mathe-
matica,w r o t e“Civilization advances by extending the number of important operations that
we can do without thinking about them, ” perhaps himself applying EBL to his understanding
of events such as Zog’s discovery. If you have understood the basic idea of the differenti-
ation example, then your brain is already busily trying to extract the general principles of
explanation-based learning from it. Notice that you hadn’t already invented EBL before you
saw the example. Like the cavemen watching Zog, you (and we) needed an example before
we could generate the basic principles. This is because explaining why something is a good
idea is much easier than coming up with the idea in the ﬁrst place.
19.3.1 Extracting general rules from examples
The basic idea behind EBL is ﬁrst to construct an explanation of the observation using prior
knowledge, and then to establish a deﬁnition of the class of cases for which the same expla-
nation structure can be used. This deﬁnition provides the basis for a rule covering all of the
cases in the class. The “explanation” can be a logical proof, but more generally it can be any
reasoning or problem-solving process whose steps are well deﬁned. The key is to be able to
identify the necessary conditions for those same steps to apply to another case.
We will use for our reasoning system the simple backward-chaining theorem prover
described in Chapter 9. The proof tree for Derivative(X
2,X )=2 X is too large to use as an
example, so we will use a simpler problem to illustrate the generalization method. Suppose
our problem is to simplify 1× (0 +X). The knowledge base includes the following rules:
Rewrite(u, v)∧Simplify(v,w ) ⇒Simplify(u, w) .
Primitive(u) ⇒Simplify(u, u) .
ArithmeticUnknown(u) ⇒Primitive(u) .
Number(u) ⇒Primitive(u) .
Rewrite(1× u, u) .
Rewrite(0 +u, u) .
...
The proof that the answer is X is shown in the top half of Figure 19.7. The EBL method
actually constructs two proof trees simultaneously. The second proof tree uses a variabilized
goal in which the constants from the original goal are replaced by variables. As the original
proof proceeds, the variabilized proof proceeds in step, using exactly the same rule applica-
tions. This could cause some of the variables to become instantiated. For example, in order
to use the rule Rewrite(1×u, u),t h ev a r i a b l ex in the subgoal Rewrite(x×(y +z),v ) must
be bound to 1. Similarly, y must be bound to 0 in the subgoal Rewrite(y + z,v ′) in order to
use the rule Rewrite(0 +u, u). Once we have the generalized proof tree, we take the leaves
782 Chapter 19. Knowledge in Learning
Primitive(X)
ArithmeticUnknown(X)
Primitive(z)
ArithmeticUnknown(z)
Simplify(X,w)
Yes, {  }
Yes, {x / 1, v / y+z}
Simplify(y+z,w)
Rewrite(y+z,v')
Yes, {y / 0, v'/ z}
{w / X}
Yes, {  }
Yes, {v / 0+X}
Yes, {v' / X}
Simplify(z,w)
{w / z}
Simplify(1 × (0+X),w)
Rewrite(x ×(y+z),v)
Simplify(x ×(y+z),w)
Rewrite( 1×( 0+X),v) Simplify(0+X,w)
Rewrite(0+X,v')
Figure 19.7 Proof trees for the simpliﬁcation problem. The ﬁrst tree shows the proof for
the original problem instance, from which we can derive
ArithmeticUnknown(z) ⇒Simplify(1× (0 +z),z ) .
The second tree shows the proof for a problem instance with all constants replaced by vari-
ables, from which we can derive a variety of other rules.
(with the necessary bindings) and form a general rule for the goal predicate:
Rewrite(1× (0 +z),0+ z)∧Rewrite(0 +z,z )∧ArithmeticUnknown(z)
⇒Simplify(1× (0 +z),z ) .
Notice that the ﬁrst two conditions on the left-hand side are true regardless of the value of z.
We can therefore drop them from the rule, yielding
ArithmeticUnknown(z) ⇒Simplify(1× (0 +z),z ) .
In general, conditions can be dropped from the ﬁnal rule if they impose no constraints on the
variables on the right-hand side of the rule, because the resulting rule will still be true and
will be more efﬁcient. Notice that we cannot drop the condition ArithmeticUnknown(z),
because not all possible values of z are arithmetic unknowns. Values other than arithmetic
unknowns might require different forms of simpliﬁcation: for example, if z were 2× 3,t h e n
the correct simpliﬁcation of 1× (0 + (2× 3)) would be 6 and not 2× 3.
To recap, the basic EBL process works as follows:
1. Given an example, construct a proof that the goal predicate applies to the example using
the available background knowledge.
Section 19.3. Explanation-Based Learning 783
2. In parallel, construct a generalized proof tree for the variabilized goal using the same
inference steps as in the original proof.
3. Construct a new rule whose left-hand side consists of the leaves of the proof tree and
whose right-hand side is the variabilized goal (after applying the necessary bindings
from the generalized proof).
4. Drop any conditions from the left-hand side that are true regardless of the values of the
variables in the goal.
19.3.2 Improving efﬁciency
The generalized proof tree in Figure 19.7 actually yields more than one generalized rule. For
example, if we terminate, or prune, the growth of the right-hand branch in the proof tree
when it reaches the Primitive step, we get the rule
Primitive(z) ⇒Simplify(1× (0 +z),z ) .
This rule is as valid as, but more general than, the rule using ArithmeticUnknown, because
it covers cases where z is a number. We can extract a still more general rule by pruning after
the step Simplify(y + z,w ), yielding the rule
Simplify(y + z,w ) ⇒Simplify(1× (y + z),w ) .
In general, a rule can be extracted fromany partial subtree of the generalized proof tree. Now
we have a problem: which of these rules do we choose?
The choice of which rule to generate comes down to the question of efﬁciency. There
are three factors involved in the analysis of efﬁciency gains from EBL:
1. Adding large numbers of rules can slow down the reasoning process, because the in-
ference mechanism must still check those rules even in cases where they do not yield a
solution. In other words, it increases the branching factor in the search space.
2. To compensate for the slowdown in reasoning, the derived rules must offer signiﬁcant
increases in speed for the cases that they do cover. These increases come about mainly
because the derived rules avoid dead ends that would otherwise be taken, but also be-
cause they shorten the proof itself.
3. Derived rules should be as general as possible, so that they apply to the largest possible
set of cases.
A common approach to ensuring that derived rules are efﬁcient is to insist on theoperational-
ity of each subgoal in the rule. A subgoal is operational if it is “easy” to solve. For example,
OPERA TIONALITY
the subgoal Primitive(z) is easy to solve, requiring at most two steps, whereas the subgoal
Simplify(y + z,w ) could lead to an arbitrary amount of inference, depending on the values
of y and z. If a test for operationality is carried out at each step in the construction of the
generalized proof, then we can prune the rest of a branch as soon as an operational subgoal is
found, keeping just the operational subgoal as a conjunct of the new rule.
Unfortunately, there is usually a tradeoff between operationality and generality. More
speciﬁc subgoals are generally easier to solve but cover fewer cases. Also, operationality
is a matter of degree: one or two steps is deﬁnitely operational, but what about 10 or 100?
784 Chapter 19. Knowledge in Learning
Finally, the cost of solving a given subgoal depends on what other rules are available in the
knowledge base. It can go up or down as more rules are added. Thus, EBL systems really
face a very complex optimization problem in trying to maximize the efﬁciency of a given
initial knowledge base. It is sometimes possible to derive a mathematical model of the effect
on overall efﬁciency of adding a given rule and to use this model to select the best rule to
add. The analysis can become very complicated, however, especially when recursive rules
are involved. One promising approach is to address the problem of efﬁciency empirically,
simply by adding several rules and seeing which ones are useful and actually speed things up.
Empirical analysis of efﬁciency is actually at the heart of EBL. What we have been
calling loosely the “efﬁciency of a given knowledge base” is actually the average-case com-
plexity on a distribution of problems. By generalizing from past example problems, EBL
makes the knowledge base more efﬁcient for the kind of problems that it is reasonable to
expect. This works as long as the distribution of past examples is roughly the same as for
future examples—the same assumption used for PAC-learning in Section 18.5. If the EBL
system is carefully engineered, it is possible to obtain signiﬁcant speedups. For example, a
very large Prolog-based natural language system designed for speech-to-speech translation
between Swedish and English was able to achieve real-time performance only by the appli-
cation of EBL to the parsing process (Samuelsson and Rayner, 1991).
19.4 L EARNING USING RELEV ANCE INFORMATION
Our traveler in Brazil seems to be able to make a conﬁdent generalization concerning the lan-
guage spoken by other Brazilians. The inference is sanctioned by her background knowledge,
namely, that people in a given country (usually) speak the same language. We can express
this in ﬁrst-order logic as follows:
2
Nationality(x, n)∧Nationality(y,n )∧Language(x, l) ⇒Language(y,l ) . (19.6)
(Literal translation: “If x and y have the same nationality n and x speaks language l,t h e ny
also speaks it.”) It is not difﬁcult to show that, from this sentence and the observation that
Nationality(Fernando,Brazil)∧Language(Fernando,Portuguese) ,
the following conclusion is entailed (see Exercise 19.1):
Nationality(x,Brazil) ⇒Language(x,Portuguese) .
Sentences such as (19.6) express a strict form of relevance: given nationality, language
is fully determined. (Put another way: language is a function of nationality.) These sentences
are called functional dependencies or determinations. They occur so commonly in certain
FUNCTIONAL
DEPENDENCY
DETERMINA TION kinds of applications (e.g., deﬁning database designs) that a special syntax is used to write
them. We adopt the notation of Davies (1985):
Nationality(x, n)≻ Language(x, l) .
2 We assume for the sake of simplicity that a person speaks only one language. Clearly, the rule would have to
be amended for countries such as Switzerland and India.
Section 19.4. Learning Using Relevance Information 785
As usual, this is simply a syntactic sugaring, but it makes it clear that the determination is
really a relationship between the predicates: nationality determines language. The relevant
properties determining conductance and density can be expressed similarly:
Material(x, m)∧Temperature(x, t)≻ Conductance(x, ρ);
Material(x, m)∧Temperature(x, t)≻ Density(x, d) .
The corresponding generalizations follow logically from the determinations and observations.
19.4.1 Determining the hypothesis space
Although the determinations sanction general conclusions concerning all Brazilians, or all
pieces of copper at a given temperature, they cannot, of course, yield a general predictive
theory for all nationalities, or for all temperatures and materials, from a single example.
Their main effect can be seen as limiting the space of hypotheses that the learning agent need
consider. In predicting conductance, for example, one need consider only material and tem-
perature and can ignore mass, ownership, day of the week, the current president, and so on.
Hypotheses can certainly include terms that are in turn determined by material and temper-
ature, such as molecular structure, thermal energy, or free-electron density. Determinations
specify a sufﬁcient basis vocabulary from which to construct hypotheses concerning the target
predicate. This statement can be proven by showing that a given determination is logically
equivalent to a statement that the correct deﬁnition of the target predicate is one of the set of
all deﬁnitions expressible using the predicates on the left-hand side of the determination.
Intuitively, it is clear that a reduction in the hypothesis space size should make it eas-
ier to learn the target predicate. Using the basic results of computational learning theory
(Section 18.5), we can quantify the possible gains. First, recall that for Boolean functions,
log(|H|) examples are required to converge to a reasonable hypothesis, where |H| is the
size of the hypothesis space. If the learner has n Boolean features with which to construct
hypotheses, then, in the absence of further restrictions, |H| = O(2
2n
), so the number of ex-
amples is O(2n). If the determination contains d predicates in the left-hand side, the learner
will require only O(2d) examples, a reduction of O(2n− d).
19.4.2 Learning and using relevance information
As we stated in the introduction to this chapter, prior knowledge is useful in learning; but
it too has to be learned. In order to provide a complete story of relevance-based learning,
we must therefore provide a learning algorithm for determinations. The learning algorithm
we now present is based on a straightforward attempt to ﬁnd the simplest determination con-
sistent with the observations. A determination P ≻ Q says that if any examples match on
P , then they must also match on Q. A determination is therefore consistent with a set of
examples if every pair that matches on the predicates on the left-hand side also matches on
the goal predicate. For example, suppose we have the following examples of conductance
measurements on material samples:
786 Chapter 19. Knowledge in Learning
function MINIMAL -CONSISTENT -DET(E,A) returns a set of attributes
inputs: E,as e to fe x a m p l e s
A, a set of attributes, of size n
for i =0 to n do
for each subset Ai of A of size i do
if CONSISTENT -DET ?(Ai,E) then return Ai
function CONSISTENT -DET ?(A,E) returns a truth value
inputs: A, a set of attributes
E,as e to fe x a m p l e s
local variables: H , a hash table
for each example e in E do
if some example in H has the same values as e for the attributes A
but a different classiﬁcation then return false
store the class of e in H , indexed by the values for attributesA of the example e
return true
Figure 19.8 An algorithm for ﬁnding a minimal consistent determination.
Sample
 Mass
 Temperature
 Material
 Size
 Conductance
S1
 12
 26
 Copper
 3
 0.59
S1
 12
 100
 Copper
 3
 0.57
S2
 24
 26
 Copper
 6
 0.59
S3
 12
 26
 Lead
 2
 0.05
S3
 12
 100
 Lead
 2
 0.04
S4
 24
 26
 Lead
 4
 0.05
The minimal consistent determination is Material∧Temperature≻ Conductance.T h e r e
is a nonminimal but consistent determination, namely, Mass ∧Size ∧Temperature ≻
Conductance. This is consistent with the examples because mass and size determine density
and, in our data set, we do not have two different materials with the same density. As usual,
we would need a larger sample set in order to eliminate a nearly correct hypothesis.
There are several possible algorithms for ﬁnding minimal consistent determinations.
The most obvious approach is to conduct a search through the space of determinations, check-
ing all determinations with one predicate, two predicates, and so on, until a consistent deter-
mination is found. We will assume a simple attribute-based representation, like that used for
decision tree learning in Chapter 18. A determination d will be represented by the set of
attributes on the left-hand side, because the target predicate is assumed to be ﬁxed. The basic
algorithm is outlined in Figure 19.8.
The time complexity of this algorithm depends on the size of the smallest consistent
determination. Suppose this determination has p attributes out of the n total attributes. Then
the algorithm will not ﬁnd it until searching the subsets ofA of size p.T h e r ea r e
⎞
n
p
⎠
= O(np)
Section 19.4. Learning Using Relevance Information 787
0.4
0.5
0.6
0.7
0.8
0.9
1
0 20 40 60 80 100 120 140
Proportion correct on test set
Training set size
RBDTL
DTL
Figure 19.9 A performance comparison between D ECISION -TREE -LEARNING and
RBDTL on randomly generated data for a target function that depends on only 5 of 16
attributes.
such subsets; hence the algorithm is exponential in the size of the minimal determination. It
turns out that the problem is NP-complete, so we cannot expect to do better in the general
case. In most domains, however, there will be sufﬁcient local structure (see Chapter 14 for a
deﬁnition of locally structured domains) that p will be small.
Given an algorithm for learning determinations, a learning agent has a way to construct
a minimal hypothesis within which to learn the target predicate. For example, we can combine
M
INIMAL -CONSISTENT -DET with the DECISION -TREE -LEARNING algorithm. This yields
a relevance-based decision-tree learning algorithm RBDTL that ﬁrst identiﬁes a minimal
set of relevant attributes and then passes this set to the decision tree algorithm for learning.
Unlike D
ECISION -TREE -LEARNING , RBDTL simultaneously learns and uses relevance in-
formation in order to minimize its hypothesis space. We expect that RBDTL will learn faster
than D
ECISION -TREE -LEARNING , and this is in fact the case. Figure 19.9 shows the learning
performance for the two algorithms on randomly generated data for a function that depends
on only 5 of 16 attributes. Obviously, in cases where all the available attributes are relevant,
RBDTL will show no advantage.
This section has only scratched the surface of the ﬁeld of declarative bias, which aimsDECLARA TIVE BIAS
to understand how prior knowledge can be used to identify the appropriate hypothesis space
within which to search for the correct target deﬁnition. There are many unanswered questions:
•How can the algorithms be extended to handle noise?
•Can we handle continuous-valued variables?
•How can other kinds of prior knowledge be used, besides determinations?
•How can the algorithms be generalized to cover any ﬁrst-order theory, rather than just
an attribute-based representation?
Some of these questions are addressed in the next section.
788 Chapter 19. Knowledge in Learning
19.5 I NDUCTIVE LOGIC PROGRAMMING
Inductive logic programming (ILP) combines inductive methods with the power of ﬁrst-order
representations, concentrating in particular on the representation of hypotheses as logic pro-
grams.
3 It has gained popularity for three reasons. First, ILP offers a rigorous approach to
the general knowledge-based inductive learning problem. Second, it offers complete algo-
rithms for inducing general, ﬁrst-order theories from examples, which can therefore learn
successfully in domains where attribute-based algorithms are hard to apply. An example is
in learning how protein structures fold (Figure 19.10). The three-dimensional conﬁguration
of a protein molecule cannot be represented reasonably by a set of attributes, because the
conﬁguration inherently refers to relationships between objects, not to attributes of a single
object. First-order logic is an appropriate language for describing the relationships. Third,
inductive logic programming produces hypotheses that are (relatively) easy for humans to
read. For example, the English translation in Figure 19.10 can be scrutinized and criticized
by working biologists. This means that inductive logic programming systems can participate
in the scientiﬁc cycle of experimentation, hypothesis generation, debate, and refutation. Such
participation would not be possible for systems that generate “black-box” classiﬁers, such as
neural networks.
19.5.1 An example
Recall from Equation (19.5) that the general knowledge-based induction problem is to “solve”
the entailment constraint
Background∧Hypothesis∧Descriptions|= Classiﬁcations
for the unknown Hypothesis,g i v e nt h eBackground knowledge and examples described by
Descriptions and Classiﬁcations. To illustrate this, we will use the problem of learning
family relationships from examples. The descriptions will consist of an extended family
tree, described in terms of Mother, Father,a n dMarried relations and Male and Female
properties. As an example, we will use the family tree from Exercise 8.14, shown here in
Figure 19.11. The corresponding descriptions are as follows:
Father(Philip,Charles) Father(Philip,Anne) ...
Mother(Mum,Margaret) Mother(Mum,Elizabeth) ...
Married(Diana,Charles) Married(Elizabeth,Philip) ...
Male(Philip) Male(Charles) ...
Female(Beatrice) Female(Margaret) ...
The sentences in Classiﬁcationsdepend on the target concept being learned. We might want
to learn Grandparent, BrotherInLaw,o r Ancestor, for example. For Grandparent,t h e
3 It might be appropriate at this point for the reader to refer to Chapter 7 for some of the underlying concepts,
including Horn clauses, conjunctive normal form, uniﬁcation, and resolution.
Section 19.5. Inductive Logic Programming 789
complete set of Classiﬁcations contains 20× 20 = 400conjuncts of the form
Grandparent(Mum,Charles) Grandparent(Elizabeth,Beatrice) ...
¬Grandparent(Mum,Harry) ¬Grandparent(Spencer,Peter) ...
We could of course learn from a subset of this complete set.
The object of an inductive learning program is to come up with a set of sentences for
the Hypothesis such that the entailment constraint is satisﬁed. Suppose, for the moment, that
the agent has no background knowledge: Background is empty. Then one possible solution
2mhr - Four-helical up-and-down bundle
H:1[19-37]
H:2[41-64]
H:3[71-84]
H:4[93-108]
H:5[111-113]
H:1[8-17]
H:2[26-33]
H:3[40-50]
H:4[61-64]
H:5[66-70]
H:6[79-88]
H:7[99-106]
E:1[57-59]
E:2[96-98]
1omd - EF-Hand
(a) (b)
Figure 19.10 (a) and (b) show positive and negative examples, respectively, of the
“four-helical up-and-down bundle” concept in the domain of protein folding. Each
example structure is coded into a logica l expression of about 100 conjuncts such as
TotalLength(D2mhr, 118)∧NumberHelices(D2mhr, 6)∧... . From these descriptions and
from classiﬁcations such as Fold(FOUR -HELICAL -UP-AND-DOWN -BUNDLE ,D 2mhr),
the ILP system PROGOL (Muggleton, 1995) learned the following rule:
Fold(FOUR -HELICAL -UP-AND-DOWN -BUNDLE ,p )⇐
Helix(p, h1)∧Length(h1, HIGH)∧Position(p, h1,n )
∧(1≤n≤3)∧Adjacent(p, h1,h2)∧Helix(p, h2) .
This kind of rule could not be learned, or even represented, by an attribute-based mechanism
such as we saw in previous chapters. The rule can be translated into English as “ Protein p
has fold class “Four-helical up-and-down-bundle” if it contains a long helixh1 at a secondary
structure position between 1 and 3 and h1 is next to a second helix.”

790 Chapter 19. Knowledge in Learning
for Hypothesis is the following:
Grandparent(x, y) ⇔ [∃z Mother(x, z)∧Mother(z,y )]
∨ [∃z Mother(x, z)∧Father(z,y )]
∨ [∃z Father(x, z)∧Mother(z,y )]
∨ [∃z Father(x, z)∧Father(z,y )] .
Notice that an attribute-based learning algorithm, such as DECISION -TREE -LEARNING , will
get nowhere in solving this problem. In order to express Grandparent as an attribute (i.e., a
unary predicate), we would need to make pairs of people into objects:
Grandparent(⟨Mum,Charles⟩)...
Then we get stuck in trying to represent the example descriptions. The only possible attributes
are horrible things such as
FirstElementIsMotherOfElizabeth(⟨Mum,Charles⟩) .
The deﬁnition of Grandparent in terms of these attributes simply becomes a large disjunc-
tion of speciﬁc cases that does not generalize to new examples at all.Attribute-based learning
algorithms are incapable of learning relational predicates. Thus, one of the principal advan-
tages of ILP algorithms is their applicability to a much wider range of problems, including
relational problems.
The reader will certainly have noticed that a little bit of background knowledge would
help in the representation of the Grandparent deﬁnition. For example, if Background in-
cluded the sentence
Parent(x, y) ⇔[Mother(x, y)∨Father(x, y)] ,
then the deﬁnition of Grandparent would be reduced to
Grandparent(x, y) ⇔[∃z Parent(x, z)∧Parent(z,y )] .
This shows how background knowledge can dramatically reduce the size of hypotheses re-
quired to explain the observations.
It is also possible for ILP algorithms to create new predicates in order to facilitate the
expression of explanatory hypotheses. Given the example data shown earlier, it is entirely
reasonable for the ILP program to propose an additional predicate, which we would call
Beatrice
Andrew
EugenieWilliam Harry
CharlesDiana
MumGeorge
PhilipElizabeth MargaretKyddSpencer
Peter
Mark
Zara
Anne Sarah Edward Sophie
Louise James
Figure 19.11 A typical family tree.

Section 19.5. Inductive Logic Programming 791
“Parent,” in order to simplify the deﬁnitions of the target predicates. Algorithms that can
generate new predicates are called constructive induction algorithms. Clearly, constructiveCONSTRUCTIVE
INDUCTION
induction is a necessary part of the picture of cumulative learning. It has been one of the
hardest problems in machine learning, but some ILP techniques provide effective mechanisms
for achieving it.
In the rest of this chapter, we will study the two principal approaches to ILP. The ﬁrst
uses a generalization of decision tree methods, and the second uses techniques based on
inverting a resolution proof.
19.5.2 Top-down inductive learning methods
The ﬁrst approach to ILP works by starting with a very general rule and gradually specializing
it so that it ﬁts the data. This is essentially what happens in decision-tree learning, where a
decision tree is gradually grown until it is consistent with the observations. To do ILP we
use ﬁrst-order literals instead of attributes, and the hypothesis is a set of clauses instead of a
decision tree. This section describes F
OIL (Quinlan, 1990), one of the ﬁrst ILP programs.
Suppose we are trying to learn a deﬁnition of the Grandfather(x, y) predicate, using
the same family data as before. As with decision-tree learning, we can divide the examples
into positive and negative examples. Positive examples are
⟨George,Anne⟩, ⟨Philip,Peter⟩, ⟨Spencer,Harry⟩, ...
and negative examples are
⟨George,Elizabeth⟩, ⟨Harry,Zara⟩, ⟨Charles,Philip⟩, ...
Notice that each example is a pair of objects, because Grandfather is a binary predicate. In
all, there are 12 positive examples in the family tree and 388 negative examples (all the other
pairs of people).
FOIL constructs a set of clauses, each withGrandfather(x, y) as the head. The clauses
must classify the 12 positive examples as instances of the Grandfather(x, y) relationship,
while ruling out the 388 negative examples. The clauses are Horn clauses, with the extension
that negated literals are allowed in the body of a clause and are interpreted using negation as
failure, as in Prolog. The initial clause has an empty body:
⇒Grandfather(x, y) .
This clause classiﬁes every example as positive, so it needs to be specialized. We do this by
adding literals one at a time to the left-hand side. Here are three potential additions:
Father(x, y) ⇒Grandfather(x, y) .
Parent(x, z) ⇒Grandfather(x, y) .
Father(x, z) ⇒Grandfather(x, y) .
(Notice that we are assuming that a clause deﬁning Parent is already part of the background
knowledge.) The ﬁrst of these three clauses incorrectly classiﬁes all of the 12 positive exam-
ples as negative and can thus be ignored. The second and third agree with all of the positive
examples, but the second is incorrect on a larger fraction of the negative examples—twice as
many, because it allows mothers as well as fathers. Hence, we prefer the third clause.
792 Chapter 19. Knowledge in Learning
Now we need to specialize this clause further, to rule out the cases in which x is the
father of some z,b u tz is not a parent of y. Adding the single literal Parent(z,y ) gives
Father(x, z)∧Parent(z,y ) ⇒Grandfather(x, y) ,
which correctly classiﬁes all the examples. F OIL will ﬁnd and choose this literal, thereby
solving the learning task. In general, the solution is a set of Horn clauses, each of which
implies the target predicate. For example, if we didn’t have the Parent predicate in our
vocabulary, then the solution might be
Father(x, z)∧Father(z,y ) ⇒Grandfather(x, y)
Father(x, z)∧Mother(z,y ) ⇒Grandfather(x, y) .
Note that each of these clauses covers some of the positive examples, that together they cover
all the positive examples, and that N
EW-CLAUSE is designed in such a way that no clause
will incorrectly cover a negative example. In general FOIL will have to search through many
unsuccessful clauses before ﬁnding a correct solution.
This example is a very simple illustration of how F OIL operates. A sketch of the com-
plete algorithm is shown in Figure 19.12. Essentially, the algorithm repeatedly constructs a
clause, literal by literal, until it agrees with some subset of the positive examples and none of
the negative examples. Then the positive examples covered by the clause are removed from
the training set, and the process continues until no positive examples remain. The two main
subroutines to be explained are N
EW-LITERALS , which constructs all possible new literals to
add to the clause, and CHOOSE -LITERAL , which selects a literal to add.
NEW-LITERALS takes a clause and constructs all possible “useful” literals that could
be added to the clause. Let us use as an example the clause
Father(x, z) ⇒Grandfather(x, y) .
There are three kinds of literals that can be added:
1. Literals using predicates: the literal can be negated or unnegated, any existing predicate
(including the goal predicate) can be used, and the arguments must all be variables. Any
variable can be used for any argument of the predicate, with one restriction: each literal
must include at least one variable from an earlier literal or from the head of the clause.
Literals such as Mother(z,u ), Married(z,z ),¬Male(y),a n dGrandfather(v,x ) are
allowed, whereas Married(u, v) is not. Notice that the use of the predicate from the
head of the clause allows FOIL to learn recursive deﬁnitions.
2. Equality and inequality literals : these relate variables already appearing in the clause.
For example, we might add z ̸= x. These literals can also include user-speciﬁed con-
stants. For learning arithmetic we might use 0 and 1, and for learning list functions we
might use the empty list [] .
3. Arithmetic comparisons: when dealing with functions of continuous variables, literals
such as x>y and y ≤z can be added. As in decision-tree learning, a constant
threshold value can be chosen to maximize the discriminatory power of the test.
The resulting branching factor in this search space is very large (see Exercise 19.6), but F
OIL
can also use type information to reduce it. For example, if the domain included numbers as
Section 19.5. Inductive Logic Programming 793
function FOIL (examples,target) returns a set of Horn clauses
inputs: examples, set of examples
target, a literal for the goal predicate
local variables: clauses, set of clauses, initially empty
while examples contains positive examples do
clause←NEW-CLAUSE (examples,target)
remove positive examples covered byclause from examples
add clause to clauses
return clauses
function NEW-CLAUSE (examples,target) returns a Horn clause
local variables: clause, a clause with target as head and an empty body
l, a literal to be added to the clause
extended
 examples, a set of examples with values for new variables
extended
 examples←examples
while extended
 examples contains negative examples do
l←CHOOSE -LITERAL (NEW-LITERALS (clause),extended
 examples)
append l to the body of clause
extended
 examples←set of examples created by applying EXTEND -EXAMPLE
to each example in extended
 examples
return clause
function EXTEND -EXAMPLE (example,literal) returns a set of examples
if example satisﬁes literal
then return the set of examples created by extending example with
each possible constant value for each new variable inliteral
else return the empty set
Figure 19.12 Sketch of the F OIL algorithm for learning sets of ﬁrst-order Horn clauses
from examples. NEW-LITERALS and CHOOSE -LITERAL are explained in the text.
well as people, type restrictions would prevent NEW-LITERALS from generating literals such
as Parent(x, n),w h e r ex is a person and n is a number.
CHOOSE -LITERAL uses a heuristic somewhat similar to information gain (see page 704)
to decide which literal to add. The exact details are not important here, and a number of
different variations have been tried. One interesting additional feature of F
OIL is the use of
Ockham’s razor to eliminate some hypotheses. If a clause becomes longer (according to some
metric) than the total length of the positive examples that the clause explains, that clause is
not considered as a potential hypothesis. This technique provides a way to avoid overcomplex
clauses that ﬁt noise in the data.
F
OIL and its relatives have been used to learn a wide variety of deﬁnitions. One of the
most impressive demonstrations (Quinlan and Cameron-Jones, 1993) involved solving a long
sequence of exercises on list-processing functions from Bratko’s (1986) Prolog textbook. In
794 Chapter 19. Knowledge in Learning
each case, the program was able to learn a correct deﬁnition of the function from a small set
of examples, using the previously learned functions as background knowledge.
19.5.3 Inductive learning with inverse deduction
The second major approach to ILP involves inverting the normal deductive proof process.
Inverse resolution is based on the observation that if the example Classiﬁcations follow
INVERSE
RESOLUTION
from Background∧Hypothesis∧Descriptions, then one must be able to prove this fact by
resolution (because resolution is complete). If we can “run the proof backward,” then we can
ﬁnd a Hypothesis such that the proof goes through. The key, then, is to ﬁnd a way to invert
the resolution process.
We will show a backward proof process for inverse resolution that consists of individual
backward steps. Recall that an ordinary resolution step takes two clauses C
1 and C2 and
resolves them to produce the resolvent C. An inverse resolution step takes a resolvent C
and produces two clauses C1 and C2, such that C is the result of resolving C1 and C2.
Alternatively, it may take a resolvent C and clause C1 and produce a clause C2 such that C
is the result of resolving C1 and C2.
The early steps in an inverse resolution process are shown in Figure 19.13, where we
focus on the positive example Grandparent(George,Anne). The process begins at the end
of the proof (shown at the bottom of the ﬁgure). We take the resolvent C to be empty
clause (i.e. a contradiction) and C2 to be¬Grandparent(George,Anne), which is the nega-
tion of the goal example. The ﬁrst inverse step takes C and C2 and generates the clause
Grandparent(George,Anne) for C1. The next step takes this clause as C and the clause
Parent(Elizabeth,Anne) as C2, and generates the clause
¬Parent(Elizabeth,y )∨Grandparent(George,y )
as C1. The ﬁnal step treats this clause as the resolvent. With Parent(George,Elizabeth) as
C2, one possible clause C1 is the hypothesis
Parent(x, z)∧Parent(z,y ) ⇒Grandparent(x, y) .
Now we have a resolution proof that the hypothesis, descriptions, and background knowledge
entail the classiﬁcation Grandparent(George,Anne).
Clearly, inverse resolution involves a search. Each inverse resolution step is nonde-
terministic, because for any C, there can be many or even an inﬁnite number of clauses
C1 and C2 that resolve to C. For example, instead of choosing ¬Parent(Elizabeth,y )∨
Grandparent(George,y ) for C1 in the last step of Figure 19.13, the inverse resolution step
might have chosen any of the following sentences:
¬Parent(Elizabeth,Anne)∨Grandparent(George,Anne) .
¬Parent(z, Anne)∨Grandparent(George,Anne) .
¬Parent(z,y )∨Grandparent(George,y ) .
...
(See Exercises 19.4 and 19.5.) Furthermore, the clauses that participate in each step can be
chosen from the Background knowledge, from the example Descriptions, from the negated
Section 19.5. Inductive Logic Programming 795
Classiﬁcations, or from hypothesized clauses that have already been generated in the inverse
resolution tree. The large number of possibilities means a large branching factor (and there-
fore an inefﬁcient search) without additional controls. A number of approaches to taming the
search have been tried in implemented ILP systems:
1. Redundant choices can be eliminated—for example, by generating only the most spe-
ciﬁc hypotheses possible and by requiring that all the hypothesized clauses be consistent
with each other, and with the observations. This last criterion would rule out the clause
¬Parent(z,y )∨Grandparent(George,y ), listed before.
2. The proof strategy can be restricted. For example, we saw in Chapter 9 that linear
resolution is a complete, restricted strategy. Linear resolution produces proof trees that
have a linear branching structure—the whole tree follows one line, with only single
clauses branching off that line (as in Figure 19.13).
3. The representation language can be restricted, for example by eliminating function sym-
bols or by allowing only Horn clauses. For instance, P
ROGOL operates with Horn
clauses using inverse entailment. The idea is to change the entailment constraintINVERSE
ENTAILMENT
Background∧Hypothesis∧Descriptions|= Classiﬁcations
to the logically equivalent form
Background∧Descriptions∧¬Classiﬁcations|=¬Hypothesis.
From this, one can use a process similar to the normal Prolog Horn-clause deduction,
with negation-as-failure to derive Hypothesis. Because it is restricted to Horn clauses,
this is an incomplete method, but it can be more efﬁcient than full resolution. It is also
possible to apply complete inference with inverse entailment (Inoue, 2001).
4. Inference can be done with model checking rather than theorem proving. The P ROGOL
system (Muggleton, 1995) uses a form of model checking to limit the search. That
{y/Anne}
Parent(Elizabeth,Anne)
Grandparent(George,Anne)Grandparent(George,Anne)
Grandparent(George,y)Parent(Elizabeth,y)
>
{x/George, z/Elizabeth}
Parent(George,Elizabeth)
>Parent(z,y) Grandparent(x,y)
>Parent(x,z)¬ ¬
¬
¬
Figure 19.13 Early steps in an inverse resolution process. The shaded clauses are
generated by inverse resolution steps from the clause to the right and the clause below.
The unshaded clauses are from the Descriptions and Classiﬁcations (including negated
Classiﬁcations).

796 Chapter 19. Knowledge in Learning
is, like answer set programming, it generates possible values for logical variables, and
checks for consistency.
5. Inference can be done with ground propositional clauses rather than in ﬁrst-order logic.
The LINUS system (Lavrauca n dDuzeroski, 1994) works by translating ﬁrst-order the-
ories into propositional logic, solving them with a propositional learning system, and
then translating back. Working with propositional formulas can be more efﬁcient on
some problems, as we saw with SATP
LAN in Chapter 10.
19.5.4 Making discoveries with inductive logic programming
An inverse resolution procedure that inverts a complete resolution strategy is, in principle, a
complete algorithm for learning ﬁrst-order theories. That is, if some unknown Hypothesis
generates a set of examples, then an inverse resolution procedure can generate Hypothesis
from the examples. This observation suggests an interesting possibility: Suppose that the
available examples include a variety of trajectories of falling bodies. Would an inverse reso-
lution program be theoretically capable of inferring the law of gravity? The answer is clearly
yes, because the law of gravity allows one to explain the examples, given suitable background
mathematics. Similarly, one can imagine that electromagnetism, quantum mechanics, and the
theory of relativity are also within the scope of ILP programs. Of course, they are also within
the scope of a monkey with a typewriter; we still need better heuristics and new ways to
structure the search space.
One thing that inverse resolution systems will do for you is invent new predicates. This
ability is often seen as somewhat magical, because computers are often thought of as “merely
working with what they are given.” In fact, new predicates fall directly out of the inverse
resolution step. The simplest case arises in hypothesizing two new clauses C
1 and C2,g i v e n
ac l a u s eC. The resolution of C1 and C2 eliminates a literal that the two clauses share; hence,
it is quite possible that the eliminated literal contained a predicate that does not appear in C.
Thus, when working backward, one possibility is to generate a new predicate from which to
reconstruct the missing literal.
Figure 19.14 shows an example in which the new predicateP is generated in the process
of learning a deﬁnition for Ancestor. Once generated, P can be used in later inverse resolu-
tion steps. For example, a later step might hypothesize thatMother(x, y) ⇒P(x, y). Thus,
the new predicate P has its meaning constrained by the generation of hypotheses that involve
it. Another example might lead to the constraint Father(x, y) ⇒P(x, y). In other words,
the predicate P is what we usually think of as the Parent relationship. As we mentioned
earlier, the invention of new predicates can signiﬁcantly reduce the size of the deﬁnition of
the goal predicate. Hence, by including the ability to invent new predicates, inverse resolution
systems can often solve learning problems that are infeasible with other techniques.
Some of the deepest revolutions in science come from the invention of new predicates
and functions—for example, Galileo’s invention of acceleration or Joule’s invention of ther-
mal energy. Once these terms are available, the discovery of new laws becomes (relatively)
easy. The difﬁcult part lies in realizing that some new entity, with a speciﬁc relationship
to existing entities, will allow an entire body of observations to be explained with a much
Section 19.6. Summary 797
{x/George}
Father(x,y) P(x,y)
>
Father(George,y) Ancestor(George,y)
>
P(George,y) Ancestor(George,y)
>¬ ¬
Figure 19.14 An inverse resolution step that generates a new predicateP .
simpler and more elegant theory than previously existed.
As yet, ILP systems have not made discoveries on the level of Galileo or Joule, but their
discoveries have been deemed publishable in the scientiﬁc literature. For example, in the
Journal of Molecular Biology, Turcotte et al. (2001) describe the automated discovery of rules
for protein folding by the ILP program P
ROGOL . Many of the rules discovered by P ROGOL
could have been derived from known principles, but most had not been previously published
as part of a standard biological database. (See Figure 19.10 for an example.). In related
work, Srinivasan et al. (1994) dealt with the problem of discovering molecular-structure-
based rules for the mutagenicity of nitroaromatic compounds. These compounds are found in
automobile exhaust fumes. For 80% of the compounds in a standard database, it is possible to
identify four important features, and linear regression on these features outperforms ILP. For
the remaining 20%, the features alone are not predictive, and ILP identiﬁes relationships that
allow it to outperform linear regression, neural nets, and decision trees. Most impressively,
King et al. (2009) endowed a robot with the ability to perform molecular biology experiments
and extended ILP techniques to include experiment design, thereby creating an autonomous
scientist that actually discovered new knowledge about the functional genomics of yeast. For
all these examples it appears that the ability both to represent relations and to use background
knowledge contribute to ILP’s high performance. The fact that the rules found by ILP can be
interpreted by humans contributes to the acceptance of these techniques in biology journals
rather than just computer science journals.
ILP has made contributions to other sciences besides biology. One of the most impor-
tant is natural language processing, where ILP has been used to extract complex relational
information from text. These results are summarized in Chapter 23.
19.6 S UMMARY
This chapter has investigated various ways in which prior knowledge can help an agent to
learn from new experiences. Because much prior knowledge is expressed in terms of rela-
tional models rather than attribute-based models, we have also covered systems that allow
learning of relational models. The important points are:
•The use of prior knowledge in learning leads to a picture of cumulative learning,i n
which learning agents improve their learning ability as they acquire more knowledge.
•Prior knowledge helps learning by eliminating otherwise consistent hypotheses and by
798 Chapter 19. Knowledge in Learning
“ﬁlling in” the explanation of examples, thereby allowing for shorter hypotheses. These
contributions often result in faster learning from fewer examples.
•Understanding the different logical roles played by prior knowledge, as expressed by
entailment constraints, helps to deﬁne a variety of learning techniques.
•Explanation-based learning (EBL) extracts general rules from single examples by ex-
plaining the examples and generalizing the explanation. It provides a deductive method
for turning ﬁrst-principles knowledge into useful, efﬁcient, special-purpose expertise.
•Relevance-based learning (RBL) uses prior knowledge in the form of determinations
to identify the relevant attributes, thereby generating a reduced hypothesis space and
speeding up learning. RBL also allows deductive generalizations from single examples.
•Knowledge-based inductive learning (KBIL) ﬁnds inductive hypotheses that explain
sets of observations with the help of background knowledge.
•Inductive logic programming (ILP) techniques perform KBIL on knowledge that is
expressed in ﬁrst-order logic. ILP methods can learn relational knowledge that is not
expressible in attribute-based systems.
•ILP can be done with a top-down approach of reﬁning a very general rule or through a
bottom-up approach of inverting the deductive process.
•ILP methods naturally generate new predicates with which concise new theories can be
expressed and show promise as general-purpose scientiﬁc theory formation systems.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Although the use of prior knowledge in learning would seem to be a natural topic for philoso-
phers of science, little formal work was done until quite recently.Fact, Fiction, and F orecast,
by the philosopher Nelson Goodman (1954), refuted the earlier supposition that induction
was simply a matter of seeing enough examples of some universally quantiﬁed proposition
and then adopting it as a hypothesis. Consider, for example, the hypothesis “All emeralds are
grue,” where grue means “green if observed before time t, but blue if observed thereafter.”
At any time up to t, we might have observed millions of instances conﬁrming the rule that
emeralds are grue, and no disconﬁrming instances, and yet we are unwilling to adopt the rule.
This can be explained only by appeal to the role of relevant prior knowledge in the induction
process. Goodman proposes a variety of different kinds of prior knowledge that might be use-
ful, including a version of determinations called overhypotheses. Unfortunately, Goodman’s
ideas were never pursued in machine learning.
The current-best-hypothesis approach is an old idea in philosophy (Mill, 1843). Early
work in cognitive psychology also suggested that it is a natural form of concept learning in
humans (Bruner et al., 1957). In AI, the approach is most closely associated with the work
of Patrick Winston, whose Ph.D. thesis (Winston, 1970) addressed the problem of learning
descriptions of complex objects. The version space method (Mitchell, 1977, 1982) takes
a different approach, maintaining the set of all consistent hypotheses and eliminating those
found to be inconsistent with new examples. The approach was used in the Meta-D
ENDRAL
Bibliographical and Historical Notes 799
expert system for chemistry (Buchanan and Mitchell, 1978), and later in Mitchell’s (1983)
LEX system, which learns to solve calculus problems. A third inﬂuential thread was formed
by the work of Michalski and colleagues on the AQ series of algorithms, which learned sets
of logical rules (Michalski, 1969; Michalski et al., 1986).
EBL had its roots in the techniques used by the S
TRIPS planner (Fikes et al. , 1972).
When a plan was constructed, a generalized version of it was saved in a plan library and
used in later planning as a macro-operator. Similar ideas appeared in Anderson’s ACT*
architecture, under the heading of knowledge compilation (Anderson, 1983), and in the
SOAR architecture, as chunking (Laird et al., 1986). Schema acquisition (DeJong, 1981),
analytical generalization (Mitchell, 1982), and constraint-based generalization (Minton,
1984) were immediate precursors of the rapid growth of interest in EBL stimulated by the
papers of Mitchell et al. (1986) and DeJong and Mooney (1986). Hirsh (1987) introduced
the EBL algorithm described in the text, showing how it could be incorporated directly into a
logic programming system. Van Harmelen and Bundy (1988) explain EBL as a variant of the
partial evaluation method used in program analysis systems (Jones et al., 1993).
Initial enthusiasm for EBL was tempered by Minton’s ﬁnding (1988) that, without ex-
tensive extra work, EBL could easily slow down a program signiﬁcantly. Formal probabilistic
analysis of the expected payoff of EBL can be found in Greiner (1989) and Subramanian and
Feldman (1990). An excellent survey of early work on EBL appears in Dietterich (1990).
Instead of using examples as foci for generalization, one can use them directly to solve
new problems, in a process known as analogical reasoning. This form of reasoning ranges
ANALOGICAL
REASONING
from a form of plausible reasoning based on degree of similarity (Gentner, 1983), through
a form of deductive inference based on determinations but requiring the participation of the
example (Davies and Russell, 1987), to a form of “lazy” EBL that tailors the direction of
generalization of the old example to ﬁt the needs of the new problem. This latter form of
analogical reasoning is found most commonly in case-based reasoning (Kolodner, 1993)
and derivational analogy (Veloso and Carbonell, 1993).
Relevance information in the form of functional dependencies was ﬁrst developed in
the database community, where it is used to structure large sets of attributes into manage-
able subsets. Functional dependencies were used for analogical reasoning by Carbonell
and Collins (1973) and rediscovered and given a full logical analysis by Davies and Rus-
sell (Davies, 1985; Davies and Russell, 1987). Their role as prior knowledge in inductive
learning was explored by Russell and Grosof (1987). The equivalence of determinations to
a restricted-vocabulary hypothesis space was proved in Russell (1988). Learning algorithms
for determinations and the improved performance obtained by RBDTL were ﬁrst shown in
the F
OCUS algorithm, due to Almuallim and Dietterich (1991). Tadepalli (1993) describes a
very ingenious algorithm for learning with determinations that shows large improvements in
learning speed.
The idea that inductive learning can be performed by inverse deduction can be traced
to W. S. Jevons (1874), who wrote, “The study both of Formal Logic and of the Theory of
Probabilities has led me to adopt the opinion that there is no such thing as a distinct method
of induction as contrasted with deduction, but that induction is simply an inverse employ-
ment of deduction.” Computational investigations began with the remarkable Ph.D. thesis by
800 Chapter 19. Knowledge in Learning
Gordon Plotkin (1971) at Edinburgh. Although Plotkin developed many of the theorems and
methods that are in current use in ILP, he was discouraged by some undecidability results for
certain subproblems in induction. MIS (Shapiro, 1981) reintroduced the problem of learning
logic programs, but was seen mainly as a contribution to the theory of automated debug-
ging. Work on rule induction, such as the ID3 (Quinlan, 1986) and CN2 (Clark and Niblett,
1989) systems, led to F
OIL (Quinlan, 1990), which for the ﬁrst time allowed practical induc-
tion of relational rules. The ﬁeld of relational learning was reinvigorated by Muggleton and
Buntine (1988), whose C
IGOL program incorporated a slightly incomplete version of inverse
resolution and was capable of generating new predicates. The inverse resolution method also
appears in (Russell, 1986), with a simple algorithm given in a footnote. The next major sys-
tem was G
OLEM (Muggleton and Feng, 1990), which uses a covering algorithm based on
Plotkin’s concept of relative least general generalization. I TOU (Rouveirol and Puget, 1989)
and CLINT (De Raedt, 1992) were other systems of that era. More recently, P ROGOL (Mug-
gleton, 1995) has taken a hybrid (top-down and bottom-up) approach to inverse entailment
and has been applied to a number of practical problems, particularly in biology and natural
language processing. Muggleton (2000) describes an extension of P
ROGOL to handle uncer-
tainty in the form of stochastic logic programs.
A formal analysis of ILP methods appears in Muggleton (1991), a large collection of
papers in Muggleton (1992), and a collection of techniques and applications in the book
by Lavrauca n dDuzeroski (1994). Page and Srinivasan (2002) give a more recent overview of
the ﬁeld’s history and challenges for the future. Early complexity results by Haussler (1989)
suggested that learning ﬁrst-order sentences was intractible. However, with better understand-
ing of the importance of syntactic restrictions on clauses, positive results have been obtained
even for clauses with recursion (D uzeroski et al. , 1992). Learnability results for ILP are
surveyed by Kietz and Duzeroski (1994) and Cohen and Page (1995).
Although ILP now seems to be the dominant approach to constructive induction, it has
not been the only approach taken. So-called discovery systems aim to model the process
DISCOVERY SYSTEM
of scientiﬁc discovery of new concepts, usually by a direct search in the space of concept
deﬁnitions. Doug Lenat’s Automated Mathematician, or AM (Davis and Lenat, 1982), used
discovery heuristics expressed as expert system rules to guide its search for concepts and
conjectures in elementary number theory. Unlike most systems designed for mathematical
reasoning, AM lacked a concept of proof and could only make conjectures. It rediscovered
Goldbach’s conjecture and the Unique Prime Factorization theorem. AM’s architecture was
generalized in the E
URISKO system (Lenat, 1983) by adding a mechanism capable of rewrit-
ing the system’s own discovery heuristics. E URISKO was applied in a number of areas other
than mathematical discovery, although with less success than AM. The methodology of AM
and E
URISKO has been controversial (Ritchie and Hanna, 1984; Lenat and Brown, 1984).
Another class of discovery systems aims to operate with real scientiﬁc data to ﬁnd new
laws. The systems D ALTON ,G LAUBER ,a n dS TAHL (Langley et al. , 1987) are rule-based
systems that look for quantitative relationships in experimental data from physical systems;
in each case, the system has been able to recapitulate a well-known discovery from the his-
tory of science. Discovery systems based on probabilistic techniques—especially clustering
algorithms that discover new categories—are discussed in Chapter 20.
Exercises 801
EXERCISES
19.1 Show, by translating into conjunctive normal form and applying resolution, that the
conclusion drawn on page 784 concerning Brazilians is sound.
19.2 For each of the following determinations, write down the logical representation and
explain why the determination is true (if it is):
a. Design and denomination determine the mass of a coin.
b. For a given program, input determines output.
c. Climate, food intake, exercise, and metabolism determine weight gain and loss.
d. Baldness is determined by the baldness (or lack thereof) of one’s maternal grandfather.
19.3 Would a probabilistic version of determinations be useful? Suggest a deﬁnition.
19.4 Fill in the missing values for the clauses C1 or C2 (or both) in the following sets of
clauses, given that C is the resolvent of C1 and C2:
a. C = True ⇒P(A, B), C1 = P(x, y) ⇒Q(x, y), C2 =??.
b. C = True ⇒P(A, B), C1 =??, C2 =??.
c. C = P(x, y) ⇒P(x, f(y)), C1 =??, C2 =??.
If there is more than one possible solution, provide one example of each different kind.
19.5 Suppose one writes a logic program that carries out a resolution inference step. That
is, let Resolve(c1,c2,c) succeed if c is the result of resolving c1 and c2. Normally, Resolve
would be used as part of a theorem prover by calling it with c1 and c2 instantiated to par-
ticular clauses, thereby generating the resolvent c. Now suppose instead that we call it with
c instantiated and c1 and c2 uninstantiated. Will this succeed in generating the appropriate
results of an inverse resolution step? Would you need any special modiﬁcations to the logic
programming system for this to work?
19.6 Suppose that F
OIL is considering adding a literal to a clause using a binary predicate
P and that previous literals (including the head of the clause) contain ﬁve different variables.
a. How many functionally different literals can be generated? Two literals are functionally
identical if they differ only in the names of the new variables that they contain.
b. Can you ﬁnd a general formula for the number of different literals with a predicate of
arity r when there are n variables previously used?
c. Why does F OIL not allow literals that contain no previously used variables?
19.7 Using the data from the family tree in Figure 19.11, or a subset thereof, apply the FOIL
algorithm to learn a deﬁnition for the Ancestor predicate.


--- BOOK CHAPTER: 12_Knowledge_Representation ---

12
KNOWLEDGE
REPRESENTATION
In which we show how to use ﬁrst-order logic to represent the most important
aspects of the real world, such as action, space, time, thoughts, and shopping.
The previous chapters described the technology for knowledge-based agents: the syntax,
semantics, and proof theory of propositional and ﬁrst-order logic, and the implementation of
agents that use these logics. In this chapter we address the question of what content to put
into such an agent’s knowledge base—how to represent facts about the world.
Section 12.1 introduces the idea of a general ontology, which organizes everything in
the world into a hierarchy of categories. Section 12.2 covers the basic categories of objects,
substances, and measures; Section 12.3 covers events, and Section 12.4 discusses knowledge
about beliefs. We then return to consider the technology for reasoning with this content:
Section 12.5 discusses reasoning systems designed for efﬁcient inference with categories,
and Section 12.6 discusses reasoning with default information. Section 12.7 brings all the
knowledge together in the context of an Internet shopping environment.
12.1 O NTOLOGICAL ENGINEERING
In “toy” domains, the choice of representation is not that important; many choices will work.
Complex domains such as shopping on the Internet or driving a car in trafﬁc require more
general and ﬂexible representations. This chapter shows how to create these representations,
concentrating on general concepts—such as Events, Time, Physical Objects ,a n d Beliefs—
that occur in many different domains. Representing these abstract concepts is sometimes
called ontological engineering.
ONTOLOGICAL
ENGINEERING
The prospect of representing everything in the world is daunting. Of course, we won’t
actually write a complete description of everything—that would be far too much for even a
1000-page textbook—but we will leave placeholders where new knowledge for any domain
can ﬁt in. For example, we will deﬁne what it means to be a physical object, and the details of
different types of objects—robots, televisions, books, or whatever—can be ﬁlled in later. This
is analogous to the way that designers of an object-oriented programming framework (such as
the Java Swing graphical framework) deﬁne general concepts likeWindow, expecting users to
437
438 Chapter 12. Knowledge Representation
Anything
AbstractObjects
Sets Numbers RepresentationalObjects Interval Places ProcessesPhysicalObjects
Humans
Categories Sentences Measurements Moments Things Stuff
Times Weights Animals Agents Solid Liquid Gas
GeneralizedEvents
Figure 12.1 The upper ontology of the world, showing the topics to be covered later in
the chapter. Each link indicates that the lower concept is a specialization of the upper one.
Specializations are not necessarily disjoint; a human is both an animal and an agent, for
example. We will see in Section 12.3.3 why physical objects come under generalized events.
use these to deﬁne more speciﬁc concepts like SpreadsheetWindow. The general framework
of concepts is called an upper ontology because of the convention of drawing graphs withUPPER ONTOLOGY
the general concepts at the top and the more speciﬁc concepts below them, as in Figure 12.1.
Before considering the ontology further, we should state one important caveat. We
have elected to use ﬁrst-order logic to discuss the content and organization of knowledge,
although certain aspects of the real world are hard to capture in FOL. The principal difﬁculty
is that most generalizations have exceptions or hold only to a degree. For example, although
“tomatoes are red” is a useful rule, some tomatoes are green, yellow, or orange. Similar
exceptions can be found to almost all the rules in this chapter. The ability to handle exceptions
and uncertainty is extremely important, but is orthogonal to the task of understanding the
general ontology. For this reason, we delay the discussion of exceptions until Section 12.5 of
this chapter, and the more general topic of reasoning with uncertainty until Chapter 13.
Of what use is an upper ontology? Consider the ontology for circuits in Section 8.4.2.
It makes many simplifying assumptions: time is omitted completely; signals are ﬁxed and do
not propagate; the structure of the circuit remains constant. A more general ontology would
consider signals at particular times, and would include the wire lengths and propagation de-
lays. This would allow us to simulate the timing properties of the circuit, and indeed such
simulations are often carried out by circuit designers. We could also introduce more inter-
esting classes of gates, for example, by descr ibing the technology (TTL, CMOS, and so on)
as well as the input–output speciﬁcation. If we wanted to discuss reliability or diagnosis, we
would include the possibility that the structure of the circuit or the properties of the gates
might change spontaneously. To account for stray capacitances, we would need to represent
where the wires are on the board.
Section 12.1. Ontological Engineering 439
If we look at the wumpus world, similar considerations apply. Although we do represent
time, it has a simple structure: Nothing happens except when the agent acts, and all changes
are instantaneous. A more general ontology, better suited for the real world, would allow for
simultaneous changes extended over time. We also used aPit predicate to say which squares
have pits. We could have allowed for different kinds of pits by having several individuals
belonging to the class of pits, each having different properties. Similarly, we might want to
allow for other animals besides wumpuses. It might not be possible to pin down the exact
species from the available percepts, so we would need to build up a biological taxonomy to
help the agent predict the behavior of cave-dwellers from scanty clues.
For any special-purpose ontology, it is possible to make changes like these to move
toward greater generality. An obvious question then arises: do all these ontologies converge
on a general-purpose ontology? After centuries of philosophical and computational inves-
tigation, the answer is “Maybe.” In this section, we present one general-purpose ontology
that synthesizes ideas from those centuries. Two major characteristics of general-purpose
ontologies distinguish them from collections of special-purpose ontologies:
•A general-purpose ontology should be applicable in more or less any special-purpose
domain (with the addition of domain-speciﬁc axioms). This means that no representa-
tional issue can be ﬁnessed or brushed under the carpet.
•In any sufﬁciently demanding domain, different areas of knowledge must be uniﬁed,
because reasoning and problem solving could involve several areas simultaneously. A
robot circuit-repair system, for instance, needs to reason about circuits in terms of elec-
trical connectivity and physical layout, and about time, both for circuit timing analysis
and estimating labor costs. The sentences describing time therefore must be capable
of being combined with those describing spatial layout and must work equally well for
nanoseconds and minutes and for angstroms and meters.
We should say up front that the enterprise of general ontological engineering has so far had
only limited success. None of the top AI applications (as listed in Chapter 1) make use
of a shared ontology—they all use special-purpose knowledge engineering. Social/political
considerations can make it difﬁcult for competing parties to agree on an ontology. As Tom
Gruber (2004) says, “Every ontology is a treaty—a social agreement—among people with
some common motive in sharing.” When competing concerns outweigh the motivation for
sharing, there can be no common ontology. Those ontologies that do exist have been created
along four routes:
1. By a team of trained ontologist/logicians, who architect the ontology and write axioms.
The CYC system was mostly built this way (Lenat and Guha, 1990).
2. By importing categories, attributes, and values from an existing database or databases.
DB
PEDIA was built by importing structured facts from Wikipedia (Bizer et al., 2007).
3. By parsing text documents and extracting information from them. T EXT RUNNER was
built by reading a large corpus of Web pages (Banko and Etzioni, 2008).
4. By enticing unskilled amateurs to enter commonsense knowledge. The O PEN MIND
system was built by volunteers who proposed facts in English (Singh et al. , 2002;
Chklovski and Gil, 2005).
440 Chapter 12. Knowledge Representation
12.2 C ATEGORIES AND OBJECTS
The organization of objects into categories is a vital part of knowledge representation. Al-CA TEGORY
though interaction with the world takes place at the level of individual objects, much reason-
ing takes place at the level of categories. For example, a shopper would normally have the
goal of buying a basketball, rather than a particular basketball such as BB9. Categories also
serve to make predictions about objects once they are classiﬁed. One infers the presence of
certain objects from perceptual input, infers category membership from the perceived proper-
ties of the objects, and then uses category information to make predictions about the objects.
For example, from its green and yellow mottled skin, one-foot diameter, ovoid shape, red
ﬂesh, black seeds, and presence in the fruit aisle, one can infer that an object is a watermelon;
from this, one infers that it would be useful for fruit salad.
There are two choices for representing categories in ﬁrst-order logic: predicates and
objects. That is, we can use the predicate Basketball(b), or we can reify
1 the category asREIFICA TION
an object, Basketballs. We could then say Member(b,Basketballs), which we will abbre-
viate as b∈Basketballs, to say that b is a member of the category of basketballs. We say
Subset(Basketballs,Balls), abbreviated as Basketballs ⊂Balls , to say that Basketballs is
a subcategory of Balls . We will use subcategory, subclass, and subset interchangeably.SUBCA TEGORY
Categories serve to organize and simplify the knowledge base through inheritance.I fINHERIT ANCE
we say that all instances of the category Food are edible, and if we assert that Fruit is a
subclass of Food and Apples is a subclass of Fruit, then we can infer that every apple is
edible. We say that the individual apples inherit the property of edibility, in this case from
their membership in the Food category.
Subclass relations organize categories into ataxonomy,o r taxonomic hierarchy.T a x -TAXONOMY
onomies have been used explicitly for centuries in technical ﬁelds. The largest such taxonomy
organizes about 10 million living and extinct species, many of them beetles,2 into a single hi-
erarchy; library science has developed a taxonomy of all ﬁelds of knowledge, encoded as the
Dewey Decimal system; and tax authorities and other government departments have devel-
oped extensive taxonomies of occupations and commercial products. Taxonomies are also an
important aspect of general commonsense knowledge.
First-order logic makes it easy to state facts about categories, either by relating ob-
jects to categories or by quantifying over their members. Here are some types of facts, with
examples of each:
•An object is a member of a category.
BB
9∈Basketballs
•A category is a subclass of another category.
Basketballs⊂Balls
•All members of a category have some properties.
(x∈Basketballs) ⇒Spherical(x)
1 Turning a proposition into an object is called reiﬁcation, from the Latin word res, or thing. John McCarthy
proposed the term “thingiﬁcation,” but it never caught on.
2 The famous biologist J. B. S. Haldane deduced “An inordinate fondness for beetles” on the part of the Creator.
Section 12.2. Categories and Objects 441
•Members of a category can be recognized by some properties.
Orange(x)∧Round(x)∧Diameter(x)=9 .5′′∧x∈Balls ⇒x∈Basketballs
•A category as a whole has some properties.
Dogs∈DomesticatedSpecies
Notice that because Dogs is a category and is a member of DomesticatedSpecies, the latter
must be a category of categories. Of course there are exceptions to many of the above rules
(punctured basketballs are not spherical); we deal with these exceptions later.
Although subclass and member relations are the most important ones for categories,
we also want to be able to state relations between categories that are not subclasses of each
other. For example, if we just say that Males and Females are subclasses of Animals,t h e n
we have not said that a male cannot be a female. We say that two or more categories are
disjoint if they have no members in common. And even if we know that males and females
DISJOINT
are disjoint, we will not know that an animal that is not a male must be a female, unless
we say that males and females constitute an exhaustive decomposition of the animals. AEXHAUSTIVE
DECOMPOSITION
disjoint exhaustive decomposition is known as apartition. The following examples illustrateP ARTITION
these three concepts:
Disjoint({Animals,Vegetables})
ExhaustiveDecomposition({Americans,Canadians,Mexicans},
NorthAmericans)
Partition({Males,Females},Animals) .
(Note that the ExhaustiveDecomposition of NorthAmericans is not a Partition, because
some people have dual citizenship.) The three predicates are deﬁned as follows:
Disjoint(s) ⇔(∀c1,c2 c1∈s∧c2∈s∧c1 ̸= c2 ⇒Intersection(c1,c2)={} )
ExhaustiveDecomposition(s,c) ⇔(∀ii ∈c ⇔∃c2 c2∈s∧i∈c2)
Partition(s,c) ⇔Disjoint(s)∧ExhaustiveDecomposition(s,c) .
Categories can also be deﬁned by providing necessary and sufﬁcient conditions for
membership. For example, a bachelor is an unmarried adult male:
x∈Bachelors ⇔Unmarried(x)∧x∈Adults∧x∈Males .
As we discuss in the sidebar on natural kinds on page 443, strict logical deﬁnitions for cate-
gories are neither always possible nor always necessary.
12.2.1 Physical composition
The idea that one object can be part of another is a familiar one. One’s nose is part of one’s
head, Romania is part of Europe, and this chapter is part of this book. We use the general
PartOf relation to say that one thing is part of another. Objects can be grouped intoPartOf
hierarchies, reminiscent of the Subset hierarchy:
PartOf (Bucharest,Romania)
PartOf (Romania,EasternEurope)
PartOf (EasternEurope,Europe)
PartOf (Europe,Earth) .
442 Chapter 12. Knowledge Representation
The PartOf relation is transitive and reﬂexive; that is,
PartOf (x, y)∧PartOf (y,z ) ⇒PartOf (x, z) .
PartOf (x, x) .
Therefore, we can conclude PartOf (Bucharest,Earth).
Categories of composite objects are often characterized by structural relations amongCOMPOSITE OBJECT
parts. For example, a biped has two legs attached to a body:
Biped(a) ⇒∃l1,l2,b Leg(l1)∧Leg(l2)∧Body(b) ∧
PartOf (l1,a)∧PartOf (l2,a)∧PartOf (b, a) ∧
Attached(l1,b)∧Attached(l2,b) ∧
l1 ̸= l2∧[∀l3 Leg(l3)∧PartOf (l3,a) ⇒(l3 = l1∨l3 = l2)] .
The notation for “exactly two” is a little awkward; we are forced to say that there are two
legs, that they are not the same, and that if anyone proposes a third leg, it must be the same
as one of the other two. In Section 12.5.2, we describe a formalism called description logic
makes it easier to represent constraints like “exactly two.”
We can deﬁne a PartPartition relation analogous to the Partition relation for cate-
gories. (See Exercise 12.8.) An object is composed of the parts in its PartPartition and can
be viewed as deriving some properties from those parts. For example, the mass of a compos-
ite object is the sum of the masses of the parts. Notice that this is not the case with categories,
which have no mass, even though their elements might.
It is also useful to deﬁne composite objects with deﬁnite parts but no particular struc-
ture. For example, we might want to say “The apples in this bag weigh two pounds.” The
temptation would be to ascribe this weight to the set of apples in the bag, but this would be
a mistake because the set is an abstract mathematical concept that has elements but does not
have weight. Instead, we need a new concept, which we will call a bunch. For example, if
BUNCH
the apples are Apple1, Apple2,a n dApple3,t h e n
BunchOf ({Apple1,Apple2,Apple3})
denotes the composite object with the three apples as parts (not elements). We can then use the
bunch as a normal, albeit unstructured, object. Notice thatBunchOf ({x})= x.F u r t h e r m o r e ,
BunchOf (Apples) is the composite object consisting of all apples—not to be confused with
Apples, the category or set of all apples.
We can deﬁne BunchOf in terms of the PartOf relation. Obviously, each element of
s is part of BunchOf (s):
∀xx ∈s ⇒PartOf (x,BunchOf (s)) .
Furthermore, BunchOf (s) is the smallest object satisfying this condition . In other words,
BunchOf (s) must be part of any object that has all the elements of s as parts:
∀y [∀xx ∈s ⇒PartOf (x, y)] ⇒PartOf (BunchOf (s),y ) .
These axioms are an example of a general technique called logical minimization ,w h i c hLOGICAL
MINIMIZA TION
means deﬁning an object as the smallest one satisfying certain conditions.
Section 12.2. Categories and Objects 443
NATURAL KINDS
Some categories have strict deﬁnitions: an object is a triangle if and only if it is
a polygon with three sides. On the other hand, most categories in the real world
have no clear-cut deﬁnition; these are callednatural kind categories. For example,
tomatoes tend to be a dull scarlet; roughly spherical; with an indentation at the top
where the stem was; about two to four inches in diameter; with a thin but tough
skin; and with ﬂesh, seeds, and juice inside. There is, however, variation: some
tomatoes are yellow or orange, unripe tomatoes are green, some are smaller or
larger than average, and cherry tomatoes are uniformly small. Rather than having
a complete deﬁnition of tomatoes, we have a set of features that serves to identify
objects that are clearly typical tomatoes, but might not be able to decide for other
objects. (Could there be a tomato that is fuzzy like a peach?)
This poses a problem for a logical agent. The agent cannot be sure that an
object it has perceived is a tomato, and even if it were sure, it could not be cer-
tain which of the properties of typical tomatoes this one has. This problem is an
inevitable consequence of operating in partially observable environments.
One useful approach is to separate what is true of all instances of a cate-
gory from what is true only of typical instances. So in addition to the category
Tomatoes, we will also have the categoryTypical(Tomatoes). Here, the Typical
function maps a category to the subclass that contains only typical instances:
Typical(c)⊆c.
Most knowledge about natural kinds will actually be about their typical instances:
x∈Typical(Tomatoes) ⇒Red(x)∧Round(x) .
Thus, we can write down useful facts about categories without exact deﬁni-
tions. The difﬁculty of providing exact deﬁnitions for most natural categories was
explained in depth by Wittgenstein (1953). He used the example of games to show
that members of a category shared “family resemblances” rather than necessary
and sufﬁcient characteristics: what strict deﬁnition encompasses chess, tag, soli-
taire, and dodgeball?
The utility of the notion of strict deﬁnition was also challenged by
Quine (1953). He pointed out that even the deﬁnition of “bachelor” as an un-
married adult male is suspect; one might, for example, question a statement such
as “the Pope is a bachelor.” While not strictly false, this usage is certainly infe-
licitous because it induces unintended inferences on the part of the listener. The
tension could perhaps be resolved by distinguishing between logical deﬁnitions
suitable for internal knowledge representation and the more nuanced criteria for
felicitous linguistic usage. The latter may be achieved by “ﬁltering” the assertions
derived from the former. It is also possible that failures of linguistic usage serve as
feedback for modifying internal deﬁnitions, so that ﬁltering becomes unnecessary.

444 Chapter 12. Knowledge Representation
12.2.2 Measurements
In both scientiﬁc and commonsense theories of the world, objects have height, mass, cost,
and so on. The values that we assign for these properties are called measures.O r d i -MEASURE
nary quantitative measures are quite easy to represent. We imagine that the universe in-
cludes abstract “measure objects,” such as the length that is the length of this line seg-
ment:
 . We can call this length 1.5 inches or 3.81 centimeters. Thus,
the same length has different names in our language.We represent the length with a units
function that takes a number as argument. (An alternative scheme is explored in Exer-UNITS FUNCTION
cise 12.9.) If the line segment is called L1, we can write
Length(L1)= Inches(1.5)= Centimeters(3.81) .
Conversion between units is done by equating multiples of one unit to another:
Centimeters(2.54× d)= Inches(d) .
Similar axioms can be written for pounds and kilograms, seconds and days, and dollars and
cents. Measures can be used to describe objects as follows:
Diameter(Basketball12)= Inches(9.5) .
ListPrice(Basketball12) = $(19).
d∈Days ⇒Duration(d)= Hours(24) .
Note that $(1) is not a dollar bill! One can have two dollar bills, but there is only one object
named $(1). Note also that, while Inches(0) and Centimeters(0) refer to the same zero
length, they are not identical to other zero measures, such as Seconds(0).
Simple, quantitative measures are easy to represent. Other measures present more of a
problem, because they have no agreed scale of values. Exercises have difﬁculty, desserts have
deliciousness, and poems have beauty, yet numbers cannot be assigned to these qualities. One
might, in a moment of pure accountancy, dismiss such properties as useless for the purpose of
logical reasoning; or, still worse, attempt to impose a numerical scale on beauty. This would
be a grave mistake, because it is unnecessary. The most important aspect of measures is not
the particular numerical values, but the fact that measures can be ordered.
Although measures are not numbers, we can still compare them, using an ordering
symbol such as >. For example, we might well believe that Norvig’s exercises are tougher
than Russell’s, and that one scores less on tougher exercises:
e
1∈Exercises∧e2∈Exercises∧Wrote(Norvig,e1)∧Wrote(Russell,e2) ⇒
Diﬃculty (e1) > Diﬃculty (e2) .
e1∈Exercises∧e2∈Exercises∧Diﬃculty (e1) > Diﬃculty (e2) ⇒
ExpectedScore(e1) < ExpectedScore(e2) .
This is enough to allow one to decide which exercises to do, even though no numerical values
for difﬁculty were ever used. (One does, however, have to discover who wrote which exer-
cises.) These sorts of monotonic relationships among measures form the basis for the ﬁeld of
qualitative physics, a subﬁeld of AI that investigates how to reason about physical systems
without plunging into detailed equations and numerical simulations. Qualitative physics is
discussed in the historical notes section.
Section 12.2. Categories and Objects 445
12.2.3 Objects: Things and stuff
The real world can be seen as consisting of primitive objects (e.g., atomic particles) and
composite objects built from them. By reasoning at the level of large objects such as apples
and cars, we can overcome the complexity involved in dealing with vast numbers of primitive
objects individually. There is, however, a signiﬁcant portion of reality that seems to defy any
obvious individuation—division into distinct objects. We give this portion the generic name
INDIVIDUA TION
stuff. For example, suppose I have some butter and an aardvark in front of me. I can saySTUFF
there is one aardvark, but there is no obvious number of “butter-objects,” because any part of
a butter-object is also a butter-object, at least until we get to very small parts indeed. This is
the major distinction between stuff and things. If we cut an aardvark in half, we do not get
two aardvarks (unfortunately).
The English language distinguishes clearly between stuff and things. We say “an aard-
vark,” but, except in pretentious California restaurants, one cannot say “a butter.” Linguists
distinguish between count nouns, such as aardvarks, holes, and theorems, and mass nouns,
COUNT NOUNS
MASS NOUN such as butter, water, and energy. Several competing ontologies claim to handle this distinc-
tion. Here we describe just one; the others are covered in the historical notes section.
To represent stuff properly, we begin with the obvious. We need to have as objects in
our ontology at least the gross “lumps” of stuff we interact with. For example, we might
recognize a lump of butter as the one left on the table the night before; we might pick it up,
weigh it, sell it, or whatever. In these senses, it is an object just like the aardvark. Let us
call it Butter
3. We also deﬁne the categoryButter. Informally, its elements will be all those
things of which one might say “It’s butter,” includingButter3. With some caveats about very
small parts that we w omit for now, any part of a butter-object is also a butter-object:
b∈Butter∧PartOf (p, b) ⇒p∈Butter .
We can now say that butter melts at around 30 degrees centigrade:
b∈Butter ⇒MeltingPoint(b,Centigrade(30)) .
We could go on to say that butter is yellow, is less dense than water, is soft at room tempera-
ture, has a high fat content, and so on. On the other hand, butter has no particular size, shape,
or weight. We can deﬁne more specialized categories of butter such as UnsaltedButter,
which is also a kind of stuff. Note that the category PoundOfButter, which includes as
members all butter-objects weighing one pound, is not a kind of stuff. If we cut a pound of
butter in half, we do not, alas, get two pounds of butter.
What is actually going on is this: some properties are intrinsic: they belong to the veryINTRINSIC
substance of the object, rather than to the object as a whole. When you cut an instance of
stuff in half, the two pieces retain the intrinsic properties—things like density, boiling point,
ﬂavor, color, ownership, and so on. On the other hand, their extrinsic properties—weight,EXTRINSIC
length, shape, and so on—are not retained under subdivision. A category of objects that
includes in its deﬁnition only intrinsic properties is then a substance, or mass noun; a class
that includes any extrinsic properties in its deﬁnition is a count noun. The category Stuﬀ is
the most general substance category, specifying no intrinsic properties. The category Thing
is the most general discrete object category, specifying no extrinsic properties.
446 Chapter 12. Knowledge Representation
12.3 E VENTS
In Section 10.4.2, we showed how situation calculus represents actions and their effects.
Situation calculus is limited in its applicability: it was designed to describe a world in which
actions are discrete, instantaneous, and happen one at a time. Consider a continuous action,
such as ﬁlling a bathtub. Situation calculus can say that the tub is empty before the action and
full when the action is done, but it can’t talk about what happens during the action. It also
can’t describe two actions happening at the same time—such as brushing one’s teeth while
waiting for the tub to ﬁll. To handle such cases we introduce an alternative formalism known
as event calculus, which is based on points of time rather than on situations.
3EVENT CALCULUS
Event calculus reiﬁes ﬂuents and events. The ﬂuent At(Shankar,Berkeley) is an ob-
ject that refers to the fact of Shankar being in Berkeley, but does not by itself say anything
about whether it is true. To assert that a ﬂuent is actually true at some point in time we use
the predicate T ,a si n T(At(Shankar,Berkeley),t).
Events are described as instances of event categories.
4 The event E1 of Shankar ﬂying
from San Francisco to Washington, D.C. is described as
E1 ∈Flyings ∧Flyer(E1,Shankar)∧Origin(E1,SF)∧Destination(E1,DC).
If this is too verbose, we can deﬁne an alternative three-argument version of the category of
ﬂying events and say
E1 ∈Flyings (Shankar,SF,DC).
We then use Happens(E1,i) to say that the event E1 took place over the time interval i,a n d
we say the same thing in functional form with Extent(E1)= i. We represent time intervals
by a (start, end) pair of times; that is, i =( t1,t2) is the time interval that starts at t1 and ends
at t2. The complete set of predicates for one version of the event calculus is
T(f,t ) Fluent f is true at time t
Happens(e, i) Event e happens over the time interval i
Initiates(e, f, t) Event e causes ﬂuent f to start to hold at time t
Terminates(e, f, t) Event e causes ﬂuent f to cease to hold at time t
Clipped(f,i ) Fluent f ceases to be true at some point during time interval i
Restored(f,i ) Fluent f becomes true sometime during time interval i
We assume a distinguished event,Start, that describes the initial state by saying which ﬂuents
are initiated or terminated at the start time. We deﬁneT by saying that a ﬂuent holds at a point
in time if the ﬂuent was initiated by an event at some time in the past and was not made false
(clipped) by an intervening event. A ﬂuent does not hold if it was terminated by an event and
3 The terms “event” and “action” may be used intercha ngeably. Informally, “action” connotes an agent while
“event” connotes the possibility of agentless actions.
4 Some versions of event calculus do not distinguish event categories from instances of the categories.
Section 12.3. Events 447
not made true (restored) by another event. Formally, the axioms are:
Happens(e,(t1,t2))∧Initiates(e, f, t1)∧¬Clipped(f, (t1,t))∧t1 <t ⇒
T(f,t )
Happens(e,(t1,t2))∧Terminates(e, f, t1)∧¬Restored(f, (t1,t))∧t1 <t ⇒
¬T(f,t )
where Clipped and Restored are deﬁned by
Clipped(f, (t1,t2)) ⇔
∃e, t, t3 Happens(e,(t, t3))∧t1 ≤t<t 2∧Terminates(e, f, t)
Restored(f, (t1,t2)) ⇔
∃e, t, t3 Happens(e,(t, t3))∧t1 ≤t<t 2∧Initiates(e, f, t)
It is convenient to extend T to work over intervals as well as time points; a ﬂuent holds over
an interval if it holds on every point within the interval:
T(f, (t1,t2)) ⇔[∀t (t1 ≤t<t 2) ⇒T(f,t )]
Fluents and actions are deﬁned with domain-speciﬁc axioms that are similar to successor-
state axioms. For example, we can say that the only way a wumpus-world agent gets an
arrow is at the start, and the only way to use up an arrow is to shoot it:
Initiates(e,HaveArrow(a),t) ⇔e = Start
Terminates(e,HaveArrow(a),t) ⇔e∈Shootings(a)
By reifying events we make it possible to add any amount of arbitrary information about
them. For example, we can say that Shankar’s ﬂight was bumpy with Bumpy(E1).I n a n
ontology where events are n-ary predicates, there would be no way to add extra information
like this; moving to an n +1 -ary predicate isn’t a scalable solution.
We can extend event calculus to make it possible to represent simultaneous events (such
as two people being necessary to ride a seesaw), exogenous events (such as the wind blowing
and changing the location of an object), continuous events (such as the level of water in the
bathtub continuously rising) and other complications.
12.3.1 Processes
The events we have seen so far are what we call discrete events—they have a deﬁnite struc-DISCRETE EVENTS
ture. Shankar’s trip has a beginning, middle, and end. If interrupted halfway, the event would
be something different—it would not be a trip from San Francisco to Washington, but instead
a trip from San Francisco to somewhere over Kansas. On the other hand, the category of
events denoted by Flyings has a different quality. If we take a small interval of Shankar’s
ﬂight, say, the third 20-minute segment (while he waits anxiously for a bag of peanuts), that
event is still a member of Flyings. In fact, this is true for any subinterval.
Categories of events with this property are called process categories or liquid eventPROCESS
LIQUID EVENT categories. Any process e that happens over an interval also happens over any subinterval:
(e∈Processes)∧Happens(e, (t1,t4))∧(t1 <t 2 <t 3 <t 4) ⇒Happens(e, (t2,t3)).
The distinction between liquid and nonliquid events is exactly analogous to the difference
between substances, or stuff, and individual objects, or things. In fact, some have called
liquid events temporal substances, whereas substances like butter are spatial substances.TEMPORAL
SUBSTANCE
SP A TIAL SUBSTANCE
448 Chapter 12. Knowledge Representation
12.3.2 Time intervals
Event calculus opens us up to the possibility of talking about time, and time intervals. We
will consider two kinds of time intervals: moments and extended intervals. The distinction is
that only moments have zero duration:
Partition({Moments,ExtendedIntervals},Intervals)
i∈Moments ⇔Duration(i)= Seconds(0) .
Next we invent a time scale and associate points on that scale with moments, giving us ab-
solute times. The time scale is arbitrary; we measure it in seconds and say that the moment
at midnight (GMT) on January 1, 1900, has time 0. The functions Begin and End pick out
the earliest and latest moments in an interval, and the functionTime delivers the point on the
time scale for a moment. The function Duration gives the difference between the end time
and the start time.
Interval(i) ⇒Duration(i)=( Time(End(i))−Time(Begin(i))) .
Time(Begin(AD1900)) =Seconds(0) .
Time(Begin(AD2001)) =Seconds(3187324800) .
Time(End(AD2001)) =Seconds(3218860800) .
Duration(AD2001) =Seconds(31536000) .
To make these numbers easier to read, we also introduce a function Date, which takes six
arguments (hours, minutes, seconds, day, month, and year) and returns a time point:
Time(Begin(
AD2001)) =Date(0,0,0,1,Jan,2001)
Date(0,20,21,24,1,1995) =Seconds(3000000000) .
Two intervals Meet if the end time of the ﬁrst equals the start time of the second. The com-
plete set of interval relations, as proposed by Allen (1983), is shown graphically in Figure 12.2
and logically below:
Meet(i, j) ⇔ End(i)= Begin(j)
Before(i, j) ⇔ End(i) < Begin(j)
After(j, i) ⇔ Before(i, j)
During(i, j) ⇔ Begin(j) < Begin(i) < End(i) < End(j)
Overlap(i, j) ⇔ Begin(i
) < Begin(j) < End(i) < End(j)
Begins(i, j) ⇔ Begin(i)= Begin(j)
Finishes(i, j) ⇔ End(i)= End(j)
Equals(i, j) ⇔ Begin(i)= Begin(j)∧End(i)= End(j)
These all have their intuitive meaning, with the exception of Overlap: we tend to think of
overlap as symmetric (if i overlaps j then j overlaps i), but in this deﬁnition, Overlap(i, j)
only holds if i begins before j. To say that the reign of Elizabeth II immediately followed that
of George VI, and the reign of Elvis overlapped with the 1950s, we can write the following:
Meets(ReignOf(GeorgeVI),ReignOf (ElizabethII)) .
Overlap(Fifties,ReignOf(Elvis)) .
Begin(Fifties)= Begin(AD1950) .
End(Fifties)= End(AD1959) .
Section 12.3. Events 449
Figure 12.2 Predicates on time intervals.
time
18011797
1789
Washington
Adams
Jefferson
Figure 12.3 A schematic view of the object President(USA) for the ﬁrst 15 years of its
existence.
12.3.3 Fluents and objects
Physical objects can be viewed as generalized events, in the sense that a physical object is
a chunk of space–time. For example, USA can be thought of as an event that began in,
say, 1776 as a union of 13 states and is still in progress today as a union of 50. We can
describe the changing properties of USA using state ﬂuents, such as Population(USA).A
property of the USA that changes every four or eight years, barring mishaps, is its president.
One might propose that President(USA) is a logical term that denotes a different object
at different times. Unfortunately, this is not possible, because a term denotes exactly one
object in a given model structure. (The termPresident(USA,t) can denote different objects,
depending on the value of t, but our ontology keeps time indices separate from ﬂuents.) The
450 Chapter 12. Knowledge Representation
only possibility is that President(USA) denotes a single object that consists of different
people at different times. It is the object that is George Washington from 1789 to 1797, John
Adams from 1797 to 1801, and so on, as in Figure 12.3. To say that George Washington was
president throughout 1790, we can write
T(Equals(President(USA),GeorgeWashington),AD1790) .
We use the function symbol Equals rather than the standard logical predicate =, because
we cannot have a predicate as an argument to T , and because the interpretation is not that
GeorgeWashington and President(USA) are logically identical in 1790; logical identity is
not something that can change over time. The identity is between the subevents of each object
that are deﬁned by the period 1790.
12.4 M ENTAL EVENTS AND MENTAL OBJECTS
The agents we have constructed so far have beliefs and can deduce new beliefs. Yet none
of them has any knowledge about beliefs or about deduction. Knowledge about one’s own
knowledge and reasoning processes is useful for controlling inference. For example, suppose
Alice asks “what is the square root of 1764” and Bob replies “I don’t know.” If Alice insists
“think harder,” Bob should realize that with some more thought, this question can in fact
be answered. On the other hand, if the question were “Is your mother sitting down right
now?” then Bob should realize that thinking harder is unlikely to help. Knowledge about
the knowledge of other agents is also important; Bob should realize that his mother knows
whether she is sitting or not, and that asking her would be a way to ﬁnd out.
What we need is a model of the mental objects that are in someone’s head (or some-
thing’s knowledge base) and of the mental processes that manipulate those mental objects.
The model does not have to be detailed. We do not have to be able to predict how many
milliseconds it will take for a particular agent to make a deduction. We will be happy just to
be able to conclude that mother knows whether or not she is sitting.
We begin with the propositional attitudes that an agent can have toward mental ob-
PROPOSITIONAL
A TTITUDE
jects: attitudes such as Believes, Knows, Wants, Intends,a n dInforms. The difﬁculty is
that these attitudes do not behave like “normal” predicates. For example, suppose we try to
assert that Lois knows that Superman can ﬂy:
Knows(Lois,CanFly(Superman)).
One minor issue with this is that we normally think ofCanFly(Superman) as a sentence, but
here it appears as a term. That issue can be patched up just be reifying CanFly(Superman);
making it a ﬂuent. A more serious problem is that, if it is true that Superman is Clark Kent,
then we must conclude that Lois knows that Clark can ﬂy:
(Superman = Clark)∧Knows(Lois,CanFly(Superman))
|= Knows(Lois,CanFly(Clark)).
This is a consequence of the fact that equality reasoning is built into logic. Normally that is
a good thing; if our agent knows that 2+2=4 and 4 < 5, then we want our agent to know
Section 12.4. Mental Events and Mental Objects 451
that 2+2 < 5. This property is called referential transparency—it doesn’t matter whatREFERENTIAL
TRANSP ARENCY
term a logic uses to refer to an object, what matters is the object that the term names. But for
propositional attitudes likebelieves and knows, we would like to have referential opacity—the
terms used do matter, because not all agents know which terms are co-referential.
Modal logic is designed to address this problem. Regular logic is concerned with a sin-MODAL LOGIC
gle modality, the modality of truth, allowing us to express “ P is true.” Modal logic includes
special modal operators that take sentences (rather than terms) as arguments. For example,
“A knows P” is represented with the notationK
AP ,w h e r eK is the modal operator for knowl-
edge. It takes two arguments, an agent (written as the subscript) and a sentence. The syntax
of modal logic is the same as ﬁrst-order logic, except that sentences can also be formed with
modal operators.
The semantics of modal logic is more complicated. In ﬁrst-order logic a model con-
tains a set of objects and an interpretation that maps each name to the appropriate object,
relation, or function. In modal logic we want to be able to consider both the possibility that
Superman’s secret identity is Clark and that it isn’t. Therefore, we will need a more com-
plicated model, one that consists of a collection of possible worlds rather than just one true
POSSIBLE WORLD
world. The worlds are connected in a graph by accessibility relations, one relation for eachACCESSIBILITY
RELA TIONS
modal operator. We say that world w1 is accessible from world w0 with respect to the modal
operator KA if everything in w1 is consistent with what A knows in w0, and we write this
as Acc(KA,w0,w1). In diagrams such as Figure 12.4 we show accessibility as an arrow be-
tween possible worlds. As an example, in the real world, Bucharest is the capital of Romania,
but for an agent that did not know that, other possible worlds are accessible, including ones
where the capital of Romania is Sibiu or Soﬁa. Presumably a world where 2+2=5 would
not be accessible to any agent.
In general, a knowledge atom K
AP is true in world w if and only if P is true in every
world accessible from w. The truth of more complex sentences is derived by recursive appli-
cation of this rule and the normal rules of ﬁrst-order logic. That means that modal logic can
be used to reason about nested knowledge sentences: what one agent knows about another
agent’s knowledge. For example, we can say that, even though Lois doesn’t know whether
Superman’s secret identity is Clark Kent, she does know that Clark knows:
K
Lois[KClarkIdentity(Superman,Clark)∨KClark¬Identity(Superman,Clark)]
Figure 12.4 shows some possible worlds for this domain, with accessibility relations for Lois
and Superman.
In the TOP -LEFT diagram, it is common knowledge that Superman knows his own iden-
tity, and neither he nor Lois has seen the weather report. So in w0 the worlds w0 and w2 are
accessible to Superman; maybe rain is predicted, maybe not. For Lois all four worlds are ac-
cessible from each other; she doesn’t know anything about the report or if Clark is Superman.
But she does know that Superman knows whether he is Clark, because in every world that is
accessible to Lois, either Superman knows I, or he knows¬I. Lois does not know which is
the case, but either way she knows Superman knows.
In the TOP -RIGHT diagram it is common knowledge that Lois has seen the weather
report. So in w4 she knows rain is predicted and in w6 she knows rain is not predicted.
452 Chapter 12. Knowledge Representation
(a) (b)
(c)
w0: I,R
w2: I,¬Rw 3: ¬I,¬R
w1: ¬I,R w4: I,R
w6: I,¬Rw 7: ¬I,¬R
w5: ¬I,R
w0: I,R
w2: I,¬R w3: ¬I,¬R
w1: ¬I,R
w4: I,R w5: ¬I,R
w6: I,¬R w7: ¬I,¬R
Figure 12.4 Possible worlds with accessibility relations KSuperman (solid arrows) and
KLois (dotted arrows). The proposition R means “the weather report for tomorrow is rain”
and I means “Superman’s secret identity is Clark Kent.” All worlds are accessible to them-
selves; the arrows from a world to itself are not shown.
Superman does not know the report, but he knows that Lois knows, because in every world
that is accessible to him, either she knows R or she knows¬R.
In the BOTTOM diagram we represent the scenario where it is common knowledge that
Superman knows his identity, and Lois might or might not have seen the weather report. We
represent this by combining the two top scenarios, and adding arrows to show that Superman
does not know which scenario actually holds. Lois does know, so we don’t need to add any
arrows for her. In w
0 Superman still knows I but not R, and now he does not know whether
Lois knows R. From what Superman knows, he might be in w0 or w2, in which case Lois
does not know whether R is true, or he could be in w4, in which case she knows R,o r w6,i n
which case she knows¬R.
There are an inﬁnite number of possible worlds, so the trick is to introduce just the ones
you need to represent what you are trying to model. A new possible world is needed to talk
about different possible facts (e.g., rain is predicted or not), or to talk about different states
of knowledge (e.g., does Lois know that rain is predicted). That means two possible worlds,
such as w4 and w0 in Figure 12.4, might have the same base facts about the world, but differ
in their accessibility relations, and therefore in facts about knowledge.
Modal logic solves some tricky issues with the interplay of quantiﬁers and knowledge.
The English sentence “Bond knows that someone is a spy” is ambiguous. The ﬁrst reading is
Section 12.5. Reasoning Systems for Categories 453
that there is a particular someone who Bond knows is a spy; we can write this as
∃x KBondSpy(x),
which in modal logic means that there is an x that, in all accessible worlds, Bond knows to
be a spy. The second reading is that Bond just knows that there is at least one spy:
KBond∃x Spy(x).
The modal logic interpretation is that in each accessible world there is an x that is a spy, but
it need not be the same x in each world.
Now that we have a modal operator for knowledge, we can write axioms for it. First,
we can say that agents are able to draw deductions; if an agent knows P and knows that P
implies Q, then the agent knows Q:
(KaP∧Ka(P ⇒Q)) ⇒KaQ.
From this (and a few other rules about logical identities) we can establish that KA(P∨¬P)
is a tautology; every agent knows every proposition P is either true or false. On the other
hand, (KAP)∨(KA¬P) is not a tautology; in general, there will be lots of propositions that
an agent does not know to be true and does not know to be false.
It is said (going back to Plato) that knowledge is justiﬁed true belief. That is, if it is
true, if you believe it, and if you have an unassailably good reason, then you know it. That
means that if you know something, it must be true, and we have the axiom:
KaP ⇒P.
Furthermore, logical agents should be able to introspect on their own knowledge. If they
know something, then they know that they know it:
KaP ⇒Ka(KaP) .
We can deﬁne similar axioms for belief (often denoted by B) and other modalities. However,
one problem with the modal logic approach is that it assumes logical omniscience on theLOGICAL
OMNISCIENCE
part of agents. That is, if an agent knows a set of axioms, then it knows all consequences of
those axioms. This is on shaky ground even for the somewhat abstract notion of knowledge,
but it seems even worse for belief, because belief has more connotation of referring to things
that are physically represented in the agent, not just potentially derivable. There have been
attempts to deﬁne a form of limited rationality for agents; to say that agents believe those
assertions that can be derived with the application of no more than k reasoning steps, or no
more than s seconds of computation. These attempts have been generally unsatisfactory.
12.5 R EASONING SYSTEMS FOR CATEGORIES
Categories are the primary building blocks of large-scale knowledge representation schemes.
This section describes systems specially designed for organizing and reasoning with cate-
gories. There are two closely related families of systems: semantic networks provide graph-
ical aids for visualizing a knowledge base and efﬁcient algorithms for inferring properties
454 Chapter 12. Knowledge Representation
of an object on the basis of its category membership; and description logics provide a for-
mal language for constructing and combining category deﬁnitions and efﬁcient algorithms
for deciding subset and superset relationships between categories.
12.5.1 Semantic networks
In 1909, Charles S. Peirce proposed a graphical notation of nodes and edges calledexistential
graphs that he called “the logic of the future.” Thus began a long-running debate betweenEXISTENTIAL
GRAPHS
advocates of “logic” and advocates of “semantic networks.” Unfortunately, the debate ob-
scured the fact that semantics networks—at least those with well-deﬁned semantics— are a
form of logic. The notation that semantic networks provide for certain kinds of sentences
is often more convenient, but if we strip away the “human interface” issues, the underlying
concepts—objects, relations, quantiﬁcation, and so on—are the same.
There are many variants of semantic networks, but all are capable of representing in-
dividual objects, categories of objects, and relations among objects. A typical graphical no-
tation displays object or category names in ovals or boxes, and connects them with labeled
links. For example, Figure 12.5 has a MemberOf link between Mary and FemalePersons,
corresponding to the logical assertion Mary∈FemalePersons; similarly, the SisterOf link
between Mary and John corresponds to the assertion SisterOf (Mary,John). We can con-
nect categories using SubsetOf links, and so on. It is such fun drawing bubbles and arrows
that one can get carried away. For example, we know that persons have female persons as
mothers, so can we draw a HasMother link from Persons to FemalePersons? The answer
is no, becauseHasMother is a relation between a person and his or her mother, and categories
do not have mothers.
5
For this reason, we have used a special notation—the double-boxed link—in Figure 12.5.
This link asserts that
∀xx ∈Persons ⇒[∀y HasMother(x, y) ⇒y∈FemalePersons].
We might also want to assert that persons have two legs—that is,
∀xx ∈Persons ⇒Legs(x,2) .
As before, we need to be careful not to assert that a category has legs; the single-boxed link
in Figure 12.5 is used to assert properties of every member of a category.
The semantic network notation makes it convenient to perform inheritance reasoning
of the kind introduced in Section 12.2. For example, by virtue of being a person, Mary inherits
the property of having two legs. Thus, to ﬁnd out how many legs Mary has, the inheritance
algorithm follows the MemberOf link from Mary to the category she belongs to, and then
follows SubsetOf links up the hierarchy until it ﬁnds a category for which there is a boxed
Legs link—in this case, thePersons category. The simplicity and efﬁciency of this inference
5 Several early systems failed to distinguish between properties of members of a category and properties of the
category as a whole. This can lead directly to inconsistencies, as pointed out by Drew McDermott (1976) in his
article “Artiﬁcial Intelligence Meets Natural Stupidity.” Another common problem was the use of IsA links for
both subset and membership relations, in correspondence with English usage: “a cat is a mammal” and “Fiﬁ is a
cat.” See Exercise 12.22 for more on these issues.
Section 12.5. Reasoning Systems for Categories 455
Mammals
JohnMary
Persons
Male
Persons
Female
Persons
1
2
SubsetOf
SubsetOfSubsetOf
MemberOf MemberOf
SisterOf Legs
LegsHasMother
Figure 12.5 A semantic network with four objects (John, Mary, 1, and 2) and four cate-
gories. Relations are denoted by labeled links.
MemberOf
FlyEvents
Fly17
Shankar NewYork NewDelhi Yesterday
Agent
Origin Destination
During
Figure 12.6 A fragment of a semantic network showing the representation of the logical
assertion Fly(Shankar,NewYork,NewDelhi,Yesterday).
mechanism, compared with logical theorem proving, has been one of the main attractions of
semantic networks.
Inheritance becomes complicated when an object can belong to more than one category
or when a category can be a subset of more than one other category; this is calledmultiple in-
heritance. In such cases, the inheritance algorithm might ﬁnd two or more conﬂicting valuesMULTIPLE
INHERIT ANCE
answering the query. For this reason, multiple inheritance is banned in someobject-oriented
programming (OOP) languages, such as Java, that use inheritance in a class hierarchy. It is
usually allowed in semantic networks, but we defer discussion of that until Section 12.6.
The reader might have noticed an obvious drawback of semantic network notation, com-
pared to ﬁrst-order logic: the fact that links between bubbles represent only binary relations.
For example, the sentence Fly(Shankar,NewYork,NewDelhi,Yesterday) cannot be as-
serted directly in a semantic network. Nonetheless, we can obtain the effect of n-ary asser-
tions by reifying the proposition itself as an event belonging to an appropriate event category.
Figure 12.6 shows the semantic network structure for this particular event. Notice that the
restriction to binary relations forces the creation of a rich ontology of reiﬁed concepts.
Reiﬁcation of propositions makes it possible to represent every ground, function-free
atomic sentence of ﬁrst-order logic in the semantic network notation. Certain kinds of univer-
456 Chapter 12. Knowledge Representation
sally quantiﬁed sentences can be asserted using inverse links and the singly boxed and doubly
boxed arrows applied to categories, but that still leaves us a long way short of full ﬁrst-order
logic. Negation, disjunction, nested function symbols, and existential quantiﬁcation are all
missing. Now it is possible to extend the notation to make it equivalent to ﬁrst-order logic—as
in Peirce’s existential graphs—but doing so negates one of the main advantages of semantic
networks, which is the simplicity and transparency of the inference processes. Designers can
build a large network and still have a good idea about what queries will be efﬁcient, because
(a) it is easy to visualize the steps that the inference procedure will go through and (b) in some
cases the query language is so simple that difﬁcult queries cannot be posed. In cases where
the expressive power proves to be too limiting, many semantic network systems provide for
procedural attachment to ﬁll in the gaps. Procedural attachment is a technique whereby
a query about (or sometimes an assertion of) a certain relation results in a call to a special
procedure designed for that relation rather than a general inference algorithm.
One of the most important aspects of semantic networks is their ability to represent
default values for categories. Examining Figure 12.5 carefully, one notices that John has one
DEFAUL T VALUE
leg, despite the fact that he is a person and all persons have two legs. In a strictly logical KB,
this would be a contradiction, but in a semantic network, the assertion that all persons have
two legs has only default status; that is, a person is assumed to have two legs unless this is
contradicted by more speciﬁc information. The default semantics is enforced naturally by the
inheritance algorithm, because it follows links upwards from the object itself (John in this
case) and stops as soon as it ﬁnds a value. We say that the default is overridden by the more
OVERRIDING
speciﬁc value. Notice that we could also override the default number of legs by creating a
category of OneLeggedPersons, a subset of Persons of which John is a member.
We can retain a strictly logical semantics for the network if we say that the Legs asser-
tion for Persons includes an exception for John:
∀xx ∈Persons∧x̸= John ⇒Legs(x,2) .
For a ﬁxed network, this is semantically adequate but will be much less concise than the
network notation itself if there are lots of exceptions. For a network that will be updated with
more assertions, however, such an approach fails—we really want to say that any persons as
yet unknown with one leg are exceptions too. Section 12.6 goes into more depth on this issue
and on default reasoning in general.
12.5.2 Description logics
The syntax of ﬁrst-order logic is designed to make it easy to say things about objects. De-
scription logics are notations that are designed to make it easier to describe deﬁnitions andDESCRIPTION LOGIC
properties of categories. Description logic systems evolved from semantic networks in re-
sponse to pressure to formalize what the networks mean while retaining the emphasis on
taxonomic structure as an organizing principle.
The principal inference tasks for description logics are subsumption (checking if one
SUBSUMPTION
category is a subset of another by comparing their deﬁnitions) and classiﬁcation (checkingCLASSIFICA TION
whether an object belongs to a category).. Some systems also include consistency of a cate-
gory deﬁnition—whether the membership criteria are logically satisﬁable.
Section 12.5. Reasoning Systems for Categories 457
Concept → Thing| ConceptName
| And(Concept,... )
| All(RoleName,Concept)
| AtLeast(Integer,RoleName)
| AtMost(Integer,RoleName)
| Fills(RoleName,IndividualName,... )
| SameAs(Path,Path)
| OneOf(IndividualName,... )
Path → [RoleName,... ]
Figure 12.7 The syntax of descriptions in a subset of the CLASSIC language.
The CLASSIC language (Borgida et al., 1989) is a typical description logic. The syntax
of C LASSIC descriptions is shown in Figure 12.7. 6 For example, to say that bachelors are
unmarried adult males we would write
Bachelor = And(Unmarried,Adult,Male) .
The equivalent in ﬁrst-order logic would be
Bachelor(x) ⇔Unmarried(x)∧Adult(x)∧Male(x) .
Notice that the description logic has an an algebra of operations on predicates, which of
course we can’t do in ﬁrst-order logic. Any description in CLASSIC can be translated into an
equivalent ﬁrst-order sentence, but some descriptions are more straightforward in C LASSIC .
For example, to describe the set of men with at least three sons who are all unemployed
and married to doctors, and at most two daughters who are all professors in physics or math
departments, we would use
And(Man,AtLeast(3,Son),AtMost(2,Daughter),
All(Son,And(Unemployed,Married,All(Spouse,Doctor))),
All(Daughter,And(Professor,Fills(Department,Physics,Math)))) .
We leave it as an exercise to translate this into ﬁrst-order logic.
Perhaps the most important aspect of description logics is their emphasis on tractability
of inference. A problem instance is solved by describing it and then asking if it is subsumed
by one of several possible solution categories. In standard ﬁrst-order logic systems, predicting
the solution time is often impossible. It is frequently left to the user to engineer the represen-
tation to detour around sets of sentences that seem to be causing the system to take several
weeks to solve a problem. The thrust in description logics, on the other hand, is to ensure that
subsumption-testing can be solved in time polynomial in the size of the descriptions.
7
6 Notice that the language does not allow one to simply state that one concept, or category, is a subset of
another. This is a deliberate policy: subsumption between categories must be derivable from some aspects of the
descriptions of the categories. If not, then something is missing from the descriptions.
7 CLASSIC provides efﬁcient subsumption testing in practice, but the worst-case run time is exponential.
458 Chapter 12. Knowledge Representation
This sounds wonderful in principle, until one realizes that it can only have one of two
consequences: either hard problems cannot be stated at all, or they require exponentially
large descriptions! However, the tractability results do shed light on what sorts of constructs
cause problems and thus help the user to understand how different representations behave.
For example, description logics usually lack negation and disjunction. Each forces ﬁrst-
order logical systems to go through a potentially exponential case analysis in order to ensure
completeness. C
LASSIC allows only a limited form of disjunction in the Fills and OneOf
constructs, which permit disjunction over explicitly enumerated individuals but not over de-
scriptions. With disjunctive descriptions, nested deﬁnitions can lead easily to an exponential
number of alternative routes by which one category can subsume another.
12.6 R EASONING WITH DEFAULT INFORMATION
In the preceding section, we saw a simple example of an assertion with default status: people
have two legs. This default can be overridden by more speciﬁc information, such as that
Long John Silver has one leg. We saw that the inheritance mechanism in semantic networks
implements the overriding of defaults in a simple and natural way. In this section, we study
defaults more generally, with a view toward understanding the semantics of defaults rather
than just providing a procedural mechanism.
12.6.1 Circumscription and default logic
We have seen two examples of reasoning processes that violate themonotonicity property of
logic that was proved in Chapter 7. 8 In this chapter we saw that a property inherited by all
members of a category in a semantic network could be overridden by more speciﬁc informa-
tion for a subcategory. In Section 9.4.5, we saw that under the closed-world assumption, if a
proposition α is not mentioned in KB then KB|=¬α,b u tKB∧α|= α.
Simple introspection suggests that these failures of monotonicity are widespread in
commonsense reasoning. It seems that humans often “jump to conclusions.” For example,
when one sees a car parked on the street, one is normally willing to believe that it has four
wheels even though only three are visible. Now, probability theory can certainly provide a
conclusion that the fourth wheel exists with high probability, yet, for most people, the possi-
bility of the car’s not having four wheels does not arise unless some new evidence presents
itself. Thus, it seems that the four-wheel conclusion is reached by default, in the absence of
any reason to doubt it. If new evidence arrives—for example, if one sees the owner carrying
a wheel and notices that the car is jacked up—then the conclusion can be retracted. This kind
of reasoning is said to exhibit nonmonotonicity, because the set of beliefs does not grow
NONMONOTONICITY
monotonically over time as new evidence arrives. Nonmonotonic logics have been devisedNONMONOTONIC
LOGIC
with modiﬁed notions of truth and entailment in order to capture such behavior. We will look
at two such logics that have been studied extensively: circumscription and default logic.
8 Recall that monotonicity requires all entailed sentences to remain entailed after new sentences are added to the
KB. That is, if KB |= α then KB ∧ β |= α.
Section 12.6. Reasoning with Default Information 459
Circumscription can be seen as a more powerful and precise version of the closed-CIRCUMSCRIPTION
world assumption. The idea is to specify particular predicates that are assumed to be “as false
as possible”—that is, false for every object except those for which they are known to be true.
For example, suppose we want to assert the default rule that birds ﬂy. We would introduce a
predicate, say Abnormal
1(x), and write
Bird(x)∧¬Abnormal1(x) ⇒Flies(x) .
If we say that Abnormal1 is to be circumscribed, a circumscriptive reasoner is entitled to
assume¬Abnormal1(x) unless Abnormal1(x) is known to be true. This allows the con-
clusion Flies(Tweety) to be drawn from the premise Bird(Tweety), but the conclusion no
longer holds if Abnormal1(Tweety) is asserted.
Circumscription can be viewed as an example of a model preference logic. In suchMODEL
PREFERENCE
logics, a sentence is entailed (with default status) if it is true in allpreferred models of the KB,
as opposed to the requirement of truth in all models in classical logic. For circumscription,
one model is preferred to another if it has fewer abnormal objects. 9 Let us see how this idea
works in the context of multiple inheritance in semantic networks. The standard example for
which multiple inheritance is problematic is called the “Nixon diamond.” It arises from the
observation that Richard Nixon was both a Quaker (and hence by default a paciﬁst) and a
Republican (and hence by default not a paciﬁst). We can write this as follows:
Republican(Nixon)∧Quaker(Nixon) .
Republican(x)∧¬Abnormal
2(x) ⇒¬Paciﬁst(x) .
Quaker(x)∧¬Abnormal3(x) ⇒Paciﬁst(x) .
If we circumscribe Abnormal2 and Abnormal3, there are two preferred models: one in
which Abnormal2(Nixon) and Paciﬁst(Nixon) hold and one in whichAbnormal3(Nixon)
and¬Paciﬁst(Nixon) hold. Thus, the circumscriptive reasoner remains properly agnostic as
to whether Nixon was a paciﬁst. If we wish, in addition, to assert that religious beliefs take
precedence over political beliefs, we can use a formalism calledprioritized circumscriptionPRIORITIZED
CIRCUMSCRIPTION
to give preference to models where Abnormal3 is minimized.
Default logic is a formalism in which default rules can be written to generate contin-DEFAUL T LOGIC
DEFAUL T RULES gent, nonmonotonic conclusions. A default rule looks like this:
Bird(x): Flies(x)/Flies(x) .
This rule means that ifBird(x) is true, and ifFlies(x) is consistent with the knowledge base,
then Flies(x) may be concluded by default. In general, a default rule has the form
P : J1,...,J n/C
where P is called the prerequisite, C is the conclusion, and Ji are the justiﬁcations—if any
one of them can be proven false, then the conclusion cannot be drawn. Any variable that
9 For the closed-world assumption, one model is preferred to another if it has fewer true atoms—that is, preferred
models are minimal models. There is a natural connection between the closed-world assumption and deﬁnite-
clause KBs, because the ﬁxed point reached by forward chaining on deﬁnite-clause KBs is the unique minimal
model. See page 258 for more on this point.
460 Chapter 12. Knowledge Representation
appears in Ji or C must also appear in P . The Nixon-diamond example can be represented
in default logic with one fact and two default rules:
Republican(Nixon)∧Quaker(Nixon) .
Republican(x): ¬Paciﬁst(x)/¬Paciﬁst(x) .
Quaker(x): Paciﬁst(x)/Paciﬁst(x) .
To interpret what the default rules mean, we deﬁne the notion of an extension of a defaultEXTENSION
theory to be a maximal set of consequences of the theory. That is, an extension S consists
of the original known facts and a set of conclusions from the default rules, such that no
additional conclusions can be drawn fromS and the justiﬁcations of every default conclusion
in S are consistent with S. As in the case of the preferred models in circumscription, we have
two possible extensions for the Nixon diamond: one wherein he is a paciﬁst and one wherein
he is not. Prioritized schemes exist in which some default rules can be given precedence over
others, allowing some ambiguities to be resolved.
Since 1980, when nonmonotonic logics were ﬁrst proposed, a great deal of progress
has been made in understanding their mathematical properties. There are still unresolved
questions, however. For example, if “Cars have four wheels” is false, what does it mean
to have it in one’s knowledge base? What is a good set of default rules to have? If we
cannot decide, for each rule separately, whether it belongs in our knowledge base, then we
have a serious problem of nonmodularity. Finally, how can beliefs that have default status be
used to make decisions? This is probably the hardest issue for default reasoning. Decisions
often involve tradeoffs, and one therefore needs to compare the strengths of belief in the
outcomes of different actions, and the costs of making a wrong decision. In cases where the
same kinds of decisions are being made repeatedly, it is possible to interpret default rules
as “threshold probability” statements. For example, the default rule “My brakes are always
OK” really means “The probability that my brakes are OK, given no other information, is
sufﬁciently high that the optimal decision is for me to drive without checking them.” When
the decision context changes—for example, when one is driving a heavily laden truck down a
steep mountain road—the default rule suddenly becomes inappropriate, even though there is
no new evidence of faulty brakes. These considerations have led some researchers to consider
how to embed default reasoning within probability theory or utility theory.
12.6.2 Truth maintenance systems
We have seen that many of the inferences drawn by a knowledge representation system will
have only default status, rather than being absolutely certain. Inevitably, some of these in-
ferred facts will turn out to be wrong and will have to be retracted in the face of new informa-
tion. This process is called belief revision.
10 Suppose that a knowledge base KB containsBELIEF REVISION
a sentence P —perhaps a default conclusion recorded by a forward-chaining algorithm, or
perhaps just an incorrect assertion—and we want to execute T ELL (KB,¬P ). To avoid cre-
ating a contradiction, we must ﬁrst execute R ETRACT (KB, P ). This sounds easy enough.
10 Belief revision is often contrasted withbelief update, which occurs when a knowledge base is revised to reﬂect
a change in the world rather than new information about a ﬁxed world. Belief update combines belief revision
with reasoning about time and change; it is also related to the process of ﬁltering described in Chapter 15.
Section 12.6. Reasoning with Default Information 461
Problems arise, however, if any additional sentences were inferred from P and asserted in
the KB. For example, the implicationP ⇒Q might have been used to addQ. The obvious
“solution”—retracting all sentences inferred fromP —fails because such sentences may have
other justiﬁcations besides P . For example, if R and R ⇒Q are also in the KB, then Q
does not have to be removed after all. Truth maintenance systems, or TMSs, are designed
TRUTH
MAINTENANCE
SYSTEM
to handle exactly these kinds of complications.
One simple approach to truth maintenance is to keep track of the order in which sen-
tences are told to the knowledge base by numbering them from P1 to Pn. When the call
RETRACT (KB, Pi) is made, the system reverts to the state just before Pi was added, thereby
removing both Pi and any inferences that were derived fromPi. The sentences Pi+1 through
Pn can then be added again. This is simple, and it guarantees that the knowledge base will
be consistent, but retracting Pi requires retracting and reasserting n−i sentences as well as
undoing and redoing all the inferences drawn from those sentences. For systems to which
many facts are being added—such as large commercial databases—this is impractical.
A more efﬁcient approach is the justiﬁcation-based truth maintenance system, orJTMS.
JTMS
In a JTMS, each sentence in the knowledge base is annotated with a justiﬁcation consistingJUSTIFICA TION
of the set of sentences from which it was inferred. For example, if the knowledge base
already contains P ⇒Q,t h e nTELL(P) will cause Q to be added with the justiﬁcation
{P, P ⇒Q}. In general, a sentence can have any number of justiﬁcations. Justiﬁca-
tions make retraction efﬁcient. Given the call R ETRACT (P), the JTMS will delete exactly
those sentences for which P is a member of every justiﬁcation. So, if a sentence Q had
the single justiﬁcation {P, P ⇒Q}, it would be removed; if it had the additional justi-
ﬁcation{P, P ∨R ⇒ Q}, it would still be removed; but if it also had the justiﬁcation
{R, P ∨R ⇒Q}, then it would be spared. In this way, the time required for retraction ofP
depends only on the number of sentences derived from P rather than on the number of other
sentences added since P entered the knowledge base.
The JTMS assumes that sentences that are considered once will probably be considered
again, so rather than deleting a sentence from the knowledge base entirely when it loses
all justiﬁcations, we merely mark the sentence as being out of the knowledge base. If a
subsequent assertion restores one of the justiﬁcations, then we mark the sentence as being
back in. In this way, the JTMS retains all the inference chains that it uses and need not
rederive sentences when a justiﬁcation becomes valid again.
In addition to handling the retraction of incorrect information, TMSs can be used to
speed up the analysis of multiple hypothetical situations. Suppose, for example, that the
Romanian Olympic Committee is choosing sites for the swimming, athletics, and eques-
trian events at the 2048 Games to be held in Romania. For example, let the ﬁrst hypothe-
sis be Site(Swimming,Pitesti), Site(Athletics,Bucharest),a n dSite(Equestrian,Arad).
A great deal of reasoning must then be done to work out the logistical consequences and
hence the desirability of this selection. If we want to consider Site(Athletics,Sibiu) in-
stead, the TMS avoids the need to start again from scratch. Instead, we simply retract
Site(Athletics,Bucharest) and assert Site(Athletics,Sibiu) and the TMS takes care of the
necessary revisions. Inference chains generated from the choice of Bucharest can be reused
with Sibiu, provided that the conclusions are the same.
462 Chapter 12. Knowledge Representation
An assumption-based truth maintenance system, or AT M S, makes this type of context-AT M S
switching between hypothetical worlds particularly efﬁcient. In a JTMS, the maintenance of
justiﬁcations allows you to move quickly from one state to another by making a few retrac-
tions and assertions, but at any time only one state is represented. An ATMS representsall the
states that have ever been considered at the same time. Whereas a JTMS simply labels each
sentence as being in or out, an ATMS keeps track, for each sentence, of which assumptions
would cause the sentence to be true. In other words, each sentence has a label that consists of
a set of assumption sets. The sentence holds just in those cases in which all the assumptions
in one of the assumption sets hold.
Truth maintenance systems also provide a mechanism for generating explanations.
EXPLANA TION
Technically, an explanation of a sentence P is a set of sentences E such that E entails P .
If the sentences in E are already known to be true, then E simply provides a sufﬁcient ba-
sis for proving that P must be the case. But explanations can also include assumptions—ASSUMPTION
sentences that are not known to be true, but would sufﬁce to prove P if they were true. For
example, one might not have enough information to prove that one’s car won’t start, but a
reasonable explanation might include the assumption that the battery is dead. This, combined
with knowledge of how cars operate, explains the observed nonbehavior. In most cases, we
will prefer an explanation E that is minimal, meaning that there is no proper subset ofE that
is also an explanation. An ATMS can generate explanations for the “car won’t start” problem
by making assumptions (such as “gas in car” or “battery dead”) in any order we like, even if
some assumptions are contradictory. Then we look at the label for the sentence “car won’t
start” to read off the sets of assumptions that would justify the sentence.
The exact algorithms used to implement truth maintenance systems are a little compli-
cated, and we do not cover them here. The computational complexity of the truth maintenance
problem is at least as great as that of propositional inference—that is, NP-hard. Therefore,
you should not expect truth maintenance to be a panacea. When used carefully, however, a
TMS can provide a substantial increase in the ability of a logical system to handle complex
environments and hypotheses.
12.7 T HE INTERNET SHOPPING WORLD
In this ﬁnal section we put together all we have learned to encode knowledge for a shopping
research agent that helps a buyer ﬁnd product offers on the Internet. The shopping agent is
given a product description by the buyer and has the task of producing a list of Web pages
that offer such a product for sale, and ranking which offers are best. In some cases the
buyer’s product description will be precise, as in Canon Rebel XTi digital camera ,a n dt h e
task is then to ﬁnd the store(s) with the best offer. In other cases the description will be only
partially speciﬁed, as in digital camera for under
$300, and the agent will have to compare
different products.
The shopping agent’s environment is the entire World Wide Web in its full complexity—
not a toy simulated environment. The agent’s percepts are Web pages, but whereas a human
Section 12.7. The Internet Shopping World 463
Example Online Store
Select from our ﬁne line of products:
•Computers
•Cameras
•Books
•Videos
•Music
<h1>Example Online Store</h1>
<i>Select</i> from our fine line of products:
<ul>
<li> <a href="http://example.com/compu">Computers</a>
<li> <a href="http://example.com/camer">Cameras</a>
<li> <a href="http://example.com/books">Books</a>
<li> <a href="http://example.com/video">Videos</a>
<li> <a href="http://example.com/music">Music</a>
</ul>
Figure 12.8 A Web page from a generic online store in the form perceived by the human
user of a browser (top), and the corresponding HTML string as perceived by the browser or
the shopping agent (bottom). In HTML, characters between < and > are markup directives
that specify how the page is displayed. For example, the string <i>Select</i> means
to switch to italic font, display the word Select, and then end the use of italic font. A page
identiﬁer such as http://example.com/books is called a uniform resource locator
(URL). The markup <a href=" url">Books</a> means to create a hypertext link to url
with the anchor text Books.
Web user would see pages displayed as an array of pixels on a screen, the shopping agent
will perceive a page as a character string consisting of ordinary words interspersed with for-
matting commands in the HTML markup language. Figure 12.8 shows a Web page and a
corresponding HTML character string. The perception problem for the shopping agent in-
volves extracting useful information from percepts of this kind.
Clearly, perception on Web pages is easier than, say, perception while driving a taxi in
Cairo. Nonetheless, there are complications to the Internet perception task. The Web page in
Figure 12.8 is simple compared to real shopping sites, which may include CSS, cookies, Java,
Javascript, Flash, robot exclusion protocols, malformed HTML, sound ﬁles, movies, and text
that appears only as part of a JPEG image. An agent that can deal with all of the Internet is
almost as complex as a robot that can move in the real world. We concentrate on a simple
agent that ignores most of these complications.
The agent’s ﬁrst task is to collect product offers that are relevant to a query. If the query
is “laptops,” then a Web page with a review of the latest high-end laptop would be relevant,
but if it doesn’t provide a way to buy, it isn’t an offer. For now, we can say a page is an offer
if it contains the words “buy” or “price” or “add to cart” within an HTML link or form on the
464 Chapter 12. Knowledge Representation
page. For example, if the page contains a string of the form “<a ... add to cart ... </a”
then it is an offer. This could be represented in ﬁrst-order logic, but it is more straightforward
to encode it into program code. We show how to do more sophisticated information extraction
in Section 22.4.
12.7.1 Following links
The strategy is to start at the home page of an online store and consider all pages that can be
reached by following relevant links.11 The agent will have knowledge of a number of stores,
for example:
Amazon∈OnlineStores∧Homepage(Amazon, “amazon.com”) .
Ebay∈OnlineStores∧Homepage(Ebay, “ebay.com”) .
ExampleStore∈OnlineStores∧Homepage(ExampleStore, “example.com”) .
These stores classify their goods into product categories, and provide links to the major cat-
egories from their home page. Minor categories can be reached through a chain of relevant
links, and eventually we will reach offers. In other words, a page is relevant to the query if it
can be reached by a chain of zero or more relevant category links from a store’s home page,
and then from one more link to the product offer. We can deﬁne relevance:
Relevant(page,query) ⇔
∃store,home store ∈OnlineStores∧Homepage(store,home)
∧∃url,url
2 RelevantChain(home,url2,query)∧Link(url2,url)
∧page = Contents(url) .
Here the predicate Link(from,to) means that there is a hyperlink from the from URL to
the to URL. To deﬁne what counts as a RelevantChain, we need to follow not just any old
hyperlinks, but only those links whose associated anchor text indicates that the link is relevant
to the product query. For this, we use LinkText(from,to,text) to mean that there is a link
between from and to with text as the anchor text. A chain of links between two URLs, start
and end, is relevant to a description d if the anchor text of each link is a relevant category
name for d. The existence of the chain itself is determined by a recursive deﬁnition, with the
empty chain (start =end) as the base case:
RelevantChain(start,end,query) ⇔(start = end)
∨(∃u,text LinkText (start,u ,text)∧RelevantCategoryName(query,text)
∧RelevantChain(u,end,query)) .
Now we must deﬁne what it means for text to be a RelevantCategoryName
for query.
First, we need to relate strings to the categories they name. This is done using the predicate
Name(s,c), which says that string s is a name for category c—for example, we might assert
that Name(“laptops”,LaptopComputers). Some more examples of the Name predicate
appear in Figure 12.9(b). Next, we deﬁne relevance. Suppose that query is “laptops.” Then
RelevantCategoryName(query,text) is true when one of the following holds:
•The text and query name the same category—e.g., “notebooks” and “laptops.”
11 An alternative to the link-following strategy is to use an Internet search engine; the technology behind Internet
search, information retrieval, will be covered in Section 22.3.
Section 12.7. The Internet Shopping World 465
Books⊂Products
MusicRecordings⊂Products
MusicCDs⊂MusicRecordings
Electronics⊂Products
DigitalCameras⊂Electronics
StereoEquipment⊂Electronics
Computers⊂Electronics
DesktopComputers⊂Computers
LaptopComputers⊂Computers
...
Name(“books”,Books)
Name(“music”,MusicRecordings)
Name(“CDs”,MusicCDs)
Name(“electronics”,Electronics)
Name(“digital cameras”,DigitalCameras)
Name(“stereos”,StereoEquipment)
Name(“computers”,Computers)
Name(“desktops”,DesktopComputers)
Name(“laptops”,LaptopComputers)
Name(“notebooks”,LaptopComputers)
...
(a) (b)
Figure 12.9 (a) Taxonomy of product categories. (b) Names for those categories.
•The text names a supercategory such as “computers.”
•The text names a subcategory such as “ultralight notebooks.”
The logical deﬁnition of RelevantCategoryName is as follows:
RelevantCategoryName(query,text) ⇔
∃c1,c2 Name(query,c1)∧Name(text,c2)∧(c1 ⊆c2∨c2 ⊆c1) . (12.1)
Otherwise, the anchor text is irrelevant because it names a category outside this line, such as
“clothes” or “lawn & garden.”
To follow relevant links, then, it is essential to have a rich hierarchy of product cate-
gories. The top part of this hierarchy might look like Figure 12.9(a). It will not be feasible to
list all possible shopping categories, because a buyer could always come up with some new
desire and manufacturers will always come out with new products to satisfy them (electric
kneecap warmers?). Nonetheless, an ontology of about a thousand categories will serve as a
very useful tool for most buyers.
In addition to the product hierarchy itself, we also need to have a rich vocabulary of
names for categories. Life would be much easier if there were a one-to-one correspon-
dence between categories and the character strings that name them. We have already seen
the problem of synonymy—two names for the same category, such as “laptop computers”
and “laptops.” There is also the problem of ambiguity—one name for two or more different
categories. For example, if we add the sentence
Name(“CDs”,CertiﬁcatesOfDeposit)
to the knowledge base in Figure 12.9(b), then “CDs” will name two different categories.
Synonymy and ambiguity can cause a signiﬁcant increase in the number of paths that
the agent has to follow, and can sometimes make it difﬁcult to determine whether a given
page is indeed relevant. A much more serious problem is the very broad range of descriptions
that a user can type and category names that a store can use. For example, the link might say
“laptop” when the knowledge base has only “laptops” or the user might ask for “a computer
466 Chapter 12. Knowledge Representation
I can ﬁt on the tray table of an economy-class airline seat.” It is impossible to enumerate in
advance all the ways a category can be named, so the agent will have to be able to do addi-
tional reasoning in some cases to determine if theName relation holds. In the worst case, this
requires full natural language understanding, a topic that we will defer to Chapter 22. In prac-
tice, a few simple rules—such as allowing “laptop” to match a category named “laptops”—go
a long way. Exercise 12.10 asks you to develop a set of such rules after doing some research
into online stores.
Given the logical deﬁnitions from the preceding paragraphs and suitable knowledge
bases of product categories and naming conventions, are we ready to apply an inference
algorithm to obtain a set of relevant offers for our query? Not quite! The missing element
is the Contents(url) function, which refers to the HTML page at a given URL. The agent
doesn’t have the page contents of every URL in its knowledge base; nor does it have explicit
rules for deducing what those contents might be. Instead, we can arrange for the right HTTP
procedure to be executed whenever a subgoal involves the Contents function. In this way, it
appears to the inference engine as if the entire Web is inside the knowledge base. This is an
example of a general technique calledprocedural attachment, whereby particular predicates
PROCEDURAL
A TT ACHMENT
and functions can be handled by special-purpose methods.
12.7.2 Comparing offers
Let us assume that the reasoning processes of the preceding section have produced a set of
offer pages for our “laptops” query. To compare those offers, the agent must extract the rele-
vant information—price, speed, disk size, weight, and so on—from the offer pages. This can
be a difﬁcult task with real Web pages, for all the reasons mentioned previously. A common
way of dealing with this problem is to use programs called wrappers to extract information
WRAPPER
from a page. The technology of information extraction is discussed in Section 22.4. For
now we assume that wrappers exist, and when given a page and a knowledge base, they add
assertions to the knowledge base. Typically, a hierarchy of wrappers would be applied to a
page: a very general one to extract dates and prices, a more speciﬁc one to extract attributes
for computer-related products, and if necessary a site-speciﬁc one that knows the format of a
particular store. Given a page on the example.com site with the text
IBM ThinkBook 970. Our price: $399.00
followed by various technical speciﬁcations, we would like a wrapper to extract information
such as the following:
∃c,oﬀer c∈LaptopComputers∧oﬀer∈ProductOﬀers∧
Manufacturer(c,IBM )∧Model(c,ThinkBook970)∧
ScreenSize(c,Inches(14))∧ScreenType(c,ColorLCD)∧
MemorySize(c,Gigabytes(2))∧CPUSpeed(c,GHz(1.2))∧
OﬀeredProduct(oﬀer,c)∧Store(oﬀer,GenStore)∧
URL(oﬀer, “example.com/computers/34356.html”)∧
Price(oﬀer,$(399))∧Date(oﬀer,Today) .
This example illustrates several issues that arise when we take seriously the task of knowledge
engineering for commercial transactions. For example, notice that the price is an attribute of
Section 12.8. Summary 467
the offer, not the product itself. This is important because the offer at a given store may
change from day to day even for the same individual laptop; for some categories—such as
houses and paintings—the same individual object may even be offered simultaneously by
different intermediaries at different prices. There are still more complications that we have
not handled, such as the possibility that the price depends on the method of payment and on
the buyer’s qualiﬁcations for certain discounts. The ﬁnal task is to compare the offers that
have been extracted. For example, consider these three offers:
A : 1.4 GHz CPU, 2GB RAM, 250 GB disk, $299 .
B : 1.2 GHz CPU, 4GB RAM, 350 GB disk, $500 .
C : 1.2 GHz CPU, 2GB RAM, 250 GB disk, $399 .
C is dominated by A;t h a ti s ,A is cheaper and faster, and they are otherwise the same. In
general, X dominates Y if X has a better value on at least one attribute, and is not worse on
any attribute. But neither A nor B dominates the other. To decide which is better we need
to know how the buyer weighs CPU speed and price against memory and disk space. The
general topic of preferences among multiple attributes is addressed in Section 16.4; for now,
our shopping agent will simply return a list of all undominated offers that meet the buyer’s
description. In this example, both A and B are undominated. Notice that this outcome relies
on the assumption that everyone prefers cheaper prices, faster processors, and more storage.
Some attributes, such as screen size on a notebook, depend on the user’s particular preference
(portability versus visibility); for these, the shopping agent will just have to ask the user.
The shopping agent we have described here is a simple one; many reﬁnements are
possible. Still, it has enough capability that with the right domain-speciﬁc knowledge it can
actually be of use to a shopper. Because of its declarative construction, it extends easily to
more complex applications. The main point of this section is to show that some knowledge
representation—in particular, the product hierarchy—is necessary for such an agent, and that
once we have some knowledge in this form, the rest follows naturally.
12.8 S UMMARY
By delving into the details of how one represents a variety of knowledge, we hope we have
given the reader a sense of how real knowledge bases are constructed and a feeling for the
interesting philosophical issues that arise. The major points are as follows:
•Large-scale knowledge representation requires a general-purpose ontology to organize
and tie together the various speciﬁc domains of knowledge.
•A general-purpose ontology needs to cover a wide variety of knowledge and should be
capable, in principle, of handling any domain.
•Building a large, general-purpose ontology is a signiﬁcant challenge that has yet to be
fully realized, although current frameworks seem to be quite robust.
•We presented an upper ontology based on categories and the event calculus. We
covered categories, subcategories, parts, structured objects, measurements, substances,
events, time and space, change, and beliefs.
468 Chapter 12. Knowledge Representation
•Natural kinds cannot be deﬁned completely in logic, but properties of natural kinds can
be represented.
•Actions, events, and time can be represented either in situation calculus or in more
expressive representations such as event calculus. Such representations enable an agent
to construct plans by logical inference.
•We presented a detailed analysis of the Internet shopping domain, exercising the general
ontology and showing how the domain knowledge can be used by a shopping agent.
•Special-purpose representation systems, such as semantic networks and description
logics, have been devised to help in organizing a hierarchy of categories. Inheritance
is an important form of inference, allowing the properties of objects to be deduced from
their membership in categories.
•The closed-world assumption, as implemented in logic programs, provides a simple
way to avoid having to specify lots of negative information. It is best interpreted as a
default that can be overridden by additional information.
•Nonmonotonic logics,s u c ha scircumscription and default logic, are intended to cap-
ture default reasoning in general.
•Truth maintenance systems handle knowledge updates and revisions efﬁciently.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Briggs (1985) claims that formal knowledge representation research began with classical In-
dian theorizing about the grammar of Shastric Sanskrit, which dates back to the ﬁrst millen-
nium
B.C. In the West, the use of deﬁnitions of terms in ancient Greek mathematics can be
regarded as the earliest instance: Aristotle’sMetaphysics (literally, what comes after the book
on physics) is a near-synonym for Ontology. Indeed, the development of technical terminol-
ogy in any ﬁeld can be regarded as a form of knowledge representation.
Early discussions of representation in AI tended to focus on “ problem representation”
rather than “knowledge representation.” (See, for example, Amarel’s (1968) discussion of the
Missionaries and Cannibals problem.) In the 1970s, AI emphasized the development of “ex-
pert systems” (also called “knowledge-based systems”) that could, if given the appropriate
domain knowledge, match or exceed the performance of human experts on narrowly deﬁned
tasks. For example, the ﬁrst expert system, D
ENDRAL (Feigenbaum et al. , 1971; Lindsay
et al., 1980), interpreted the output of a mass spectrometer (a type of instrument used to ana-
lyze the structure of organic chemical compounds) as accurately as expert chemists. Although
the success of D
ENDRAL was instrumental in convincing the AI research community of the
importance of knowledge representation, the representational formalisms used in D ENDRAL
are highly speciﬁc to the domain of chemistry. Over time, researchers became interested in
standardized knowledge representation formalisms and ontologies that could streamline the
process of creating new expert systems. In so doing, they ventured into territory previously
explored by philosophers of science and of language. The discipline imposed in AI by the
need for one’s theories to “work” has led to more rapid and deeper progress than was the case
Bibliographical and Historical Notes 469
when these problems were the exclusive domain of philosophy (although it has at times also
led to the repeated reinvention of the wheel).
The creation of comprehensive taxonomies or classiﬁcations dates back to ancient times.
Aristotle (384–322 B.C.) strongly emphasized classiﬁcation and categorization schemes. His
Organon, a collection of works on logic assembled by his students after his death, included a
treatise called Categories in which he attempted to construct what we would now call an upper
ontology. He also introduced the notions of genus and species for lower-level classiﬁcation.
Our present system of biological classiﬁcation, including the use of “binomial nomenclature”
(classiﬁcation via genus and species in the technical sense), was invented by the Swedish
biologist Carolus Linnaeus, or Carl von Linne (1707–1778). The problems associated with
natural kinds and inexact category boundaries have been addressed by Wittgenstein (1953),
Quine (1953), Lakoff (1987), and Schwartz (1977), among others.
Interest in larger-scale ontologies is increasing, as documented by the Handbook on
Ontologies (Staab, 2004). The O
PEN CYC project (Lenat and Guha, 1990; Matuszek et al.,
2006) has released a 150,000-concept ontology, with an upper ontology similar to the one in
Figure 12.1 as well as speciﬁc concepts like “OLED Display” and “iPhone,” which is a type
of “cellular phone,” which in turn is a type of “consumer electronics,” “phone,” “wireless
communication device,” and other concepts. The DB
PEDIA project extracts structured data
from Wikipedia; speciﬁcally from Infoboxes: the boxes of attribute/value pairs that accom-
pany many Wikipedia articles (Wu and Weld, 2008; Bizer et al. , 2007). As of mid-2009,
DB
PEDIA contains 2.6 million concepts, with about 100 facts per concept. The IEEE work-
ing group P1600.1 created the Suggested Upper Merged Ontology (SUMO) (Niles and Pease,
2001; Pease and Niles, 2002), which contains about 1000 terms in the upper ontology and
links to over 20,000 domain-speciﬁc terms. Stoffel et al. (1997) describe algorithms for ef-
ﬁciently managing a very large ontology. A survey of techniques for extracting knowledge
from Web pages is given by Etzioni et al. (2008).
On the Web, representation languages are emerging. RDF (Brickley and Guha, 2004)
allows for assertions to be made in the form of relational triples, and provides some means
for evolving the meaning of names over time. OWL (Smithet al., 2004) is a description logic
that supports inferences over these triples. So far, usage seems to be inversely proportional to
representational complexity: the traditional HTML and CSS formats account for over 99% of
Web content, followed by the simplest representation schemes, such as microformats (Khare,
2006) and RDFa (Adida and Birbeck, 2008), which use HTML and XHTML markup to
add attributes to literal text. Usage of sophisticated RDF and OWL ontologies is not yet
widespread, and the full vision of the Semantic Web (Berners-Lee et al., 2001) has not yet
been realized. The conferences on F ormal Ontology in Information Systems (FOIS) contain
many interesting papers on both general and domain-speciﬁc ontologies.
The taxonomy used in this chapter was developed by the authors and is based in part
on their experience in the CYC project and in part on work by Hwang and Schubert (1993)
and Davis (1990, 2005). An inspirational discussion of the general project of commonsense
knowledge representation appears in Hayes’s (1978, 1985b) “Naive Physics Manifesto.”
Successful deep ontologies within a speciﬁc ﬁeld include the Gene Ontology project
(Consortium, 2008) and CML, the Chemical Markup Language (Murray-Rust et al., 2003).
470 Chapter 12. Knowledge Representation
Doubts about the feasibility of a single ontology for all knowledge are expressed by
Doctorow (2001), Gruber (2004), Halevy et al. (2009), and Smith (2004), who states, “the
initial project of building one single ontology . . . has . . . largely been abandoned.”
The event calculus was introduced by Kowalski and Sergot (1986) to handle continuous
time, and there have been several variations (Sadri and Kowalski, 1995; Shanahan, 1997) and
overviews (Shanahan, 1999; Mueller, 2006). van Lambalgen and Hamm (2005) show how
the logic of events maps onto the language we use to talk about events. An alternative to the
event and situation calculi is the ﬂuent calculus (Thielscher, 1999). James Allen introduced
time intervals for the same reason (Allen, 1984), arguing that intervals were much more natu-
ral than situations for reasoning about extended and concurrent events. Peter Ladkin (1986a,
1986b) introduced “concave” time intervals (intervals with gaps; essentially, unions of ordi-
nary “convex” time intervals) and applied the techniques of mathematical abstract algebra to
time representation. Allen (1991) systematically investigates the wide variety of techniques
available for time representation; van Beek and Manchak (1996) analyze algorithms for tem-
poral reasoning. There are signiﬁcant commonalities between the event-based ontology given
in this chapter and an analysis of events due to the philosopher Donald Davidson (1980).
The histories in Pat Hayes’s (1985a) ontology of liquids and the chronicles in McDermott’s
(1985) theory of plans were also important inﬂuences on the ﬁeld and this chapter.
The question of the ontological status of substances has a long history. Plato proposed
that substances were abstract entities entirely distinct from physical objects; he would say
MadeOf (Butter
3,Butter) rather than Butter3∈Butter. This leads to a substance hierar-
chy in which, for example,UnsaltedButter is a more speciﬁc substance thanButter. The po-
sition adopted in this chapter, in which substances are categories of objects, was championed
by Richard Montague (1973). It has also been adopted in the CYC project. Copeland (1993)
mounts a serious, but not invincible, attack. The alternative approach mentioned in the chap-
ter, in which butter is one object consisting of all buttery objects in the universe, was proposed
originally by the Polish logician Le´sniewski (1916). His mereology (the name is derived from
MEREOLOGY
the Greek word for “part”) used the part–whole relation as a substitute for mathematical set
theory, with the aim of eliminating abstract entities such as sets. A more readable exposition
of these ideas is given by Leonard and Goodman (1940), and Goodman’s The Structure of
Appearance (1977) applies the ideas to various problems in knowledge representation. While
some aspects of the mereological approach are awkward—for example, the need for a sepa-
rate inheritance mechanism based on part–whole relations—the approach gained the support
of Quine (1960). Harry Bunt (1985) has provided an extensive analysis of its use in knowl-
edge representation. Casati and Varzi (1999) cover parts, wholes, and the spatial locations.
Mental objects have been the subject of intensive study in philosophy and AI. There
are three main approaches. The one taken in this chapter, based on modal logic and possible
worlds, is the classical approach from philosophy (Hintikka, 1962; Kripke, 1963; Hughes
and Cresswell, 1996). The book Reasoning about Knowledge (Fagin et al., 1995) provides a
thorough introduction. The second approach is a ﬁrst-order theory in which mental objects
are ﬂuents. Davis (2005) and Davis and Morgenstern (2005) describe this approach. It relies
on the possible-worlds formalism, and builds on work by Robert Moore (1980, 1985). The
third approach is a syntactic theory, in which mental objects are represented by character
SYNTACTIC THEORY
Bibliographical and Historical Notes 471
strings. A string is just a complex term denoting a list of symbols, so CanFly(Clark) can
be represented by the list of symbols [C,a,n,F,l,y, (,C,l,a ,r ,k, )]. The syntactic theory
of mental objects was ﬁrst studied in depth by Kaplan and Montague (1960), who showed
that it led to paradoxes if not handled carefully. Ernie Davis (1990) provides an excellent
comparison of the syntactic and modal theories of knowledge.
The Greek philosopher Porphyry (c. 234–305
A.D.), commenting on Aristotle’s Cat-
egories, drew what might qualify as the ﬁrst semantic network. Charles S. Peirce (1909)
developed existential graphs as the ﬁrst semantic network formalism using modern logic.
Ross Quillian (1961), driven by an interest in human memory and language processing, ini-
tiated work on semantic networks within AI. An inﬂuential paper by Marvin Minsky (1975)
presented a version of semantic networks called frames; a frame was a representation of
an object or category, with attributes and relations to other objects or categories. The ques-
tion of semantics arose quite acutely with respect to Quillian’s semantic networks (and those
of others who followed his approach), with their ubiquitous and very vague “IS-A links”
Woods’s (1975) famous article “What’s In a Link?” drew the attention of AI researchers to the
need for precise semantics in knowledge representation formalisms. Brachman (1979) elab-
orated on this point and proposed solutions. Patrick Hayes’s (1979) “The Logic of Frames”
cut even deeper, claiming that “Most of ‘frames’ is just a new syntax for parts of ﬁrst-order
logic.” Drew McDermott’s (1978b) “Tarskian Semantics, or, No Notation without Denota-
tion!” argued that the model-theoretic approach to semantics used in ﬁrst-order logic should
be applied to all knowledge representation formalisms. This remains a controversial idea;
notably, McDermott himself has reversed his position in “A Critique of Pure Reason” (Mc-
Dermott, 1987). Selman and Levesque (1993) discuss the complexity of inheritance with
exceptions, showing that in most formulations it is NP-complete.
The development of description logics is the most recent stage in a long line of re-
search aimed at ﬁnding useful subsets of ﬁrst-order logic for which inference is computa-
tionally tractable. Hector Levesque and Ron Brachman (1987) showed that certain logical
constructs—notably, certain uses of disjunction and negation—were primarily responsible
for the intractability of logical inference. Building on the KL-O
NE system (Schmolze and
Lipkis, 1983), several researchers developed systems that incorporate theoretical complex-
ity analysis, most notably K
RYPTON (Brachman et al. , 1983) and Classic (Borgida et al. ,
1989). The result has been a marked increase in the speed of inference and a much better
understanding of the interaction between complexity and expressiveness in reasoning sys-
tems. Calvanese et al. (1999) summarize the state of the art, and Baader et al. (2007) present
a comprehensive handbook of description logic. Against this trend, Doyle and Patil (1991)
have argued that restricting the expressiveness of a language either makes it impossible to
solve certain problems or encourages the user to circumvent the language restrictions through
nonlogical means.
The three main formalisms for dealing with nonmonotonic inference—circumscription
(McCarthy, 1980), default logic (Reiter, 1980), and modal nonmonotonic logic (McDermott
and Doyle, 1980)—were all introduced in one special issue of the AI Journal. Delgrande and
Schaub (2003) discuss the merits of the variants, given 25 years of hindsight. Answer set
programming can be seen as an extension of negation as failure or as a reﬁnement of circum-
472 Chapter 12. Knowledge Representation
scription; the underlying theory of stable model semantics was introduced by Gelfond and
Lifschitz (1988), and the leading answer set programming systems areDLV (Eiter et al., 1998)
and SMODELS (Niemel¨a et al., 2000). The disk drive example comes from the SMODELS user
manual (Syrj¨anen, 2000). Lifschitz (2001) discusses the use of answer set programming for
planning. Brewka et al. (1997) give a good overview of the various approaches to nonmono-
tonic logic. Clark (1978) covers the negation-as-failure approach to logic programming and
Clark completion. Van Emden and Kowalski (1976) show that every Prolog program without
negation has a unique minimal model. Recent years have seen renewed interest in applica-
tions of nonmonotonic logics to large-scale knowledge representation systems. The B
ENINQ
systems for handling insurance-beneﬁt inquiries was perhaps the ﬁrst commercially success-
ful application of a nonmonotonic inheritance system (Morgenstern, 1998). Lifschitz (2001)
discusses the application of answer set programming to planning. A variety of nonmonotonic
reasoning systems based on logic programming are documented in the proceedings of the
conferences on Logic Programming and Nonmonotonic Reasoning (LPNMR).
The study of truth maintenance systems began with the TMS (Doyle, 1979) and RUP
(McAllester, 1980) systems, both of which were essentially JTMSs. Forbus and de Kleer
(1993) explain in depth how TMSs can be used in AI applications. Nayak and Williams
(1997) show how an efﬁcient incremental TMS called an ITMS makes it feasible to plan the
operations of a NASA spacecraft in real time.
This chapter could not coverevery area of knowledge representation in depth. The three
principal topics omitted are the following:
Qualitative physics: Qualitative physics is a subﬁeld of knowledge representation concerned
QUALIT A TIVE
PHYSICS
speciﬁcally with constructing a logical, nonnumeric theory of physical objects and processes.
The term was coined by Johan de Kleer (1975), although the enterprise could be said to
have started in Fahlman’s (1974) B UILD , a sophisticated planner for constructing complex
towers of blocks. Fahlman discovered in the process of designing it that most of the effort
(80%, by his estimate) went into modeling the physics of the blocks world to calculate the
stability of various subassemblies of blocks, rather than into planning per se. He sketches a
hypothetical naive-physics-like process to explain why young children can solve BUILD -like
problems without access to the high-speed ﬂoating-point arithmetic used in BUILD ’s physical
modeling. Hayes (1985a) uses “histories”—four-dimensional slices of space-time similar to
Davidson’s events—to construct a fairly complex naive physics of liquids. Hayes was the
ﬁrst to prove that a bath with the plug in will eventually overﬂow if the tap keeps running and
that a person who falls into a lake will get wet all over. Davis (2008) gives an update to the
ontology of liquids that describes the pouring of liquids into containers.
De Kleer and Brown (1985), Ken Forbus (1985), and Benjamin Kuipers (1985) inde-
pendently and almost simultaneously developed systems that can reason about a physical
system based on qualitative abstractions of the underlying equations. Qualitative physics
soon developed to the point where it became possible to analyze an impressive variety of
complex physical systems (Yip, 1991). Qualitative techniques have been used to construct
novel designs for clocks, windshield wipers, and six-legged walkers (Subramanian and Wang,
1994). The collection Readings in Qualitative Reasoning about Physical Systems (Weld and
Exercises 473
de Kleer, 1990) an encyclopedia article by Kuipers (2001), and a handbook article by Davis
(2007) introduce to the ﬁeld.
Spatial reasoning: The reasoning necessary to navigate in the wumpus world and shoppingSP A TIAL REASONING
world is trivial in comparison to the rich spatial structure of the real world. The earliest
serious attempt to capture commonsense reasoning about space appears in the work of Ernest
Davis (1986, 1990). The region connection calculus of Cohn et al. (1997) supports a form of
qualitative spatial reasoning and has led to new kinds of geographical information systems;
see also (Davis, 2006). As with qualitative physics, an agent can go a long way, so to speak,
without resorting to a full metric representation. When such a representation is necessary,
techniques developed in robotics (Chapter 25) can be used.
Psychological reasoning: Psychological reasoning involves the development of a working
PSYCHOLOGICAL
REASONING
psychology for artiﬁcial agents to use in reasoning about themselves and other agents. This
is often based on so-called folk psychology, the theory that humans in general are believed
to use in reasoning about themselves and other humans. When AI researchers provide their
artiﬁcial agents with psychological theories for reasoning about other agents, the theories are
frequently based on the researchers’ description of the logical agents’ own design. Psycholog-
ical reasoning is currently most useful within the context of natural language understanding,
where divining the speaker’s intentions is of paramount importance.
Minker (2001) collects papers by leading researchers in knowledge representation, sum-
marizing 40 years of work in the ﬁeld. The proceedings of the international conferences on
Principles of Knowledge Representation and Reasoning provide the most up-to-date sources
f o rw o r ki nt h i sa r e a .Readings in Knowledge Representation (Brachman and Levesque,
1985) and F ormal Theories of the Commonsense World (Hobbs and Moore, 1985) are ex-
cellent anthologies on knowledge representation; the former focuses more on historically
important papers in representation languages and formalisms, the latter on the accumulation
of the knowledge itself. Davis (1990), Steﬁk (1995), and Sowa (1999) provide textbook in-
troductions to knowledge representation, van Harmelen et al. (2007) contributes a handbook,
and a special issue of AI Journal covers recent progress (Davis and Morgenstern, 2004). The
biennial conference on Theoretical Aspects of Reasoning About Knowledge (TARK) covers
applications of the theory of knowledge in AI, economics, and distributed systems.
EXERCISES
12.1 Deﬁne an ontology in ﬁrst-order logic for tic-tac-toe. The ontology should contain
situations, actions, squares, players, marks (X, O, or blank), and the notion of winning, losing,
or drawing a game. Also deﬁne the notion of a forced win (or draw): a position from which
a player can force a win (or draw) with the right sequence of actions. Write axioms for the
domain. (Note: The axioms that enumerate the different squares and that characterize the
winning positions are rather long. You need not write these out in full, but indicate clearly
what they look like.)
474 Chapter 12. Knowledge Representation
12.2 Figure 12.1 shows the top levels of a hierarchy for everything. Extend it to include
as many real categories as possible. A good way to do this is to cover all the things in your
everyday life. This includes objects and events. Start with waking up, and proceed in an
orderly fashion noting everything that you see, touch, do, and think about. For example,
a random sampling produces music, news, milk, walking, driving, gas, Soda Hall, carpet,
talking, Professor Fateman, chicken curry, tongue, $7, sun, the daily newspaper, and so on.
You should produce both a single hierarchy chart (on a large sheet of paper) and a
listing of objects and categories with the relations satisﬁed by members of each category.
Every object should be in a category, and every category should be in the hierarchy.
12.3 Develop a representational system for reasoning about windows in a window-based
computer interface. In particular, your representation should be able to describe:
•The state of a window: minimized, displayed, or nonexistent.
•Which window (if any) is the active window.
•The position of every window at a given time.
•The order (front to back) of overlapping windows.
•The actions of creating, destroying, resizing, and moving windows; changing the state
of a window; and bringing a window to the front. Treat these actions as atomic; that is,
do not deal with the issue of relating them to mouse actions. Give axioms describing
the effects of actions on ﬂuents. You may use either event or situation calculus.
Assume an ontology containing situations, actions, integers (for x and y coordinates) and
windows. Deﬁne a language over this ontology; that is, a list of constants, function symbols,
and predicates with an English description of each. If you need to add more categories to the
ontology (e.g., pixels), you may do so, but be sure to specify these in your write-up. You may
(and should) use symbols deﬁned in the text, but be sure to list these explicitly.
12.4 State the following in the language you developed for the previous exercise:
a. In situation S
0, window W1 is behind W2 but sticks out on the left and right. Do not
state exact coordinates for these; describe the general situation.
b. If a window is displayed, then its top edge is higher than its bottom edge.
c. After you create a window w, it is displayed.
d. A window can be minimized if it is displayed.
12.5 (Adapted from an example by Doug Lenat.) Your mission is to capture, in logical
form, enough knowledge to answer a series of questions about the following simple scenario:
Yesterday John went to the North Berkeley Safeway supermarket and bought two
pounds of tomatoes and a pound of ground beef.
Start by trying to represent the content of the sentence as a series of assertions. You should
write sentences that have straightforward logical structure (e.g., statements that objects have
certain properties, that objects are related in certain ways, that all objects satisfying one prop-
erty satisfy another). The following might help you get started:
Exercises 475
•Which classes, objects, and relations would you need? What are their parents, siblings
and so on? (You will need events and temporal ordering, among other things.)
•Where would they ﬁt in a more general hierarchy?
•What are the constraints and interrelationships among them?
•How detailed must you be about each of the various concepts?
To answer the questions below, your knowledge base must include background knowledge.
You’ll have to deal with what kind of things are at a supermarket, what is involved with
purchasing the things one selects, what the purchases will be used for, and so on. Try to make
your representation as general as possible. To give a trivial example: don’t say “People buy
food from Safeway,” because that won’t help you with those who shop at another supermarket.
Also, don’t turn the questions into answers; for example, question (c) asks “Did John buy any
meat?”—not “Did John buy a pound of ground beef?”
Sketch the chains of reasoning that would answer the questions. If possible, use a
logical reasoning system to demonstrate the sufﬁciency of your knowledge base. Many of the
things you write might be only approximately correct in reality, but don’t worry too much;
the idea is to extract the common sense that lets you answer these questions at all. A truly
complete answer to this question is extremely difﬁcult, probably beyond the state of the art of
current knowledge representation. But you should be able to put together a consistent set of
axioms for the limited questions posed here.
a. Is John a child or an adult? [Adult]
b. Does John now have at least two tomatoes? [Yes]
c. Did John buy any meat? [Yes]
d. If Mary was buying tomatoes at the same time as John, did he see her? [Yes]
e. Are the tomatoes made in the supermarket? [No]
f. What is John going to do with the tomatoes? [Eat them]
g. Does Safeway sell deodorant? [Yes]
h. Did John bring some money or a credit card to the supermarket? [Yes]
i. Does John have less money after going to the supermarket? [Yes]
12.6 Make the necessary additions or changes to your knowledge base from the previous
exercise so that the questions that follow can be answered. Include in your report a discussion
of your changes, explaining why they were needed, whether they were minor or major, and
what kinds of questions would necessitate further changes.
a. Are there other people in Safeway while John is there? [Yes—staff!]
b. Is John a vegetarian? [No]
c. Who owns the deodorant in Safeway? [Safeway Corporation]
d. Did John have an ounce of ground beef? [Yes]
e. Does the Shell station next door have any gas? [Yes]
f. Do the tomatoes ﬁt in John’s car trunk? [Yes]
476 Chapter 12. Knowledge Representation
12.7 Represent the following seven sentences using and extending the representations de-
veloped in the chapter:
a. Water is a liquid between 0 and 100 degrees.
b. Water boils at 100 degrees.
c. The water in John’s water bottle is frozen.
d. Perrier is a kind of water.
e. John has Perrier in his water bottle.
f. All liquids have a freezing point.
g. A liter of water weighs more than a liter of alcohol.
12.8 Write deﬁnitions for the following:
a. ExhaustivePartDecomposition
b. PartPartition
c. PartwiseDisjoint
These should be analogous to the deﬁnitions for ExhaustiveDecomposition, Partition,a n d
Disjoint. Is it the case that PartPartition(s,BunchOf (s))? If so, prove it; if not, give a
counterexample and deﬁne sufﬁcient conditions under which it does hold.
12.9 An alternative scheme for representing measures involves applying the units function
to an abstract length object. In such a scheme, one would write Inches(Length(L1)) =
1.5. How does this scheme compare with the one in the chapter? Issues include conversion
axioms, names for abstract quantities (such as “50 dollars”), and comparisons of abstract
measures in different units (50 inches is more than 50 centimeters).
12.10 Add sentences to extend the deﬁnition of the predicate Name(s,c) so that a string
such as “laptop computer” matches the appropriate category names from a variety of stores.
Try to make your deﬁnition general. Test it by looking at ten online stores, and at the category
names they give for three different categories. For example, for the category of laptops, we
found the names “Notebooks,” “Laptops,” “Notebook Computers,” “Notebook,” “Laptops
and Notebooks,” and “Notebook PCs.” Some of these can be covered by explicitName facts,
while others could be covered by sentences for handling plurals, conjunctions, etc.
12.11 Write event calculus axioms to describe the actions in the wumpus world.
12.12 State the interval-algebra relation that holds between every pair of the following real-
world events:
LK: The life of President Kennedy.
IK : The infancy of President Kennedy.
PK : The presidency of President Kennedy.
LJ: The life of President Johnson.
PJ : The presidency of President Johnson.
LO: The life of President Obama.
Exercises 477
12.13 Investigate ways to extend the event calculus to handle simultaneous events. Is it
possible to avoid a combinatorial explosion of axioms?
12.14 Construct a representation for exchange rates between currencies that allows for daily
ﬂuctuations.
12.15 Deﬁne the predicate Fixed,w h e r eFixed(Location(x)) means that the location of
object x is ﬁxed over time.
12.16 Describe the event of trading something for something else. Describe buying as a
kind of trading in which one of the objects traded is a sum of money.
12.17 The two preceding exercises assume a fairly primitive notion of ownership. For ex-
ample, the buyer starts by owning the dollar bills. This picture begins to break down when,
for example, one’s money is in the bank, because there is no longer any speciﬁc collection
of dollar bills that one owns. The picture is complicated still further by borrowing, leasing,
renting, and bailment. Investigate the various commonsense and legal concepts of ownership,
and propose a scheme by which they can be represented formally.
12.18 (Adapted from Fagin et al. (1995).) Consider a game played with a deck of just 8
cards, 4 aces and 4 kings. The three players, Alice, Bob, and Carlos, are dealt two cards each.
Without looking at them, they place the cards on their foreheads so that the other players can
see them. Then the players take turns either announcing that they know what cards are on
their own forehead, thereby winning the game, or saying “I don’t know.” Everyone knows
the players are truthful and are perfect at reasoning about beliefs.
a. Game 1. Alice and Bob have both said “I don’t know.” Carlos sees that Alice has two
aces (A-A) and Bob has two kings (K-K). What should Carlos say? ( Hint: consider all
three possible cases for Carlos: A-A, K-K, A-K.)
b. Describe each step of Game 1 using the notation of modal logic.
c. Game 2. Carlos, Alice, and Bob all said “I don’t know” on their ﬁrst turn. Alice holds
K-K and Bob holds A-K. What should Carlos say on his second turn?
d. Game 3. Alice, Carlos, and Bob all say “I don’t know” on their ﬁrst turn, as does Alice
on her second turn. Alice and Bob both hold A-K. What should Carlos say?
e. Prove that there will always be a winner to this game.
12.19 The assumption of logical omniscience, discussed on page 453, is of course not true
of any actual reasoners. Rather, it is an idealization of the reasoning process that may be
more or less acceptable depending on the applications. Discuss the reasonableness of the
assumption for each of the following applications of reasoning about knowledge:
a. Partial knowledge adversary games, such as card games. Here one player wants to
reason about what his opponent knows about the state of the game.
b. Chess with a clock. Here the player may wish to reason about the limits of his oppo-
nent’s or his own ability to ﬁnd the best move in the time available. For instance, if
player A has much more time left than player B, then A will sometimes make a move
that greatly complicates the situation, in the hopes of gaining an advantage because he
has more time to work out the proper strategy.
478 Chapter 12. Knowledge Representation
c. A shopping agent in an environment in which there are costs of gathering information.
d. Reasoning about public key cryptography, which rests on the intractability of certain
computational problems.
12.20 Translate the following description logic expression (from page 457) into ﬁrst-order
logic, and comment on the result:
And(Man,AtLeast(3,Son),AtMost(2,Daughter),
All(Son,And(Unemployed,Married,All(Spouse,Doctor))),
All(Daughter,And(Professor,Fills(Department,Physics,Math)))) .
12.21 Recall that inheritance information in semantic networks can be captured logically
by suitable implication sentences. This exercise investigates the efﬁciency of using such
sentences for inheritance.
a. Consider the information in a used-car catalog such as Kelly’s Blue Book—for exam-
ple, that 1973 Dodge vans are (or perhaps were once) worth $575. Suppose all this
information (for 11,000 models) is encoded as logical sentences, as suggested in the
chapter. Write down three such sentences, including that for 1973 Dodge vans. How
would you use the sentences to ﬁnd the value of a particular car, given a backward-
chaining theorem prover such as Prolog?
b. Compare the time efﬁciency of the backward-chaining method for solving this problem
with the inheritance method used in semantic nets.
c. Explain how forward chaining allows a logic-based system to solve the same problem
efﬁciently, assuming that the KB contains only the 11,000 sentences about prices.
d. Describe a situation in which neither forward nor backward chaining on the sentences
will allow the price query for an individual car to be handled efﬁciently.
e. Can you suggest a solution enabling this type of query to be solved efﬁciently in all
cases in logic systems? ( Hint: Remember that two cars of the same year and model
have the same price.)
12.22 One might suppose that the syntactic distinction between unboxed links and singly
boxed links in semantic networks is unnecessary, because singly boxed links are always at-
tached to categories; an inheritance algorithm could simply assume that an unboxed link
attached to a category is intended to apply to all members of that category. Show that this
argument is fallacious, giving examples of errors that would arise.
12.23 One part of the shopping process that was not covered in this chapter is checking
for compatibility between items. For example, if a digital camera is ordered, what accessory
batteries, memory cards, and cases are compatible with the camera? Write a knowledge base
that can determine the compatibility of a set of items and suggest replacements or additional
items if the shopper makes a choice that is not compatible. The knowledge base should works
with at least one line of products and extend easily to other lines.
12.24 A complete solution to the problem of inexact matches to the buyer’s description
in shopping is very difﬁcult and requires a full array of natural language processing and
Exercises 479
information retrieval techniques. (See Chapters 22 and 23.) One small step is to allow the
user to specify minimum and maximum values for various attributes. The buyer must use the
following grammar for product descriptions:
Description → Category [Connector Modiﬁer]∗
Connector → “with”| “and”| “,”
Modiﬁer → Attribute| Attribute Op Value
Op → “=”| “>”| “<”
Here, Category names a product category, Attribute is some feature such as “CPU” or
“price,” andValue is the target value for the attribute. So the query “computer with at least a
2.5 GHz CPU for under $500” must be re-expressed as “computer with CPU > 2.5 GHz and
price < $500.” Implement a shopping agent that accepts descriptions in this language.
12.25 Our description of Internet shopping omitted the all-important step of actuallybuying
the product. Provide a formal logical description of buying, using event calculus. That is,
deﬁne the sequence of events that occurs when a buyer submits a credit-card purchase and
then eventually gets billed and receives the product.


END_INSTRUCTION
