
START_INSTRUCTION
You are an expert AI tutor creating a comprehensive study guide.
Your task is to rewrite and expand the "Lesson Notes" provided below into a cohesive, readable study text.

1. **Structure:** Follow the exact structure and order of the topics in the "Lesson Notes". Do not reorder them.
2. **Enrichment:** Use the "Textbook Context" provided to expand on every bullet point in the notes.
    - If the notes mention a concept (e.g., "Rationality"), define it precisely using the textbook.
    - If the notes list algorithms (e.g., "A*", "Simulated Annealing"), explain how they work, their complexity, and pros/cons using the textbook details.
    - Add examples from the textbook where appropriate.
3. **Tone:** Academic but accessible study material. Not just bullet points—write paragraphs where necessary to explain complex ideas.
4. **Language:** The output must be in **Czech** (since the lesson notes are in Czech), but you can keep standard English AI terminology (like "Overfitting") in parentheses.

--- LESSON NOTES (Skeleton) ---
Agenti a roboti

Robotika - disciplína o vytváření inteligentních strojů
Typy robotů
Průmyslové roboty (manipulátory)
○ Pevně umístěné na pracovních pozicích
•
Mobilní roboty
○ Roboty pohybující se v prostředí
•
Roboty humanoidní
○ vzhledem napodobující člověka
•
Autonomní robot je inteligentní stroj schopen vykonávat úkoly samostatně bez lidské pomoci
• nejdůležitější vlastnost je schopnost reagovat na změny prostředí
základní části:
○ senzory
○ řídící jednotka
○ efektory (aktuátory)
•
Prostředí agenta
• pozorovatelné plně / částečně
• deterministické / stochastické
• epizodické / sekvenční
• statické / dynamické
• diskrétní / spojité
• jednoagentové / multiagentní
Sensorický subsystém
sensory
pasivní (využívají fyzikální vlastnosti okolí)
▪ kamera, gyroskop, tlačítka
○
aktivní (vysílají signál a detekují jeho návrat)
▪ dálkoměrové senzory, GPS
○
•
sensory
○ lokální (součástí robota)
○ distribuované
•
Příklady robotů:
shakey - první univerzální mobilní robot
○ vedlejším produktem vývoje byl algoritmus A*
•
• TJ2 - robot schopný komunikovat v přirozeném jazyce
Genghis - chodící robot
○ záporná zpětná vazba když se dotkl země tělem
○ kladná zpě5tná vazba když se pohyboval vpřed nebo vzad
•
• Hannibal - vylepšená verze zamýšlená jako planetární mobilní robot
• Cog - Humanoidní robot aproximující vnímání a dynamiku lidského těla
• Kismet - humanoidní robot simulující sociální interakce
Coco - humanoidní robot, který se dokáže pohybovat
○ inspirací pro tvar těla bylo tělo gorily
•
Typy agentů
• Reaktivní agent
Reaktivní agent
○ nejjednodušší typ agenta
○ jedná přímo na základě aktuálního stavu prostředí bez vytváření složitých plánů
○ př. hloupý robotický vysavač
•
Deliberativní agent
○ na nedostatky reaktivních agentů navazuje agent deliberativní
○ plánuje své akce na základě modelu prostředí a svých cílů
○ dokáže plánovat cestu k množině svých cílů
○ plány tvoří na základě databáze svých znalostí a vnitřních stavů
V interakci s prostředím si tvoří jeho symbolickou reprezentaci, která je uložena ve 
formě jednotlivých tvrzení o světě
○
př. Robot, který plánuje trasu, aby doručil balík na specifické místo, a při tom zvažuje 
překážky a alternativní cesty.
○
•
Subsumpční architektura
• dekompozice agenta dle jeho aktivit do vrstev
• jednotlivé vrstvy spolu "soupeří" o řízení agenta
• Nejspodnější vrstva má přednost
•
Celulární automat
• dynamický systém, diskrétní v prostoru a čase
• pravidelná struktura buněk v N-rozměrném prostoru (nejčastěji N=2)
• každá buňka se nachází v jednom z K stavů
hodnoty stavů buňky se počítají na základě lokální přechodové funkce
○ funkce obsahuje aktuální stav a stav buněk v okolí
•
využití:
simulace přírodních jevů
▪ růst krystalů
▪ šíření požáru
▪ šíření nemoci
○
teoretická informatika
▪ modelování výpočetních procesů
○
Fyzika
▪ modelování plynových dynamik
▪ simulace termodynamických procesů
○
Kryptografie
náhodnost generovaná CA může být použita v šifrovacích algoritmech nebo k 
tvorbě pseudonáhodných čísel

--- TEXTBOOK CONTEXT (Source Material) ---


--- BOOK CHAPTER: 25_Robotics ---

25 ROBOTICS
In which agents are endowed with physical effectors with which to do mischief.
25.1 I NTRODUCTION
Robots are physical agents that perform tasks by manipulating the physical world. To do so,ROBOT
they are equipped with effectors such as legs, wheels, joints, and grippers. Effectors haveEFFECTOR
a single purpose: to assert physical forces on the environment. 1 Robots are also equipped
with sensors, which allow them to perceive their environment. Present day robotics em-SENSOR
ploys a diverse set of sensors, including cameras and lasers to measure the environment, and
gyroscopes and accelerometers to measure the robot’s own motion.
Most of today’s robots fall into one of three primary categories.Manipulators, or robotMANIPULA TOR
arms (Figure 25.1(a)), are physically anchored to their workplace, for example in a factory
assembly line or on the International Space Station. Manipulator motion usually involves
a chain of controllable joints, enabling such robots to place their effectors in any position
within the workplace. Manipulators are by far the most common type of industrial robots,
with approximately one million units installed worldwide. Some mobile manipulators are
used in hospitals to assist surgeons. Few car manufacturers could survive without robotic
manipulators, and some manipulators have even been used to generate original artwork.
The second category is the mobile robot. Mobile robots move about their environment
MOBILE ROBOT
using wheels, legs, or similar mechanisms. They have been put to use delivering food in
hospitals, moving containers at loading docks, and similar tasks. Unmanned ground vehi-
cles, or UGVs, drive autonomously on streets, highways, and off-road. The planetary roverUGV
PLANETARY ROVER shown in Figure 25.2(b) explored Mars for a period of 3 months in 1997. Subsequent NASA
robots include the twin Mars Exploration Rovers (one is depicted on the cover of this book),
which landed in 2003 and were still operating six years later. Other types of mobile robots
include unmanned air vehicles (UA Vs), commonly used for surveillance, crop-spraying, and
UAV
1 In Chapter 2 we talked about actuators, not effectors. Here we distinguish the effector (the physical device)
from the actuator (the control line that communicates a command to the effector).
971
972 Chapter 25. Robotics
(a) (b)
Figure 25.1 (a) An industrial robotic manipulator for stacking bags on a pallet. Image
courtesy of Nachi Robotic Systems. (b) Honda’s P3 and Asimo humanoid robots.
(a) (b)
Figure 25.2 (a) Predator, an unmanned aerial vehicle (UA V) used by the U.S. Military.
Image courtesy of General Atomics Aeronautical Systems. (b) NASA’s Sojourner, a mobile
robot that explored the surface of Mars in July 1997.
military operations. Figure 25.2(a) shows a UA V commonly used by the U.S. military. Au-
tonomous underwater vehicles (AUVs) are used in deep sea exploration. Mobile robotsAUV
deliver packages in the workplace and vacuum the ﬂoors at home.
The third type of robot combines mobility with manipulation, and is often called a
mobile manipulator. Humanoid robots mimic the human torso. Figure 25.1(b) shows twoMOBILE
MANIPULA TOR
HUMANOID ROBOT early humanoid robots, both manufactured by Honda Corp. in Japan. Mobile manipulators
Section 25.2. Robot Hardware 973
can apply their effectors further aﬁeld than anchored manipulators can, but their task is made
harder because they don’t have the rigidity that the anchor provides.
The ﬁeld of robotics also includes prosthetic devices (artiﬁcial limbs, ears, and eyes
for humans), intelligent environments (such as an entire house that is equipped with sensors
and effectors), and multibody systems, wherein robotic action is achieved through swarms of
small cooperating robots.
Real robots must cope with environments that are partially observable, stochastic, dy-
namic, and continuous. Many robot environments are sequential and multiagent as well.
Partial observability and stochasticity are the result of dealing with a large, complex world.
Robot cameras cannot see around corners, and motion commands are subject to uncertainty
due to gears slipping, friction, etc. Also, the real world stubbornly refuses to operate faster
than real time. In a simulated environment, it is possible to use simple algorithms (such as the
Q-learning algorithm described in Chapter 21) to learn in a few CPU hours from millions of
trials. In a real environment, it might take years to run these trials. Furthermore, real crashes
really hurt, unlike simulated ones. Practical robotic systems need to embody prior knowledge
about the robot, its physical environment, and the tasks that the robot will perform so that the
robot can learn quickly and perform safely.
Robotics brings together many of the concepts we have seen earlier in the book, in-
cluding probabilistic state estimation, perception, planning, unsupervised learning, and re-
inforcement learning. For some of these concepts robotics serves as a challenging example
application. For other concepts this chapter breaks new ground in introducing the continuous
version of techniques that we previously saw only in the discrete case.
25.2 R OBOT HARDW ARE
So far in this book, we have taken the agent architecture—sensors, effectors, and processors—
as given, and we have concentrated on the agent program. The success of real robots depends
at least as much on the design of sensors and effectors that are appropriate for the task.
25.2.1 Sensors
Sensors are the perceptual interface between robot and environment. Passive sensors,s u c hP ASSIVE SENSOR
as cameras, are true observers of the environment: they capture signals that are generated by
other sources in the environment. Active sensors, such as sonar, send energy into the envi-ACTIVE SENSOR
ronment. They rely on the fact that this energy is reﬂected back to the sensor. Active sensors
tend to provide more information than passive sensors, but at the expense of increased power
consumption and with a danger of interference when multiple active sensors are used at the
same time. Whether active or passive, sensors can be divided into three types, depending on
whether they sense the environment, the robot’s location, or the robot’s internal conﬁguration.
Range ﬁnders are sensors that measure the distance to nearby objects. In the early
RANGE FINDER
days of robotics, robots were commonly equipped with sonar sensors. Sonar sensors emitSONAR SENSORS
directional sound waves, which are reﬂected by objects, with some of the sound making it
974 Chapter 25. Robotics
(a) (b)
Figure 25.3 (a) Time of ﬂight camera; image courtesy of Mesa Imaging GmbH. (b) 3D
range image obtained with this camera. The range image makes it possible to detect obstacles
and objects in a robot’s vicinity.
back into the sensor. The time and intensity of the returning signal indicates the distance
to nearby objects. Sonar is the technology of choice for autonomous underwater vehicles.
Stereo vision (see Section 24.4.2) relies on multiple cameras to image the environment from
STEREO VISION
slightly different viewpoints, analyzing the resulting parallax in these images to compute the
range of surrounding objects. For mobile ground robots, sonar and stereo vision are now
rarely used, because they are not reliably accurate.
Most ground robots are now equipped with optical range ﬁnders. Just like sonar sensors,
optical range sensors emit active signals (light) and measure the time until a reﬂection of this
signal arrives back at the sensor. Figure 25.3(a) shows a time of ﬂight camera . This camera
TIME OF FLIGHT
CAMERA
acquires range images like the one shown in Figure 25.3(b) at up to 60 frames per second.
Other range sensors use laser beams and special 1-pixel cameras that can be directed using
complex arrangements of mirrors or rotating elements. These sensors are called scanning
lidars (short for light detection and ranging ). Scanning lidars tend to provide longer rangesSCANNING LIDARS
than time of ﬂight cameras, and tend to perform better in bright daylight.
Other common range sensors include radar, which is often the sensor of choice for
UA Vs. Radar sensors can measure distances of multiple kilometers. On the other extreme
end of range sensing are tactile sensors such as whiskers, bump panels, and touch-sensitiveTACTILE SENSORS
skin. These sensors measure range based on physical contact, and can be deployed only for
sensing objects very close to the robot.
A second important class of sensors is location sensors . Most location sensors useLOCA TION SENSORS
range sensing as a primary component to determine location. Outdoors, theGlobal Position-
ing System (GPS) is the most common solution to the localization problem. GPS measures
GLOBAL
POSITIONING
SYSTEM
the distance to satellites that emit pulsed signals. At present, there are 31 satellites in orbit,
transmitting signals on multiple frequencies. GPS receivers can recover the distance to these
satellites by analyzing phase shifts. By triangulating signals from multiple satellites, GPS
Section 25.2. Robot Hardware 975
receivers can determine their absolute location on Earth to within a few meters. Differential
GPS involves a second ground receiver with known location, providing millimeter accuracyDIFFERENTIAL GPS
under ideal conditions. Unfortunately, GPS does not work indoors or underwater. Indoors,
localization is often achieved by attaching beacons in the environment at known locations.
Many indoor environments are full of wireless base stations, which can help robots localize
through the analysis of the wireless signal. Underwater, active sonar beacons can provide a
sense of location, using sound to inform AUVs of their relative distances to those beacons.
The third important class is proprioceptive sensors, which inform the robot of its own
PROPRIOCEPTIVE
SENSOR
motion. To measure the exact conﬁguration of a robotic joint, motors are often equipped
with shaft decoders that count the revolution of motors in small increments. On robot arms,SHAFT DECODER
shaft decoders can provide accurate information over any period of time. On mobile robots,
shaft decoders that report wheel revolutions can be used for odometry—the measurement ofODOMETRY
distance traveled. Unfortunately, wheels tend to drift and slip, so odometry is accurate only
over short distances. External forces, such as the current for AUVs and the wind for UA Vs,
increase positional uncertainty. Inertial sensors, such as gyroscopes, rely on the resistance
INERTIAL SENSOR
of mass to the change of velocity. They can help reduce uncertainty.
Other important aspects of robot state are measured by force sensors and torque sen-FORCE SENSOR
sors. These are indispensable when robots handle fragile objects or objects whose exact shapeTORQUE SENSOR
and location is unknown. Imagine a one-ton robotic manipulator screwing in a light bulb. It
would be all too easy to apply too much force and break the bulb. Force sensors allow the
robot to sense how hard it is gripping the bulb, and torque sensors allow it to sense how hard
it is turning. Good sensors can measure forces in all three translational and three rotational
directions. They do this at a frequency of several hundred times a second, so that a robot can
quickly detect unexpected forces and correct its actions before it breaks a light bulb.
25.2.2 Effectors
Effectors are the means by which robots move and change the shape of their bodies. To
understand the design of effectors, it will help to talk about motion and shape in the abstract,
using the concept of a degree of freedom (DOF) We count one degree of freedom for each
DEGREE OF
FREEDOM
independent direction in which a robot, or one of its effectors, can move. For example, a rigid
mobile robot such as an AUV has six degrees of freedom, three for its (x, y, z) location in
space and three for its angular orientation, known as yaw, roll,a n dpitch. These six degrees
deﬁne the kinematic state2 or pose of the robot. The dynamic state of a robot includes theseKINEMA TIC ST A TE
POSE
DYNAMIC STATE
six plus an additional six dimensions for the rate of change of each kinematic dimension, that
is, their velocities.
For nonrigid bodies, there are additional degrees of freedom within the robot itself. For
example, the elbow of a human arm possesses two degree of freedom. It can ﬂex the upper
arm towards or away, and can rotate right or left. The wrist has three degrees of freedom. It
can move up and down, side to side, and can also rotate. Robot joints also have one, two,
or three degrees of freedom each. Six degrees of freedom are required to place an object,
such as a hand, at a particular point in a particular orientation. The arm in Figure 25.4(a)
2 “Kinematic” is from the Greek word for motion, as is “cinema.”
976 Chapter 25. Robotics
has exactly six degrees of freedom, created by ﬁve revolute joints that generate rotationalREVOLUTE JOINT
motion and one prismatic joint that generates sliding motion. You can verify that the humanPRISMA TIC JOINT
arm as a whole has more than six degrees of freedom by a simple experiment: put your hand
on the table and notice that you still have the freedom to rotate your elbow without changing
the conﬁguration of your hand. Manipulators that have extra degrees of freedom are easier to
control than robots with only the minimum number of DOFs. Many industrial manipulators
therefore have seven DOFs, not six.
R
RR
P
R R
θ
(x, y)
(a) (b)
Figure 25.4 (a) The Stanford Manipulator, an early robot arm with ﬁve revolute joints (R)
and one prismatic joint (P), for a total of six degrees of freedom. (b) Motion of a nonholo-
nomic four-wheeled vehicle with front-wheel steering.
For mobile robots, the DOFs are not necessarily the same as the number of actuated ele-
ments. Consider, for example, your average car: it can move forward or backward, and it can
turn, giving it two DOFs. In contrast, a car’s kinematic conﬁguration is three-dimensional:
on an open ﬂat surface, one can easily maneuver a car to any (x, y) point, in any orientation.
(See Figure 25.4(b).) Thus, the car has three effective degrees of freedom but two control-
EFFECTIVE DOF
lable degrees of freedom . We say a robot is nonholonomic if it has more effective DOFsCONTROLLABLE DOF
NONHOLONOMIC than controllable DOFs and holonomic if the two numbers are the same. Holonomic robots
are easier to control—it would be much easier to park a car that could move sideways as well
as forward and backward—but holonomic robots are also mechanically more complex. Most
robot arms are holonomic, and most mobile robots are nonholonomic.
Mobile robots have a range of mechanisms for locomotion, including wheels, tracks,
and legs. Differential drive robots possess two independently actuated wheels (or tracks),
DIFFERENTIAL DRIVE
one on each side, as on a military tank. If both wheels move at the same velocity, the robot
moves on a straight line. If they move in opposite directions, the robot turns on the spot. An
alternative is the synchro drive, in which each wheel can move and turn around its own axis.
SYNCHRO DRIVE
To avoid chaos, the wheels are tightly coordinated. When moving straight, for example, all
wheels point in the same direction and move at the same speed. Both differential and synchro
drives are nonholonomic. Some more expensive robots use holonomic drives, which have
three or more wheels that can be oriented and moved independently.
Some mobile robots possess arms. Figure 25.5(a) displays a two-armed robot. This
robot’s arms use springs to compensate for gravity, and they provide minimal resistance to
Section 25.2. Robot Hardware 977
(a) (b)
Figure 25.5 (a) Mobile manipulator plugging its charge cable into a wall outlet. Image
courtesy of Willow Garage, c⃝ 2009. (b) One of Marc Raibert’s legged robots in motion.
external forces. Such a design minimizes the physical danger to people who might stumble
into such a robot. This is a key consideration in deploying robots in domestic environments.
Legs, unlike wheels, can handle rough terrain. However, legs are notoriously slow on
ﬂat surfaces, and they are mechanically difﬁcult to build. Robotics researchers have tried de-
signs ranging from one leg up to dozens of legs. Legged robots have been made to walk, run,
and even hop—as we see with the legged robot in Figure 25.5(b). This robot is dynamically
stable, meaning that it can remain upright while hopping around. A robot that can remain
DYNAMICALL Y
STABLE
upright without moving its legs is called statically stable. A robot is statically stable if itsSTA TICALL Y STABLE
center of gravity is above the polygon spanned by its legs. The quadruped (four-legged) robot
shown in Figure 25.6(a) may appear statically stable. However, it walks by lifting multiple
legs at the same time, which renders it dynamically stable. The robot can walk on snow and
ice, and it will not fall over even if you kick it (as demonstrated in videos available online).
Two-legged robots such as those in Figure 25.6(b) are dynamically stable.
Other methods of movement are possible: air vehicles use propellers or turbines; un-
derwater vehicles use propellers or thrusters, similar to those used on submarines. Robotic
blimps rely on thermal effects to keep themselves aloft.
Sensors and effectors alone do not make a robot. A complete robot also needs a source
of power to drive its effectors. The electric motor is the most popular mechanism for both
ELECTRIC MOTOR
manipulator actuation and locomotion, but pneumatic actuation using compressed gas andPNEUMA TIC
ACTUA TION
hydraulic actuation using pressurized ﬂuids also have their application niches.HYDRAULIC
ACTUA TION
978 Chapter 25. Robotics
(a) (b)
Figure 25.6 (a) Four-legged dynamically-stable robot “Big Dog.” Image courtesy Boston
Dynamics, c⃝ 2009. (b) 2009 RoboCup Standard Platform League competition, showing the
winning team, B-Human, from the DFKI center at the University of Bremen. Throughout the
match, B-Human outscored th eir opponents 64:1. Their success was built on probabilistic
state estimation using particle ﬁlters and Kalman ﬁlters; on machine-learning models for gait
optimization; and on dynamic kicking moves. Image courtesy DFKI, c⃝ 2009.
25.3 R OBOTIC PERCEPTION
Perception is the process by which robots map sensor measurements into internal representa-
tions of the environment. Perception is difﬁcult because sensors are noisy, and the environ-
ment is partially observable, unpredictable, and often dynamic. In other words, robots have
all the problems of state estimation (or ﬁltering) that we discussed in Section 15.2. As a
rule of thumb, good internal representations for robots have three properties: they contain
enough information for the robot to make good decisions, they are structured so that they can
be updated efﬁciently, and they are natural in the sense that internal variables correspond to
natural state variables in the physical world.
In Chapter 15, we saw that Kalman ﬁlters, HMMs, and dynamic Bayes nets can repre-
sent the transition and sensor models of a partially observable environment, and we described
both exact and approximate algorithms for updating the belief state—the posterior probabil-
ity distribution over the environment state variables. Several dynamic Bayes net models for
this process were shown in Chapter 15. For robotics problems, we include the robot’s own
past actions as observed variables in the model. Figure 25.7 shows the notation used in this
chapter: X
t is the state of the environment (including the robot) at timet, Zt is the observation
received at time t,a n dAt is the action taken after the observation is received.
Section 25.3. Robotic Perception 979
Xt+1Xt
At−2 At−1 At
Zt−1
Xt−1
Zt Zt+1
Figure 25.7 Robot perception can be viewed as temporal inference from sequences of
actions and measurements, as illustrated by this dynamic Bayes network.
We would like to compute the new belief state, P(Xt+1 | z1:t+1,a1:t), from the current
belief state P(Xt | z1:t,a1:t−1) and the new observation zt+1. We did this in Section 15.2,
but here there are two differences: we condition explicitly on the actions as well as the ob-
servations, and we deal with continuous rather than discrete variables. Thus, we modify the
recursive ﬁltering equation (15.5 on page 572) to use integration rather than summation:
P(X
t+1 | z1:t+1,a1:t)
= αP(zt+1 | Xt+1)
∫
P(Xt+1 | xt,at) P(xt | z1:t,a1:t−1) dxt . (25.1)
This equation states that the posterior over the state variables X at time t +1 is calculated
recursively from the corresponding estimate one time step earlier. This calculation involves
the previous action at and the current sensor measurement zt+1. For example, if our goal
is to develop a soccer-playing robot, Xt+1 might be the location of the soccer ball relative
to the robot. The posterior P(Xt| z1:t,a1:t−1) is a probability distribution over all states that
captures what we know from past sensor measurements and controls. Equation (25.1) tells us
how to recursively estimate this location, by incrementally folding in sensor measurements
(e.g., camera images) and robot motion commands. The probability P(Xt+1 | xt,at) is called
the transition model or motion model,a n dP(zt+1 | Xt+1) is the sensor model.MOTION MODEL
25.3.1 Localization and mapping
Localization is the problem of ﬁnding out where things are—including the robot itself.LOCALIZA TION
Knowledge about where things are is at the core of any successful physical interaction with
the environment. For example, robot manipulators must know the location of objects they
seek to manipulate; navigating robots must know where they are to ﬁnd their way around.
To keep things simple, let us consider a mobile robot that moves slowly in a ﬂat 2D
world. Let us also assume the robot is given an exact map of the environment. (An example
of such a map appears in Figure 25.10.) The pose of such a mobile robot is deﬁned by its
two Cartesian coordinates with values x and y and its heading with value θ, as illustrated in
Figure 25.8(a). If we arrange those three values in a vector, then any particular state is given
by X
t =(xt,y t,θt)⊤. So far so good.
980 Chapter 25. Robotics
xi, yi
vt Δt
xt+1
h(xt)
xt
θt
t+1θ
t Δtω
Z1 Z2 Z3 Z4
(a) (b)
Figure 25.8 (a) A simpliﬁed kinematic model of a mobile robot. The robot is shown as a
circle with an interior line marking the forward direction. The state xt consists of the (xt,y t)
position (shown implicitly) and the orientation θt. The new state xt+1 is obtained by an
update in position of vtΔt and in orientation of ωtΔt. Also shown is a landmark at (xi,y i)
observed at timet. (b) The range-scan sensor model. Two possible robot poses are shown for
a given range scan (z1,z2,z3,z4). It is much more likely that the pose on the left generated
the range scan than the pose on the right.
In the kinematic approximation, each action consists of the “instantaneous” speciﬁca-
tion of two velocities—a translational velocity vt and a rotational velocity ωt. For small time
intervals Δ t, a crude deterministic model of the motion of such robots is given by
ˆXt+1 = f(Xt,vt,ωt
 
 
at
)= Xt +
⎛
⎝
vtΔ t cosθt
vtΔ t sinθt
ωtΔ t
⎞
⎠ .
The notation ˆX refers to a deterministic state prediction. Of course, physical robots are
somewhat unpredictable. This is commonly modeled by a Gaussian distribution with mean
f(X
t,vt,ωt) and covariance Σx. (See Appendix A for a mathematical deﬁnition.)
P(Xt+1 | Xt,vt,ωt)= N(ˆXt+1,Σx) .
This probability distribution is the robot’s motion model. It models the effects of the motion
at on the location of the robot.
Next, we need a sensor model. We will consider two kinds of sensor model. The
ﬁrst assumes that the sensors detect stable, recognizable features of the environment called
landmarks. For each landmark, the range and bearing are reported. Suppose the robot’s stateLANDMARK
is xt =(xt,y t,θt)⊤ and it senses a landmark whose location is known to be(xi,y i)⊤. Without
noise, the range and bearing can be calculated by simple geometry. (See Figure 25.8(a).) The
exact prediction of the observed range and bearing would be
ˆz
t = h(xt)=
⎞ √
(xt−xi)2 +( yt−yi)2
arctan yi− yt
xi− xt
−θt
⎠
.
Section 25.3. Robotic Perception 981
Again, noise distorts our measurements. To keep things simple, one might assume Gaussian
noise with covariance Σz, giving us the sensor model
P(zt | xt)= N(ˆzt,Σz) .
A somewhat different sensor model is used for an array of range sensors, each of which
has a ﬁxed bearing relative to the robot. Such sensors produce a vector of range values
zt =( z1,...,z M )⊤. Given a pose xt,l e tˆzj be the exact range along the jth beam direction
from xt to the nearest obstacle. As before, this will be corrupted by Gaussian noise. Typically,
we assume that the errors for the different beam directions are independent and identically
distributed, so we have
P(z
t | xt)= α
M∏
j =1
e−(zj − ˆzj)/2σ2
.
Figure 25.8(b) shows an example of a four-beam range scan and two possible robot poses,
one of which is reasonably likely to have produced the observed scan and one of which is not.
Comparing the range-scan model to the landmark model, we see that the range-scan model
has the advantage that there is no need to identify a landmark before the range scan can be
interpreted; indeed, in Figure 25.8(b), the robot faces a featureless wall. On the other hand,
if there are visible, identiﬁable landmarks, they may provide instant localization.
Chapter 15 described the Kalman ﬁlter, which represents the belief state as a single
multivariate Gaussian, and the particle ﬁlter, which represents the belief state by a collection
of particles that correspond to states. Most modern localization algorithms use one of two
representations of the robot’s belief P(X
t | z1:t,a1:t−1).
Localization using particle ﬁltering is called Monte Carlo localization ,o rM C L .T h eMONTE CARLO
LOCALIZA TION
MCL alfgorithm is an instance of the particle-ﬁltering algorithm of Figure 15.17 (page 598).
All we need to do is supply the appropriate motion model and sensor model. Figure 25.9
shows one version using the range-scan model. The operation of the algorithm is illustrated in
Figure 25.10 as the robot ﬁnds out where it is inside an ofﬁce building. In the ﬁrst image, the
particles are uniformly distributed based on the prior, indicating global uncertainty about the
robot’s position. In the second image, the ﬁrst set of measurements arrives and the particles
form clusters in the areas of high posterior belief. In the third, enough measurements are
available to push all the particles to a single location.
The Kalman ﬁlter is the other major way to localize. A Kalman ﬁlter represents the
posterior P(X
t | z1:t,a1:t−1) by a Gaussian. The mean of this Gaussian will be denotedμt and
its covariance Σt. The main problem with Gaussian beliefs is that they are only closed under
linear motion models f and linear measurement models h. For nonlinear f or h, the result of
updating a ﬁlter is in general not Gaussian. Thus, localization algorithms using the Kalman
ﬁlter linearize the motion and sensor models. Linearization is a local approximation of a
LINEARIZA TION
nonlinear function by a linear function. Figure 25.11 illustrates the concept of linearization
for a (one-dimensional) robot motion model. On the left, it depicts a nonlinear motion model
f(xt,at) (the control at is omitted in this graph since it plays no role in the linearization).
On the right, this function is approximated by a linear function ˜f(xt,at). This linear function
is tangent to f at the point μt, the mean of our state estimate at time t. Such a linearization
982 Chapter 25. Robotics
function MONTE -CARLO -LOCALIZATION (a, z, N , P(X′|X, v, ω ), P(z|z∗), m) returns
a set of samples for the next time step
inputs: a, robot velocities v and ω
z, range scan z1,...,z M
P(X′|X, v, ω ), motion model
P(z|z∗), range sensor noise model
m, 2D map of the environment
persistent: S, a vector of samples of size N
local variables: W , a vector of weights of size N
S′, a temporary vector of particles of size N
W ′, a vector of weights of size N
if S is empty then /* initialization phase */
for i =1 to N do
S[i]←sample from P(X0)
for i =1 to N do /* update cycle */
S′[i]←sample from P(X′|X = S[i],v,ω )
W ′[i]←1
for j =1 to M do
z∗←RAYCAST (j, X = S′[i], m)
W ′[i]←W ′[i] · P(zj| z∗)
S←WEIGHTED -SAMPLE -WITH -REPLACEMENT (N ,S ′,W ′)
return S
Figure 25.9 A Monte Carlo localization algorithm using a range-scan sensor model with
independent noise.
is called (ﬁrst degree) Taylor expansion. A Kalman ﬁlter that linearizes f and h via TaylorTAYLOR EXP ANSION
expansion is called an extended Kalman ﬁlter (or EKF). Figure 25.12 shows a sequence
of estimates of a robot running an extended Kalman ﬁlter localization algorithm. As the
robot moves, the uncertainty in its location estimate increases, as shown by the error ellipses.
Its error decreases as it senses the range and bearing to a landmark with known location
and increases again as the robot loses sight of the landmark. EKF algorithms work well if
landmarks are easily identiﬁed. Otherwise, the posterior distribution may be multimodal, as
in Figure 25.10(b). The problem of needing to know the identity of landmarks is an instance
of the data association problem discussed in Figure 15.6.
In some situations, no map of the environment is available. Then the robot will have to
acquire a map. This is a bit of a chicken-and-egg problem: the navigating robot will have to
determine its location relative to a map it doesn’t quite know, at the same time building this
map while it doesn’t quite know its actual location. This problem is important for many robot
applications, and it has been studied extensively under the name simultaneous localization
and mapping, abbreviated as SLAM.
SIMULTANEOUS
LOCALIZA TION AND
MAPPING
SLAM problems are solved using many different probabilistic techniques, including
the extended Kalman ﬁlter discussed above. Using the EKF is straightforward: just augment
Section 25.3. Robotic Perception 983
Robot position
(a)
Robot position
(b)
Robot position
(c)
Figure 25.10 Monte Carlo localization, a particle ﬁltering algorithm for mobile robot lo-
calization. (a) Initial, global uncertainty. (b) Approximately bimodal uncertainty after navi-
gating in the (symmetric) corridor. (c) Unimodal uncertainty after entering a room and ﬁnding
it to be distinctive.

984 Chapter 25. Robotics
Xt+1
Xtμt
Σt
f(Xt, at)
f(μt, at)Σt+1
Xt+1
Xtμt
Σt
f(Xt, at)
f(μt, at)Σt+1 Σt+1
~
f(Xt, at) = f(μt, at) + Ft(Xt − μt)
~
(a) (b)
Figure 25.11 One-dimensional illustration of a linearized motion model: (a) The function
f, and the projection of a mean μt and a covariance interval (based on Σt) into time t +1 .
(b) The linearized version is the tangent of f at μt. The projection of the mean μt is correct.
However, the projected covariance ˜Σt+1 differs from Σt+1.
robot
landmark
Figure 25.12 Example of localization using the extended Kalman ﬁlter. The robot moves
on a straight line. As it progresses, its uncertainty increases gradually, as illustrated by the
error ellipses. When it observes a landmark with known position, the uncertainty is reduced.
the state vector to include the locations of the landmarks in the environment. Luckily, the
EKF update scales quadratically, so for small maps (e.g., a few hundred landmarks) the com-
putation is quite feasible. Richer maps are often obtained using graph relaxation methods,
similar to the Bayesian network inference techniques discussed in Chapter 14. Expectation–
maximization is also used for SLAM.
25.3.2 Other types of perception
Not all of robot perception is about localization or mapping. Robots also perceive the tem-
perature, odors, acoustic signals, and so on. Many of these quantities can be estimated using
variants of dynamic Bayes networks. All that is required for such estimators are conditional
probability distributions that characterize the evolution of state variables over time, and sen-
sor models that describe the relation of measurements to state variables.
It is also possible to program a robot as a reactive agent, without explicitly reasoning
about probability distributions over states. We cover that approach in Section 25.6.3.
The trend in robotics is clearly towards representations with well-deﬁned semantics.
Section 25.3. Robotic Perception 985
(a) (b) (c)
Figure 25.13 Sequence of “drivable surface” classiﬁer results using adaptive vision. In
(a) only the road is classiﬁed as drivable (striped area). The V-shaped dark line shows where
the vehicle is heading. In (b) the vehicle is commanded to drive off the road, onto a grassy
surface, and the classiﬁer is beginning to classify some of the grass as drivable. In (c) the
vehicle has updated its model of drivable surface to correspond to grass as well as road.
Probabilistic techniques outperform other approaches in many hard perceptual problems such
as localization and mapping. However, statistical techniques are sometimes too cumbersome,
and simpler solutions may be just as effective in practice. To help decide which approach to
take, experience working with real physical robots is your best teacher.
25.3.3 Machine learning in robot perception
Machine learning plays an important role in robot perception. This is particularly the case
when the best internal representation is not known. One common approach is to map high-
dimensional sensor streams into lower-dimensional spaces using unsupervised machine learn-
ing methods (see Chapter 18). Such an approach is called low-dimensional embedding .
LOW-DIMENSIONAL
EMBEDDING
Machine learning makes it possible to learn sensor and motion models from data, while si-
multaneously discovering a suitable internal representations.
Another machine learning technique enables robots to continuously adapt to broad
changes in sensor measurements. Picture yourself walking from a sun-lit space into a dark
neon-lit room. Clearly things are darker inside. But the change of light source also affects all
the colors: Neon light has a stronger component of green light than sunlight. Yet somehow
we seem not to notice the change. If we walk together with people into a neon-lit room, we
don’t think that suddenly their faces turned green. Our perception quickly adapts to the new
lighting conditions, and our brain ignores the differences.
Adaptive perception techniques enable robots to adjust to such changes. One example
is shown in Figure 25.13, taken from the autonomous driving domain. Here an unmanned
ground vehicle adapts its classiﬁer of the concept “drivable surface.” How does this work?
The robot uses a laser to provide classiﬁcation for a small area right in front of the robot.
When this area is found to be ﬂat in the laser range scan, it is used as a positive training
example for the concept “drivable surface.” A mixture-of-Gaussians technique similar to the
EM algorithm discussed in Chapter 20 is then trained to recognize the speciﬁc color and
texture coefﬁcients of the small sample patch. The images in Figure 25.13 are the result of
applying this classiﬁer to the full image.
986 Chapter 25. Robotics
Methods that make robots collect their own training data (with labels!) are called self-
supervised. In this instance, the robot uses machine learning to leverage a short-range sensorSELF-SUPERVISED
LEARNING
that works well for terrain classiﬁcation into a sensor that can see much farther. That allows
the robot to drive faster, slowing down only when the sensor model says there is a change in
the terrain that needs to be examined more carefully by the short-range sensors.
25.4 P LANNING TO MOVE
All of a robot’s deliberations ultimately come down to deciding how to move effectors. The
point-to-point motion problem is to deliver the robot or its end effector to a designated targetPOINT -TO-POINT
MOTION
location. A greater challenge is the compliant motion problem, in which a robot movesCOMPLIANT MOTION
while being in physical contact with an obstacle. An example of compliant motion is a robot
manipulator that screws in a light bulb, or a robot that pushes a box across a table top.
We begin by ﬁnding a suitable representation in which motion-planning problems can
be described and solved. It turns out that the conﬁguration space—the space of robot states
deﬁned by location, orientation, and joint angles—is a better place to work than the original
3D space. The path planning problem is to ﬁnd a path from one conﬁguration to another in
P A TH PLANNING
conﬁguration space. We have already encountered various versions of the path-planning prob-
lem throughout this book; the complication added by robotics is that path planning involves
continuous spaces. There are two main approaches: cell decomposition and skeletonization.
Each reduces the continuous path-planning problem to a discrete graph-search problem. In
this section, we assume that motion is deterministic and that localization of the robot is exact.
Subsequent sections will relax these assumptions.
25.4.1 Conﬁguration space
We will start with a simple representation for a simple robot motion problem. Consider the
robot arm shown in Figure 25.14(a). It has two joints that move independently. Moving
the joints alters the (x, y) coordinates of the elbow and the gripper. (The arm cannot move
in the z direction.) This suggests that the robot’s conﬁguration can be described by a four-
dimensional coordinate: (xe,y e) for the location of the elbow relative to the environment and
(xg,y g) for the location of the gripper. Clearly, these four coordinates characterize the full
state of the robot. They constitute what is known as workspace representation, since theWORKSP ACE
REPRESENTA TION
coordinates of the robot are speciﬁed in the same coordinate system as the objects it seeks to
manipulate (or to avoid). Workspace representations are well-suited for collision checking,
especially if the robot and all objects are represented by simple polygonal models.
The problem with the workspace representation is that not all workspace coordinates
are actually attainable, even in the absence of obstacles. This is because of the linkage con-
straints on the space of attainable workspace coordinates. For example, the elbow position
LINKAGE
CONSTRAINTS
(xe,y e) and the gripper position (xg,y g) are always a ﬁxed distance apart, because they are
joined by a rigid forearm. A robot motion planner deﬁned over workspace coordinates faces
the challenge of generating paths that adhere to these constraints. This is particularly tricky
Section 25.4. Planning to Move 987
shou
elb
s
e
e
table
table
left wall
vertical
obstacle
s
e
s
(a) (b)
Figure 25.14 (a) Workspace representation of a robot arm with 2 DOFs. The workspace
is a box with a ﬂat obstacle hanging from the ceiling. (b) Conﬁguration space of the same
robot. Only white regions in the space are conﬁ gurations that are free of collisions. The dot
in this diagram corresponds to the conﬁguration of the robot shown on the left.
because the state space is continuous and the constraints are nonlinear. It turns out to be eas-
ier to plan with a conﬁguration space representation. Instead of representing the state of theCONFIGURA TION
SP ACE
robot by the Cartesian coordinates of its elements, we represent the state by a conﬁguration
of the robot’s joints. Our example robot possesses two joints. Hence, we can represent its
state with the two angles ϕ
s and ϕe for the shoulder joint and elbow joint, respectively. In
the absence of any obstacles, a robot could freely take on any value in conﬁguration space. In
particular, when planning a path one could simply connect the present conﬁguration and the
target conﬁguration by a straight line. In following this path, the robot would then move its
joints at a constant velocity, until a target location is reached.
Unfortunately, conﬁguration spaces have their own problems. The task of a robot is usu-
ally expressed in workspace coordinates, not in conﬁguration space coordinates. This raises
the question of how to map between workspace coordinates and conﬁguration space. Trans-
forming conﬁguration space coordinates into workspace coordinates is simple: it involves
a series of straightforward coordinate transformations. These transformations are linear for
prismatic joints and trigonometric for revolute joints. This chain of coordinate transformation
is known as kinematics.
KINEMA TICS
The inverse problem of calculating the conﬁguration of a robot whose effector location
is speciﬁed in workspace coordinates is known asinverse kinematics. Calculating the inverseINVERSE
KINEMA TICS
kinematics is hard, especially for robots with many DOFs. In particular, the solution is seldom
unique. Figure 25.14(a) shows one of two possible conﬁgurations that put the gripper in the
same location. (The other conﬁguration would has the elbow below the shoulder.)
988 Chapter 25. Robotics
conf-3
conf-1
conf-2
conf-3
conf-2
conf-1
e
s
s
e
(a) (b)
Figure 25.15 Three robot conﬁgurations, shown in workspace and conﬁguration space.
In general, this two-link robot arm has between zero and two inverse kinematic solu-
tions for any set of workspace coordinates. Most industrial robots have sufﬁcient degrees
of freedom to ﬁnd inﬁnitely many solutions to motion problems. To see how this is possi-
ble, simply imagine that we added a third revolute joint to our example robot, one whose
rotational axis is parallel to the ones of the existing joints. In such a case, we can keep the
location (but not the orientation!) of the gripper ﬁxed and still freely rotate its internal joints,
for most conﬁgurations of the robot. With a few more joints (how many?) we can achieve the
same effect while keeping the orientation of the gripper constant as well. We have already
seen an example of this in the “experiment” of placing your hand on the desk and moving
your elbow. The kinematic constraint of your hand position is insufﬁcient to determine the
conﬁguration of your elbow. In other words, the inverse kinematics of your shoulder–arm
assembly possesses an inﬁnite number of solutions.
The second problem with conﬁguration space representations arises from the obsta-
cles that may exist in the robot’s workspace. Our example in Figure 25.14(a) shows several
such obstacles, including a free-hanging obstacle that protrudes into the center of the robot’s
workspace. In workspace, such obstacles take on simple geometric forms—especially in
most robotics textbooks, which tend to focus on polygonal obstacles. But how do they look
in conﬁguration space?
Figure 25.14(b) shows the conﬁguration space for our example robot, under the speciﬁc
obstacle conﬁguration shown in Figure 25.14(a). The conﬁguration space can be decomposed
into two subspaces: the space of all conﬁgurations that a robot may attain, commonly called
free space, and the space of unattainable conﬁgurations, called occupied space. The white
FREE SP ACE
OCCUPIED SP ACE area in Figure 25.14(b) corresponds to the free space. All other regions correspond to occu-
Section 25.4. Planning to Move 989
pied space. The different shadings of the occupied space corresponds to the different objects
in the robot’s workspace; the black region surrounding the entire free space corresponds to
conﬁgurations in which the robot collides with itself. It is easy to see that extreme values of
the shoulder or elbow angles cause such a violation. The two oval-shaped regions on both
sides of the robot correspond to the table on which the robot is mounted. The third oval region
corresponds to the left wall. Finally, the most interesting object in conﬁguration space is the
vertical obstacle that hangs from the ceiling and impedes the robot’s motions. This object has
a funny shape in conﬁguration space: it is highly nonlinear and at places even concave. With
a little bit of imagination the reader will recognize the shape of the gripper at the upper left
end. We encourage the reader to pause for a moment and study this diagram. The shape of
this obstacle is not at all obvious! The dot inside Figure 25.14(b) marks the conﬁguration of
the robot, as shown in Figure 25.14(a). Figure 25.15 depicts three additional conﬁgurations,
both in workspace and in conﬁguration space. In conﬁguration conf-1, the gripper encloses
the vertical obstacle.
Even if the robot’s workspace is represented by ﬂat polygons, the shape of the free space
can be very complicated. In practice, therefore, one usually probes a conﬁguration space
instead of constructing it explicitly. A planner may generate a conﬁguration and then test to
see if it is in free space by applying the robot kinematics and then checking for collisions in
workspace coordinates.
25.4.2 Cell decomposition methods
The ﬁrst approach to path planning uses cell decomposition —that is, it decomposes theCELL
DECOMPOSITION
free space into a ﬁnite number of contiguous regions, called cells. These regions have the
important property that the path-planning problem within a single region can be solved by
simple means (e.g., moving along a straight line). The path-planning problem then becomes
a discrete graph-search problem, very much like the search problems introduced in Chapter 3.
The simplest cell decomposition consists of a regularly spaced grid. Figure 25.16(a)
shows a square grid decomposition of the space and a solution path that is optimal for this
grid size. Grayscale shading indicates the value of each free-space grid cell—i.e., the cost of
the shortest path from that cell to the goal. (These values can be computed by a deterministic
form of the V
ALUE -ITERATION algorithm given in Figure 17.4 on page 653.) Figure 25.16(b)
shows the corresponding workspace trajectory for the arm. Of course, we can also use the A∗
algorithm to ﬁnd a shortest path.
Such a decomposition has the advantage that it is extremely simple to implement, but
it also suffers from three limitations. First, it is workable only for low-dimensional conﬁgu-
ration spaces, because the number of grid cells increases exponentially withd, the number of
dimensions. Sounds familiar? This is the curse!dimensionality@of dimensionality. Second,
there is the problem of what to do with cells that are “mixed”—that is, neither entirely within
free space nor entirely within occupied space. A solution path that includes such a cell may
not be a real solution, because there may be no way to cross the cell in the desired direction
in a straight line. This would make the path planner unsound. On the other hand, if we insist
that only completely free cells may be used, the planner will be incomplete, because it might
990 Chapter 25. Robotics
start
goal
start
goal
(a) (b)
Figure 25.16 (a) Value function and path found for a discrete grid cell approximation of
the conﬁguration space. (b) The same path visualized in workspace coordinates. Notice how
the robot bends its elbow to avoid a collision with the vertical obstacle.
be the case that the only paths to the goal go through mixed cells—especially if the cell size
is comparable to that of the passageways and clearances in the space. And third, any path
through a discretized state space will not be smooth. It is generally difﬁcult to guarantee that
a smooth solution exists near the discrete path. So a robot may not be able to execute the
solution found through this decomposition.
Cell decomposition methods can be improved in a number of ways, to alleviate some
of these problems. The ﬁrst approach allows further subdivision of the mixed cells—perhaps
using cells of half the original size. This can be continued recursively until a path is found
that lies entirely within free cells. (Of course, the method only works if there is a way to
decide if a given cell is a mixed cell, which is easy only if the conﬁguration space boundaries
have relatively simple mathematical descriptions.) This method is complete provided there is
a bound on the smallest passageway through which a solution must pass. Although it focuses
most of the computational effort on the tricky areas within the conﬁguration space, it still
fails to scale well to high-dimensional problems because each recursive splitting of a cell
creates 2
d smaller cells. A second way to obtain a complete algorithm is to insist on an exact
cell decomposition of the free space. This method must allow cells to be irregularly shapedEXACT CELL
DECOMPOSITION
where they meet the boundaries of free space, but the shapes must still be “simple” in the
sense that it should be easy to compute a traversal of any free cell. This technique requires
some quite advanced geometric ideas, so we shall not pursue it further here.
Examining the solution path shown in Figure 25.16(a), we can see an additional difﬁ-
culty that will have to be resolved. The path contains arbitrarily sharp corners; a robot moving
at any ﬁnite speed could not execute such a path. This problem is solved by storing certain
continuous values for each grid cell. Consider an algorithm which stores, for each grid cell,
Section 25.4. Planning to Move 991
the exact, continuous state that was attained with the cell was ﬁrst expanded in the search.
Assume further, that when propagating information to nearby grid cells, we use this continu-
ous state as a basis, and apply the continuous robot motion model for jumping to nearby cells.
In doing so, we can now guarantee that the resulting trajectory is smooth and can indeed be
executed by the robot. One algorithm that implements this is hybrid A*.
HYBRID A*
25.4.3 Modiﬁed cost functions
Notice that in Figure 25.16, the path goes very close to the obstacle. Anyone who has driven
a car knows that a parking space with one millimeter of clearance on either side is not really a
parking space at all; for the same reason, we would prefer solution paths that are robust with
respect to small motion errors.
This problem can be solved by introducing a potential ﬁeld . A potential ﬁeld is a
POTENTIAL FIELD
function deﬁned over state space, whose value grows with the distance to the closest obstacle.
Figure 25.17(a) shows such a potential ﬁeld—the darker a conﬁguration state, the closer it is
to an obstacle.
The potential ﬁeld can be used as an additional cost term in the shortest-path calculation.
This induces an interesting tradeoff. On the one hand, the robot seeks to minimize path length
to the goal. On the other hand, it tries to stay away from obstacles by virtue of minimizing the
potential function. With the appropriate weight balancing the two objectives, a resulting path
may look like the one shown in Figure 25.17(b). This ﬁgure also displays the value function
derived from the combined cost function, again calculated by value iteration. Clearly, the
resulting path is longer, but it is also safer.
There exist many other ways to modify the cost function. For example, it may be
desirable to smooth the control parameters over time. For example, when driving a car, a
smooth path is better than a jerky one. In general, such higher-order constraints are not easy
to accommodate in the planning process, unless we make the most recent steering command
a part of the state. However, it is often easy to smooth the resulting trajectory after planning,
using conjugate gradient methods. Such post-planning smoothing is essential in many real-
world applications.
25.4.4 Skeletonization methods
The second major family of path-planning algorithms is based on the idea ofskeletonization.SKELETONIZA TION
These algorithms reduce the robot’s free space to a one-dimensional representation, for which
the planning problem is easier. This lower-dimensional representation is called a skeleton of
the conﬁguration space.
Figure 25.18 shows an example skeletonization: it is a V oronoi graph of the freeVORONOI GRAPH
space—the set of all points that are equidistant to two or more obstacles. To do path plan-
ning with a V oronoi graph, the robot ﬁrst changes its present conﬁguration to a point on the
V oronoi graph. It is easy to show that this can always be achieved by a straight-line motion
in conﬁguration space. Second, the robot follows the V oronoi graph until it reaches the point
nearest to the target conﬁguration. Finally, the robot leaves the V oronoi graph and moves to
the target. Again, this ﬁnal step involves straight-line motion in conﬁguration space.
992 Chapter 25. Robotics
start goal
(a) (b)
Figure 25.17 (a) A repelling potential ﬁeld pushe s the robot away from obstacles. (b)
Path found by simultaneously minimizing path length and the potential.
(a) (b)
Figure 25.18 (a) The V oronoi graph is the set of points equidistant to two or more obsta-
cles in conﬁguration space. (b) A probabilistic roadmap, composed of 400 randomly chosen
points in free space.
In this way, the original path-planning problem is reduced to ﬁnding a path on the
V oronoi graph, which is generally one-dimensional (except in certain nongeneric cases) and
has ﬁnitely many points where three or more one-dimensional curves intersect. Thus, ﬁnding
Section 25.5. Planning Uncertain Movements 993
the shortest path along the V oronoi graph is a discrete graph-search problem of the kind
discussed in Chapters 3 and 4. Following the V oronoi graph may not give us the shortest
path, but the resulting paths tend to maximize clearance. Disadvantages of V oronoi graph
techniques are that they are difﬁcult to apply to higher-dimensional conﬁguration spaces, and
that they tend to induce unnecessarily large detours when the conﬁguration space is wide
open. Furthermore, computing the V oronoi graph can be difﬁcult, especially in conﬁguration
space, where the shapes of obstacles can be complex.
An alternative to the V oronoi graphs is the probabilistic roadmap, a skeletonization
PROBABILISTIC
ROADMAP
approach that offers more possible routes, and thus deals better with wide-open spaces. Fig-
ure 25.18(b) shows an example of a probabilistic roadmap. The graph is created by randomly
generating a large number of conﬁgurations, and discarding those that do not fall into free
space. Two nodes are joined by an arc if it is “easy” to reach one node from the other–for
example, by a straight line in free space. The result of all this is a randomized graph in the
robot’s free space. If we add the robot’s start and goal conﬁgurations to this graph, path
planning amounts to a discrete graph search. Theoretically, this approach is incomplete, be-
cause a bad choice of random points may leave us without any paths from start to goal. It
is possible to bound the probability of failure in terms of the number of points generated
and certain geometric properties of the conﬁguration space. It is also possible to direct the
generation of sample points towards the areas where a partial search suggests that a good
path may be found, working bidirectionally from both the start and the goal positions. With
these improvements, probabilistic roadmap planning tends to scale better to high-dimensional
conﬁguration spaces than most alternative path-planning techniques.
25.5 P LANNING UNCERTAIN MOVEMENTS
None of the robot motion-planning algorithms discussed thus far addresses a key characteris-
tic of robotics problems: uncertainty. In robotics, uncertainty arises from partial observability
of the environment and from the stochastic (or unmodeled) effects of the robot’s actions. Er-
rors can also arise from the use of approximation algorithms such as particle ﬁltering, which
does not provide the robot with an exact belief state even if the stochastic nature of the envi-
ronment is modeled perfectly.
Most of today’s robots use deterministic algorithms for decision making, such as the
path-planning algorithms of the previous section. To do so, it is common practice to extract
the most likely state from the probability distribution produced by the state estimation al-
MOST LIKEL Y STA TE
gorithm. The advantage of this approach is purely computational. Planning paths through
conﬁguration space is already a challenging problem; it would be worse if we had to work
with a full probability distribution over states. Ignoring uncertainty in this way works when
the uncertainty is small. In fact, when the environment model changes over time as the result
of incorporating sensor measurements, many robots plan paths online during plan execution.
This is the online replanning technique of Section 11.3.3.
ONLINE REPLANNING
994 Chapter 25. Robotics
Unfortunately, ignoring the uncertainty does not always work. In some problems the
robot’s uncertainty is simply too massive: How can we use a deterministic path planner to
control a mobile robot that has no clue where it is? In general, if the robot’s true state is not
the one identiﬁed by the maximum likelihood rule, the resulting control will be suboptimal.
Depending on the magnitude of the error this can lead to all sorts of unwanted effects, such
as collisions with obstacles.
The ﬁeld of robotics has adopted a range of techniques for accommodating uncertainty.
Some are derived from the algorithms given in Chapter 17 for decision making under uncer-
tainty. If the robot faces uncertainty only in its state transition, but its state is fully observable,
the problem is best modeled as a Markov decision process (MDP). The solution of an MDP is
an optimal policy, which tells the robot what to do in every possible state. In this way, it can
handle all sorts of motion errors, whereas a single-path solution from a deterministic planner
would be much less robust. In robotics, policies are called navigation functions.T h ev a l u e
NAVIGA TION
FUNCTION
function shown in Figure 25.16(a) can be converted into such a navigation function simply
by following the gradient.
Just as in Chapter 17, partial observability makes the problem much harder. The result-
ing robot control problem is a partially observable MDP, or POMDP. In such situations, the
robot maintains an internal belief state, like the ones discussed in Section 25.3. The solution
to a POMDP is a policy deﬁned over the robot’s belief state. Put differently, the input to
the policy is an entire probability distribution. This enables the robot to base its decision not
only on what it knows, but also on what it does not know. For example, if it is uncertain
about a critical state variable, it can rationally invoke aninformation gathering action.T h i s
INFORMA TION
GA THERING ACTION
is impossible in the MDP framework, since MDPs assume full observability. Unfortunately,
techniques that solve POMDPs exactly are inapplicable to robotics—there are no known tech-
niques for high-dimensional continuous spaces. Discretization produces POMDPs that are far
too large to handle. One remedy is to make the minimization of uncertainty a control objec-
tive. For example, the coastal navigation heuristic requires the robot to stay near known
COASTAL
NAVIGA TION
landmarks to decrease its uncertainty. Another approach applies variants of the probabilis-
tic roadmap planning method to the belief space representation. Such methods tend to scale
better to large discrete POMDPs.
25.5.1 Robust methods
Uncertainty can also be handled using so-calledrobust control methods (see page 836) ratherROBUST CONTROL
than probabilistic methods. A robust method is one that assumes a bounded amount of un-
certainty in each aspect of a problem, but does not assign probabilities to values within the
allowed interval. A robust solution is one that works no matter what actual values occur,
provided they are within the assumed interval. An extreme form of robust method is thecon-
formant planning approach given in Chapter 11—it produces plans that work with no state
information at all.
Here, we look at a robust method that is used for ﬁne-motion planning (or FMP) in
FINE-MOTION
PLANNING
robotic assembly tasks. Fine-motion planning involves moving a robot arm in very close
proximity to a static environment object. The main difﬁculty with ﬁne-motion planning is
Section 25.5. Planning Uncertain Movements 995
v
Cv
motion
envelope
initial
configuration
Figure 25.19 A two-dimensional environment, velocity uncertainty cone, and envelope of
possible robot motions. The intended velocity is v, but with uncertainty the actual velocity
could be anywhere inCv, resulting in a ﬁnal conﬁguration somewhere in the motion envelope,
which means we wouldn’t know if we hit the hole or not.
v
Cv
motion
envelope
initial
configuration
Figure 25.20 The ﬁrst motion command and the resulting envelope of possible robot mo-
tions. No matter what the error, we know the ﬁnal conﬁguration will be to the left of the
hole.
that the required motions and the relevant features of the environment are very small. At such
small scales, the robot is unable to measure or control its position accurately and may also be
uncertain of the shape of the environment itself; we will assume that these uncertainties are
all bounded. The solutions to FMP problems will typically be conditional plans or policies
that make use of sensor feedback during execution and are guaranteed to work in all situations
consistent with the assumed uncertainty bounds.
A ﬁne-motion plan consists of a series of guarded motions . Each guarded motion
GUARDED MOTION
consists of (1) a motion command and (2) a termination condition, which is a predicate on the
robot’s sensor values, and returns true to indicate the end of the guarded move. The motion
commands are typically compliant motions that allow the effector to slide if the motion
COMPLIANT MOTION
command would cause collision with an obstacle. As an example, Figure 25.19 shows a two-
dimensional conﬁguration space with a narrow vertical hole. It could be the conﬁguration
space for insertion of a rectangular peg into a hole or a car key into the ignition. The motion
commands are constant velocities. The termination conditions are contact with a surface. To
model uncertainty in control, we assume that instead of moving in the commanded direction,
the robot’s actual motion lies in the cone C
v about it. The ﬁgure shows what would happen
996 Chapter 25. Robotics
v
Cv
motion
envelope
Figure 25.21 The second motion command and the envelope of possible motions. Even
with error, we will eventually get into the hole.
if we commanded a velocity straight down from the initial conﬁguration. Because of the
uncertainty in velocity, the robot could move anywhere in the conical envelope, possibly
going into the hole, but more likely landing to one side of it. Because the robot would not
then know which side of the hole it was on, it would not know which way to move.
A more sensible strategy is shown in Figures 25.20 and 25.21. In Figure 25.20, the
robot deliberately moves to one side of the hole. The motion command is shown in the ﬁgure,
and the termination test is contact with any surface. In Figure 25.21, a motion command is
given that causes the robot to slide along the surface and into the hole. Because all possible
velocities in the motion envelope are to the right, the robot will slide to the right whenever it
is in contact with a horizontal surface. It will slide down the right-hand vertical edge of the
hole when it touches it, because all possible velocities are down relative to a vertical surface.
It will keep moving until it reaches the bottom of the hole, because that is its termination
condition. In spite of the control uncertainty, all possible trajectories of the robot terminate
in contact with the bottom of the hole—that is, unless surface irregularities cause the robot to
stick in one place.
As one might imagine, the problem of constructing ﬁne-motion plans is not trivial; in
fact, it is a good deal harder than planning with exact motions. One can either choose a
ﬁxed number of discrete values for each motion or use the environment geometry to choose
directions that give qualitatively different behavior. A ﬁne-motion planner takes as input the
conﬁguration-space description, the angle of the velocity uncertainty cone, and a speciﬁcation
of what sensing is possible for termination (surface contact in this case). It should produce a
multistep conditional plan or policy that is guaranteed to succeed, if such a plan exists.
Our example assumes that the planner has an exact model of the environment, but it is
possible to allow for bounded error in this model as follows. If the error can be described in
terms of parameters, those parameters can be added as degrees of freedom to the conﬁguration
space. In the last example, if the depth and width of the hole were uncertain, we could add
them as two degrees of freedom to the conﬁguration space. It is impossible to move the
robot in these directions in the conﬁguration space or to sense its position directly. But
both those restrictions can be incorporated when describing this problem as an FMP problem
by appropriately specifying control and sensor uncertainties. This gives a complex, four-
dimensional planning problem, but exactly the same planning techniques can be applied.
Section 25.6. Moving 997
Notice that unlike the decision-theoretic methods in Chapter 17, this kind of robust approach
results in plans designed for the worst-case outcome, rather than maximizing the expected
quality of the plan. Worst-case plans are optimal in the decision-theoretic sense only if failure
during execution is much worse than any of the other costs involved in execution.
25.6 M OVING
So far, we have talked about how to plan motions, but not about how to move. Our plans—
particularly those produced by deterministic path planners—assume that the robot can simply
follow any path that the algorithm produces. In the real world, of course, this is not the case.
Robots have inertia and cannot execute arbitrary paths except at arbitrarily slow speeds. In
most cases, the robot gets to exert forces rather than specify positions. This section discusses
methods for calculating these forces.
25.6.1 Dynamics and control
Section 25.2 introduced the notion of dynamic state, which extends the kinematic state of a
robot by its velocity. For example, in addition to the angle of a robot joint, the dynamic state
also captures the rate of change of the angle, and possibly even its momentary acceleration.
The transition model for a dynamic state representation includes the effect of forces on this
rate of change. Such models are typically expressed via differential equations, which are
DIFFERENTIAL
EQUA TION
equations that relate a quantity (e.g., a kinematic state) to the change of the quantity over
time (e.g., velocity). In principle, we could have chosen to plan robot motion using dynamic
models, instead of our kinematic models. Such a methodology would lead to superior robot
performance, if we could generate the plans. However, the dynamic state has higher dimen-
sion than the kinematic space, and the curse of dimensionality would render many motion
planning algorithms inapplicable for all but the most simple robots. For this reason, practical
robot system often rely on simpler kinematic path planners.
A common technique to compensate for the limitations of kinematic plans is to use a
separate mechanism, a controller, for keeping the robot on track. Controllers are techniques
CONTROLLER
for generating robot controls in real time using feedback from the environment, so as to
achieve a control objective. If the objective is to keep the robot on a preplanned path, it is
o f t e nr e f e r r e dt oa sareference controller and the path is called areference path. ControllersREFERENCE
CONTROLLER
REFERENCE P A TH that optimize a global cost function are known as optimal controllers. Optimal policies for
OPTIMAL
CONTROLLERS continuous MDPs are, in effect, optimal controllers.
On the surface, the problem of keeping a robot on a prespeciﬁed path appears to be
relatively straightforward. In practice, however, even this seemingly simple problem has its
pitfalls. Figure 25.22(a) illustrates what can go wrong; it shows the path of a robot that
attempts to follow a kinematic path. Whenever a deviation occurs—whether due to noise or
to constraints on the forces the robot can apply—the robot provides an opposing force whose
magnitude is proportional to this deviation. Intuitively, this might appear plausible, since
deviations should be compensated by a counterforce to keep the robot on track. However,
998 Chapter 25. Robotics
(a) (b) (c)
Figure 25.22 Robot arm control using (a) proportional control with gain factor 1.0, (b)
proportional control with gain factor 0.1, and (c) PD (proportional derivative) control with
gain factors 0.3 for the proportional component and 0.8 for the differential component. In all
cases the robot arm tries to follow the path shown in gray.
as Figure 25.22(a) illustrates, our controller causes the robot to vibrate rather violently. The
vibration is the result of a natural inertia of the robot arm: once driven back to its reference
position the robot then overshoots, which induces a symmetric error with opposite sign. Such
overshooting may continue along an entire trajectory, and the resulting robot motion is far
from desirable.
Before we can deﬁne a better controller, let us formally describe what went wrong.
Controllers that provide force in negative proportion to the observed error are known as P
controllers. The letter ‘P’ stands for proportional, indicating that the actual control is pro-
P CONTROLLER
portional to the error of the robot manipulator. More formally, let y(t) be the reference path,
parameterized by time index t. The control at generated by a P controller has the form:
at = KP (y(t)−xt) .
Here xt is the state of the robot at timet and KP is a constant known as thegain parameter ofGAIN P ARAMETER
the controller and its value is called the gain factor);Kp regulates how strongly the controller
corrects for deviations between the actual state xt and the desired one y(t). In our example,
KP =1 . At ﬁrst glance, one might think that choosing a smaller value for KP would
remedy the problem. Unfortunately, this is not the case. Figure 25.22(b) shows a trajectory
for KP = .1, still exhibiting oscillatory behavior. Lower values of the gain parameter may
simply slow down the oscillation, but do not solve the problem. In fact, in the absence of
friction, the P controller is essentially a spring law; so it will oscillate indeﬁnitely around a
ﬁxed target location.
Traditionally, problems of this type fall into the realm of control theory ,aﬁ e l do f
increasing importance to researchers in AI. Decades of research in this ﬁeld have led to a large
number of controllers that are superior to the simple control law given above. In particular, a
reference controller is said to bestable if small perturbations lead to a bounded error between
STABLE
the robot and the reference signal. It is said to be strictly stable if it is able to return to andSTRICTL Y STABLE
Section 25.6. Moving 999
then stay on its reference path upon such perturbations. Our P controller appears to be stable
but not strictly stable, since it fails to stay anywhere near its reference trajectory.
The simplest controller that achieves strict stability in our domain is a PD controller.PD CONTROLLER
The letter ‘P’ stands again for proportional, and ‘D’ stands for derivative. PD controllers are
described by the following equation:
at = KP (y(t)−xt)+ KD
∂(y(t)−xt)
∂t . (25.2)
As this equation suggests, PD controllers extend P controllers by a differential component,
which adds to the value of at a term that is proportional to the ﬁrst derivative of the error
y(t)−xt over time. What is the effect of such a term? In general, a derivative term dampens
the system that is being controlled. To see this, consider a situation where the error(y(t)−xt)
is changing rapidly over time, as is the case for our P controller above. The derivative of this
error will then counteract the proportional term, which will reduce the overall response to
the perturbation. However, if the same error persists and does not change, the derivative will
vanish and the proportional term dominates the choice of control.
Figure 25.22(c) shows the result of applying this PD controller to our robot arm, using
as gain parameters K
P = .3 and KD = .8. Clearly, the resulting path is much smoother, and
does not exhibit any obvious oscillations.
PD controllers do have failure modes, however. In particular, PD controllers may fail
to regulate an error down to zero, even in the absence of external perturbations. Often such
a situation is the result of a systematic external force that is not part of the model. An au-
tonomous car driving on a banked surface, for example, may ﬁnd itself systematically pulled
to one side. Wear and tear in robot arms cause similar systematic errors. In such situations,
an over-proportional feedback is required to drive the error closer to zero. The solution to this
problem lies in adding a third term to the control law, based on the integrated error over time:
a
t = KP (y(t)−xt)+ KI
∫
(y(t)−xt)dt + KD
∂(y(t)−xt)
∂t . (25.3)
Here KI is yet another gain parameter. The term
∫
(y(t)−xt)dt calculates the integral of the
error over time. The effect of this term is that long-lasting deviations between the reference
signal and the actual state are corrected. If, for example, x
t is smaller than y(t) for a long
period of time, this integral will grow until the resulting control at forces this error to shrink.
Integral terms, then, ensure that a controller does not exhibit systematic error, at the expense
of increased danger of oscillatory behavior. A controller with all three terms is called a PID
controller (for proportional integral derivative). PID controllers are widely used in industry,PID CONTROLLER
for a variety of control problems.
25.6.2 Potential-ﬁeld control
We introduced potential ﬁelds as an additional cost function in robot motion planning, but
they can also be used for generating robot motion directly, dispensing with the path planning
phase altogether. To achieve this, we have to deﬁne an attractive force that pulls the robot
towards its goal conﬁguration and a repellent potential ﬁeld that pushes the robot away from
obstacles. Such a potential ﬁeld is shown in Figure 25.23. Its single global minimum is
1000 Chapter 25. Robotics
start
goal
 start goal
(a) (b)
Figure 25.23 Potential ﬁeld control. The robot ascends a potential ﬁeld composed of
repelling forces asserted from the obstacles and an attracting force that corresponds to the
goal conﬁguration. (a) Successful path. (b) Local optimum.
the goal conﬁguration, and the value is the sum of the distance to this goal conﬁguration
and the proximity to obstacles. No planning was involved in generating the potential ﬁeld
shown in the ﬁgure. Because of this, potential ﬁelds are well suited to real-time control.
Figure 25.23(a) shows a trajectory of a robot that performs hill climbing in the potential
ﬁeld. In many applications, the potential ﬁeld can be calculated efﬁciently for any given
conﬁguration. Moreover, optimizing the potential amounts to calculating the gradient of the
potential for the present robot conﬁguration. These calculations can be extremely efﬁcient,
especially when compared to path-planning algorithms, all of which are exponential in the
dimensionality of the conﬁguration space (the DOFs) in the worst case.
The fact that the potential ﬁeld approach manages to ﬁnd a path to the goal in such
an efﬁcient manner, even over long distances in conﬁguration space, raises the question as
to whether there is a need for planning in robotics at all. Are potential ﬁeld techniques
sufﬁcient, or were we just lucky in our example? The answer is that we were indeed lucky.
Potential ﬁelds have many local minima that can trap the robot. In Figure 25.23(b), the robot
approaches the obstacle by simply rotating its shoulder joint, until it gets stuck on the wrong
side of the obstacle. The potential ﬁeld is not rich enough to make the robot bend its elbow
so that the arm ﬁts under the obstacle. In other words, potential ﬁeld control is great for local
robot motion but sometimes we still need global planning. Another important drawback with
potential ﬁelds is that the forces they generate depend only on the obstacle and robot positions,
not on the robot’s velocity. Thus, potential ﬁeld control is really a kinematic method and may
fail if the robot is moving quickly.
Section 25.6. Moving 1001
S1S2
S4S3
push backward
lift up set down
retract, lift higher
move
forward no
yes
stuck?
(a) (b)
Figure 25.24 (a) Genghis, a hexapod robot. (b) An augmented ﬁnite state machine
(AFSM) for the control of a single leg. Notice that this AFSM reacts to sensor feedback:
if a leg is stuck during the forward swinging phase, it will be lifted increasingly higher.
25.6.3 Reactive control
So far we have considered control decisions that require some model of the environment for
constructing either a reference path or a potential ﬁeld. There are some difﬁculties with this
approach. First, models that are sufﬁciently accurate are often difﬁcult to obtain, especially
in complex or remote environments, such as the surface of Mars, or for robots that have
few sensors. Second, even in cases where we can devise a model with sufﬁcient accuracy,
computational difﬁculties and localization error might render these techniques impractical.
In some cases, a reﬂex agent architecture using reactive control is more appropriate.
REACTIVE CONTROL
For example, picture a legged robot that attempts to lift a leg over an obstacle. We could
give this robot a rule that says lift the leg a small heighth and move it forward, and if the leg
encounters an obstacle, move it back and start again at a higher height. You could say that h
is modeling an aspect of the world, but we can also think of h as an auxiliary variable of the
robot controller, devoid of direct physical meaning.
One such example is the six-legged (hexapod) robot, shown in Figure 25.24(a), de-
signed for walking through rough terrain. The robot’s sensors are inadequate to obtain mod-
els of the terrain for path planning. Moreover, even if we added sufﬁciently accurate sensors,
the twelve degrees of freedom (two for each leg) would render the resulting path planning
problem computationally intractable.
It is possible, nonetheless, to specify a controller directly without an explicit environ-
mental model. (We have already seen this with the PD controller, which was able to keep a
complex robot arm on targetwithout an explicit model of the robot dynamics; it did, however,
require a reference path generated from a kinematic model.) For the hexapod robot we ﬁrst
choose a gait, or pattern of movement of the limbs. One statically stable gait is to ﬁrst move
GAIT
the right front, right rear, and left center legs forward (keeping the other three ﬁxed), and
then move the other three. This gait works well on ﬂat terrain. On rugged terrain, obstacles
may prevent a leg from swinging forward. This problem can be overcome by a remarkably
simple control rule: when a leg’s forward motion is blocked, simply retract it, lift it higher ,
1002 Chapter 25. Robotics
Figure 25.25 Multiple exposures of an RC helicopter executing a ﬂip based on a policy
learned with reinforcement learning. Images courtesy of Andrew Ng, Stanford University.
and try again. The resulting controller is shown in Figure 25.24(b) as a ﬁnite state machine;
it constitutes a reﬂex agent with state, where the internal state is represented by the index of
the current machine state (s
1 through s4).
Variants of this simple feedback-driven controller have been found to generate remark-
ably robust walking patterns, capable of maneuvering the robot over rugged terrain. Clearly,
such a controller is model-free, and it does not deliberate or use search for generating con-
trols. Environmental feedback plays a crucial role in the controller’s execution. The software
alone does not specify what will actually happen when the robot is placed in an environment.
Behavior that emerges through the interplay of a (simple) controller and a (complex) envi-
ronment is often referred to as emergent behavior. Strictly speaking, all robots discussed
EMERGENT
BEHAVIOR
in this chapter exhibit emergent behavior, due to the fact that no model is perfect. Histori-
cally, however, the term has been reserved for control techniques that do not utilize explicit
environmental models. Emergent behavior is also characteristic of biological organisms.
25.6.4 Reinforcement learning control
One particularly exciting form of control is based on thepolicy search form of reinforcement
learning (see Section 21.5). This work has been enormously inﬂuential in recent years, at
is has solved challenging robotics problems for which previously no solution existed. An
example is acrobatic autonomous helicopter ﬂight. Figure 25.25 shows an autonomous ﬂip
of a small RC (radio-controlled) helicopter. This maneuver is challenging due to the highly
nonlinear nature of the aerodynamics involved. Only the most experienced of human pilots
are able to perform it. Yet a policy search method (as described in Chapter 21), using only a
few minutes of computation, learned a policy that can safely execute a ﬂip every time.
Policy search needs an accurate model of the domain before it can ﬁnd a policy. The
input to this model is the state of the helicopter at time t, the controls at time t,a n dt h e
resulting state at timet+Δ t. The state of a helicopter can be described by the 3D coordinates
of the vehicle, its yaw, pitch, and roll angles, and the rate of change of these six variables.
The controls are the manual controls of of the helicopter: throttle, pitch, elevator, aileron,
and rudder. All that remains is the resulting state—how are we going to deﬁne a model that
accurately says how the helicopter responds to each control? The answer is simple: Let an
expert human pilot ﬂy the helicopter, and record the controls that the expert transmits over
the radio and the state variables of the helicopter. About four minutes of human-controlled
ﬂight sufﬁces to build a predictive model that is sufﬁciently accurate to simulate the vehicle.
Section 25.7. Robotic Software Architectures 1003
What is remarkable about this example is the ease with which this learning approach
solves a challenging robotics problem. This is one of the many successes of machine learning
in scientiﬁc ﬁelds previously dominated by careful mathematical analysis and modeling.
25.7 R OBOTIC SOFTW ARE ARCHITECTURES
A methodology for structuring algorithms is called a software architecture. An architectureSOFTWARE
ARCHITECTURE
includes languages and tools for writing programs, as well as an overall philosophy for how
programs can be brought together.
Modern-day software architectures for robotics must decide how to combine reactive
control and model-based deliberative planning. In many ways, reactive and deliberate tech-
niques have orthogonal strengths and weaknesses. Reactive control is sensor-driven and ap-
propriate for making low-level decisions in real time. However, it rarely yields a plausible
solution at the global level, because global control decisions depend on information that can-
not be sensed at the time of decision making. For such problems, deliberate planning is a
more appropriate choice.
Consequently, most robot architectures use reactive techniques at the lower levels of
control and deliberative techniques at the higher levels. We encountered such a combination
in our discussion of PD controllers, where we combined a (reactive) PD controller with a
(deliberate) path planner. Architectures that combine reactive and deliberate techniques are
called hybrid architectures.
HYBRID
ARCHITECTURE
25.7.1 Subsumption architecture
The subsumption architecture (Brooks, 1986) is a framework for assembling reactive con-SUBSUMPTION
ARCHITECTURE
trollers out of ﬁnite state machines. Nodes in these machines may contain tests for certain
sensor variables, in which case the execution trace of a ﬁnite state machine is conditioned on
the outcome of such a test. Arcs can be tagged with messages that will be generated when
traversing them, and that are sent to the robot’s motors or to other ﬁnite state machines. Addi-
tionally, ﬁnite state machines possess internal timers (clocks) that control the time it takes to
traverse an arc. The resulting machines are refereed to as augmented ﬁnite state machines ,
AUGMENTED FINITE
ST A TE MACHINE
or AFSMs, where the augmentation refers to the use of clocks.
An example of a simple AFSM is the four-state machine shown in Figure 25.24(b),
which generates cyclic leg motion for a hexapod walker. This AFSM implements a cyclic
controller, whose execution mostly does not rely on environmental feedback. The forward
swing phase, however, does rely on sensor feedback. If the leg is stuck, meaning that it has
failed to execute the forward swing, the robot retracts the leg, lifts it up a little higher, and
attempts to execute the forward swing once again. Thus, the controller is able to react to
contingencies arising from the interplay of the robot and its environment.
The subsumption architecture offers additional primitives for synchronizing AFSMs,
and for combining output values of multiple, possibly conﬂicting AFSMs. In this way, it
enables the programmer to compose increasingly complex controllers in a bottom-up fashion.
1004 Chapter 25. Robotics
In our example, we might begin with AFSMs for individual legs, followed by an AFSM for
coordinating multiple legs. On top of this, we might implement higher-level behaviors such
as collision avoidance, which might involve backing up and turning.
The idea of composing robot controllers from AFSMs is quite intriguing. Imagine
how difﬁcult it would be to generate the same behavior with any of the conﬁguration-space
path-planning algorithms described in the previous section. First, we would need an accu-
rate model of the terrain. The conﬁguration space of a robot with six legs, each of which
is driven by two independent motors, totals eighteen dimensions (twelve dimensions for the
conﬁguration of the legs, and six for the location and orientation of the robot relative to its
environment). Even if our computers were fast enough to ﬁnd paths in such high-dimensional
spaces, we would have to worry about nasty effects such as the robot sliding down a slope.
Because of such stochastic effects, a single path through conﬁguration space would almost
certainly be too brittle, and even a PID controller might not be able to cope with such con-
tingencies. In other words, generating motion behavior deliberately is simply too complex a
problem for present-day robot motion planning algorithms.
Unfortunately, the subsumption architecture has its own problems. First, the AFSMs
are driven by raw sensor input, an arrangement that works if the sensor data is reliable and
contains all necessary information for decision making, but fails if sensor data has to be inte-
grated in nontrivial ways over time. Subsumption-style controllers have therefore mostly been
applied to simple tasks, such as following a wall or moving towards visible light sources. Sec-
ond, the lack of deliberation makes it difﬁcult to change the task of the robot. A subsumption-
style robot usually does just one task, and it has no notion of how to modify its controls to
accommodate different goals (just like the dung beetle on page 39). Finally, subsumption-
style controllers tend to be difﬁcult to understand. In practice, the intricate interplay between
dozens of interacting AFSMs (and the environment) is beyond what most human program-
mers can comprehend. For all these reasons, the subsumption architecture is rarely used in
robotics, despite its great historical importance. However, it has had an inﬂuence on other
architectures, and on individual components of some architectures.
25.7.2 Three-layer architecture
Hybrid architectures combine reaction with deliberation. The most popular hybrid architec-
ture is the three-layer architecture, which consists of a reactive layer, an executive layer,
THREE-LAYER
ARCHITECTURE
and a deliberative layer.
The reactive layer provides low-level control to the robot. It is characterized by a tightREACTIVE LAYER
sensor–action loop. Its decision cycle is often on the order of milliseconds.
The executive layer (or sequencing layer) serves as the glue between the reactive layerEXECUTIVE LAYER
and the deliberative layer. It accepts directives by the deliberative layer, and sequences them
for the reactive layer. For example, the executive layer might handle a set of via-points
generated by a deliberative path planner, and make decisions as to which reactive behavior
to invoke. Decision cycles at the executive layer are usually in the order of a second. The
executive layer is also responsible for integrating sensor information into an internal state
representation. For example, it may host the robot’s localization and online mapping routines.
Section 25.7. Robotic Software Architectures 1005
Touareg interface 
Laser mapper 
Wireless E-Stop 
Top level control 
Laser 2 interface 
Laser 3 interface 
Laser 4 interface 
Laser 1 interface 
Laser 5 interface 
Camera interface 
Radar interface Radar mapper 
Vision mapper 
UKF Pose estimation 
Wheel velocity 
GPS position 
GPS compass 
IMU interface Surface assessment 
Health monitor 
Road finder 
Touch screen UI 
Throttle/brake control 
Steering control 
Path planner 
laser map 
vehicle state (pose, velocity) 
velocity limit 
map 
vision map 
vehicle 
state 
obstacle list 
trajectory 
road center 
RDDF database 
driving mode 
pause/disable command 
Power server interface 
clocks 
emergency stop 
power on/off 
Linux processes start/stop heart beats 
corridor 
    SENSOR INTERFACE                PERCEPTION                  PLANNING&CONTROL        USER INTERFACE 
VEHICLE 
INTERFACE 
RDDF corridor (smoothed and original) 
Process controller 
GLOBAL 
SERVICES 
health status 
data 
Data logger File system 
Communication requests 
vehicle state (pose, velocity) 
Brake/steering 
Communication channels 
Inter-process communication (IPC) server Time server 
Figure 25.26 Software architecture of a robot car. This software implements a data
pipeline, in which all modules process data simultaneously.
The deliberative layer generates global solutions to complex tasks using planning.DELIBERA TIVE LAYER
Because of the computational complexity involved in generating such solutions, its decision
cycle is often in the order of minutes. The deliberative layer (or planning layer) uses models
for decision making. Those models might be either learned from data or supplied and may
utilize state information gathered at the executive layer.
Variants of the three-layer architecture can be found in most modern-day robot software
systems. The decomposition into three layers is not very strict. Some robot software systems
possess additional layers, such as user interface layers that control the interaction with people,
or a multiagent level for coordinating a robot’s actions with that of other robots operating in
the same environment.
25.7.3 Pipeline architecture
Another architecture for robots is known as thepipeline architecture. Just like the subsump-PIPELINE
ARCHITECTURE
tion architecture, the pipeline architecture executes multiple process in parallel. However, the
speciﬁc modules in this architecture resemble those in the three-layer architecture.
Figure 25.26 shows an example pipeline architecture, which is used to control an au-
tonomous car. Data enters this pipeline at the sensor interface layer.T h e perception layerSENSOR INTERFACE
LAYER
PERCEPTION LAYER
1006 Chapter 25. Robotics
(a) (b)
Figure 25.27 (a) The Helpmate robot transports food and other medical items in dozens
of hospitals worldwide. (b) Kiva robots are part of a material-handling system for moving
shelves in fulﬁllment centers. Image courtesy of Kiva Systems.
then updates the robot’s internal models of the environment based on this data. Next, these
models are handed to the planning and control layer , which adjusts the robot’s internalPLANNING AND
CONTROL LAYER
plans turns them into actual controls for the robot. Those are then communicated back to the
vehicle through the vehicle interface layer.VEHICLE INTERFACE
LAYER
The key to the pipeline architecture is that this all happens in parallel. While the per-
ception layer processes the most recent sensor data, the control layer bases its choices on
slightly older data. In this way, the pipeline architecture is similar to the human brain. We
don’t switch off our motion controllers when we digest new sensor data. Instead, we perceive,
plan, and act all at the same time. Processes in the pipeline architecture run asynchronously,
and all computation is data-driven. The resulting system is robust, and it is fast.
The architecture in Figure 25.26 also contains other, cross-cutting modules, responsible
for establishing communication between the different elements of the pipeline.
25.8 A PPLICA TION DOMAINS
Here are some of the prime application domains for robotic technology.
Industry and Agriculture. Traditionally, robots have been ﬁelded in areas that require
difﬁcult human labor, yet are structured enough to be amenable to robotic automation. The
best example is the assembly line, where manipulators routinely perform tasks such as as-
sembly, part placement, material handling, welding, and painting. In many of these tasks,
robots have become more cost-effective than human workers. Outdoors, many of the heavy
machines that we use to harvest, mine, or excavate earth have been turned into robots. For
Section 25.8. Application Domains 1007
(a) (b)
Figure 25.28 (a) Robotic car B OSS , which won the DARPA Urban Challenge. Courtesy
of Carnegie Mellon University. (b) Surgical robots in the operating room. Image courtesy of
da Vinci Surgical Systems.
example, a project at Carnegie Mellon University has demonstrated that robots can strip paint
off large ships about 50 times faster than people can, and with a much reduced environmental
impact. Prototypes of autonomous mining robots have been found to be faster and more pre-
cise than people in transporting ore in underground mines. Robots have been used to generate
high-precision maps of abandoned mines and sewer systems. While many of these systems
are still in their prototype stages, it is only a matter of time until robots will take over much
of the semimechanical work that is presently performed by people.
Transportation. Robotic transportation has many facets: from autonomous helicopters
that deliver payloads to hard-to-reach locations, to automatic wheelchairs that transport peo-
ple who are unable to control wheelchairs by themselves, to autonomous straddle carriers that
outperform skilled human drivers when transporting containers from ships to trucks on load-
ing docks. A prime example of indoor transportation robots, or gofers, is the Helpmate robot
shown in Figure 25.27(a). This robot has been deployed in dozens of hospitals to transport
food and other items. In factory settings, autonomous vehicles are now routinely deployed
to transport goods in warehouses and between production lines. The Kiva system, shown in
Figure 25.27(b), helps workers at fulﬁllment centers package goods into shipping containers.
Many of these robots require environmental modiﬁcations for their operation. The most
common modiﬁcations are localization aids such as inductive loops in the ﬂoor, active bea-
cons, or barcode tags. An open challenge in robotics is the design of robots that can use
natural cues, instead of artiﬁcial devices, to navigate, particularly in environments such as the
deep ocean where GPS is unavailable.
Robotic cars. Most of use cars every day. Many of us make cell phone calls while
driving. Some of us even text. The sad result: more than a million people die every year in
trafﬁc accidents. Robotic cars like B
OSS and STANLEY offer hope: Not only will they make
driving much safer, but they will also free us from the need to pay attention to the road during
our daily commute.
Progress in robotic cars was stimulated by the DARPA Grand Challenge, a race over
100 miles of unrehearsed desert terrain, which represented a much more challenging task than
1008 Chapter 25. Robotics
(a) (b)
Figure 25.29 (a) A robot mapping an abandoned coal mine. (b) A 3D map of the mine
acquired by the robot.
had ever been accomplished before. Stanford’s STANLEY vehicle completed the course in less
than seven hours in 2005, winning a $2 million prize and a place in the National Museum of
American History. Figure 25.28(a) depicts B OSS , which in 2007 won the DARPA Urban
Challenge, a complicated road race on city streets where robots faced other robots and had to
obey trafﬁc rules.
Health care. Robots are increasingly used to assist surgeons with instrument placement
when operating on organs as intricate as brains, eyes, and hearts. Figure 25.28(b) shows such
a system. Robots have become indispensable tools in a range of surgical procedures, such as
hip replacements, thanks to their high precision. In pilot studies, robotic devices have been
found to reduce the danger of lesions when performing colonoscopy. Outside the operating
room, researchers have begun to develop robotic aides for elderly and handicapped people,
such as intelligent robotic walkers and intelligent toys that provide reminders to take medica-
tion and provide comfort. Researchers are also working on robotic devices for rehabilitation
that aid people in performing certain exercises.
Hazardous environments. Robots have assisted people in cleaning up nuclear waste,
most notably in Chernobyl and Three Mile Island. Robots were present after the collapse
of the World Trade Center, where they entered structures deemed too dangerous for human
search and rescue crews.
Some countries have used robots to transport ammunition and to defuse bombs—a no-
toriously dangerous task. A number of research projects are presently developing prototype
robots for clearing mineﬁelds, on land and at sea. Most existing robots for these tasks are
teleoperated—a human operates them by remote control. Providing such robots with auton-
omy is an important next step.
Exploration. Robots have gone where no one has gone before, including the surface
of Mars (see Figure 25.2(b) and the cover). Robotic arms assist astronauts in deploying
and retrieving satellites and in building the International Space Station. Robots also help
explore under the sea. They are routinely used to acquire maps of sunken ships. Figure 25.29
shows a robot mapping an abandoned coal mine, along with a 3D model of the mine acquired
Section 25.8. Application Domains 1009
(a) (b)
Figure 25.30 (a) Roomba, the world’s best-selling mobile robot, vacuums ﬂoors. Image
courtesy of iRobot, c⃝ 2009. (b) Robotic hand modeled after human hand. Image courtesy
of University of Washington and Carnegie Mellon University.
using range sensors. In 1996, a team of researches released a legged robot into the crater
of an active volcano to acquire data for climate research. Unmanned air vehicles known as
drones are used in military operations. Robots are becoming very effective tools for gathering
DRONE
information in domains that are difﬁcult (or dangerous) for people to access.
Personal Services. Service is an up-and-coming application domain of robotics. Ser-
vice robots assist individuals in performing daily tasks. Commercially available domestic
service robots include autonomous vacuum cleaners, lawn mowers, and golf caddies. The
world’s most popular mobile robot is a personal service robot: the robotic vacuum cleaner
Roomba, shown in Figure 25.30(a). More than three million Roombas have been sold.
ROOMBA
Roomba can navigate autonomously and perform its tasks without human help.
Other service robots operate in public places, such as robotic information kiosks that
have been deployed in shopping malls and trade fairs, or in museums as tour guides. Ser-
vice tasks require human interaction, and the ability to cope robustly with unpredictable and
dynamic environments.
Entertainment. Robots have begun to conquer the entertainment and toy industry.
In Figure 25.6(b) we see robotic soccer, a competitive game very much like human soc-
ROBOTIC SOCCER
cer, but played with autonomous mobile robots. Robot soccer provides great opportunities
for research in AI, since it raises a range of problems relevant to many other, more serious
robot applications. Annual robotic soccer competitions have attracted large numbers of AI
researchers and added a lot of excitement to the ﬁeld of robotics.
Human augmentation. A ﬁnal application domain of robotic technology is that of
human augmentation. Researchers have developed legged walking machines that can carry
people around, very much like a wheelchair. Several research efforts presently focus on the
development of devices that make it easier for people to walk or move their arms by providing
additional forces through extraskeletal attachments. If such devices are attached permanently,
1010 Chapter 25. Robotics
they can be thought of as artiﬁcial robotic limbs. Figure 25.30(b) shows a robotic hand that
may serve as a prosthetic device in the future.
Robotic teleoperation, or telepresence, is another form of human augmentation. Tele-
operation involves carrying out tasks over long distances with the aid of robotic devices.
A popular conﬁguration for robotic teleoperation is the master–slave conﬁguration, where
a robot manipulator emulates the motion of a remote human operator, measured through a
haptic interface. Underwater vehicles are often teleoperated; the vehicles can go to a depth
that would be dangerous for humans but can still be guided by the human operator. All these
systems augment people’s ability to interact with their environments. Some projects go as far
as replicating humans, at least at a very superﬁcial level. Humanoid robots are now available
commercially through several companies in Japan.
25.9 S UMMARY
Robotics concerns itself with intelligent agents that manipulate the physical world. In this
chapter, we have learned the following basics of robot hardware and software.
•Robots are equipped with sensors for perceiving their environment and effectors with
which they can assert physical forces on their environment. Most robots are either
manipulators anchored at ﬁxed locations or mobile robots that can move.
•Robotic perception concerns itself with estimating decision-relevant quantities from
sensor data. To do so, we need an internal representation and a method for updating
this internal representation over time. Common examples of hard perceptual problems
include localization, mapping, and object recognition .
•Probabilistic ﬁltering algorithms such as Kalman ﬁlters and particle ﬁlters are useful
for robot perception. These techniques maintain the belief state, a posterior distribution
over state variables.
•The planning of robot motion is usually done inconﬁguration space, where each point
speciﬁes the location and orientation of the robot and its joint angles.
•Conﬁguration space search algorithms include cell decomposition techniques, which
decompose the space of all conﬁgurations into ﬁnitely many cells, and skeletonization
techniques, which project conﬁguration spaces onto lower-dimensional manifolds. The
motion planning problem is then solved using search in these simpler structures.
•A path found by a search algorithm can be executed by using the path as the reference
trajectory for a PID controller. Controllers are necessary in robotics to accommodate
small perturbations; path planning alone is usually insufﬁcient.
•Potential ﬁeld techniques navigate robots by potential functions, deﬁned over the dis-
tance to obstacles and the goal location. Potential ﬁeld techniques may get stuck in
local minima, but they can generate motion directly without the need for path planning.
•Sometimes it is easier to specify a robot controller directly, rather than deriving a path
from an explicit model of the environment. Such controllers can often be written as
simple ﬁnite state machines.
Bibliographical and Historical Notes 1011
•There exist different architectures for software design. The subsumption architec-
ture enables programmers to compose robot controllers from interconnected ﬁnite state
machines. Three-layer architectures are common frameworks for developing robot
software that integrate deliberation, sequencing of subgoals, and control. The related
pipeline architecture processes data in parallel through a sequence of modules, corre-
sponding to perception, modeling, planning, control, and robot interfaces.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
The word robot was popularized by Czech playwright Karel Capek in his 1921 play R.U.R.
(Rossum’s Universal Robots). The robots, which were grown chemically rather than con-
structed mechanically, end up resenting their masters and decide to take over. It appears
(Glanc, 1978) it was Capek’s brother, Josef, who ﬁrst combined the Czech words “robota”
(obligatory work) and “robotnik” (serf) to yield “robot” in his 1917 short story Opilec.
The term robotics was ﬁrst used by Asimov (1950). Robotics (under other names) has
a much longer history, however. In ancient Greek mythology, a mechanical man named Talos
was supposedly designed and built by Hephaistos, the Greek god of metallurgy. Wonderful
automata were built in the 18th century—Jacques Vaucanson’s mechanical duck from 1738
being one early example—but the complex behaviors they exhibited were entirely ﬁxed in
advance. Possibly the earliest example of a programmable robot-like device was the Jacquard
loom (1805), described on page 14.
The ﬁrst commercial robot was a robot arm calledUnimate, short for universal automa-
UNIMA TE
tion, developed by Joseph Engelberger and George Devol. In 1961, the ﬁrst Unimate robot
was sold to General Motors, where it was used for manufacturing TV picture tubes. 1961
was also the year when Devol obtained the ﬁrst U.S. patent on a robot. Eleven years later, in
1972, Nissan Corp. was among the ﬁrst to automate an entire assembly line with robots, de-
veloped by Kawasaki with robots supplied by Engelberger and Devol’s company Unimation.
This development initiated a major revolution that took place mostly in Japan and the U.S.,
and that is still ongoing. Unimation followed up in 1978 with the development of thePUMA
PUMA
robot, short for Programmable Universal Machine for Assembly. The PUMA robot, initially
developed for General Motors, was thede facto standard for robotic manipulation for the two
decades that followed. At present, the number of operating robots is estimated at one million
worldwide, more than half of which are installed in Japan.
The literature on robotics research can be divided roughly into two parts: mobile robots
and stationary manipulators. Grey Walter’s “turtle,” built in 1948, could be considered the
ﬁrst autonomous mobile robot, although its control system was not programmable. The “Hop-
kins Beast,” built in the early 1960s at Johns Hopkins University, was much more sophisti-
cated; it had pattern-recognition hardware and could recognize the cover plate of a standard
AC power outlet. It was capable of searching for outlets, plugging itself in, and then recharg-
ing its batteries! Still, the Beast had a limited repertoire of skills. The ﬁrst general-purpose
mobile robot was “Shakey,” developed at what was then the Stanford Research Institute (now
1012 Chapter 25. Robotics
SRI) in the late 1960s (Fikes and Nilsson, 1971; Nilsson, 1984). Shakey was the ﬁrst robot
to integrate perception, planning, and execution, and much subsequent research in AI was
inﬂuenced by this remarkable achievement. Shakey appears on the cover of this book with
project leader Charlie Rosen (1917–2002). Other inﬂuential projects include the Stanford
Cart and the CMU Rover (Moravec, 1983). Cox and Wilfong (1990) describes classic work
on autonomous vehicles.
The ﬁeld of robotic mapping has evolved from two distinct origins. The ﬁrst thread
began with work by Smith and Cheeseman (1986), who applied Kalman ﬁlters to the si-
multaneous localization and mapping problem. This algorithm was ﬁrst implemented by
Moutarlier and Chatila (1989), and later extended by Leonard and Durrant-Whyte (1992);
see Dissanayake et al. (2001) for an overview of early Kalman ﬁlter variations. The second
thread began with the development of the occupancy grid representation for probabilistic
OCCUP ANCY GRID
mapping, which speciﬁes the probability that each (x, y) location is occupied by an obsta-
cle (Moravec and Elfes, 1985). Kuipers and Levitt (1988) were among the ﬁrst to propose
topological rather than metric mapping, motivated by models of human spatial cognition. A
seminal paper by Lu and Milios (1997) recognized the sparseness of the simultaneous local-
ization and mapping problem, which gave rise to the development of nonlinear optimization
techniques by Konolige (2004) and Montemerlo and Thrun (2004), as well as hierarchical
methods by Bosse et al. (2004). Shatkay and Kaelbling (1997) and Thrun et al. (1998) intro-
duced the EM algorithm into the ﬁeld of robotic mapping for data association. An overview
of probabilistic mapping methods can be found in (Thrun et al., 2005).
Early mobile robot localization techniques are surveyed by Borenstein et al. (1996).
Although Kalman ﬁltering was well known as a localization method in control theory for
decades, the general probabilistic formulation of the localization problem did not appear
in the AI literature until much later, through the work of Tom Dean and colleagues (Dean
et al., 1990, 1990) and of Simmons and Koenig (1995). The latter work introduced the term
Markov localization. The ﬁrst real-world application of this technique was by Burgardet al.
MARKOV
LOCALIZA TION
(1999), through a series of robots that were deployed in museums. Monte Carlo localiza-
tion based on particle ﬁlters was developed by Fox et al. (1999) and is now widely used.
The Rao-Blackwellized particle ﬁlter combines particle ﬁltering for robot localization with
RAO-
BLACKWELLIZED
PA RT I C L E F I LT E R
exact ﬁltering for map building (Murphy and Russell, 2001; Montemerlo et al., 2002).
The study of manipulator robots, originally called hand–eye machines ,h a se v o l v e dHAND–EYE
MACHINES
along quite different lines. The ﬁrst major effort at creating a hand–eye machine was Hein-
rich Ernst’s MH-1, described in his MIT Ph.D. thesis (Ernst, 1961). The Machine Intelligence
project at Edinburgh also demonstrated an impressive early system for vision-based assem-
bly called F
REDDY (Michie, 1972). After these pioneering efforts, a great deal of work fo-
cused on geometric algorithms for deterministic and fully observable motion planning prob-
lems. The PSPACE-hardness of robot motion planning was shown in a seminal paper by
Reif (1979). The conﬁguration space representation is due to Lozano-Perez (1983). A series
of papers by Schwartz and Sharir on what they called piano movers problems (Schwartz
PIANO MOVERS
et al., 1987) was highly inﬂuential.
Recursive cell decomposition for conﬁguration space planning was originated by Brooks
and Lozano-Perez (1985) and improved signiﬁcantly by Zhu and Latombe (1991). The ear-
Bibliographical and Historical Notes 1013
liest skeletonization algorithms were based on V oronoi diagrams (Rowat, 1979) and visi-
bility graphs (Wesley and Lozano-Perez, 1979). Guibas et al. (1992) developed efﬁcientVISIBILITY GRAPH
techniques for calculating V oronoi diagrams incrementally, and Choset (1996) generalized
V oronoi diagrams to broader motion-planning problems. John Canny (1988) established the
ﬁrst singly exponential algorithm for motion planning. The seminal text by Latombe (1991)
covers a variety of approaches to motion-planning, as do the texts by Chosetet al. (2004) and
LaValle (2006). Kavraki et al. (1996) developed probabilistic roadmaps, which are currently
one of the most effective methods. Fine-motion planning with limited sensing was investi-
gated by Lozano-Perez et al. (1984) and Canny and Reif (1987). Landmark-based naviga-
tion (Lazanas and Latombe, 1992) uses many of the same ideas in the mobile robot arena.
Key work applying POMDP methods (Section 17.4) to motion planning under uncertainty in
robotics is due to Pineau et al. (2003) and Roy et al. (2005).
The control of robots as dynamical systems—whether for manipulation or navigation—
has generated a huge literature that is barely touched on by this chapter. Important works
include a trilogy on impedance control by Hogan (1985) and a general study of robot dy-
namics by Featherstone (1987). Dean and Wellman (1991) were among the ﬁrst to try to tie
together control theory and AI planning systems. Three classic textbooks on the mathematics
of robot manipulation are due to Paul (1981), Craig (1989), and Yoshikawa (1990). The area
of grasping is also important in robotics—the problem of determining a stable grasp is quite
GRASPING
difﬁcult (Mason and Salisbury, 1985). Competent grasping requires touch sensing, or haptic
feedback, to determine contact forces and detect slip (Fearing and Hollerbach, 1985).HAPTIC FEEDBACK
Potential-ﬁeld control, which attempts to solve the motion planning and control prob-
lems simultaneously, was introduced into the robotics literature by Khatib (1986). In mobile
robotics, this idea was viewed as a practical solution to the collision avoidance problem, and
was later extended into an algorithm called vector ﬁeld histograms by Borenstein (1991).
VECTOR FIELD
HISTOGRAMS
Navigation functions, the robotics version of a control policy for deterministic MDPs, were
introduced by Koditschek (1987). Reinforcement learning in robotics took off with the semi-
nal work by Bagnell and Schneider (2001) and Nget al. (2004), who developed the paradigm
in the context of autonomous helicopter control.
The topic of software architectures for robots engenders much religious debate. The
good old-fashioned AI candidate—the three-layer architecture—dates back to the design of
Shakey and is reviewed by Gat (1998). The subsumption architecture is due to Brooks (1986),
although similar ideas were developed independently by Braitenberg (1984), whose book,
V ehicles, describes a series of simple robots based on the behavioral approach. The suc-
cess of Brooks’s six-legged walking robot was followed by many other projects. Connell,
in his Ph.D. thesis (1989), developed a mobile robot capable of retrieving objects that was
entirely reactive. Extensions of the behavior-based paradigm to multirobot systems can be
found in (Mataric, 1997) and (Parker, 1996). GRL (Horswill, 2000) and C
OLBERT (Kono-
lige, 1997) abstract the ideas of concurrent behavior-based robotics into general robot control
languages. Arkin (1998) surveys some of the most popular approaches in this ﬁeld.
Research on mobile robotics has been stimulated over the last decade by several impor-
tant competitions. The earliest competition, AAAI’s annual mobile robot competition, began
in 1992. The ﬁrst competition winner was C ARMEL (Congdon et al., 1992). Progress has
1014 Chapter 25. Robotics
been steady and impressive: in more recent competitions robots entered the conference com-
plex, found their way to the registration desk, registered for the conference, and even gave a
short talk. The Robocup competition, launched in 1995 by Kitano and colleagues (1997a),
ROBOCUP
aims to “develop a team of fully autonomous humanoid robots that can win against the hu-
man world champion team in soccer” by 2050. Play occurs in leagues for simulated robots,
wheeled robots of different sizes, and humanoid robots. In 2009 teams from 43 countries
participated and the event was broadcast to millions of viewers. Visser and Burkhard (2007)
track the improvements that have been made in perception, team coordination, and low-level
skills over the past decade.
The DARPA Grand Challenge , organized by DARPA in 2004 and 2005, required
DARP A GRAND
CHALLENGE
autonomous robots to travel more than 100 miles through unrehearsed desert terrain in less
than 10 hours (Buehler et al., 2006). In the original event in 2004, no robot traveled more
than 8 miles, leading many to believe the prize would never be claimed. In 2005, Stanford’s
robot S
TANLEY won the competition in just under 7 hours of travel (Thrun, 2006). DARPA
then organized the Urban Challenge, a competition in which robots had to navigate 60 milesURBAN CHALLENGE
in an urban environment with other trafﬁc. Carnegie Mellon University’s robot B OSS took
ﬁrst place and claimed the $2 million prize (Urmson and Whittaker, 2008). Early pioneers in
the development of robotic cars included Dickmanns and Zapp (1987) and Pomerleau (1993).
Two early textbooks, by Dudek and Jenkin (2000) and Murphy (2000), cover robotics
generally. A more recent overview is due to Bekey (2008). An excellent book on robot
manipulation addresses advanced topics such as compliant motion (Mason, 2001). Robot
motion planning is covered in Choset et al. (2004) and LaValle (2006). Thrun et al. (2005)
provide an introduction into probabilistic robotics. The premiere conference for robotics is
Robotics: Science and Systems Conference, followed by the IEEE International Conference
on Robotics and Automation. Leading robotics journals include IEEE Robotics and Automa-
tion,t h eInternational Journal of Robotics Research,a n dRobotics and Autonomous Systems.
EXERCISES
25.1 Monte Carlo localization is biased for any ﬁnite sample size—i.e., the expected value
of the location computed by the algorithm differs from the true expected value—because of
the way particle ﬁltering works. In this question, you are asked to quantify this bias.
To simplify, consider a world with four possible robot locations:X ={x1,x2,x3,x4}.
Initially, we draw N ≥1 samples uniformly from among those locations. As usual, it is
perfectly acceptable if more than one sample is generated for any of the locations X.L e tZ
be a Boolean sensor variable characterized by the following conditional probabilities:
P(z| x1)=0 .8 P(¬z| x1)=0 .2
P(z| x2)=0 .4 P(¬z| x2)=0 .6
P(z| x3)=0 .1 P(¬z| x3)=0 .9
P(z| x4)=0 .1 P(¬z| x4)=0 .9 .
Exercises 1015
B
A A
B
Starting configuration <−0.5, 7> Ending configuration <−0.5, −7> 
Figure 25.31 A Robot manipulator in two of its possible conﬁgurations.
MCL uses these probabilities to generate particle weights, which are subsequently normalized
and used in the resampling process. For simplicity, let us assume we generate only one new
sample in the resampling process, regardless of N. This sample might correspond to any of
the four locations in X. Thus, the sampling process deﬁnes a probability distribution overX.
a. What is the resulting probability distribution over X for this new sample? Answer this
question separately for N =1 ,..., 10,a n df o rN =∞.
b. The difference between two probability distributions P and Q can be measured by the
KL divergence, which is deﬁned as
KL(P,Q )=
∑
i
P(xi)l o gP(xi)
Q(xi) .
What are the KL divergences between the distributions in (a) and the true posterior?
c. What modiﬁcation of the problem formulation (not the algorithm!) would guarantee
that the speciﬁc estimator above is unbiased even for ﬁnite values of N? Provide at
least two such modiﬁcations (each of which should be sufﬁcient).
25.2 Implement Monte Carlo localization for a simulated robot with range sensors. A grid
map and range data are available from the code repository at aima.cs.berkeley.edu.
You should demonstrate successful global localization of the robot.
25.3 Consider a robot with two simple manipulators, as shown in ﬁgure 25.31. Manipulator
A is a square block of side 2 which can slide back and on a rod that runs along the x-axis
from x=−10 to x=10. Manipulator B is a square block of side 2 which can slide back and
on a rod that runs along the y-axis from y= −10 to y=10. The rods lie outside the plane of
1016 Chapter 25. Robotics
manipulation, so the rods do not interfere with the movement of the blocks. A conﬁguration
is then a pair⟨x, y⟩ where x is the x-coordinate of the center of manipulator A and wherey is
the y-coordinate of the center of manipulator B. Draw the conﬁguration space for this robot,
indicating the permitted and excluded zones.
25.4 Suppose that you are working with the robot in Exercise 25.3 and you are given the
problem of ﬁnding a path from the starting conﬁguration of ﬁgure 25.31 to the ending con-
ﬁguration. Consider a potential function
D(A,Goal)
2 + D(B, Goal)2 + 1
D(A, B)2
where D(A, B) is the distance between the closest points of A and B.
a. Show that hill climbing in this potential ﬁeld will get stuck in a local minimum.
b. Describe a potential ﬁeld where hill climbing will solve this particular problem. You
need not work out the exact numerical coefﬁcients needed, just the general form of the
solution. (Hint: Add a term that “rewards” the hill climber for moving A out of B’s
way, even in a case like this where this does not reduce the distance from A to B in the
above sense.)
25.5 Consider the robot arm shown in Figure 25.14. Assume that the robot’s base element
is 60cm long and that its upper arm and forearm are each 40cm long. As argued on page 987,
the inverse kinematics of a robot is often not unique. State an explicit closed-form solution of
the inverse kinematics for this arm. Under what exact conditions is the solution unique?
25.6 Implement an algorithm for calculating the V oronoi diagram of an arbitrary 2D en-
vironment, described by an n×n Boolean array. Illustrate your algorithm by plotting the
V oronoi diagram for 10 interesting maps. What is the complexity of your algorithm?
25.7 This exercise explores the relationship between workspace and conﬁguration space
using the examples shown in Figure 25.32.
a. Consider the robot conﬁgurations shown in Figure 25.32(a) through (c), ignoring the
obstacle shown in each of the diagrams. Draw the corresponding arm conﬁgurations in
conﬁguration space. (Hint: Each arm conﬁguration maps to a single point in conﬁgura-
tion space, as illustrated in Figure 25.14(b).)
b. Draw the conﬁguration space for each of the workspace diagrams in Figure 25.32(a)–
(c). ( Hint: The conﬁguration spaces share with the one shown in Figure 25.32(a) the
region that corresponds to self-collision, but differences arise from the lack of enclosing
obstacles and the different locations of the obstacles in these individual ﬁgures.)
c. For each of the black dots in Figure 25.32(e)–(f), draw the corresponding conﬁgurations
of the robot arm in workspace. Please ignore the shaded regions in this exercise.
d. The conﬁguration spaces shown in Figure 25.32(e)–(f) have all been generated by a
single workspace obstacle (dark shading), plus the constraints arising from the self-
collision constraint (light shading). Draw, for each diagram, the workspace obstacle
that corresponds to the darkly shaded area.
Exercises 1017
(a) (b) (c)
(d) (e) (f)
Figure 25.32 Diagrams for Exercise 25.7.
e. Figure 25.32(d) illustrates that a single planar obstacle can decompose the workspace
into two disconnected regions. What is the maximum number of disconnected re-
gions that can be created by inserting a planar obstacle into an obstacle-free, connected
workspace, for a 2DOF robot? Give an example, and argue why no larger number of
disconnected regions can be created. How about a non-planar obstacle?
25.8 Consider a mobile robot moving on a horizontal surface. Suppose that the robot can
execute two kinds of motions:
•Rolling forward a speciﬁed distance.
•Rotating in place through a speciﬁed angle.
The state of such a robot can be characterized in terms of three parameters ⟨x, y, φ, the x-
coordinate and y-coordinate of the robot (more precisely, of its center of rotation) and the
robot’s orientation expressed as the angle from the positive x direction. The action “Roll(D)”
has the effect of changing state ⟨x, y, φto⟨x + D cos(φ),y + D sin(φ),φ⟩, and the action
Rotate(θ) has the effect of changing state⟨x, y, φ⟩ to⟨x, y, φ+ θ⟩.
a. Suppose that the robot is initially at⟨0,0,0⟩ and then executes the actionsRotate(60
◦),
Roll(1), Rotate(25◦), Roll(2). What is the ﬁnal state of the robot?
1018 Chapter 25. Robotics
robot
sensor
range
goal
Figure 25.33 Simpliﬁed robot in a maze. See Exercise 25.9.
b. Now suppose that the robot has imperfect control of its own rotation, and that, if it
attempts to rotate byθ, it may actually rotate by any angle betweenθ−10◦ and θ+10 ◦.
In that case, if the robot attempts to carry out the sequence of actions in (A), there is
a range of possible ending states. What are the minimal and maximal values of the
x-coordinate, the y-coordinate and the orientation in the ﬁnal state?
c. Let us modify the model in (B) to a probabilistic model in which, when the robot
attempts to rotate by θ, its actual angle of rotation follows a Gaussian distribution
with mean θ and standard deviation 10
◦. Suppose that the robot executes the actions
Rotate(90◦), Roll(1). Give a simple argument that (a) the expected value of the loca-
tion at the end is not equal to the result of rotating exactly 90◦ and then rolling forward
1 unit, and (b) that the distribution of locations at the end does not follow a Gaussian.
(Do not attempt to calculate the true mean or the true distribution.)
The point of this exercise is that rotational uncertainty quickly gives rise to a lot of
positional uncertainty and that dealing with rotational uncertainty is painful, whether
uncertainty is treated in terms of hard intervals or probabilistically, due to the fact that
the relation between orientation and position is both non-linear and non-monotonic.
25.9 Consider the simpliﬁed robot shown in Figure 25.33. Suppose the robot’s Cartesian
coordinates are known at all times, as are those of its goal location. However, the locations
of the obstacles are unknown. The robot can sense obstacles in its immediate proximity, as
illustrated in this ﬁgure. For simplicity, let us assume the robot’s motion is noise-free, and
the state space is discrete. Figure 25.33 is only one example; in this exercise you are required
to address all possible grid worlds with a valid path from the start to the goal location.
a. Design a deliberate controller that guarantees that the robot always reaches its goal
location if at all possible. The deliberate controller can memorize measurements in the
form of a map that is being acquired as the robot moves. Between individual moves, it
may spend arbitrary time deliberating.
Exercises 1019
b. Now design a reactive controller for the same task. This controller may not memorize
past sensor measurements. (It may not build a map!) Instead, it has to make all decisions
based on the current measurement, which includes knowledge of its own location and
that of the goal. The time to make a decision must be independent of the environment
size or the number of past time steps. What is the maximum number of steps that it may
take for your robot to arrive at the goal?
c. How will your controllers from (a) and (b) perform if any of the following six conditions
apply: continuous state space, noise in perception, noise in motion, noise in both per-
ception and motion, unknown location of the goal (the goal can be detected only when
within sensor range), or moving obstacles. For each condition and each controller, give
an example of a situation where the robot fails (or explain why it cannot fail).
25.10 In Figure 25.24(b) on page 1001, we encountered an augmented ﬁnite state machine
for the control of a single leg of a hexapod robot. In this exercise, the aim is to design an
AFSM that, when combined with six copies of the individual leg controllers, results in efﬁ-
cient, stable locomotion. For this purpose, you have to augment the individual leg controller
to pass messages to your new AFSM and to wait until other messages arrive. Argue why your
controller is efﬁcient, in that it does not unnecessarily waste energy (e.g., by sliding legs),
and in that it propels the robot at reasonably high speeds. Prove that your controller satisﬁes
the dynamic stability condition given on page 977.
25.11 (This exercise was ﬁrst devised by Michael Genesereth and Nils Nilsson. It works
for ﬁrst graders through graduate students.) Humans are so adept at basic household tasks
that they often forget how complex these tasks are. In this exercise you will discover the
complexity and recapitulate the last 30 years of developments in robotics. Consider the task
of building an arch out of three blocks. Simulate a robot with four humans as follows:
Brain. The Brain direct the hands in the execution of a plan to achieve the goal. The
Brain receives input from the Eyes, but cannot see the scene directly . The brain is the only
one who knows what the goal is.
Eyes. The Eyes report a brief description of the scene to the Brain: “There is a red box
standing on top of a green box, which is on its side” Eyes can also answer questions from the
Brain such as, “Is there a gap between the Left Hand and the red box?” If you have a video
camera, point it at the scene and allow the eyes to look at the viewﬁnder of the video camera,
but not directly at the scene.
Left hand and right hand. One person plays each Hand. The two Hands stand next to
each other, each wearing an oven mitt on one hand, Hands execute only simple commands
from the Brain—for example, “Left Hand, move two inches forward.” They cannot execute
commands other than motions; for example, they cannot be commanded to “Pick up the box.”
The Hands must be blindfolded. The only sensory capability they have is the ability to tell
when their path is blocked by an immovable obstacle such as a table or the other Hand. In
such cases, they can beep to inform the Brain of the difﬁculty.


--- BOOK CHAPTER: 2_Intelligent_Agents ---

2 INTELLIGENT AGENTS
In which we discuss the nature of agents, perfect or otherwise, the diversity of
environments, and the resulting menagerie of agent types.
Chapter 1 identiﬁed the concept of rational agents as central to our approach to artiﬁcial
intelligence. In this chapter, we make this notion more concrete. We will see that the concept
of rationality can be applied to a wide variety of agents operating in any imaginable environ-
ment. Our plan in this book is to use this concept to develop a small set of design principles
for building successful agents—systems that can reasonably be called intelligent.
We begin by examining agents, environments, and the coupling between them. The
observation that some agents behave better than others leads naturally to the idea of a rational
agent—one that behaves as well as possible. How well an agent can behave depends on
the nature of the environment; some environments are more difﬁcult than others. We give a
crude categorization of environments and show how properties of an environment inﬂuence
the design of suitable agents for that environment. We describe a number of basic “skeleton”
agent designs, which we ﬂesh out in the rest of the book.
2.1 A GENTS AND ENVIRONMENTS
An agent is anything that can be viewed as perceiving its environment through sensors andENVIRONMENT
SENSOR acting upon that environment through actuators. This simple idea is illustrated in Figure 2.1.
ACTUATOR A human agent has eyes, ears, and other organs for sensors and hands, legs, vocal tract, and so
on for actuators. A robotic agent might have cameras and infrared range ﬁnders for sensors
and various motors for actuators. A software agent receives keystrokes, ﬁle contents, and
network packets as sensory inputs and acts on the environment by displaying on the screen,
writing ﬁles, and sending network packets.
We use the termpercept to refer to the agent’s perceptual inputs at any given instant. An
PERCEPT
agent’s percept sequence is the complete history of everything the agent has ever perceived.PERCEPT SEQUENCE
In general, an agent’s choice of action at any given instant can depend on the entire percept
sequence observed to date, but not on anything it hasn’t perceived. By specifying the agent’s
choice of action for every possible percept sequence, we have said more or less everything
34
Section 2.1. Agents and Environments 35
Agent Sensors
Actuators
Environment
Percepts
Actions
?
Figure 2.1 Agents interact with environments through sensors and actuators.
there is to say about the agent. Mathematically speaking, we say that an agent’s behavior is
described by the agent function that maps any given percept sequence to an action.AGENT FUNCTION
We can imagine tabulating the agent function that describes any given agent; for most
agents, this would be a very large table—inﬁnite, in fact, unless we place a bound on the
length of percept sequences we want to consider. Given an agent to experiment with, we can,
in principle, construct this table by trying out all possible percept sequences and recording
which actions the agent does in response.
1 The table is, of course, anexternal characterization
of the agent. Internally, the agent function for an artiﬁcial agent will be implemented by an
agent program. It is important to keep these two ideas distinct. The agent function is anAGENT PROGRAM
abstract mathematical description; the agent program is a concrete implementation, running
within some physical system.
To illustrate these ideas, we use a very simple example—the vacuum-cleaner world
shown in Figure 2.2. This world is so simple that we can describe everything that happens;
it’s also a made-up world, so we can invent many variations. This particular world has just two
locations: squares A and B. The vacuum agent perceives which square it is in and whether
there is dirt in the square. It can choose to move left, move right, suck up the dirt, or do
nothing. One very simple agent function is the following: if the current square is dirty, then
suck; otherwise, move to the other square. A partial tabulation of this agent function is shown
in Figure 2.3 and an agent program that implements it appears in Figure 2.8 on page 48.
Looking at Figure 2.3, we see that various vacuum-world agents can be deﬁned simply
by ﬁlling in the right-hand column in various ways. The obvious question, then, is this: What
is the right way to ﬁll out the table? In other words, what makes an agent good or bad,
intelligent or stupid? We answer these questions in the next section.
1 If the agent uses some randomization to choose its actions, then we would have to try each sequence many
times to identify the probability of each action. One might imagine that acting randomly is rather silly, but we
show later in this chapter that it can be very intelligent.
36 Chapter 2. Intelligent Agents
AB
Figure 2.2 A vacuum-cleaner world with just two locations.
Percept sequence
 Action
[A,Clean]
 Right
[A,Dirty]
 Suck
[B, Clean]
 Left
[B, Dirty]
 Suck
[A,Clean], [A,Clean]
 Right
[A,Clean], [A,Dirty]
 Suck
...
 ...
[A,Clean], [A,Clean], [A,Clean]
 Right
[A,Clean], [A,Clean], [A,Dirty]
 Suck
...
 ...
Figure 2.3 Partial tabulation of a simple agent function for the vacuum-cleaner world
shown in Figure 2.2.
Before closing this section, we should emphasize that the notion of an agent is meant to
be a tool for analyzing systems, not an absolute characterization that divides the world into
agents and non-agents. One could view a hand-held calculator as an agent that chooses the
action of displaying “4” when given the percept sequence “2 + 2 =,” but such an analysis
would hardly aid our understanding of the calculator. In a sense, all areas of engineering can
be seen as designing artifacts that interact with the world; AI operates at (what the authors
consider to be) the most interesting end of the spectrum, where the artifacts have signiﬁcant
computational resources and the task environment requires nontrivial decision making.
2.2 G OOD BEHA VIOR:T HE CONCEPT OF RATIONALITY
A rational agent is one that does the right thing—conceptually speaking, every entry in theRA TIONAL AGENT
table for the agent function is ﬁlled out correctly. Obviously, doing the right thing is better
than doing the wrong thing, but what does it mean to do the right thing?
Section 2.2. Good Behavior: The Concept of Rationality 37
We answer this age-old question in an age-old way: by considering the consequences
of the agent’s behavior. When an agent is plunked down in an environment, it generates a
sequence of actions according to the percepts it receives. This sequence of actions causes the
environment to go through a sequence of states. If the sequence is desirable, then the agent
has performed well. This notion of desirability is captured by a performance measure that
PERFORMANCE
MEASURE
evaluates any given sequence of environment states.
Notice that we said environment states, not agent states. If we deﬁne success in terms
of agent’s opinion of its own performance, an agent could achieve perfect rationality simply
by deluding itself that its performance was perfect. Human agents in particular are notorious
for “sour grapes”—believing they did not really want something (e.g., a Nobel Prize) after
not getting it.
Obviously, there is not one ﬁxed performance measure for all tasks and agents; typically,
a designer will devise one appropriate to the circumstances. This is not as easy as it sounds.
Consider, for example, the vacuum-cleaner agent from the preceding section. We might
propose to measure performance by the amount of dirt cleaned up in a single eight-hour shift.
With a rational agent, of course, what you ask for is what you get. A rational agent can
maximize this performance measure by cleaning up the dirt, then dumping it all on the ﬂoor,
then cleaning it up again, and so on. A more suitable performance measure would reward the
agent for having a clean ﬂoor. For example, one point could be awarded for each clean square
at each time step (perhaps with a penalty for electricity consumed and noise generated). As
a general rule, it is better to design performance measures according to what one actually
wants in the environment, rather than according to how one thinks the agent should behave.
Even when the obvious pitfalls are avoided, there remain some knotty issues to untangle.
For example, the notion of “clean ﬂoor” in the preceding paragraph is based on average
cleanliness over time. Yet the same average cleanliness can be achieved by two different
agents, one of which does a mediocre job all the time while the other cleans energetically but
takes long breaks. Which is preferable might seem to be a ﬁne point of janitorial science, but
in fact it is a deep philosophical question with far-reaching implications. Which is better—
a reckless life of highs and lows, or a safe but humdrum existence? Which is better—an
economy where everyone lives in moderate poverty, or one in which some live in plenty
while others are very poor? We leave these questions as an exercise for the diligent reader.
2.2.1 Rationality
What is rational at any given time depends on four things:
•The performance measure that deﬁnes the criterion of success.
•The agent’s prior knowledge of the environment.
•The actions that the agent can perform.
•The agent’s percept sequence to date.
This leads to a deﬁnition of a rational agent :DEFINITION OF A
RA TIONAL AGENT
F or each possible percept sequence, a rational agent should select an action that is ex-
pected to maximize its performance measure, given the evidence provided by the percept
sequence and whatever built-in knowledge the agent has.
38 Chapter 2. Intelligent Agents
Consider the simple vacuum-cleaner agent that cleans a square if it is dirty and moves to the
other square if not; this is the agent function tabulated in Figure 2.3. Is this a rational agent?
That depends! First, we need to say what the performance measure is, what is known about
the environment, and what sensors and actuators the agent has. Let us assume the following:
•The performance measure awards one point for each clean square at each time step,
over a “lifetime” of 1000 time steps.
•The “geography” of the environment is known ap r i o r i(Figure 2.2) but the dirt distri-
bution and the initial location of the agent are not. Clean squares stay clean and sucking
cleans the current square. The Left and Right actions move the agent left and right
except when this would take the agent outside the environment, in which case the agent
remains where it is.
•The only available actions are Left, Right,a n dSuck.
•The agent correctly perceives its location and whether that location contains dirt.
We claim that under these circumstances the agent is indeed rational; its expected perfor-
mance is at least as high as any other agent’s. Exercise 2.2 asks you to prove this.
One can see easily that the same agent would be irrational under different circum-
stances. For example, once all the dirt is cleaned up, the agent will oscillate needlessly back
and forth; if the performance measure includes a penalty of one point for each movement left
or right, the agent will fare poorly. A better agent for this case would do nothing once it is
sure that all the squares are clean. If clean squares can become dirty again, the agent should
occasionally check and re-clean them if needed. If the geography of the environment is un-
known, the agent will need to explore it rather than stick to squares A and B.E x e r c i s e 2 . 2
asks you to design agents for these cases.
2.2.2 Omniscience, learning, and autonomy
We need to be careful to distinguish between rationality and omniscience. An omniscientOMNISCIENCE
agent knows the actual outcome of its actions and can act accordingly; but omniscience is
impossible in reality. Consider the following example: I am walking along the Champs
Elys´ees one day and I see an old friend across the street. There is no trafﬁc nearby and I’m
not otherwise engaged, so, being rational, I start to cross the street. Meanwhile, at 33,000
feet, a cargo door falls off a passing airliner, 2 and before I make it to the other side of the
street I am ﬂattened. Was I irrational to cross the street? It is unlikely that my obituary would
read “Idiot attempts to cross street.”
This example shows that rationality is not the same as perfection. Rationality max-
imizes expected performance, while perfection maximizes actual performance. Retreating
from a requirement of perfection is not just a question of being fair to agents. The point is
that if we expect an agent to do what turns out to be the best action after the fact, it will be
impossible to design an agent to fulﬁll this speciﬁcation—unless we improve the performance
of crystal balls or time machines.
2 See N. Henderson, “New door latches urged for Boeing 747 jumbo jets,” Washington Post, August 24, 1989.
Section 2.2. Good Behavior: The Concept of Rationality 39
Our deﬁnition of rationality does not require omniscience, then, because the rational
choice depends only on the percept sequence to date. We must also ensure that we haven’t
inadvertently allowed the agent to engage in decidedly underintelligent activities. For exam-
ple, if an agent does not look both ways before crossing a busy road, then its percept sequence
will not tell it that there is a large truck approaching at high speed. Does our deﬁnition of
rationality say that it’s now OK to cross the road? Far from it! First, it would not be rational
to cross the road given this uninformative percept sequence: the risk of accident from cross-
ing without looking is too great. Second, a rational agent should choose the “looking” action
before stepping into the street, because looking helps maximize the expected performance.
Doing actions in order to modify future percepts —sometimes called information gather-
ing—is an important part of rationality and is covered in depth in Chapter 16. A second
INFORMA TION
GA THERING
example of information gathering is provided by the exploration that must be undertaken byEXPLORA TION
a vacuum-cleaning agent in an initially unknown environment.
Our deﬁnition requires a rational agent not only to gather information but also to learnLEARNING
as much as possible from what it perceives. The agent’s initial conﬁguration could reﬂect
some prior knowledge of the environment, but as the agent gains experience this may be
modiﬁed and augmented. There are extreme cases in which the environment is completely
known ap r i o r i. In such cases, the agent need not perceive or learn; it simply acts correctly.
Of course, such agents are fragile. Consider the lowly dung beetle. After digging its nest and
laying its eggs, it fetches a ball of dung from a nearby heap to plug the entrance. If the ball of
dung is removed from its grasp en route, the beetle continues its task and pantomimes plug-
ging the nest with the nonexistent dung ball, never noticing that it is missing. Evolution has
built an assumption into the beetle’s behavior, and when it is violated, unsuccessful behavior
results. Slightly more intelligent is the sphex wasp. The female sphex will dig a burrow, go
out and sting a caterpillar and drag it to the burrow, enter the burrow again to check all is
well, drag the caterpillar inside, and lay its eggs. The caterpillar serves as a food source when
the eggs hatch. So far so good, but if an entomologist moves the caterpillar a few inches
away while the sphex is doing the check, it will revert to the “drag” step of its plan and will
continue the plan without modiﬁcation, even after dozens of caterpillar-moving interventions.
The sphex is unable to learn that its innate plan is failing, and thus will not change it.
To the extent that an agent relies on the prior knowledge of its designer rather than
on its own percepts, we say that the agent lacks autonomy. A rational agent should be
AUTONOMY
autonomous—it should learn what it can to compensate for partial or incorrect prior knowl-
edge. For example, a vacuum-cleaning agent that learns to foresee where and when additional
dirt will appear will do better than one that does not. As a practical matter, one seldom re-
quires complete autonomy from the start: when the agent has had little or no experience, it
would have to act randomly unless the designer gave some assistance. So, just as evolution
provides animals with enough built-in reﬂexes to survive long enough to learn for themselves,
it would be reasonable to provide an artiﬁcial intelligent agent with some initial knowledge
as well as an ability to learn. After sufﬁcient experience of its environment, the behavior
of a rational agent can become effectively independent of its prior knowledge. Hence, the
incorporation of learning allows one to design a single rational agent that will succeed in a
vast variety of environments.
40 Chapter 2. Intelligent Agents
2.3 T HE NATUR E OF ENVIRONMENTS
Now that we have a deﬁnition of rationality, we are almost ready to think about building
rational agents. First, however, we must think about task environments, which are essen-TASK ENVIRONMENT
tially the “problems” to which rational agents are the “solutions.” We begin by showing how
to specify a task environment, illustrating the process with a number of examples. We then
show that task environments come in a variety of ﬂavors. The ﬂavor of the task environment
directly affects the appropriate design for the agent program.
2.3.1 Specifying the task environment
In our discussion of the rationality of the simple vacuum-cleaner agent, we had to specify
the performance measure, the environment, and the agent’s actuators and sensors. We group
all these under the heading of the task environment. For the acronymically minded, we call
this the PEAS (Performance, Environment, Actuators, Sensors) description. In designing an
PEAS
agent, the ﬁrst step must always be to specify the task environment as fully as possible.
The vacuum world was a simple example; let us consider a more complex problem: an
automated taxi driver. We should point out, before the reader becomes alarmed, that a fully
automated taxi is currently somewhat beyond the capabilities of existing technology. (page 28
describes an existing driving robot.) The full driving task is extremely open-ended.T h e r e i s
no limit to the novel combinations of circumstances that can arise—another reason we chose
it as a focus for discussion. Figure 2.4 summarizes the PEAS description for the taxi’s task
environment. We discuss each element in more detail in the following paragraphs.
Agent Type
 Performance
Measure
Environment
 Actuators
 Sensors
Taxi driver
 Safe, fast, legal,
comfortable trip,
maximize proﬁts
Roads, other
trafﬁc,
pedestrians,
customers
Steering,
accelerator,
brake, signal,
horn, display
Cameras, sonar,
speedometer,
GPS, odometer,
accelerometer,
engine sensors,
keyboard
Figure 2.4 PEAS description of the task environment for an automated taxi.
First, what is the performance measure to which we would like our automated driver
to aspire? Desirable qualities include getting to the correct destination; minimizing fuel con-
sumption and wear and tear; minimizing the trip time or cost; minimizing violations of trafﬁc
laws and disturbances to other drivers; maximizing safety and passenger comfort; maximiz-
ing proﬁts. Obviously, some of these goals conﬂict, so tradeoffs will be required.
Next, what is the driving environment that the taxi will face? Any taxi driver must
deal with a variety of roads, ranging from rural lanes and urban alleys to 12-lane freeways.
The roads contain other trafﬁc, pedestrians, stray animals, road works, police cars, puddles,
Section 2.3. The Nature of Environments 41
and potholes. The taxi must also interact with potential and actual passengers. There are also
some optional choices. The taxi might need to operate in Southern California, where snow
is seldom a problem, or in Alaska, where it seldom is not. It could always be driving on the
right, or we might want it to be ﬂexible enough to drive on the left when in Britain or Japan.
Obviously, the more restricted the environment, the easier the design problem.
The actuators for an automated taxi include those available to a human driver: control
over the engine through the accelerator and control over steering and braking. In addition, it
will need output to a display screen or voice synthesizer to talk back to the passengers, and
perhaps some way to communicate with other vehicles, politely or otherwise.
The basic sensors for the taxi will include one or more controllable video cameras so
that it can see the road; it might augment these with infrared or sonar sensors to detect dis-
tances to other cars and obstacles. To avoid speeding tickets, the taxi should have a speedome-
ter, and to control the vehicle properly, especially on curves, it should have an accelerometer.
To determine the mechanical state of the vehicle, it will need the usual array of engine, fuel,
and electrical system sensors. Like many human drivers, it might want a global positioning
system (GPS) so that it doesn’t get lost. Finally, it will need a keyboard or microphone for
the passenger to request a destination.
In Figure 2.5, we have sketched the basic PEAS elements for a number of additional
agent types. Further examples appear in Exercise 2.4. It may come as a surprise to some read-
ers that our list of agent types includes some programs that operate in the entirely artiﬁcial
environment deﬁned by keyboard input and character output on a screen. “Surely,” one might
say, “this is not a real environment, is it?” In fact, what matters is not the distinction between
“real” and “artiﬁcial” environments, but the complexity of the relationship among the behav-
ior of the agent, the percept sequence generated by the environment, and the performance
measure. Some “real” environments are actually quite simple. For example, a robot designed
to inspect parts as they come by on a conveyor belt can make use of a number of simplifying
assumptions: that the lighting is always just so, that the only thing on the conveyor belt will
be parts of a kind that it knows about, and that only two actions (accept or reject) are possible.
In contrast, some software agents (or software robots or softbots) exist in rich, unlim-
SOFTWARE AGENT
SOFTBOT ited domains. Imagine a softbot Web site operator designed to scan Internet news sources and
show the interesting items to its users, while selling advertising space to generate revenue.
To do well, that operator will need some natural language processing abilities, it will need
to learn what each user and advertiser is interested in, and it will need to change its plans
dynamically—for example, when the connection for one news source goes down or when a
new one comes online. The Internet is an environment whose complexity rivals that of the
physical world and whose inhabitants include many artiﬁcial and human agents.
2.3.2 Properties of task environments
The range of task environments that might arise in AI is obviously vast. We can, however,
identify a fairly small number of dimensions along which task environments can be catego-
rized. These dimensions determine, to a large extent, the appropriate agent design and the
applicability of each of the principal families of techniques for agent implementation. First,
42 Chapter 2. Intelligent Agents
Agent Type
 Performance
Measure
Environment
 Actuators
 Sensors
Medical
diagnosis system
Healthy patient,
reduced costs
Patient, hospital,
staff
Display of
questions, tests,
diagnoses,
treatments,
referrals
Keyboard entry
of symptoms,
ﬁndings, patient’s
answers
Satellite image
analysis system
Correct image
categorization
Downlink from
orbiting satellite
Display of scene
categorization
Color pixel
arrays
Part-picking
robot
Percentage of
parts in correct
bins
Conveyor belt
with parts; bins
Jointed arm and
hand
Camera, joint
angle sensors
Reﬁnery
controller
Purity, yield,
safety
Reﬁnery,
operators
Valves, pumps,
heaters, displays
Temperature,
pressure,
chemical sensors
Interactive
English tutor
Student’s score
on test
Set of students,
testing agency
Display of
exercises,
suggestions,
corrections
Keyboard entry
Figure 2.5 Examples of agent types and their PEAS descriptions.
we list the dimensions, then we analyze several task environments to illustrate the ideas. The
deﬁnitions here are informal; later chapters provide more precise statements and examples of
each kind of environment.
Fully observable vs. partially observable: If an agent’s sensors give it access to the
FULL Y OBSERVABLE
PA RT I A L LY
OBSERVABLE complete state of the environment at each point in time, then we say that the task environ-
ment is fully observable. A task environment is effectively fully observable if the sensors
detect all aspects that are relevant to the choice of action; relevance, in turn, depends on the
performance measure. Fully observable environments are convenient because the agent need
not maintain any internal state to keep track of the world. An environment might be partially
observable because of noisy and inaccurate sensors or because parts of the state are simply
missing from the sensor data—for example, a vacuum agent with only a local dirt sensor
cannot tell whether there is dirt in other squares, and an automated taxi cannot see what other
drivers are thinking. If the agent has no sensors at all then the environment is unobserv-
able. One might think that in such cases the agent’s plight is hopeless, but, as we discuss in
UNOBSERVABLE
Chapter 4, the agent’s goals may still be achievable, sometimes with certainty.
Single agent vs. multiagent: The distinction between single-agent and multiagent en-SINGLE AGENT
MULTIAGENT
Section 2.3. The Nature of Environments 43
vironments may seem simple enough. For example, an agent solving a crossword puzzle by
itself is clearly in a single-agent environment, whereas an agent playing chess is in a two-
agent environment. There are, however, some subtle issues. First, we have described how an
entity may be viewed as an agent, but we have not explained which entities must be viewed
as agents. Does an agent A (the taxi driver for example) have to treat an object B (another
vehicle) as an agent, or can it be treated merely as an object behaving according to the laws of
physics, analogous to waves at the beach or leaves blowing in the wind? The key distinction
is whether B’s behavior is best described as maximizing a performance measure whose value
depends on agent A’s behavior. For example, in chess, the opponent entity B is trying to
maximize its performance measure, which, by the rules of chess, minimizes agent A’s per-
formance measure. Thus, chess is a competitive multiagent environment. In the taxi-driving
COMPETITIVE
environment, on the other hand, avoiding collisions maximizes the performance measure of
all agents, so it is a partially cooperative multiagent environment. It is also partially com-COOPERA TIVE
petitive because, for example, only one car can occupy a parking space. The agent-design
problems in multiagent environments are often quite different from those in single-agent en-
vironments; for example, communication often emerges as a rational behavior in multiagent
environments; in some competitive environments, randomized behavior is rational because
it avoids the pitfalls of predictability.
Deterministic vs. stochastic. If the next state of the environment is completely deter-
DETERMINISTIC
STOCHASTIC mined by the current state and the action executed by the agent, then we say the environment
is deterministic; otherwise, it is stochastic. In principle, an agent need not worry about uncer-
tainty in a fully observable, deterministic environment. (In our deﬁnition, we ignore uncer-
tainty that arises purely from the actions of other agents in a multiagent environment; thus,
a game can be deterministic even though each agent may be unable to predict the actions of
the others.) If the environment is partially observable, however, then it could appear to be
stochastic. Most real situations are so complex that it is impossible to keep track of all the
unobserved aspects; for practical purposes, they must be treated as stochastic. Taxi driving is
clearly stochastic in this sense, because one can never predict the behavior of trafﬁc exactly;
moreover, one’s tires blow out and one’s engine seizes up without warning. The vacuum
world as we described it is deterministic, but variations can include stochastic elements such
as randomly appearing dirt and an unreliable suction mechanism (Exercise 2.13). We say an
environment is uncertain if it is not fully observable or not deterministic. One ﬁnal note:
UNCERT AIN
our use of the word “stochastic” generally implies that uncertainty about outcomes is quan-
tiﬁed in terms of probabilities; a nondeterministic environment is one in which actions areNONDETERMINISTIC
characterized by their possible outcomes, but no probabilities are attached to them. Nonde-
terministic environment descriptions are usually associated with performance measures that
require the agent to succeed for all possible outcomes of its actions.
Episodic vs. sequential: In an episodic task environment, the agent’s experience is
EPISODIC
SEQUENTIAL divided into atomic episodes. In each episode the agent receives a percept and then performs
a single action. Crucially, the next episode does not depend on the actions taken in previous
episodes. Many classiﬁcation tasks are episodic. For example, an agent that has to spot
defective parts on an assembly line bases each decision on the current part, regardless of
previous decisions; moreover, the current decision doesn’t affect whether the next part is
44 Chapter 2. Intelligent Agents
defective. In sequential environments, on the other hand, the current decision could affect
all future decisions.3 Chess and taxi driving are sequential: in both cases, short-term actions
can have long-term consequences. Episodic environments are much simpler than sequential
environments because the agent does not need to think ahead.
Static vs. dynamic: If the environment can change while an agent is deliberating, then
STA TIC
DYNAMIC we say the environment is dynamic for that agent; otherwise, it is static. Static environments
are easy to deal with because the agent need not keep looking at the world while it is deciding
on an action, nor need it worry about the passage of time. Dynamic environments, on the
other hand, are continuously asking the agent what it wants to do; if it hasn’t decided yet,
that counts as deciding to do nothing. If the environment itself does not change with the
passage of time but the agent’s performance score does, then we say the environment is
semidynamic. Taxi driving is clearly dynamic: the other cars and the taxi itself keep moving
SEMIDYNAMIC
while the driving algorithm dithers about what to do next. Chess, when played with a clock,
is semidynamic. Crossword puzzles are static.
Discrete vs. continuous: The discrete/continuous distinction applies to the state of theDISCRETE
CONTINUOUS environment, to the way time is handled, and to the percepts and actions of the agent. For
example, the chess environment has a ﬁnite number of distinct states (excluding the clock).
Chess also has a discrete set of percepts and actions. Taxi driving is a continuous-state and
continuous-time problem: the speed and location of the taxi and of the other vehicles sweep
through a range of continuous values and do so smoothly over time. Taxi-driving actions are
also continuous (steering angles, etc.). Input from digital cameras is discrete, strictly speak-
ing, but is typically treated as representing continuously varying intensities and locations.
Known vs. unknown: Strictly speaking, this distinction refers not to the environment
KNOWN
UNKNOWN itself but to the agent’s (or designer’s) state of knowledge about the “laws of physics” of
the environment. In a known environment, the outcomes (or outcome probabilities if the
environment is stochastic) for all actions are given. Obviously, if the environment is unknown,
the agent will have to learn how it works in order to make good decisions. Note that the
distinction between known and unknown environments is not the same as the one between
fully and partially observable environments. It is quite possible for a known environment
to be partially observable—for example, in solitaire card games, I know the rules but am
still unable to see the cards that have not yet been turned over. Conversely, an unknown
environment can be fully observable—in a new video game, the screen may show the entire
game state but I still don’t know what the buttons do until I try them.
As one might expect, the hardest case is partially observable, multiagent, stochastic,
sequential, dynamic, continuous,a n dunknown. Taxi driving is hard in all these senses, except
that for the most part the driver’s environment is known. Driving a rented car in a new country
with unfamiliar geography and trafﬁc laws is a lot more exciting.
Figure 2.6 lists the properties of a number of familiar environments. Note that the
answers are not always cut and dried. For example, we describe the part-picking robot as
episodic, because it normally considers each part in isolation. But if one day there is a large
3 The word “sequential” is also used in computer science as the antonym of “parallel.” The two meanings are
largely unrelated.
Section 2.3. The Nature of Environments 45
Task Environment
 Observable Agents Deterministic Episodic Static Discrete
Crossword puzzle
 Fully Single Deterministic Sequential Static Discrete
Chess with a clock
 Fully Multi Deterministic Sequential Semi Discrete
Poker
 Partially Multi Stochastic Sequential Static Discrete
Backgammon
 Fully Multi Stochastic Sequential Static Discrete
Taxi driving
 Partially Multi Stochastic Sequential Dynamic Continuous
Medical diagnosis
 Partially Single Stochastic Sequential Dynamic Continuous
Image analysis
 Fully Single Deterministic Episodic Semi Continuous
Part-picking robot
 Partially Single Stochastic Episodic Dynamic Continuous
Reﬁnery controller
 Partially Single Stochastic Sequential Dynamic Continuous
Interactive English tutor
 Partially Multi Stochastic Sequential Dynamic Discrete
Figure 2.6 Examples of task environments and their characteristics.
batch of defective parts, the robot should learn from several observations that the distribution
of defects has changed, and should modify its behavior for subsequent parts. We have not
included a “known/unknown” column because, as explained earlier, this is not strictly a prop-
erty of the environment. For some environments, such as chess and poker, it is quite easy to
supply the agent with full knowledge of the rules, but it is nonetheless interesting to consider
how an agent might learn to play these games without such knowledge.
Several of the answers in the table depend on how the task environment is deﬁned. We
have listed the medical-diagnosis task as single-agent because the disease process in a patient
is not proﬁtably modeled as an agent; but a medical-diagnosis system might also have to
deal with recalcitrant patients and skeptical staff, so the environment could have a multiagent
aspect. Furthermore, medical diagnosis is episodic if one conceives of the task as selecting a
diagnosis given a list of symptoms; the problem is sequential if the task can include proposing
a series of tests, evaluating progress over the course of treatment, and so on. Also, many
environments are episodic at higher levels than the agent’s individual actions. For example,
a chess tournament consists of a sequence of games; each game is an episode because (by
and large) the contribution of the moves in one game to the agent’s overall performance is
not affected by the moves in its previous game. On the other hand, decision making within a
single game is certainly sequential.
The code repository associated with this book (aima.cs.berkeley.edu) includes imple-
mentations of a number of environments, together with a general-purpose environment simu-
lator that places one or more agents in a simulated environment, observes their behavior over
time, and evaluates them according to a given performance measure. Such experiments are
often carried out not for a single environment but for many environments drawn from an en-
vironment class. For example, to evaluate a taxi driver in simulated trafﬁc, we would want to
ENVIRONMENT
CLASS
run many simulations with different trafﬁc, lighting, and weather conditions. If we designed
the agent for a single scenario, we might be able to take advantage of speciﬁc properties
of the particular case but might not identify a good design for driving in general. For this
46 Chapter 2. Intelligent Agents
reason, the code repository also includes an environment generator for each environmentENVIRONMENT
GENERA TOR
class that selects particular environments (with certain likelihoods) in which to run the agent.
For example, the vacuum environment generator initializes the dirt pattern and agent location
randomly. We are then interested in the agent’s average performance over the environment
class. A rational agent for a given environment class maximizes this average performance.
Exercises 2.8 to 2.13 take you through the process of developing an environment class and
evaluating various agents therein.
2.4 T HE STRUCTURE OF AGENTS
So far we have talked about agents by describingbehavior—the action that is performed after
any given sequence of percepts. Now we must bite the bullet and talk about how the insides
work. The job of AI is to design an agent program that implements the agent function—
AGENT PROGRAM
the mapping from percepts to actions. We assume this program will run on some sort of
computing device with physical sensors and actuators—we call this the architecture:ARCHITECTURE
agent = architecture+ program .
Obviously, the program we choose has to be one that is appropriate for the architecture. If the
program is going to recommend actions like Walk, the architecture had better have legs. The
architecture might be just an ordinary PC, or it might be a robotic car with several onboard
computers, cameras, and other sensors. In general, the architecture makes the percepts from
the sensors available to the program, runs the program, and feeds the program’s action choices
to the actuators as they are generated. Most of this book is about designing agent programs,
although Chapters 24 and 25 deal directly with the sensors and actuators.
2.4.1 Agent programs
The agent programs that we design in this book all have the same skeleton: they take the
current percept as input from the sensors and return an action to the actuators.
4 Notice the
difference between the agent program, which takes the current percept as input, and the agent
function, which takes the entire percept history. The agent program takes just the current
percept as input because nothing more is available from the environment; if the agent’s actions
need to depend on the entire percept sequence, the agent will have to remember the percepts.
We describe the agent programs in the simple pseudocode language that is deﬁned in
Appendix B. (The online code repository contains implementations in real programming
languages.) For example, Figure 2.7 shows a rather trivial agent program that keeps track of
the percept sequence and then uses it to index into a table of actions to decide what to do.
The table—an example of which is given for the vacuum world in Figure 2.3—represents
explicitly the agent function that the agent program embodies. To build a rational agent in
4 There are other choices for the agent program skelet on; for example, we could have the agent programs be
coroutines that run asynchronously with the environment. Each such coroutine has an input and output port and
consists of a loop that reads the input port for percepts and writes actions to the output port.
Section 2.4. The Structure of Agents 47
function TABLE -DRIVEN -AGENT (percept) returns an action
persistent: percepts, a sequence, initially empty
table, a table of actions, indexed by percept sequences, initially fully speciﬁed
append percept to the end of percepts
action←LOOKUP (percepts,table)
return action
Figure 2.7 The TABLE -DRIVEN -AGENT program is invoked for each new percept and
returns an action each time. It retains the complete percept sequence in memory.
this way, we as designers must construct a table that contains the appropriate action for every
possible percept sequence.
It is instructive to consider why the table-driven approach to agent construction is
doomed to failure. Let P be the set of possible percepts and let T be the lifetime of the
agent (the total number of percepts it will receive). The lookup table will contain ∑T
t=1 |P|t
entries. Consider the automated taxi: the visual input from a single camera comes in at the
rate of roughly 27 megabytes per second (30 frames per second, 640× 480 pixels with 24
bits of color information). This gives a lookup table with over 10250,000,000,000 entries for an
hour’s driving. Even the lookup table for chess—a tiny, well-behaved fragment of the real
world—would have at least 10
150 entries. The daunting size of these tables (the number of
atoms in the observable universe is less than 1080) means that (a) no physical agent in this
universe will have the space to store the table, (b) the designer would not have time to create
the table, (c) no agent could ever learn all the right table entries from its experience, and (d)
even if the environment is simple enough to yield a feasible table size, the designer still has
no guidance about how to ﬁll in the table entries.
Despite all this, T
ABLE -DRIVEN -AGENT does do what we want: it implements the
desired agent function. The key challenge for AI is to ﬁnd out how to write programs that,
to the extent possible, produce rational behavior from a smallish program rather than from
a vast table. We have many examples showing that this can be done successfully in other
areas: for example, the huge tables of square roots used by engineers and schoolchildren prior
to the 1970s have now been replaced by a ﬁve-line program for Newton’s method running
on electronic calculators. The question is, can AI do for general intelligent behavior what
Newton did for square roots? We believe the answer is yes.
In the remainder of this section, we outline four basic kinds of agent programs that
embody the principles underlying almost all intelligent systems:
•Simple reﬂex agents;
•Model-based reﬂex agents;
•Goal-based agents; and
•Utility-based agents.
Each kind of agent program combines particular components in particular ways to generate
actions. Section 2.4.6 explains in general terms how to convert all these agents into learning
48 Chapter 2. Intelligent Agents
function REFLEX -VACUUM -AGENT ([location,status]) returns an action
if status = Dirty then return Suck
else if location = A then return Right
else if location = B then return Left
Figure 2.8 The agent program for a simple reﬂex agent in the two-state vacuum environ-
ment. This program implements the agent function tabulated in Figure 2.3.
agents that can improve the performance of their components so as to generate better actions.
Finally, Section 2.4.7 describes the variety of ways in which the components themselves can
be represented within the agent. This variety provides a major organizing principle for the
ﬁeld and for the book itself.
2.4.2 Simple reﬂex agents
The simplest kind of agent is thesimple reﬂex agent. These agents select actions on the basisSIMPLE REFLEX
AGENT
of the current percept, ignoring the rest of the percept history. For example, the vacuum agent
whose agent function is tabulated in Figure 2.3 is a simple reﬂex agent, because its decision
is based only on the current location and on whether that location contains dirt. An agent
program for this agent is shown in Figure 2.8.
Notice that the vacuum agent program is very small indeed compared to the correspond-
ing table. The most obvious reduction comes from ignoring the percept history, which cuts
down the number of possibilities from 4
T to just 4. A further, small reduction comes from
the fact that when the current square is dirty, the action does not depend on the location.
Simple reﬂex behaviors occur even in more complex environments. Imagine yourself
as the driver of the automated taxi. If the car in front brakes and its brake lights come on, then
you should notice this and initiate braking. In other words, some processing is done on the
visual input to establish the condition we call “The car in front is braking.” Then, this triggers
some established connection in the agent program to the action “initiate braking.” We call
such a connection a condition–action rule,
5 written asCONDITION–ACTION
RULE
if car-in-front-is-braking then initiate-braking.
Humans also have many such connections, some of which are learned responses (as for driv-
ing) and some of which are innate reﬂexes (such as blinking when something approaches the
eye). In the course of the book, we show several different ways in which such connections
can be learned and implemented.
The program in Figure 2.8 is speciﬁc to one particular vacuum environment. A more
general and ﬂexible approach is ﬁrst to build a general-purpose interpreter for condition–
action rules and then to create rule sets for speciﬁc task environments. Figure 2.9 gives the
structure of this general program in schematic form, showing how the condition–action rules
allow the agent to make the connection from percept to action. (Do not worry if this seems
5 Also called situation–action rules, productions,o r if–then rules.
Section 2.4. The Structure of Agents 49
Agent
Environment
Sensors
What action I
should do nowCondition-action rules
Actuators
What the world
is like now
Figure 2.9 Schematic diagram of a simple reﬂex agent.
function SIMPLE -REFLEX -AGENT (percept) returns an action
persistent: rules, a set of condition–action rules
state←INTERPRET -INPUT (percept)
rule←RULE -MATCH(state,rules)
action←rule.ACTION
return action
Figure 2.10 A simple reﬂex agent. It acts according to a rule whose condition matches
the current state, as deﬁned by the percept.
trivial; it gets more interesting shortly.) We use rectangles to denote the current internal state
of the agent’s decision process, and ovals to represent the background information used in
the process. The agent program, which is also very simple, is shown in Figure 2.10. The
I
NTERPRET -INPUT function generates an abstracted description of the current state from the
percept, and the RULE -MATCH function returns the ﬁrst rule in the set of rules that matches
the given state description. Note that the description in terms of “rules” and “matching” is
purely conceptual; actual implementations can be as simple as a collection of logic gates
implementing a Boolean circuit.
Simple reﬂex agents have the admirable property of being simple, but they turn out to be
of limited intelligence. The agent in Figure 2.10 will work only if the correct decision can be
made on the basis of only the current percept—that is, only if the environment is fully observ-
able. Even a little bit of unobservability can cause serious trouble. For example, the braking
rule given earlier assumes that the condition car-in-front-is-braking can be determined from
the current percept—a single frame of video. This works if the car in front has a centrally
mounted brake light. Unfortunately, older models have different conﬁgurations of taillights,
50 Chapter 2. Intelligent Agents
brake lights, and turn-signal lights, and it is not always possible to tell from a single image
whether the car is braking. A simple reﬂex agent driving behind such a car would either brake
continuously and unnecessarily, or, worse, never brake at all.
We can see a similar problem arising in the vacuum world. Suppose that a simple reﬂex
vacuum agent is deprived of its location sensor and has only a dirt sensor. Such an agent
has just two possible percepts: [Dirty] and [Clean]. It can Suck in response to [Dirty];w h a t
should it do in response to[Clean]?M o v i n gLeft fails (forever) if it happens to start in square
A, and moving Right fails (forever) if it happens to start in squareB. Inﬁnite loops are often
unavoidable for simple reﬂex agents operating in partially observable environments.
Escape from inﬁnite loops is possible if the agent can randomize its actions. For ex-
RANDOMIZA TION
ample, if the vacuum agent perceives[Clean], it might ﬂip a coin to choose betweenLeft and
Right. It is easy to show that the agent will reach the other square in an average of two steps.
Then, if that square is dirty, the agent will clean it and the task will be complete. Hence, a
randomized simple reﬂex agent might outperform a deterministic simple reﬂex agent.
We mentioned in Section 2.3 that randomized behavior of the right kind can be rational
in some multiagent environments. In single-agent environments, randomization is usuallynot
rational. It is a useful trick that helps a simple reﬂex agent in some situations, but in most
cases we can do much better with more sophisticated deterministic agents.
2.4.3 Model-based reﬂex agents
The most effective way to handle partial observability is for the agent to keep track of the
part of the world it can’t see now . That is, the agent should maintain some sort of internal
state that depends on the percept history and thereby reﬂects at least some of the unobservedINTERNAL ST A TE
aspects of the current state. For the braking problem, the internal state is not too extensive—
just the previous frame from the camera, allowing the agent to detect when two red lights at
the edge of the vehicle go on or off simultaneously. For other driving tasks such as changing
lanes, the agent needs to keep track of where the other cars are if it can’t see them all at once.
And for any driving to be possible at all, the agent needs to keep track of where its keys are.
Updating this internal state information as time goes by requires two kinds of knowl-
edge to be encoded in the agent program. First, we need some information about how the
world evolves independently of the agent—for example, that an overtaking car generally will
be closer behind than it was a moment ago. Second, we need some information about how
the agent’s own actions affect the world—for example, that when the agent turns the steering
wheel clockwise, the car turns to the right, or that after driving for ﬁve minutes northbound
on the freeway, one is usually about ﬁve miles north of where one was ﬁve minutes ago. This
knowledge about “how the world works”—whether implemented in simple Boolean circuits
or in complete scientiﬁc theories—is called a model of the world. An agent that uses such a
model is called a model-based agent.
MODEL-BASED
AGENT
Figure 2.11 gives the structure of the model-based reﬂex agent with internal state, show-
ing how the current percept is combined with the old internal state to generate the updated
description of the current state, based on the agent’s model of how the world works. The agent
program is shown in Figure 2.12. The interesting part is the function U
PDATE-STATE,w h i c h
Section 2.4. The Structure of Agents 51
Agent
Environment
Sensors
State
How the world evolves
What my actions do
Condition-action rules
Actuators
What the world
is like now
What action I
should do now
Figure 2.11 A model-based reﬂex agent.
function MODEL -BASED -REFLEX -AGENT (percept) returns an action
persistent: state, the agent’s current conception of the world state
model, a description of how the next state depends on current state and action
rules, a set of condition–action rules
action, the most recent action, initially none
state←UPDATE -STATE(state,action,percept,model)
rule←RULE -MATCH(state,rules)
action←rule.ACTION
return action
Figure 2.12 A model-based reﬂex agent. It keeps track of the current state of the world,
using an internal model. It then chooses an action in the same way as the reﬂex agent.
is responsible for creating the new internal state description. The details of how models and
states are represented vary widely depending on the type of environment and the particular
technology used in the agent design. Detailed examples of models and updating algorithms
appear in Chapters 4, 12, 11, 15, 17, and 25.
Regardless of the kind of representation used, it is seldom possible for the agent to
determine the current state of a partially observable environment exactly. Instead, the box
labeled “what the world is like now” (Figure 2.11) represents the agent’s “best guess” (or
sometimes best guesses). For example, an automated taxi may not be able to see around the
large truck that has stopped in front of it and can only guess about what may be causing the
hold-up. Thus, uncertainty about the current state may be unavoidable, but the agent still has
to make a decision.
A perhaps less obvious point about the internal “state” maintained by a model-based
agent is that it does not have to describe “what the world is like now” in a literal sense. For
52 Chapter 2. Intelligent Agents
Agent
Environment
Sensors
What action I
should do now
State
How the world evolves
What my actions do
Actuators
What the world
is like now
What it will be like
  if I do action A
Goals
Figure 2.13 A model-based, goal-based agent. It keeps track of the world state as well as
a set of goals it is trying to achieve, and chooses an action that will (eventually) lead to the
achievement of its goals.
example, the taxi may be driving back home, and it may have a rule telling it to ﬁll up with
gas on the way home unless it has at least half a tank. Although “driving back home” may
seem to an aspect of the world state, the fact of the taxi’s destination is actually an aspect of
the agent’s internal state. If you ﬁnd this puzzling, consider that the taxi could be in exactly
the same place at the same time, but intending to reach a different destination.
2.4.4 Goal-based agents
Knowing something about the current state of the environment is not always enough to decide
what to do. For example, at a road junction, the taxi can turn left, turn right, or go straight
on. The correct decision depends on where the taxi is trying to get to. In other words, as well
as a current state description, the agent needs some sort of goal information that describes
GOAL
situations that are desirable—for example, being at the passenger’s destination. The agent
program can combine this with the model (the same information as was used in the model-
based reﬂex agent) to choose actions that achieve the goal. Figure 2.13 shows the goal-based
agent’s structure.
Sometimes goal-based action selection is straightforward—for example, when goal sat-
isfaction results immediately from a single action. Sometimes it will be more tricky—for
example, when the agent has to consider long sequences of twists and turns in order to ﬁnd a
way to achieve the goal. Search (Chapters 3 to 5) and planning (Chapters 10 and 11) are the
subﬁelds of AI devoted to ﬁnding action sequences that achieve the agent’s goals.
Notice that decision making of this kind is fundamentally different from the condition–
action rules described earlier, in that it involves consideration of the future—both “What will
happen if I do such-and-such?” and “Will that make me happy?” In the reﬂex agent designs,
this information is not explicitly represented, because the built-in rules map directly from
Section 2.4. The Structure of Agents 53
percepts to actions. The reﬂex agent brakes when it sees brake lights. A goal-based agent, in
principle, could reason that if the car in front has its brake lights on, it will slow down. Given
the way the world usually evolves, the only action that will achieve the goal of not hitting
other cars is to brake.
Although the goal-based agent appears less efﬁcient, it is more ﬂexible because the
knowledge that supports its decisions is represented explicitly and can be modiﬁed. If it starts
to rain, the agent can update its knowledge of how effectively its brakes will operate; this will
automatically cause all of the relevant behaviors to be altered to suit the new conditions.
For the reﬂex agent, on the other hand, we would have to rewrite many condition–action
rules. The goal-based agent’s behavior can easily be changed to go to a different destination,
simply by specifying that destination as the goal. The reﬂex agent’s rules for when to turn
and when to go straight will work only for a single destination; they must all be replaced to
go somewhere new.
2.4.5 Utility-based agents
Goals alone are not enough to generate high-quality behavior in most environments. For
example, many action sequences will get the taxi to its destination (thereby achieving the
goal) but some are quicker, safer, more reliable, or cheaper than others. Goals just provide a
crude binary distinction between “happy” and “unhappy” states. A more general performance
measure should allow a comparison of different world states according to exactly how happy
they would make the agent. Because “happy” does not sound very scientiﬁc, economists and
computer scientists use the term utility instead.
6UTILITY
We have already seen that a performance measure assigns a score to any given sequence
of environment states, so it can easily distinguish between more and less desirable ways of
getting to the taxi’s destination. An agent’s utility function is essentially an internalizationUTILITY FUNCTION
of the performance measure. If the internal utility function and the external performance
measure are in agreement, then an agent that chooses actions to maximize its utility will be
rational according to the external performance measure.
Let us emphasize again that this is not the only way to be rational—we have already
seen a rational agent program for the vacuum world (Figure 2.8) that has no idea what its
utility function is—but, like goal-based agents, a utility-based agent has many advantages in
terms of ﬂexibility and learning. Furthermore, in two kinds of cases, goals are inadequate but
a utility-based agent can still make rational decisions. First, when there are conﬂicting goals,
only some of which can be achieved (for example, speed and safety), the utility function
speciﬁes the appropriate tradeoff. Second, when there are several goals that the agent can
aim for, none of which can be achieved with certainty, utility provides a way in which the
likelihood of success can be weighed against the importance of the goals.
Partial observability and stochasticity are ubiquitous in the real world, and so, therefore,
is decision making under uncertainty. Technically speaking, a rational utility-based agent
chooses the action that maximizes the expected utility of the action outcomes—that is, the
EXPECTED UTILITY
utility the agent expects to derive, on average, given the probabilities and utilities of each
6 The word “utility” here refers to “the quality of being useful,” not to the electric company or waterworks.
54 Chapter 2. Intelligent Agents
Agent
Environment
Sensors
How happy I will be
in such a state
State
How the world evolves
What my actions do
Utility
Actuators
What action I
should do now
What it will be like
if I do action A
What the world
is like now
Figure 2.14 A model-based, utility-based agent. It uses a model of the world, along with
a utility function that measures its preferences among states of the world. Then it chooses the
action that leads to the best expected utility, where expected utility is computed by averaging
over all possible outcome states, weighted by the probability of the outcome.
outcome. (Appendix A deﬁnes expectation more precisely.) In Chapter 16, we show that any
rational agent must behave as if it possesses a utility function whose expected value it tries
to maximize. An agent that possesses an explicit utility function can make rational decisions
with a general-purpose algorithm that does not depend on the speciﬁc utility function being
maximized. In this way, the “global” deﬁnition of rationality—designating as rational those
agent functions that have the highest performance—is turned into a “local” constraint on
rational-agent designs that can be expressed in a simple program.
The utility-based agent structure appears in Figure 2.14. Utility-based agent programs
appear in Part IV, where we design decision-making agents that must handle the uncertainty
inherent in stochastic or partially observable environments.
At this point, the reader may be wondering, “Is it that simple? We just build agents that
maximize expected utility, and we’re done?” It’s true that such agents would be intelligent,
but it’s not simple. A utility-based agent has to model and keep track of its environment,
tasks that have involved a great deal of research on perception, representation, reasoning,
and learning. The results of this research ﬁll many of the chapters of this book. Choosing
the utility-maximizing course of action is also a difﬁcult task, requiring ingenious algorithms
that ﬁll several more chapters. Even with these algorithms, perfect rationality is usually
unachievable in practice because of computational complexity, as we noted in Chapter 1.
2.4.6 Learning agents
We have described agent programs with various methods for selecting actions. We have
not, so far, explained how the agent programs come into being . In his famous early paper,
Turing (1950) considers the idea of actually programming his intelligent machines by hand.
Section 2.4. The Structure of Agents 55
Performance standard
Agent
Environment
Sensors
Performance
element
changes
knowledge
learning
  goals
Problem
generator
feedback
  Learning
element
Critic
Actuators
Figure 2.15 A general learning agent.
He estimates how much work this might take and concludes “Some more expeditious method
seems desirable.” The method he proposes is to build learning machines and then to teach
them. In many areas of AI, this is now the preferred method for creating state-of-the-art
systems. Learning has another advantage, as we noted earlier: it allows the agent to operate
in initially unknown environments and to become more competent than its initial knowledge
alone might allow. In this section, we brieﬂy introduce the main ideas of learning agents.
Throughout the book, we comment on opportunities and methods for learning in particular
kinds of agents. Part V goes into much more depth on the learning algorithms themselves.
A learning agent can be divided into four conceptual components, as shown in Fig-
ure 2.15. The most important distinction is between the learning element ,w h i c hi sr e -
LEARNING ELEMENT
sponsible for making improvements, and the performance element, which is responsible forPERFORMANCE
ELEMENT
selecting external actions. The performance element is what we have previously considered
to be the entire agent: it takes in percepts and decides on actions. The learning element uses
feedback from the critic on how the agent is doing and determines how the performance
CRITIC
element should be modiﬁed to do better in the future.
The design of the learning element depends very much on the design of the performance
element. When trying to design an agent that learns a certain capability, the ﬁrst question is
not “How am I going to get it to learn this?” but “What kind of performance element will my
agent need to do this once it has learned how?” Given an agent design, learning mechanisms
can be constructed to improve every part of the agent.
The critic tells the learning element how well the agent is doing with respect to a ﬁxed
performance standard. The critic is necessary because the percepts themselves provide no
indication of the agent’s success. For example, a chess program could receive a percept
indicating that it has checkmated its opponent, but it needs a performance standard to know
that this is a good thing; the percept itself does not say so. It is important that the performance
56 Chapter 2. Intelligent Agents
standard be ﬁxed. Conceptually, one should think of it as being outside the agent altogether
because the agent must not modify it to ﬁt its own behavior.
The last component of the learning agent is the problem generator. It is responsiblePROBLEM
GENERA TOR
for suggesting actions that will lead to new and informative experiences. The point is that
if the performance element had its way, it would keep doing the actions that are best, given
what it knows. But if the agent is willing to explore a little and do some perhaps suboptimal
actions in the short run, it might discover much better actions for the long run. The problem
generator’s job is to suggest these exploratory actions. This is what scientists do when they
carry out experiments. Galileo did not think that dropping rocks from the top of a tower in
Pisa was valuable in itself. He was not trying to break the rocks or to modify the brains of
unfortunate passers-by. His aim was to modify his own brain by identifying a better theory
of the motion of objects.
To make the overall design more concrete, let us return to the automated taxi example.
The performance element consists of whatever collection of knowledge and procedures the
taxi has for selecting its driving actions. The taxi goes out on the road and drives, using
this performance element. The critic observes the world and passes information along to the
learning element. For example, after the taxi makes a quick left turn across three lanes of traf-
ﬁc, the critic observes the shocking language used by other drivers. From this experience, the
learning element is able to formulate a rule saying this was a bad action, and the performance
element is modiﬁed by installation of the new rule. The problem generator might identify
certain areas of behavior in need of improvement and suggest experiments, such as trying out
the brakes on different road surfaces under different conditions.
The learning element can make changes to any of the “knowledge” components shown
in the agent diagrams (Figures 2.9, 2.11, 2.13, and 2.14). The simplest cases involve learning
directly from the percept sequence. Observation of pairs of successive states of the environ-
ment can allow the agent to learn “How the world evolves,” and observation of the results of
its actions can allow the agent to learn “What my actions do.” For example, if the taxi exerts
a certain braking pressure when driving on a wet road, then it will soon ﬁnd out how much
deceleration is actually achieved. Clearly, these two learning tasks are more difﬁcult if the
environment is only partially observable.
The forms of learning in the preceding paragraph do not need to access the external
performance standard—in a sense, the standard is the universal one of making predictions
that agree with experiment. The situation is slightly more complex for a utility-based agent
that wishes to learn utility information. For example, suppose the taxi-driving agent receives
no tips from passengers who have been thoroughly shaken up during the trip. The external
performance standard must inform the agent that the loss of tips is a negative contribution to
its overall performance; then the agent might be able to learn that violent maneuvers do not
contribute to its own utility. In a sense, the performance standard distinguishes part of the
incoming percept as a reward (or penalty) that provides direct feedback on the quality of the
agent’s behavior. Hard-wired performance standards such as pain and hunger in animals can
be understood in this way. This issue is discussed further in Chapter 21.
In summary, agents have a variety of components, and those components can be repre-
sented in many ways within the agent program, so there appears to be great variety among
Section 2.4. The Structure of Agents 57
learning methods. There is, however, a single unifying theme. Learning in intelligent agents
can be summarized as a process of modiﬁcation of each component of the agent to bring the
components into closer agreement with the available feedback information, thereby improv-
ing the overall performance of the agent.
2.4.7 How the components of agent programs work
We have described agent programs (in very high-level terms) as consisting of various compo-
nents, whose function it is to answer questions such as: “What is the world like now?” “What
action should I do now?” “What do my actions do?” The next question for a student of AI
is, “How on earth do these components work?” It takes about a thousand pages to begin to
answer that question properly, but here we want to draw the reader’s attention to some basic
distinctions among the various ways that the components can represent the environment that
the agent inhabits.
Roughly speaking, we can place the representations along an axis of increasing com-
plexity and expressive power—atomic, factored,a n d structured. To illustrate these ideas,
it helps to consider a particular agent component, such as the one that deals with “What my
actions do.” This component describes the changes that might occur in the environment as
the result of taking an action, and Figure 2.16 provides schematic depictions of how those
transitions might be represented.
B C
(a) Atomic (b) Factored (b) Structured
BC
Figure 2.16 Three ways to represent states and the transitions between them. (a) Atomic
representation: a state (such as B or C) is a black box with no internal structure; (b) Factored
representation: a state consists of a vector of attribute values; values can be Boolean, real-
valued, or one of a ﬁxed set of symbols. (c) Structured representation: a state includes
objects, each of which may have attributes of its own as well as relationships to other objects.
In an atomic representation each state of the world is indivisible—it has no internalATO M I C
REPRESENTA TION
structure. Consider the problem of ﬁnding a driving route from one end of a country to the
other via some sequence of cities (we address this problem in Figure 3.2 on page 68). For the
purposes of solving this problem, it may sufﬁce to reduce the state of world to just the name
of the city we are in—a single atom of knowledge; a “black box” whose only discernible
property is that of being identical to or different from another black box. The algorithms
58 Chapter 2. Intelligent Agents
underlying search and game-playing (Chapters 3–5), Hidden Markov models (Chapter 15),
and Markov decision processes (Chapter 17) all work with atomic representations—or, at
least, they treat representations as if they were atomic.
Now consider a higher-ﬁdelity description for the same problem, where we need to be
concerned with more than just atomic location in one city or another; we might need to pay
attention to how much gas is in the tank, our current GPS coordinates, whether or not the oil
warning light is working, how much spare change we have for toll crossings, what station is
on the radio, and so on. A factored representation splits up each state into a ﬁxed set of
FACTORED
REPRESENTA TION
variables or attributes, each of which can have a value. While two different atomic statesVARIABLE
A TTRIBUTE
VALUE
have nothing in common—they are just different black boxes—two different factored states
can share some attributes (such as being at some particular GPS location) and not others (such
as having lots of gas or having no gas); this makes it much easier to work out how to turn
one state into another. With factored representations, we can also represent uncertainty—for
example, ignorance about the amount of gas in the tank can be represented by leaving that
attribute blank. Many important areas of AI are based on factored representations, including
constraint satisfaction algorithms (Chapter 6), propositional logic (Chapter 7), planning
(Chapters 10 and 11), Bayesian networks (Chapters 13–16), and the machine learning al-
gorithms in Chapters 18, 20, and 21.
For many purposes, we need to understand the world as having things in it that are
related to each other, not just variables with values. For example, we might notice that a
large truck ahead of us is reversing into the driveway of a dairy farm but a cow has got loose
and is blocking the truck’s path. A factored representation is unlikely to be pre-equipped
with the attributeTruckAheadBackingIntoDairyFarmDrivewayBlockedByLooseCowwith
value true or false. Instead, we would need a structured representation, in which ob-
STRUCTURED
REPRESENTA TION
jects such as cows and trucks and their various and varying relationships can be described
explicitly. (See Figure 2.16(c).) Structured representations underlie relational databases
and ﬁrst-order logic (Chapters 8, 9, and 12), ﬁrst-order probability models (Chapter 14),
knowledge-based learning (Chapter 19) and much of natural language understanding
(Chapters 22 and 23). In fact, almost everything that humans express in natural language
concerns objects and their relationships.
As we mentioned earlier, the axis along which atomic, factored, and structured repre-
sentations lie is the axis of increasing expressiveness. Roughly speaking, a more expressiveEXPRESSIVENESS
representation can capture, at least as concisely, everything a less expressive one can capture,
plus some more. Often, the more expressive language ismuch more concise; for example, the
rules of chess can be written in a page or two of a structured-representation language such
as ﬁrst-order logic but require thousands of pages when written in a factored-representation
language such as propositional logic. On the other hand, reasoning and learning become
more complex as the expressive power of the representation increases. To gain the beneﬁts
of expressive representations while avoiding their drawbacks, intelligent systems for the real
world may need to operate at all points along the axis simultaneously.
Section 2.5. Summary 59
2.5 S UMMARY
This chapter has been something of a whirlwind tour of AI, which we have conceived of as
the science of agent design. The major points to recall are as follows:
•An agent is something that perceives and acts in an environment. The agent function
for an agent speciﬁes the action taken by the agent in response to any percept sequence.
•The performance measure evaluates the behavior of the agent in an environment. A
rational agent acts so as to maximize the expected value of the performance measure,
given the percept sequence it has seen so far.
•A task environment speciﬁcation includes the performance measure, the external en-
vironment, the actuators, and the sensors. In designing an agent, the ﬁrst step must
always be to specify the task environment as fully as possible.
•Task environments vary along several signiﬁcant dimensions. They can be fully or
partially observable, single-agent or multiagent, deterministic or stochastic, episodic or
sequential, static or dynamic, discrete or continuous, and known or unknown.
•The agent program implements the agent function. There exists a variety of basic
agent-program designs reﬂecting the kind of information made explicit and used in the
decision process. The designs vary in efﬁciency, compactness, and ﬂexibility. The
appropriate design of the agent program depends on the nature of the environment.
•Simple reﬂex agents respond directly to percepts, whereas model-based reﬂex agents
maintain internal state to track aspects of the world that are not evident in the current
percept. Goal-based agents act to achieve their goals, and utility-based agents try to
maximize their own expected “happiness.”
•All agents can improve their performance through learning.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
The central role of action in intelligence—the notion of practical reasoning—goes back at
least as far as Aristotle’s Nicomachean Ethics. Practical reasoning was also the subject of
McCarthy’s (1958) inﬂuential paper “Programs with Common Sense.” The ﬁelds of robotics
and control theory are, by their very nature, concerned principally with physical agents. The
concept of a controller in control theory is identical to that of an agent in AI. Perhaps sur-
CONTROLLER
prisingly, AI has concentrated for most of its history on isolated components of agents—
question-answering systems, theorem-provers, vision systems, and so on—rather than on
whole agents. The discussion of agents in the text by Genesereth and Nilsson (1987) was an
inﬂuential exception. The whole-agent view is now widely accepted and is a central theme in
recent texts (Poole et al., 1998; Nilsson, 1998; Padgham and Winikoff, 2004; Jones, 2007).
Chapter 1 traced the roots of the concept of rationality in philosophy and economics. In
AI, the concept was of peripheral interest until the mid-1980s, when it began to suffuse many
60 Chapter 2. Intelligent Agents
discussions about the proper technical foundations of the ﬁeld. A paper by Jon Doyle (1983)
predicted that rational agent design would come to be seen as the core mission of AI, while
other popular topics would spin off to form new disciplines.
Careful attention to the properties of the environment and their consequences for ra-
tional agent design is most apparent in the control theory tradition—for example, classical
control systems (Dorf and Bishop, 2004; Kirk, 2004) handle fully observable, deterministic
environments; stochastic optimal control (Kumar and Varaiya, 1986; Bertsekas and Shreve,
2007) handles partially observable, stochastic environments; and hybrid control (Henzinger
and Sastry, 1998; Cassandras and Lygeros, 2006) deals with environments containing both
discrete and continuous elements. The distinction between fully and partially observable en-
vironments is also central in the dynamic programming literature developed in the ﬁeld of
operations research (Puterman, 1994), which we discuss in Chapter 17.
Reﬂex agents were the primary model for psychological behaviorists such as Skinner
(1953), who attempted to reduce the psychology of organisms strictly to input/output or stim-
ulus/response mappings. The advance from behaviorism to functionalism in psychology,
which was at least partly driven by the application of the computer metaphor to agents (Put-
nam, 1960; Lewis, 1966), introduced the internal state of the agent into the picture. Most
work in AI views the idea of pure reﬂex agents with state as too simple to provide much
leverage, but work by Rosenschein (1985) and Brooks (1986) questioned this assumption
(see Chapter 25). In recent years, a great deal of work has gone into ﬁnding efﬁcient algo-
rithms for keeping track of complex environments (Hamscheret al., 1992; Simon, 2006). The
Remote Agent program (described on page 28) that controlled the Deep Space One spacecraft
is a particularly impressive example (Muscettola et al., 1998; Jonsson et al., 2000).
Goal-based agents are presupposed in everything from Aristotle’s view of practical rea-
soning to McCarthy’s early papers on logical AI. Shakey the Robot (Fikes and Nilsson,
1971; Nilsson, 1984) was the ﬁrst robotic embodiment of a logical, goal-based agent. A
full logical analysis of goal-based agents appeared in Genesereth and Nilsson (1987), and a
goal-based programming methodology called agent-oriented programming was developed by
Shoham (1993). The agent-based approach is now extremely popular in software engineer-
ing (Ciancarini and Wooldridge, 2001). It has also inﬁltrated the area of operating systems,
where autonomic computing refers to computer systems and networks that monitor and con-
AUTONOMIC
COMPUTING
trol themselves with a perceive–act loop and machine learning methods (Kephart and Chess,
2003). Noting that a collection of agent programs designed to work well together in a true
multiagent environment necessarily exhibits modularity—the programs share no internal state
and communicate with each other only through the environment—it is common within the
ﬁeld of multiagent systems to design the agent program of a single agent as a collection of
MULTIAGENT
SYSTEMS
autonomous sub-agents. In some cases, one can even prove that the resulting system gives
the same optimal solutions as a monolithic design.
The goal-based view of agents also dominates the cognitive psychology tradition in the
area of problem solving, beginning with the enormously inﬂuential Human Problem Solv-
ing (Newell and Simon, 1972) and running through all of Newell’s later work (Newell, 1990).
Goals, further analyzed as desires (general) and intentions (currently pursued), are central to
the theory of agents developed by Bratman (1987). This theory has been inﬂuential both in
Exercises 61
natural language understanding and multiagent systems.
Horvitz et al. (1988) speciﬁcally suggest the use of rationality conceived as the maxi-
mization of expected utility as a basis for AI. The text by Pearl (1988) was the ﬁrst in AI to
cover probability and utility theory in depth; its exposition of practical methods for reasoning
and decision making under uncertainty was probably the single biggest factor in the rapid
shift towards utility-based agents in the 1990s (see Part IV).
The general design for learning agents portrayed in Figure 2.15 is classic in the machine
learning literature (Buchanan et al., 1978; Mitchell, 1997). Examples of the design, as em-
bodied in programs, go back at least as far as Arthur Samuel’s (1959, 1967) learning program
for playing checkers. Learning agents are discussed in depth in Part V.
Interest in agents and in agent design has risen rapidly in recent years, partly because of
the growth of the Internet and the perceived need for automated and mobile softbot (Etzioni
and Weld, 1994). Relevant papers are collected in Readings in Agents (Huhns and Singh,
1998) and F oundations of Rational Agency(Wooldridge and Rao, 1999). Texts on multiagent
systems usually provide a good introduction to many aspects of agent design (Weiss, 2000a;
Wooldridge, 2002). Several conference series devoted to agents began in the 1990s, including
the International Workshop on Agent Theories, Architectures, and Languages (ATAL), the
International Conference on Autonomous Agents (AGENTS), and the International Confer-
ence on Multi-Agent Systems (ICMAS). In 2002, these three merged to form the International
Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS). The journal
Autonomous Agents and Multi-Agent Systems was founded in 1998. Finally, Dung Beetle
Ecology (Hanski and Cambefort, 1991) provides a wealth of interesting information on the
behavior of dung beetles. YouTube features inspiring video recordings of their activities.
EXERCISES
2.1 Suppose that the performance measure is concerned with just the ﬁrst T time steps of
the environment and ignores everything thereafter. Show that a rational agent’s action may
depend not just on the state of the environment but also on the time step it has reached.
2.2 Let us examine the rationality of various vacuum-cleaner agent functions.
a. Show that the simple vacuum-cleaner agent function described in Figure 2.3 is indeed
rational under the assumptions listed on page 38.
b. Describe a rational agent function for the case in which each movement costs one point.
Does the corresponding agent program require internal state?
c. Discuss possible agent designs for the cases in which clean squares can become dirty
and the geography of the environment is unknown. Does it make sense for the agent to
learn from its experience in these cases? If so, what should it learn? If not, why not?
2.3 For each of the following assertions, say whether it is true or false and support your
answer with examples or counterexamples where appropriate.
a. An agent that senses only partial information about the state cannot be perfectly rational.
62 Chapter 2. Intelligent Agents
b. There exist task environments in which no pure reﬂex agent can behave rationally.
c. There exists a task environment in which every agent is rational.
d. The input to an agent program is the same as the input to the agent function.
e. Every agent function is implementable by some program/machine combination.
f. Suppose an agent selects its action uniformly at random from the set of possible actions.
There exists a deterministic task environment in which this agent is rational.
g. It is possible for a given agent to be perfectly rational in two distinct task environments.
h. Every agent is rational in an unobservable environment.
i. A perfectly rational poker-playing agent never loses.
2.4 For each of the following activities, give a PEAS description of the task environment
and characterize it in terms of the properties listed in Section 2.3.2.
•Playing soccer.
•Exploring the subsurface oceans of Titan.
•Shopping for used AI books on the Internet.
•Playing a tennis match.
•Practicing tennis against a wall.
•Performing a high jump.
•Knitting a sweater.
•Bidding on an item at an auction.
2.5 Deﬁne in your own words the following terms: agent, agent function, agent program,
rationality, autonomy, reﬂex agent, model-based agent, goal-based agent, utility-based agent,
learning agent.
2.6 This exercise explores the differences between agent functions and agent programs.
a. Can there be more than one agent program that implements a given agent function?
Give an example, or show why one is not possible.
b. Are there agent functions that cannot be implemented by any agent program?
c. Given a ﬁxed machine architecture, does each agent program implement exactly one
agent function?
d. Given an architecture with n bits of storage, how many different possible agent pro-
grams are there?
e. Suppose we keep the agent program ﬁxed but speed up the machine by a factor of two.
Does that change the agent function?
2.7 Write pseudocode agent programs for the goal-based and utility-based agents.
The following exercises all concern the implementation of environments and agents for the
vacuum-cleaner world.
Exercises 63
2.8 Implement a performance-measuring environment simulator for the vacuum-cleaner
world depicted in Figure 2.2 and speciﬁed on page 38. Your implementation should be modu-
lar so that the sensors, actuators, and environment characteristics (size, shape, dirt placement,
etc.) can be changed easily. (Note: for some choices of programming language and operating
system there are already implementations in the online code repository.)
2.9 Implement a simple reﬂex agent for the vacuum environment in Exercise 2.8. Run the
environment with this agent for all possible initial dirt conﬁgurations and agent locations.
Record the performance score for each conﬁguration and the overall average score.
2.10 Consider a modiﬁed version of the vacuum environment in Exercise 2.8, in which the
agent is penalized one point for each movement.
a. Can a simple reﬂex agent be perfectly rational for this environment? Explain.
b. What about a reﬂex agent with state? Design such an agent.
c. How do your answers to a and b change if the agent’s percepts give it the clean/dirty
status of every square in the environment?
2.11 Consider a modiﬁed version of the vacuum environment in Exercise 2.8, in which the
geography of the environment—its extent, boundaries, and obstacles—is unknown, as is the
initial dirt conﬁguration. (The agent can go Up and Down as well as Left and Right.)
a. Can a simple reﬂex agent be perfectly rational for this environment? Explain.
b. Can a simple reﬂex agent with a randomized agent function outperform a simple reﬂex
agent? Design such an agent and measure its performance on several environments.
c. Can you design an environment in which your randomized agent will perform poorly?
Show your results.
d. Can a reﬂex agent with state outperform a simple reﬂex agent? Design such an agent
and measure its performance on several environments. Can you design a rational agent
of this type?
2.12 Repeat Exercise 2.11 for the case in which the location sensor is replaced with a
“bump” sensor that detects the agent’s attempts to move into an obstacle or to cross the
boundaries of the environment. Suppose the bump sensor stops working; how should the
agent behave?
2.13 The vacuum environments in the preceding exercises have all been deterministic. Dis-
cuss possible agent programs for each of the following stochastic versions:
a. Murphy’s law: twenty-ﬁve percent of the time, the Suck action fails to clean the ﬂoor if
it is dirty and deposits dirt onto the ﬂoor if the ﬂoor is clean. How is your agent program
affected if the dirt sensor gives the wrong answer 10% of the time?
b. Small children: At each time step, each clean square has a 10% chance of becoming
dirty. Can you come up with a rational agent design for this case?


END_INSTRUCTION
