
START_INSTRUCTION
You are an expert AI tutor creating a comprehensive study guide.
Your task is to rewrite and expand the "Lesson Notes" provided below into a cohesive, readable study text.

1. **Structure:** Follow the exact structure and order of the topics in the "Lesson Notes". Do not reorder them.
2. **Enrichment:** Use the "Textbook Context" provided to expand on every bullet point in the notes.
    - If the notes mention a concept (e.g., "Rationality"), define it precisely using the textbook.
    - If the notes list algorithms (e.g., "A*", "Simulated Annealing"), explain how they work, their complexity, and pros/cons using the textbook details.
    - Add examples from the textbook where appropriate.
3. **Tone:** Academic but accessible study material. Not just bullet points—write paragraphs where necessary to explain complex ideas.
4. **Language:** The output must be in **Czech** (since the lesson notes are in Czech), but you can keep standard English AI terminology (like "Overfitting") in parentheses.

--- LESSON NOTES (Skeleton) ---
Splňování podmínek

Úloha je tvořena:
• konečnou množinou proměnných, popisujících stav světa
• definičním oborem pro každou proměnnou
• podmínkami, které musí hledané řešení splnit
• řešení jsou pak takové hodnoty proměnných, aby všechny podmínky byly splněny
• př. - sudoku, 8 queens, barvení mapy, Einsteinova zebra, problém obchodního cestujícího
• reálnější úlohy - rozvrhování výroby, tvorba rozvrhů
Způsoby řešení
Matematické metody
Operační výzkum
▪ snaha najít optimální řešení daného problému při daných omezeních
▪ používá se matematické modelování (rovnice, nerovnice apod.)
○
•
Booleovská splnitelnost
○ → převod podmínek na logické formule
řešíme otázku nalezení interpretace (dosazení konstant za proměnné)
▪ tak aby formule byla pravdivá
○
Metody:
▪ DPLL algoritmus: slepé prohledávání do hloubky
▪ Tablová metoda
▪ Rezoluční princip
○
•
Využití stavového prostoru
○ Stav: částečné přiřazení hodnot proměnným
○ Konzistentní stav: stav, který neporušuje žádnou podmínku
○ Úplný stav: stav u kterého jsou přiřazeny hodnoty všem proměnným
○ Koncový stav: stav, který je úplný a konzistentní

--- TEXTBOOK CONTEXT (Source Material) ---


--- BOOK CHAPTER: 6_Constraint_Satisfaction_Problems ---

6
CONSTRAINT
SATISFACTION PROBLEMS
In which we see how treating states as more than just little black boxes leads to the
invention of a range of powerful new search methods and a deeper understanding
of problem structure and complexity.
Chapters 3 and 4 explored the idea that problems can be solved by searching in a space of
states. These states can be evaluated by domain-speciﬁc heuristics and tested to see whether
they are goal states. From the point of view of the search algorithm, however, each state is
atomic, or indivisible—a black box with no internal structure.
This chapter describes a way to solve a wide variety of problems more efﬁciently. We
use a factored representation for each state: a set of variables, each of which has a value.
A problem is solved when each variable has a value that satisﬁes all the constraints on the
variable. A problem described this way is called a constraint satisfaction problem,o rC S P .
CONSTRAINT
SA TISFACTION
PROBLEM
CSP search algorithms take advantage of the structure of states and usegeneral-purpose
rather than problem-speciﬁc heuristics to enable the solution of complex problems. The main
idea is to eliminate large portions of the search space all at once by identifying variable/value
combinations that violate the constraints.
6.1 D EFINING CONSTRAINT SA TISFACTION PROBLEMS
A constraint satisfaction problem consists of three components, X,D, and C:
X is a set of variables,{X1,...,X n}.
D is a set of domains,{D1,...,D n}, one for each variable.
C is a set of constraints that specify allowable combinations of values.
Each domain Di consists of a set of allowable values, {v1,...,v k} for variable Xi. Each
constraint Ci consists of a pair⟨scope,rel⟩,w h e r escope is a tuple of variables that participate
in the constraint andrel is a relation that deﬁnes the values that those variables can take on. A
relation can be represented as an explicit list of all tuples of values that satisfy the constraint,
or as an abstract relation that supports two operations: testing if a tuple is a member of the
relation and enumerating the members of the relation. For example, if X
1 and X2 both have
202
Section 6.1. Deﬁning Constraint Satisfaction Problems 203
the domain {A,B}, then the constraint saying the two variables must have different values
can be written as⟨(X1,X2),[(A, B),(B,A )]⟩ or as⟨(X1,X2),X1 ̸= X2⟩.
To solve a CSP, we need to deﬁne a state space and the notion of a solution. Each
state in a CSP is deﬁned by an assignment of values to some or all of the variables, {Xi =ASSIGNMENT
vi,Xj = vj,... }. An assignment that does not violate any constraints is called a consistentCONSISTENT
or legal assignment. A complete assignment is one in which every variable is assigned, andCOMPLETE
ASSIGNMENT
a solution to a CSP is a consistent, complete assignment. A partial assignment is one thatSOLUTION
PA RT I A L
ASSIGNMENT assigns values to only some of the variables.
6.1.1 Example problem: Map coloring
Suppose that, having tired of Romania, we are looking at a map of Australia showing each
of its states and territories (Figure 6.1(a)). We are given the task of coloring each region
either red, green, or blue in such a way that no neighboring regions have the same color. To
formulate this as a CSP, we deﬁne the variables to be the regions
X ={WA,NT,Q ,NSW ,V, SA,T} .
The domain of each variable is the set D
i = {red,green,blue}. The constraints require
neighboring regions to have distinct colors. Since there are nine places where regions border,
there are nine constraints:
C = {SA̸= WA,SA̸= NT,SA̸= Q,SA̸= NSW ,SA̸= V,
WA̸= NT,NT ̸= Q, Q̸= NSW ,NSW ̸= V} .
Here we are using abbreviations; SA̸= WA is a shortcut for⟨(SA,WA),SA̸= WA⟩,w h e r e
SA̸= WA can be fully enumerated in turn as
{(red,green),(red,blue),(green,red),(green,blue),(blue,red),(blue,
green)} .
There are many possible solutions to this problem, such as
{WA=red,NT =green,Q =red,NSW =green,V =red,SA =blue,T =red}.
It can be helpful to visualize a CSP as a constraint graph, as shown in Figure 6.1(b). TheCONSTRAINT GRAPH
nodes of the graph correspond to variables of the problem, and a link connects any two vari-
ables that participate in a constraint.
Why formulate a problem as a CSP? One reason is that the CSPs yield a natural rep-
resentation for a wide variety of problems; if you already have a CSP-solving system, it is
often easier to solve a problem using it than to design a custom solution using another search
technique. In addition, CSP solvers can be faster than state-space searchers because the CSP
solver can quickly eliminate large swatches of the search space. For example, once we have
chosen{SA =blue} in the Australia problem, we can conclude that none of the ﬁve neighbor-
ing variables can take on the valueblue. Without taking advantage of constraint propagation,
a search procedure would have to consider 3
5 = 243 assignments for the ﬁve neighboring
variables; with constraint propagation we never have to consider blue as a value, so we have
only 25 =3 2assignments to look at, a reduction of 87%.
In regular state-space search we can only ask: is this speciﬁc state a goal? No? What
about this one? With CSPs, once we ﬁnd out that a partial assignment is not a solution, we can
204 Chapter 6. Constraint Satisfaction Problems
Western
Australia
Northern
Territory
South
Australia
Queensland
New
South
Wales
Victoria
Tasmania
WA
NT
SA
Q
NSW
V
T
(a) (b)
Figure 6.1 (a) The principal states and territories of Australia. Coloring this map can
be viewed as a constraint satisfaction problem (CSP). The goal is to assign colors to each
region so that no neighboring regions have the same color. (b) The map-coloring problem
represented as a constraint graph.
immediately discard further reﬁnements of the partial assignment. Furthermore, we can see
why the assignment is not a solution—we see which variables violate a constraint—so we can
focus attention on the variables that matter. As a result, many problems that are intractable
for regular state-space search can be solved quickly when formulated as a CSP.
6.1.2 Example problem: Job-shop scheduling
Factories have the problem of scheduling a day’s worth of jobs, subject to various constraints.
In practice, many of these problems are solved with CSP techniques. Consider the problem of
scheduling the assembly of a car. The whole job is composed of tasks, and we can model each
task as a variable, where the value of each variable is the time that the task starts, expressed
as an integer number of minutes. Constraints can assert that one task must occur before
another—for example, a wheel must be installed before the hubcap is put on—and that only
so many tasks can go on at once. Constraints can also specify that a task takes a certain
amount of time to complete.
We consider a small part of the car assembly, consisting of 15 tasks: install axles (front
and back), afﬁx all four wheels (right and left, front and back), tighten nuts for each wheel,
afﬁx hubcaps, and inspect the ﬁnal assembly. We can represent the tasks with 15 variables:
X = {Axle
F ,AxleB,WheelRF ,WheelLF ,WheelRB,WheelLB,NutsRF ,
NutsLF ,NutsRB,NutsLB,CapRF ,CapLF ,CapRB,CapLB,Inspect} .
The value of each variable is the time that the task starts. Next we represent precedence
constraints between individual tasks. Whenever a task T1 must occur before task T2,a n dPRECEDENCE
CONSTRAINTS
task T1 takes duration d1 to complete, we add an arithmetic constraint of the form
T1 + d1 ≤T2 .
Section 6.1. Deﬁning Constraint Satisfaction Problems 205
In our example, the axles have to be in place before the wheels are put on, and it takes 10
minutes to install an axle, so we write
AxleF +1 0≤WheelRF ; AxleF +1 0≤WheelLF ;
AxleB +1 0≤WheelRB; AxleB +1 0≤WheelLB .
Next we say that, for each wheel, we must afﬁx the wheel (which takes 1 minute), then tighten
the nuts (2 minutes), and ﬁnally attach the hubcap (1 minute, but not represented yet):
WheelRF +1 ≤NutsRF ; NutsRF +2 ≤CapRF ;
WheelLF +1 ≤NutsLF ; NutsLF +2 ≤CapLF ;
WheelRB +1 ≤NutsRB; NutsRB +2 ≤CapRB;
WheelLB +1 ≤NutsLB; NutsLB +2 ≤CapLB .
Suppose we have four workers to install wheels, but they have to share one tool that helps put
the axle in place. We need a disjunctive constraint to say that AxleF and AxleB must notDISJUNCTIVE
CONSTRAINT
overlap in time; either one comes ﬁrst or the other does:
(AxleF +1 0≤AxleB) or (AxleB +1 0≤AxleF ) .
This looks like a more complicated constraint, combining arithmetic and logic. But it still
reduces to a set of pairs of values that AxleF and AxleF can take on.
We also need to assert that the inspection comes last and takes 3 minutes. For every
variable exceptInspect we add a constraint of the formX + dX ≤Inspect. Finally, suppose
there is a requirement to get the whole assembly done in 30 minutes. We can achieve that by
limiting the domain of all variables:
Di ={1,2,3,..., 27} .
This particular problem is trivial to solve, but CSPs have been applied to job-shop schedul-
ing problems like this with thousands of variables. In some cases, there are complicated
constraints that are difﬁcult to specify in the CSP formalism, and more advanced planning
techniques are used, as discussed in Chapter 11.
6.1.3 Variations on the CSP formalism
The simplest kind of CSP involves variables that have discrete, ﬁnite domains .M a p -DISCRETE DOMAIN
FINITE DOMAIN coloring problems and scheduling with time limits are both of this kind. The 8-queens prob-
lem described in Chapter 3 can also be viewed as a ﬁnite-domain CSP, where the variables
Q
1,...,Q 8 are the positions of each queen in columns 1,..., 8 and each variable has the
domain Di ={1,2,3,4,5,6,7, 8}.
A discrete domain can beinﬁnite, such as the set of integers or strings. (If we didn’t putINFINITE
a deadline on the job-scheduling problem, there would be an inﬁnite number of start times
for each variable.) With inﬁnite domains, it is no longer possible to describe constraints by
enumerating all allowed combinations of values. Instead, a constraint language must be
CONSTRAINT
LANGUAGE
used that understands constraints such as T1 + d1 ≤T2 directly, without enumerating the
set of pairs of allowable values for (T1,T2). Special solution algorithms (which we do not
discuss here) exist for linear constraints on integer variables—that is, constraints, such asLINEAR
CONSTRAINTS
the one just given, in which each variable appears only in linear form. It can be shown that
no algorithm exists for solving general nonlinear constraints on integer variables.NONLINEAR
CONSTRAINTS
206 Chapter 6. Constraint Satisfaction Problems
Constraint satisfaction problems with continuous domains are common in the realCONTINUOUS
DOMAINS
world and are widely studied in the ﬁeld of operations research. For example, the scheduling
of experiments on the Hubble Space Telescope requires very precise timing of observations;
the start and ﬁnish of each observation and maneuver are continuous-valued variables that
must obey a variety of astronomical, precedence, and power constraints. The best-known
category of continuous-domain CSPs is that of linear programming problems, where con-
straints must be linear equalities or inequalities. Linear programming problems can be solved
in time polynomial in the number of variables. Problems with different types of constraints
and objective functions have also been studied—quadratic programming, second-order conic
programming, and so on.
In addition to examining the types of variables that can appear in CSPs, it is useful to
look at the types of constraints. The simplest type is the unary constraint, which restricts
UNARY CONSTRAINT
the value of a single variable. For example, in the map-coloring problem it could be the case
that South Australians won’t tolerate the color green; we can express that with the unary
constraint⟨(SA),SA̸= green⟩
A binary constraint relates two variables. For example, SA ̸= NSW is a binary
BINARY CONSTRAINT
constraint. A binary CSP is one with only binary constraints; it can be represented as a
constraint graph, as in Figure 6.1(b).
We can also describe higher-order constraints, such as asserting that the value of Y is
between X and Z, with the ternary constraint Between(X,Y,Z ).
A constraint involving an arbitrary number of variables is called a global constraint.GLOBAL
CONSTRAINT
(The name is traditional but confusing because it need not involve all the variables in a prob-
lem). One of the most common global constraints is Alldiﬀ , which says that all of the
variables involved in the constraint must have different values. In Sudoku problems (see
Section 6.2.6), all variables in a row or column must satisfy an Alldiﬀ constraint. An-
other example is provided by cryptarithmetic puzzles. (See Figure 6.2(a).) Each letter in a
CRYPTARITHMETIC
cryptarithmetic puzzle represents a different digit. For the case in Figure 6.2(a), this would
be represented as the global constraint Alldiﬀ(F,T,U,W,R,O ). The addition constraints
on the four columns of the puzzle can be written as the following n-ary constraints:
O + O = R +1 0· C10
C10 + W + W = U +1 0· C100
C100 + T + T = O +1 0· C1000
C1000 = F,
where C10, C100,a n dC1000 are auxiliary variables representing the digit carried over into the
tens, hundreds, or thousands column. These constraints can be represented in a constraint
hypergraph, such as the one shown in Figure 6.2(b). A hypergraph consists of ordinary nodesCONSTRAINT
HYPERGRAPH
(the circles in the ﬁgure) and hypernodes (the squares), which represent n-ary constraints.
Alternatively, as Exercise 6.6 asks you to prove, every ﬁnite-domain constraint can be
reduced to a set of binary constraints if enough auxiliary variables are introduced, so we could
transform any CSP into one with only binary constraints; this makes the algorithms simpler.
Another way to convert ann-ary CSP to a binary one is thedual graph transformation: create
DUAL GRAPH
a new graph in which there will be one variable for each constraint in the original graph, and
Section 6.1. Deﬁning Constraint Satisfaction Problems 207
(a)
OWTF U R
(b)
+
F
T
T
O
W
W
U
O
O
R
C3 C1C2
Figure 6.2 (a) A cryptarithmetic problem. Each letter stands for a distinct digit; the aim is
to ﬁnd a substitution of digits for letters such that the resulting sum is arithmetically correct,
with the added restriction that no leading zeroes are allowed. (b) The constraint hypergraph
for the cryptarithmetic problem, showing the Alldiﬀ constraint (square box at the top) as
well as the column addition constraints (four square boxes in the middle). The variables C
1,
C2,a n dC3 represent the carry digits for the three columns.
one binary constraint for each pair of constraints in the original graph that share variables. For
example, if the original graph has variables {X,Y,Z } and constraints ⟨(X,Y,Z ),C1⟩ and
⟨(X,Y ),C2⟩ then the dual graph would have variables {C1,C2} with the binary constraint
⟨(X,Y ),R1⟩,w h e r e(X,Y ) are the shared variables andR1 is a new relation that deﬁnes the
constraint between the shared variables, as speciﬁed by the original C1 and C2.
There are however two reasons why we might prefer a global constraint such asAlldiﬀ
rather than a set of binary constraints. First, it is easier and less error-prone to write the
problem description using Alldiﬀ . Second, it is possible to design special-purpose inference
algorithms for global constraints that are not available for a set of more primitive constraints.
We describe these inference algorithms in Section 6.2.5.
The constraints we have described so far have all been absolute constraints, violation of
which rules out a potential solution. Many real-world CSPs include preference constraints
PREFERENCE
CONSTRAINTS
indicating which solutions are preferred. For example, in a university class-scheduling prob-
lem there are absolute constraints that no professor can teach two classes at the same time.
But we also may allow preference constraints: Prof. R might prefer teaching in the morning,
whereas Prof. N prefers teaching in the afternoon. A schedule that has Prof. R teaching at
2 p.m. would still be an allowable solution (unless Prof. R happens to be the department chair)
but would not be an optimal one. Preference constraints can often be encoded as costs on in-
dividual variable assignments—for example, assigning an afternoon slot for Prof. R costs
2 points against the overall objective function, whereas a morning slot costs 1. With this
formulation, CSPs with preferences can be solved with optimization search methods, either
path-based or local. We call such a problem a constraint optimization problem , or COP.
CONSTRAINT
OPTIMIZA TION
PROBLEM
Linear programming problems do this kind of optimization.
208 Chapter 6. Constraint Satisfaction Problems
6.2 C ONSTRAINT PROPAGATION :I NFERENCE IN CSP S
In regular state-space search, an algorithm can do only one thing: search. In CSPs there is a
choice: an algorithm can search (choose a new variable assignment from several possibilities)
or do a speciﬁc type of inference called constraint propagation: using the constraints toINFERENCE
CONSTRAINT
PROP AGA TION reduce the number of legal values for a variable, which in turn can reduce the legal values
for another variable, and so on. Constraint propagation may be intertwined with search, or it
may be done as a preprocessing step, before search starts. Sometimes this preprocessing can
solve the whole problem, so no search is required at all.
The key idea is local consistency. If we treat each variable as a node in a graph (see
LOCAL
CONSISTENCY
Figure 6.1(b)) and each binary constraint as an arc, then the process of enforcing local con-
sistency in each part of the graph causes inconsistent values to be eliminated throughout the
graph. There are different types of local consistency, which we now cover in turn.
6.2.1 Node consistency
A single variable (corresponding to a node in the CSP network) is node-consistent if allNODE CONSISTENCY
the values in the variable’s domain satisfy the variable’s unary constraints. For example,
in the variant of the Australia map-coloring problem (Figure 6.1) where South Australians
dislike green, the variable SA starts with domain {red,green,blue}, and we can make it
node consistent by eliminating green, leaving SA with the reduced domain{red,blue}.W e
say that a network is node-consistent if every variable in the network is node-consistent.
It is always possible to eliminate all the unary constraints in a CSP by running node
consistency. It is also possible to transform all n-ary constraints into binary ones (see Ex-
ercise 6.6). Because of this, it is common to deﬁne CSP solvers that work with only binary
constraints; we make that assumption for the rest of this chapter, except where noted.
6.2.2 Arc consistency
Av a r i a b l ei naC S Pi sarc-consistent if every value in its domain satisﬁes the variable’sARC CONSISTENCY
binary constraints. More formally, Xi is arc-consistent with respect to another variable Xj if
for every value in the current domain Di there is some value in the domain Dj that satisﬁes
the binary constraint on the arc (Xi,Xj). A network is arc-consistent if every variable is arc
consistent with every other variable. For example, consider the constraintY = X2 where the
domain of both X and Y is the set of digits. We can write this constraint explicitly as
⟨(X,Y ),{(0,0),(1,1),(2,4),(3,9))}⟩ .
To make X arc-consistent with respect to Y , we reduce X’s domain to {0,1,2,3}.I f w e
also make Y arc-consistent with respect to X,t h e nY ’s domain becomes{0,1,4,9} and the
whole CSP is arc-consistent.
On the other hand, arc consistency can do nothing for the Australia map-coloring prob-
lem. Consider the following inequality constraint on (SA,WA):
{(red,green),(red,blue),(green,red),(green,blue),(blue,red),(blue,green)} .
Section 6.2. Constraint Propagation: Inference in CSPs 209
function AC-3( csp) returns false if an inconsistency is found and true otherwise
inputs: csp, a binary CSP with components (X, D, C )
local variables: queue, a queue of arcs, initially all the arcs in csp
while queue is not empty do
(Xi,X j)←REMOVE -FIRST (queue)
if REVISE (csp, Xi,X j ) then
if size of Di =0 then return false
for each Xk in Xi.NEIGHBORS -{Xj} do
add (Xk,X i)t o queue
return true
function REVISE (csp, Xi,X j ) returns true iff we revise the domain of Xi
revised←false
for each x in Di do
if no value y in Dj allows (x ,y) to satisfy the constraint between Xi and Xj then
delete x from Di
revised←true
return revised
Figure 6.3 The arc-consistency algorithm AC-3. After applying AC-3, either every arc
is arc-consistent, or some variable has an empty domain, indicating that the CSP cannot be
solved. The name “AC-3” was used by the algorithm’s inventor (Mackworth, 1977) because
it’s the third version developed in the paper.
No matter what value you choose for SA (or for WA), there is a valid value for the other
variable. So applying arc consistency has no effect on the domains of either variable.
The most popular algorithm for arc consistency is called AC-3 (see Figure 6.3). To
make every variable arc-consistent, the AC-3 algorithm maintains a queue of arcs to consider.
(Actually, the order of consideration is not important, so the data structure is really a set, but
tradition calls it a queue.) Initially, the queue contains all the arcs in the CSP. AC-3 then pops
off an arbitrary arc (X
i,Xj) from the queue and makes Xi arc-consistent with respect to Xj .
If this leaves Di unchanged, the algorithm just moves on to the next arc. But if this revises
Di (makes the domain smaller), then we add to the queue all arcs (Xk,Xi) where Xk is a
neighbor of Xi. We need to do that because the change inDi might enable further reductions
in the domains of Dk, even if we have previously considered Xk.I f Di is revised down to
nothing, then we know the whole CSP has no consistent solution, and AC-3 can immediately
return failure. Otherwise, we keep checking, trying to remove values from the domains of
variables until no more arcs are in the queue. At that point, we are left with a CSP that is
equivalent to the original CSP—they both have the same solutions—but the arc-consistent
CSP will in most cases be faster to search because its variables have smaller domains.
The complexity of AC-3 can be analyzed as follows. Assume a CSP with n variables,
each with domain size at most d, and with c binary constraints (arcs). Each arc (X
k,Xi) can
be inserted in the queue only d times because Xi has at most d values to delete. Checking
210 Chapter 6. Constraint Satisfaction Problems
consistency of an arc can be done in O(d2) time, so we get O(cd3) total worst-case time.1
It is possible to extend the notion of arc consistency to handle n-ary rather than just
binary constraints; this is called generalized arc consistency or sometimes hyperarc consis-
tency, depending on the author. A variable X
i is generalized arc consistent with respect toGENERALIZED ARC
CONSISTENT
an n-ary constraint if for every value v in the domain of Xi there exists a tuple of values that
is a member of the constraint, has all its values taken from the domains of the corresponding
variables, and has its Xi component equal to v. For example, if all variables have the do-
main{0,1,2,3},t h e nt om a k et h ev a r i a b l eX consistent with the constraint X<Y<Z ,
we would have to eliminate 2 and 3 from the domain of X because the constraint cannot be
satisﬁed when X is 2 or 3.
6.2.3 Path consistency
Arc consistency can go a long way toward reducing the domains of variables, sometimes
ﬁnding a solution (by reducing every domain to size 1) and sometimes ﬁnding that the CSP
cannot be solved (by reducing some domain to size 0). But for other networks, arc consistency
fails to make enough inferences. Consider the map-coloring problem on Australia, but with
only two colors allowed, red and blue. Arc consistency can do nothing because every variable
is already arc consistent: each can be red with blue at the other end of the arc (or vice versa).
But clearly there is no solution to the problem: because Western Australia, Northern Territory
and South Australia all touch each other, we need at least three colors for them alone.
Arc consistency tightens down the domains (unary constraints) using the arcs (binary
constraints). To make progress on problems like map coloring, we need a stronger notion of
consistency. Path consistency tightens the binary constraints by using implicit constraints
P A TH CONSISTENCY
that are inferred by looking at triples of variables.
A two-variable set {Xi,Xj} is path-consistent with respect to a third variable Xm if,
for every assignment{Xi = a, Xj = b} consistent with the constraints on{Xi,Xj},t h e r ei s
an assignment toXm that satisﬁes the constraints on{Xi,Xm} and{Xm,Xj}. This is called
path consistency because one can think of it as looking at a path from Xi to Xj with Xm in
the middle.
Let’s see how path consistency fares in coloring the Australia map with two colors. We
will make the set{WA,SA} path consistent with respect toNT . We start by enumerating the
consistent assignments to the set. In this case, there are only two: {WA = red,SA = blue}
and{WA = blue,SA = red}. We can see that with both of these assignments NT can be
neither red nor blue (because it would conﬂict with either WA or SA). Because there is no
valid choice forNT, we eliminate both assignments, and we end up with no valid assignments
for{WA,SA}. Therefore, we know that there can be no solution to this problem. The PC-2
algorithm (Mackworth, 1977) achieves path consistency in much the same way that AC-3
achieves arc consistency. Because it is so similar, we do not show it here.
1 The AC-4 algorithm (Mohr and Henderson, 1986) runs inO(cd2) worst-case time but can be slower than AC-3
on average cases. See Exercise 6.13.
Section 6.2. Constraint Propagation: Inference in CSPs 211
6.2.4 K-consistency
Stronger forms of propagation can be deﬁned with the notion of k-consistency.A C S P i sK-CONSISTENCY
k-consistent if, for any set of k−1 variables and for any consistent assignment to those
variables, a consistent value can always be assigned to any kth variable. 1-consistency says
that, given the empty set, we can make any set of one variable consistent: this is what we
called node consistency. 2-consistency is the same as arc consistency. For binary constraint
networks, 3-consistency is the same as path consistency.
AC S Pi s strongly k-consistent if it is k-consistent and is also (k−1)-consistent,
STRONGL Y
K-CONSISTENT
(k−2)-consistent, ... all the way down to 1-consistent. Now suppose we have a CSP with
n nodes and make it strongly n-consistent (i.e., strongly k-consistent for k = n). We can
then solve the problem as follows: First, we choose a consistent value for X1.W e a r e t h e n
guaranteed to be able to choose a value for X2 because the graph is 2-consistent, for X3
because it is 3-consistent, and so on. For each variableXi, we need only search through thed
values in the domain to ﬁnd a value consistent withX1,...,X i−1. We are guaranteed to ﬁnd
a solution in time O(n2d). Of course, there is no free lunch: any algorithm for establishing
n-consistency must take time exponential in n in the worst case. Worse, n-consistency also
requires space that is exponential in n. The memory issue is even more severe than the time.
In practice, determining the appropriate level of consistency checking is mostly an empirical
science. It can be said practitioners commonly compute 2-consistency and less commonly
3-consistency.
6.2.5 Global constraints
Remember that a global constraint is one involving an arbitrary number of variables (but not
necessarily all variables). Global constraints occur frequently in real problems and can be
handled by special-purpose algorithms that are more efﬁcient than the general-purpose meth-
ods described so far. For example, the Alldiﬀ constraint says that all the variables involved
must have distinct values (as in the cryptarithmetic problem above and Sudoku puzzles be-
low). One simple form of inconsistency detection for Alldiﬀ constraints works as follows:
if m variables are involved in the constraint, and if they have n possible distinct values alto-
gether, and m>n , then the constraint cannot be satisﬁed.
This leads to the following simple algorithm: First, remove any variable in the con-
straint that has a singleton domain, and delete that variable’s value from the domains of the
remaining variables. Repeat as long as there are singleton variables. If at any point an empty
domain is produced or there are more variables than domain values left, then an inconsistency
has been detected.
This method can detect the inconsistency in the assignment{WA=red, NSW =red}
for Figure 6.1. Notice that the variables SA, NT ,a n d Q are effectively connected by an
Alldiﬀ constraint because each pair must have two different colors. After applying AC-3
with the partial assignment, the domain of each variable is reduced to {green,blue}.T h a t
is, we have three variables and only two colors, so the Alldiﬀ constraint is violated. Thus,
a simple consistency procedure for a higher-order constraint is sometimes more effective
than applying arc consistency to an equivalent set of binary constraints. There are more
212 Chapter 6. Constraint Satisfaction Problems
complex inference algorithms for Alldiﬀ (see van Hoeve and Katriel, 2006) that propagate
more constraints but are more computationally expensive to run.
Another important higher-order constraint is theresource constraint, sometimes calledRESOURCE
CONSTRAINT
the atmost constraint. For example, in a scheduling problem, let P1,...,P 4 denote the
numbers of personnel assigned to each of four tasks. The constraint that no more than 10
personnel are assigned in total is written as Atmost(10,P1,P2,P3,P4). We can detect an
inconsistency simply by checking the sum of the minimum values of the current domains;
for example, if each variable has the domain {3,4,5,6},t h e Atmost constraint cannot be
satisﬁed. We can also enforce consistency by deleting the maximum value of any domain if it
is not consistent with the minimum values of the other domains. Thus, if each variable in our
example has the domain{2,3,4,5,6}, the values 5 and 6 can be deleted from each domain.
For large resource-limited problems with integer values—such as logistical problems
involving moving thousands of people in hundreds of vehicles—it is usually not possible to
represent the domain of each variable as a large set of integers and gradually reduce that set by
consistency-checking methods. Instead, domains are represented by upper and lower bounds
and are managed by bounds propagation. For example, in an airline-scheduling problem,
BOUNDS
PROP AGA TION
let’s suppose there are two ﬂights, F1 and F2, for which the planes have capacities 165 and
385, respectively. The initial domains for the numbers of passengers on each ﬂight are then
D1 =[ 0,165] and D2 =[ 0,385] .
Now suppose we have the additional constraint that the two ﬂights together must carry 420
people: F1 + F2 = 420. Propagating bounds constraints, we reduce the domains to
D1 =[ 3 5,165] and D2 = [255,385] .
We say that a CSP is bounds consistent if for every variable X, and for both the lower-BOUNDS
CONSISTENT
bound and upper-bound values ofX, there exists some value ofY that satisﬁes the constraint
between X and Y for every variable Y . This kind of bounds propagation is widely used in
practical constraint problems.
6.2.6 Sudoku example
The popular Sudoku puzzle has introduced millions of people to constraint satisfaction prob-SUDOKU
lems, although they may not recognize it. A Sudoku board consists of 81 squares, some of
which are initially ﬁlled with digits from 1 to 9. The puzzle is to ﬁll in all the remaining
squares such that no digit appears twice in any row, column, or3× 3 box (see Figure 6.4). A
row, column, or box is called a unit.
The Sudoku puzzles that are printed in newspapers and puzzle books have the property
that there is exactly one solution. Although some can be tricky to solve by hand, taking tens
of minutes, even the hardest Sudoku problems yield to a CSP solver in less than 0.1 second.
A Sudoku puzzle can be considered a CSP with 81 variables, one for each square. We
use the variable names A1 through A9 for the top row (left to right), down to I1 through I9
for the bottom row. The empty squares have the domain {1,2,3,4,5,6,7,8,9} and the pre-
ﬁlled squares have a domain consisting of a single value. In addition, there are 27 different
Section 6.2. Constraint Propagation: Inference in CSPs 213
326
93 51
18 64
81 29
78
67 82
26 95
82 39
513
326
93 51
18 64
81 29
78
67 82
26 95
82 39
513
4 8915 7
6 748 2
2 579 3
5 437 6
2956413
1 394 5
3 781 4
1 457 6
6 9478 2
123456789
A
B
C
D
E
F
G
H
 I
A
B
C
D
E
F
G
H
 I
123456789
(a) (b)
Figure 6.4 (a) A Sudoku puzzle and (b) its solution.
Alldiﬀ constraints: one for each row, column, and box of 9 squares.
Alldiﬀ(A1,A2,A3,A4,A5,A6,A 7,A 8,A 9)
Alldiﬀ(B1,B 2,B 3,B 4,B 5,B 6,B 7,B 8,B 9)
···
Alldiﬀ(A1,B 1,C 1,D 1,E 1,F 1,G1,H 1,I 1)
Alldiﬀ(A2,B 2,C 2,D 2,E 2,F 2,G2,H 2,I 2)
···
Alldiﬀ(A1,A2,A3,B 1,B 2,B 3,C 1,C 2,C
3)
Alldiﬀ(A4,A5,A6,B 4,B 5,B 6,C 4,C 5,C 6)
···
Let us see how far arc consistency can take us. Assume that theAlldiﬀ constraints have been
expanded into binary constraints (such asA1 ̸= A2 ) so that we can apply the AC-3 algorithm
directly. Consider variable E6 from Figure 6.4(a)—the empty square between the 2 and the
8 in the middle box. From the constraints in the box, we can remove not only 2 and 8 but also
1a n d7f r o mE6 ’s domain. From the constraints in its column, we can eliminate 5, 6, 2, 8,
9, and 3. That leaves E6 with a domain of{4}; in other words, we know the answer for E6 .
Now consider variable I6 —the square in the bottom middle box surrounded by 1, 3, and 3.
Applying arc consistency in its column, we eliminate 5, 6, 2, 4 (since we now knowE6 must
be 4), 8, 9, and 3. We eliminate 1 by arc consistency with I5 , and we are left with only the
value 7 in the domain of I6 . Now there are 8 known values in column 6, so arc consistency
can infer that A6 must be 1. Inference continues along these lines, and eventually, AC-3 can
solve the entire puzzle—all the variables have their domains reduced to a single value, as
shown in Figure 6.4(b).
Of course, Sudoku would soon lose its appeal if every puzzle could be solved by a
214 Chapter 6. Constraint Satisfaction Problems
mechanical application of AC-3, and indeed AC-3 works only for the easiest Sudoku puzzles.
Slightly harder ones can be solved by PC-2, but at a greater computational cost: there are
255,960 different path constraints to consider in a Sudoku puzzle. To solve the hardest puzzles
and to make efﬁcient progress, we will have to be more clever.
Indeed, the appeal of Sudoku puzzles for the human solver is the need to be resourceful
in applying more complex inference strategies. Aﬁcionados give them colorful names, such
as “naked triples.” That strategy works as follows: in any unit (row, column or box), ﬁnd
three squares that each have a domain that contains the same three numbers or a subset of
those numbers. For example, the three domains might be {1,8},{3,8},a n d{1,3,8}.F r o m
that we don’t know which square contains 1, 3, or 8, but we do know that the three numbers
must be distributed among the three squares. Therefore we can remove 1, 3, and 8 from the
domains of every other square in the unit.
It is interesting to note how far we can go without saying much that is speciﬁc to Su-
doku. We do of course have to say that there are 81 variables, that their domains are the digits
1t o9 ,a n dt h a tt h e r ea r e2 7Alldiﬀ constraints. But beyond that, all the strategies—arc con-
sistency, path consistency, etc.—apply generally to all CSPs, not just to Sudoku problems.
Even naked triples is really a strategy for enforcing consistency of Alldiﬀ constraints and
has nothing to do with Sudoku per se. This is the power of the CSP formalism: for each new
problem area, we only need to deﬁne the problem in terms of constraints; then the general
constraint-solving mechanisms can take over.
6.3 B ACKTRACKING SEARCH FOR CSP S
Sudoku problems are designed to be solved by inference over constraints. But many other
CSPs cannot be solved by inference alone; there comes a time when we must search for a
solution. In this section we look at backtracking search algorithms that work on partial as-
signments; in the next section we look at local search algorithms over complete assignments.
We could apply a standard depth-limited search (from Chapter 3). A state would be a
partial assignment, and an action would be adding var = value to the assignment. But for a
CSP with n variables of domain size d, we quickly notice something terrible: the branching
factor at the top level is nd because any of d values can be assigned to any of n variables. At
the next level, the branching factor is (n−1)d, and so on for n levels. We generate a tree
with n!· d
n leaves, even though there are only dn possible complete assignments!
Our seemingly reasonable but naive formulation ignores crucial property common to
all CSPs: commutativity. A problem is commutative if the order of application of any givenCOMMUT A TIVITY
set of actions has no effect on the outcome. CSPs are commutative because when assigning
values to variables, we reach the same partial assignment regardless of order. Therefore, we
need only consider a single variable at each node in the search tree. For example, at the root
node of a search tree for coloring the map of Australia, we might make a choice between
SA =red, SA =green,a n dSA =blue, but we would never choose between SA =red and
WA=blue. With this restriction, the number of leaves is dn, as we would hope.
Section 6.3. Backtracking Search for CSPs 215
function BACKTRACKING -SEARCH (csp) returns a solution, or failure
return BACKTRACK ({} ,csp)
function BACKTRACK (assignment,csp) returns a solution, or failure
if assignment is complete then return assignment
var←SELECT -UNASSIGNED -VARIABLE (csp)
for each value in ORDER -DOMAIN -VALUES (var,assignment,csp) do
if value is consistent with assignment then
add{var = value} to assignment
inferences←INFERENCE (csp,var,value)
if inferences̸= failure then
add inferences to assignment
result←BACKTRACK (assignment,csp)
if result̸= failure then
return result
remove{var = value} and inferences from assignment
return failure
Figure 6.5 A simple backtracking algorithm for constraint satisfaction problems. The al-
gorithm is modeled on the recursive depth-ﬁrst search of Chapter 3. By varying the functions
SELECT -UNASSIGNED -VARIABLE and O RDER -DOMAIN -VALUES , we can implement the
general-purpose heuristics discussed in the text. The function I NFERENCE can optionally be
used to impose arc-, path-, or k-consistency, as desired. If a value choice leads to failure
(noticed either by INFERENCE or by BACKTRACK ), then value assignments (including those
made by INFERENCE ) are removed from the current assignment and a new value is tried.
The term backtracking search is used for a depth-ﬁrst search that chooses values forBACKTRACKING
SEARCH
one variable at a time and backtracks when a variable has no legal values left to assign. The
algorithm is shown in Figure 6.5. It repeatedly chooses an unassigned variable, and then tries
all values in the domain of that variable in turn, trying to ﬁnd a solution. If an inconsistency is
detected, then B
ACKTRACK returns failure, causing the previous call to try another value. Part
of the search tree for the Australia problem is shown in Figure 6.6, where we have assigned
variables in the order WA,NT,Q ,... . Because the representation of CSPs is standardized,
there is no need to supply B ACKTRACKING -SEARCH with a domain-speciﬁc initial state,
action function, transition model, or goal test.
Notice that BACKTRACKING -SEARCH keeps only a single representation of a state and
alters that representation rather than creating new ones, as described on page 87.
In Chapter 3 we improved the poor performance of uninformed search algorithms by
supplying them with domain-speciﬁc heuristic functions derived from our knowledge of the
problem. It turns out that we can solve CSPs efﬁciently without such domain-speciﬁc knowl-
edge. Instead, we can add some sophistication to the unspeciﬁed functions in Figure 6.5,
using them to address the following questions:
1. Which variable should be assigned next (S
ELECT -UNASSIGNED -VARIABLE ), and in
what order should its values be tried (ORDER -DOMAIN -VALUES )?
216 Chapter 6. Constraint Satisfaction Problems
WA=red WA=blueWA=green
WA=red
NT=blue
WA=red
NT=green
WA=red
NT=green
Q=red
WA=red
NT=green
Q=blue
Figure 6.6 Part of the search tree for the map-coloring problem in Figure 6.1.
2. What inferences should be performed at each step in the search (I NFERENCE )?
3. When the search arrives at an assignment that violates a constraint, can the search avoid
repeating this failure?
The subsections that follow answer each of these questions in turn.
6.3.1 Variable and value ordering
The backtracking algorithm contains the line
var←SELECT -UNASSIGNED -VARIABLE (csp).
The simplest strategy for SELECT -UNASSIGNED -VARIABLE is to choose the next unassigned
variable in order,{X1,X2,... }. This static variable ordering seldom results in the most efﬁ-
cient search. For example, after the assignments forWA=red and NT =green in Figure 6.6,
there is only one possible value forSA, so it makes sense to assignSA =blue next rather than
assigning Q. In fact, afterSA is assigned, the choices forQ, NSW ,a n dV are all forced. This
intuitive idea—choosing the variable with the fewest “legal” values—is called theminimum-
remaining-values (MRV) heuristic. It also has been called the “most constrained variable” orMINIMUM-
REMAINING-VALUES
“fail-ﬁrst” heuristic, the latter because it picks a variable that is most likely to cause a failure
soon, thereby pruning the search tree. If some variable X has no legal values left, the MRV
heuristic will select X and failure will be detected immediately—avoiding pointless searches
through other variables. The MRV heuristic usually performs better than a random or static
ordering, sometimes by a factor of 1,000 or more, although the results vary widely depending
on the problem.
The MRV heuristic doesn’t help at all in choosing the ﬁrst region to color in Australia,
because initially every region has three legal colors. In this case, the degree heuristic comes
DEGREE HEURISTIC
in handy. It attempts to reduce the branching factor on future choices by selecting the vari-
able that is involved in the largest number of constraints on other unassigned variables. In
Figure 6.1, SA is the variable with highest degree, 5; the other variables have degree 2 or 3,
except for T , which has degree 0. In fact, once SA is chosen, applying the degree heuris-
tic solves the problem without any false steps—you can choose any consistent color at each
choice point and still arrive at a solution with no backtracking. The minimum-remaining-
Section 6.3. Backtracking Search for CSPs 217
values heuristic is usually a more powerful guide, but the degree heuristic can be useful as a
tie-breaker.
Once a variable has been selected, the algorithm must decide on the order in which to
examine its values. For this, the least-constraining-value heuristic can be effective in some
LEAST -
CONSTRAINING-
VALUE
cases. It prefers the value that rules out the fewest choices for the neighboring variables in
the constraint graph. For example, suppose that in Figure 6.1 we have generated the partial
assignment with WA=red and NT =green and that our next choice is for Q. Blue would
be a bad choice because it eliminates the last legal value left for Q’s neighbor, SA.T h e
least-constraining-value heuristic therefore prefers red to blue. In general, the heuristic is
trying to leave the maximum ﬂexibility for subsequent variable assignments. Of course, if we
are trying to ﬁnd all the solutions to a problem, not just the ﬁrst one, then the ordering does
not matter because we have to consider every value anyway. The same holds if there are no
solutions to the problem.
Why should variable selection be fail-ﬁrst, but value selection be fail-last? It turns out
that, for a wide variety of problems, a variable ordering that chooses a variable with the
minimum number of remaining values helps minimize the number of nodes in the search tree
by pruning larger parts of the tree earlier. For value ordering, the trick is that we only need
one solution; therefore it makes sense to look for the most likely values ﬁrst. If we wanted to
enumerate all solutions rather than just ﬁnd one, then value ordering would be irrelevant.
6.3.2 Interleaving search and inference
So far we have seen how AC-3 and other algorithms can infer reductions in the domain of
variables before we begin the search. But inference can be even more powerful in the course
of a search: every time we make a choice of a value for a variable, we have a brand-new
opportunity to infer new domain reductions on the neighboring variables.
One of the simplest forms of inference is called forward checking. Whenever a vari-
FORWARD
CHECKING
able X is assigned, the forward-checking process establishes arc consistency for it: for each
unassigned variable Y that is connected to X by a constraint, delete from Y ’s domain any
value that is inconsistent with the value chosen for X. Because forward checking only does
arc consistency inferences, there is no reason to do forward checking if we have already done
arc consistency as a preprocessing step.
Figure 6.7 shows the progress of backtracking search on the Australia CSP with for-
ward checking. There are two important points to notice about this example. First, notice
that after WA=red and Q=green are assigned, the domains of NT and SA are reduced
to a single value; we have eliminated branching on these variables altogether by propagat-
ing information from WA and Q. A second point to notice is that after V =blue, the do-
main of SA is empty. Hence, forward checking has detected that the partial assignment
{WA=red,Q =green,V =blue} is inconsistent with the constraints of the problem, and
the algorithm will therefore backtrack immediately.
For many problems the search will be more effective if we combine the MRV heuris-
tic with forward checking. Consider Figure 6.7 after assigning {WA=red}. Intuitively, it
seems that that assignment constrains its neighbors, NT and SA, so we should handle those
218 Chapter 6. Constraint Satisfaction Problems
Initial domains
AfterWA=red
AfterQ=green
AfterV=blue
R G B
R
RB
R G B
R G B
B
R G B
R G B
R G B
R
R
R
R G B
B
B
G B
R G B
G
G
R G B
R G B
B
G B
R G B
R G B
R G B
R G B
WA T SAVNSWQNT
Figure 6.7 The progress of a map-coloring search with forward checking. WA=red
is assigned ﬁrst; then forward checking deletes red from the domains of the neighboring
variables NT and SA.A f t e rQ =green is assigned, green is deleted from the domains of
NT , SA,a n dNSW .A f t e rV =blue is assigned, blue is deleted from the domains of NSW
and SA, leaving SA with no legal values.
variables next, and then all the other variables will fall into place. That’s exactly what hap-
pens with MRV:NT and SA have two values, so one of them is chosen ﬁrst, then the other,
then Q, NSW ,a n dV in order. Finally T still has three values, and any one of them works.
We can view forward checking as an efﬁcient way to incrementally compute the information
that the MRV heuristic needs to do its job.
Although forward checking detects many inconsistencies, it does not detect all of them.
The problem is that it makes the current variable arc-consistent, but doesn’t look ahead and
make all the other variables arc-consistent. For example, consider the third row of Figure 6.7.
It shows that whenWAis red and Q is green, bothNT and SA are forced to be blue. Forward
checking does not look far enough ahead to notice that this is an inconsistency: NT and SA
are adjacent and so cannot have the same value.
The algorithm called MAC (for Maintaining Arc Consistency (MAC) ) detects this
MAINT AINING ARC
CONSISTENCY (MAC)
inconsistency. After a variable Xi is assigned a value, the INFERENCE procedure calls AC-3,
but instead of a queue of all arcs in the CSP, we start with only the arcs (Xj,Xi) for all
Xj that are unassigned variables that are neighbors of Xi. From there, AC-3 does constraint
propagation in the usual way, and if any variable has its domain reduced to the empty set, the
call to AC-3 fails and we know to backtrack immediately. We can see that MAC is strictly
more powerful than forward checking because forward checking does the same thing as MAC
on the initial arcs in MAC’s queue; but unlike MAC, forward checking does not recursively
propagate constraints when changes are made to the domains of variables.
6.3.3 Intelligent backtracking: Looking backward
The BACKTRACKING -SEARCH algorithm in Figure 6.5 has a very simple policy for what to
do when a branch of the search fails: back up to the preceding variable and try a different
value for it. This is called chronological backtracking because the most recent decision
CHRONOLOGICAL
BACKTRACKING
point is revisited. In this subsection, we consider better possibilities.
Consider what happens when we apply simple backtracking in Figure 6.1 with a ﬁxed
variable ordering Q, NSW , V , T , SA, WA, NT . Suppose we have generated the partial
assignment{Q=red,NSW =green,V =blue,T =red}. When we try the next variable,
SA, we see that every value violates a constraint. We back up to T and try a new color for
Section 6.3. Backtracking Search for CSPs 219
Tasmania! Obviously this is silly—recoloring Tasmania cannot possibly resolve the problem
with South Australia.
A more intelligent approach to backtracking is to backtrack to a variable that might ﬁx
the problem—a variable that was responsible for making one of the possible values of SA
impossible. To do this, we will keep track of a set of assignments that are in conﬂict with
some value for SA. The set (in this case{Q=red,NSW =green,V =blue,}), is called the
conﬂict set for SA.T h e backjumping method backtracks to the most recent assignment inCONFLICT SET
BACKJUMPING the conﬂict set; in this case, backjumping would jump over Tasmania and try a new value
for V . This method is easily implemented by a modiﬁcation to B ACKTRACK such that it
accumulates the conﬂict set while checking for a legal value to assign. If no legal value is
found, the algorithm should return the most recent element of the conﬂict set along with the
failure indicator.
The sharp-eyed reader will have noticed that forward checking can supply the conﬂict
set with no extra work: whenever forward checking based on an assignment X = x deletes a
value from Y ’s domain, it should add X = x to Y ’s conﬂict set. If the last value is deleted
from Y ’s domain, then the assignments in the conﬂict set of Y are added to the conﬂict set
of X. Then, when we get to Y , we know immediately where to backtrack if needed.
The eagle-eyed reader will have noticed something odd: backjumping occurs when
every value in a domain is in conﬂict with the current assignment; but forward checking
detects this event and prevents the search from ever reaching such a node! In fact, it can be
shown that every branch pruned by backjumping is also pruned by forward checking. Hence,
simple backjumping is redundant in a forward-checking search or, indeed, in a search that
uses stronger consistency checking, such as MAC.
Despite the observations of the preceding paragraph, the idea behind backjumping re-
mains a good one: to backtrack based on the reasons for failure. Backjumping notices failure
when a variable’s domain becomes empty, but in many cases a branch is doomed long before
this occurs. Consider again the partial assignment {WA=red,NSW =red} (which, from
our earlier discussion, is inconsistent). Suppose we try T =red next and then assign NT , Q,
V , SA. We know that no assignment can work for these last four variables, so eventually we
run out of values to try atNT . Now, the question is, where to backtrack? Backjumping cannot
work, because NT does have values consistent with the preceding assigned variables—NT
doesn’t have a complete conﬂict set of preceding variables that caused it to fail. We know,
however, that the four variablesNT , Q, V ,a n dSA, taken together, failed because of a set of
preceding variables, which must be those variables that directly conﬂict with the four. This
leads to a deeper notion of the conﬂict set for a variable such as NT : it is that set of preced-
ing variables that caused NT , together with any subsequent variables , to have no consistent
solution. In this case, the set is WA and NSW , so the algorithm should backtrack to NSW
and skip over Tasmania. A backjumping algorithm that uses conﬂict sets deﬁned in this way
is called conﬂict-directed backjumping.
CONFLICT -DIRECTED
BACKJUMPING
We must now explain how these new conﬂict sets are computed. The method is in
fact quite simple. The “terminal” failure of a branch of the search always occurs because a
variable’s domain becomes empty; that variable has a standard conﬂict set. In our example,
SA fails, and its conﬂict set is (say) {WA,NT,Q}. We backjump to Q,a n d Q absorbs
220 Chapter 6. Constraint Satisfaction Problems
the conﬂict set from SA (minus Q itself, of course) into its own direct conﬂict set, which is
{NT,NSW}; the new conﬂict set is {WA,NT,NSW}. That is, there is no solution from
Q onward, given the preceding assignment to {WA,NT,NSW}. Therefore, we backtrack
to NT, the most recent of these. NT absorbs {WA,NT,NSW}−{NT} into its own
direct conﬂict set {WA},g i v i n g{WA,NSW} (as stated in the previous paragraph). Now
the algorithm backjumps to NSW , as we would hope. To summarize: let Xj be the current
variable, and let conf(Xj) be its conﬂict set. If every possible value for Xj fails, backjump
to the most recent variable Xi in conf(Xj), and set
conf(Xi)←conf(Xi)∪conf(Xj)−{Xi} .
When we reach a contradiction, backjumping can tell us how far to back up, so we don’t
waste time changing variables that won’t ﬁx the problem. But we would also like to avoid
running into the same problem again. When the search arrives at a contradiction, we know
that some subset of the conﬂict set is responsible for the problem.Constraint learning is theCONSTRAINT
LEARNING
idea of ﬁnding a minimum set of variables from the conﬂict set that causes the problem. This
set of variables, along with their corresponding values, is called a no-good. We then recordNO-GOOD
the no-good, either by adding a new constraint to the CSP or by keeping a separate cache of
no-goods.
For example, consider the state {WA = red,NT = green,Q = blue} in the bottom
row of Figure 6.6. Forward checking can tell us this state is a no-good because there is no
valid assignment toSA. In this particular case, recording the no-good would not help, because
once we prune this branch from the search tree, we will never encounter this combination
again. But suppose that the search tree in Figure 6.6 were actually part of a larger search tree
that started by ﬁrst assigning values for V and T . Then it would be worthwhile to record
{WA = red,NT = green,Q = blue} as a no-good because we are going to run into the
same problem again for each possible set of assignments to V and T .
No-goods can be effectively used by forward checking or by backjumping. Constraint
learning is one of the most important techniques used by modern CSP solvers to achieve
efﬁciency on complex problems.
6.4 L OCAL SEARCH FOR CSP S
Local search algorithms (see Section 4.1) turn out to be effective in solving many CSPs. They
use a complete-state formulation: the initial state assigns a value to every variable, and the
search changes the value of one variable at a time. For example, in the 8-queens problem (see
Figure 4.3), the initial state might be a random conﬁguration of 8 queens in 8 columns, and
each step moves a single queen to a new position in its column. Typically, the initial guess
violates several constraints. The point of local search is to eliminate the violated constraints.
2
In choosing a new value for a variable, the most obvious heuristic is to select the value
that results in the minimum number of conﬂicts with other variables—the min-conﬂictsMIN-CONFLICTS
2 Local search can easily be extended to constraint optimization problems (COPs). In that case, all the techniques
for hill climbing and simulated annealing can be applied to optimize the objective function.
Section 6.4. Local Search for CSPs 221
function MIN-CONFLICTS (csp,max
 steps) returns a solution or failure
inputs: csp, a constraint satisfaction problem
max
 steps, the number of steps allowed before giving up
current←an initial complete assignment for csp
for i =1t o max
 steps do
if current is a solution for csp then return current
var←a randomly chosen conﬂicted variable from csp.VARIABLES
value←the value v for var that minimizes CONFLICTS (var,v,current,csp)
set var =value in current
return failure
Figure 6.8 The MIN-CONFLICTS algorithm for solving CSPs by local search. The initial
state may be chosen randomly or by a greedy assignment process that chooses a minimal-
conﬂict value for each variable in turn. The C ONFLICTS function counts the number of
constraints violated by a particular value, given the rest of the current assignment.
2
2
1
2
3
1
2
3
3
2
3
2
3
0
Figure 6.9 A two-step solution using min-conﬂicts for an 8-queens problem. At each
stage, a queen is chosen for reassignment in its column. The number of conﬂicts (in this
case, the number of attacking queens) is shown in each square. The algorithm moves the
queen to the min-conﬂicts square, breaking ties randomly.
heuristic. The algorithm is shown in Figure 6.8 and its application to an 8-queens problem is
diagrammed in Figure 6.9.
Min-conﬂicts is surprisingly effective for many CSPs. Amazingly, on the n-queens
problem, if you don’t count the initial placement of queens, the run time of min-conﬂicts is
roughly independent of problem size . It solves even the million-queens problem in an aver-
age of 50 steps (after the initial assignment). This remarkable observation was the stimulus
leading to a great deal of research in the 1990s on local search and the distinction between
easy and hard problems, which we take up in Chapter 7. Roughly speaking, n-queens is
easy for local search because solutions are densely distributed throughout the state space.
Min-conﬂicts also works well for hard problems. For example, it has been used to schedule
observations for the Hubble Space Telescope, reducing the time taken to schedule a week of
observations from three weeks (!) to around 10 minutes.
222 Chapter 6. Constraint Satisfaction Problems
All the local search techniques from Section 4.1 are candidates for application to CSPs,
and some of those have proved especially effective. The landscape of a CSP under the min-
conﬂicts heuristic usually has a series of plateaux. There may be millions of variable as-
signments that are only one conﬂict away from a solution. Plateau search—allowing side-
ways moves to another state with the same score—can help local search ﬁnd its way off this
plateau. This wandering on the plateau can be directed with tabu search: keeping a small
list of recently visited states and forbidding the algorithm to return to those states. Simulated
annealing can also be used to escape from plateaux.
Another technique, called constraint weighting, can help concentrate the search on the
CONSTRAINT
WEIGHTING
important constraints. Each constraint is given a numeric weight, Wi, initially all 1. At each
step of the search, the algorithm chooses a variable/value pair to change that will result in the
lowest total weight of all violated constraints. The weights are then adjusted by incrementing
the weight of each constraint that is violated by the current assignment. This has two beneﬁts:
it adds topography to plateaux, making sure that it is possible to improve from the current
state, and it also, over time, adds weight to the constraints that are proving difﬁcult to solve.
Another advantage of local search is that it can be used in an online setting when the
problem changes. This is particularly important in scheduling problems. A week’s airline
schedule may involve thousands of ﬂights and tens of thousands of personnel assignments,
but bad weather at one airport can render the schedule infeasible. We would like to repair the
schedule with a minimum number of changes. This can be easily done with a local search
algorithm starting from the current schedule. A backtracking search with the new set of
constraints usually requires much more time and might ﬁnd a solution with many changes
from the current schedule.
6.5 T HE STRUCTURE OF PROBLEMS
In this section, we examine ways in which the structure of the problem, as represented by
the constraint graph, can be used to ﬁnd solutions quickly. Most of the approaches here also
apply to other problems besides CSPs, such as probabilistic reasoning. After all, the only way
we can possibly hope to deal with the real world is to decompose it into many subproblems.
Looking again at the constraint graph for Australia (Figure 6.1(b), repeated as Figure 6.12(a)),
one fact stands out: Tasmania is not connected to the mainland.
3 Intuitively, it is obvious that
coloring Tasmania and coloring the mainland are independent subproblems—any solutionINDEPENDENT
SUBPROBLEMS
for the mainland combined with any solution for Tasmania yields a solution for the whole
map. Independence can be ascertained simply by ﬁnding connected components of theCONNECTED
COMPONENT
constraint graph. Each component corresponds to a subproblem CSPi. If assignment Si is
a solution of CSPi,t h e n⋃
i Si is a solution of ⋃
i CSPi. Why is this important? Consider
the following: suppose each CSPi has c variables from the total of n variables, where c is
a constant. Then there are n/c subproblems, each of which takes at most dc work to solve,
3 A careful cartographer or patriotic Tasmanian might object that Tasmania should not be colored the same as
its nearest mainland neighbor, to avoid the impression that it might be part of that state.
Section 6.5. The Structure of Problems 223
where d is the size of the domain. Hence, the total work is O(dcn/c),w h i c hi slinear in n;
without the decomposition, the total work is O(dn), which is exponential in n.L e t ’ s m a k e
this more concrete: dividing a Boolean CSP with 80 variables into four subproblems reduces
the worst-case solution time from the lifetime of the universe down to less than a second.
Completely independent subproblems are delicious, then, but rare. Fortunately, some
other graph structures are also easy to solve. For example, a constraint graph is a tree when
any two variables are connected by only one path. We show thatany tree-structured CSP can
be solved in time linear in the number of variables. 4 The key is a new notion of consistency,
called directed arc consistency or DAC. A CSP is deﬁned to be directed arc-consistent underDIRECTED ARC
CONSISTENCY
an ordering of variables X1,X2,...,X n if and only if every Xi is arc-consistent with each
Xj for j>i .
To solve a tree-structured CSP, ﬁrst pick any variable to be the root of the tree, and
choose an ordering of the variables such that each variable appears after its parent in the tree.
Such an ordering is called a topological sort. Figure 6.10(a) shows a sample tree and (b)
TOPOLOGICAL SORT
shows one possible ordering. Any tree withn nodes has n−1 arcs, so we can make this graph
directed arc-consistent in O(n) steps, each of which must compare up to d possible domain
values for two variables, for a total time of O(nd2). Once we have a directed arc-consistent
graph, we can just march down the list of variables and choose any remaining value. Since
each link from a parent to its child is arc consistent, we know that for any value we choose for
the parent, there will be a valid value left to choose for the child. That means we won’t have
to backtrack; we can move linearly through the variables. The complete algorithm is shown
in Figure 6.11.
A
C
B D
E
F
(a)
AC BD E F
(b)
Figure 6.10 (a) The constraint graph of a tree-structured CSP. (b) A linear ordering of the
variables consistent with the tree with A as the root. This is known as a topological sort of
the variables.
Now that we have an efﬁcient algorithm for trees, we can consider whether more general
constraint graphs can be reduced to trees somehow. There are two primary ways to do this,
one based on removing nodes and one based on collapsing nodes together.
The ﬁrst approach involves assigning values to some variables so that the remaining
variables form a tree. Consider the constraint graph for Australia, shown again in Fig-
ure 6.12(a). If we could delete South Australia, the graph would become a tree, as in (b).
Fortunately, we can do this (in the graph, not the continent) by ﬁxing a value for SA and
4 Sadly, very few regions of the world have tree-structured maps, although Sulawesi comes close.
224 Chapter 6. Constraint Satisfaction Problems
function TREE -CSP-S OLVER (csp) returns a solution, or failure
inputs: csp, a CSP with components X, D, C
n←number of variables in X
assignment←an empty assignment
root←any variable in X
X←TOPOLOGICAL SORT(X ,root)
for j = n down to 2 do
MAKE -ARC-CONSISTENT (PARENT (Xj),Xj )
if it cannot be made consistent then return failure
for i =1 to n do
assignment[Xi]←any consistent value from Di
if there is no consistent value then return failure
return assignment
Figure 6.11 The TREE -CSP-S OLVER algorithm for solving tree-structured CSPs. If the
CSP has a solution, we will ﬁnd it in linear time; if not, we will detect a contradiction.
WA
NT
SA
Q
NSW
V
T
WA
NT
Q
NSW
V
T
(a) (b)
Figure 6.12 (a) The original constraint graph from Figure 6.1. (b) The constraint graph
after the removal of SA.
deleting from the domains of the other variables any values that are inconsistent with the
value chosen for SA.
Now, any solution for the CSP after SA and its constraints are removed will be con-
sistent with the value chosen for SA. (This works for binary CSPs; the situation is more
complicated with higher-order constraints.) Therefore, we can solve the remaining tree with
the algorithm given above and thus solve the whole problem. Of course, in the general case
(as opposed to map coloring), the value chosen for SA could be the wrong one, so we would
need to try each possible value. The general algorithm is as follows:
Section 6.5. The Structure of Problems 225
1. Choose a subset S of the CSP’s variables such that the constraint graph becomes a tree
after removal of S. S is called a cycle cutset.CYCLE CUTSET
2. For each possible assignment to the variables in S that satisﬁes all constraints on S,
(a) remove from the domains of the remaining variables any values that are inconsis-
tent with the assignment for S,a n d
(b) If the remaining CSP has a solution, return it together with the assignment for S.
If the cycle cutset has size c, then the total run time is O(dc· (n−c)d2): we have to try each
of the dc combinations of values for the variables in S, and for each combination we must
solve a tree problem of size n−c. If the graph is “nearly a tree,” then c will be small and the
savings over straight backtracking will be huge. In the worst case, however,c can be as large
as (n−2). Finding the smallest cycle cutset is NP-hard, but several efﬁcient approximation
algorithms are known. The overall algorithmic approach is called cutset conditioning ;i tCUTSET
CONDITIONING
comes up again in Chapter 14, where it is used for reasoning about probabilities.
The second approach is based on constructing a tree decomposition of the constraintTREE
DECOMPOSITION
graph into a set of connected subproblems. Each subproblem is solved independently, and the
resulting solutions are then combined. Like most divide-and-conquer algorithms, this works
well if no subproblem is too large. Figure 6.13 shows a tree decomposition of the map-
coloring problem into ﬁve subproblems. A tree decomposition must satisfy the following
three requirements:
•Every variable in the original problem appears in at least one of the subproblems.
•If two variables are connected by a constraint in the original problem, they must appear
together (along with the constraint) in at least one of the subproblems.
•If a variable appears in two subproblems in the tree, it must appear in every subproblem
along the path connecting those subproblems.
The ﬁrst two conditions ensure that all the variables and constraints are represented in the
decomposition. The third condition seems rather technical, but simply reﬂects the constraint
that any given variable must have the same value in every subproblem in which it appears;
the links joining subproblems in the tree enforce this constraint. For example, SA appears in
all four of the connected subproblems in Figure 6.13. You can verify from Figure 6.12 that
this decomposition makes sense.
We solve each subproblem independently; if any one has no solution, we know the en-
tire problem has no solution. If we can solve all the subproblems, then we attempt to construct
a global solution as follows. First, we view each subproblem as a “mega-variable” whose do-
main is the set of all solutions for the subproblem. For example, the leftmost subproblem in
Figure 6.13 is a map-coloring problem with three variables and hence has six solutions—one
is{WA = red,SA = blue,NT = green}. Then, we solve the constraints connecting the
subproblems, using the efﬁcient algorithm for trees given earlier. The constraints between
subproblems simply insist that the subproblem solutions agree on their shared variables. For
example, given the solution{WA = red,SA = blue,NT = green} for the ﬁrst subproblem,
the only consistent solution for the next subproblem is{SA = blue,NT = green,Q = red}.
A given constraint graph admits many tree decompositions; in choosing a decompo-
sition, the aim is to make the subproblems as small as possible. The tree width of a tree
TREE WIDTH
226 Chapter 6. Constraint Satisfaction Problems
T
WA
NT
SA
NT
SA
Q
SA
Q
NSW
SA NSW
V
Figure 6.13 A tree decomposition of the constraint graph in Figure 6.12(a).
decomposition of a graph is one less than the size of the largest subproblem; the tree width
of the graph itself is deﬁned to be the minimum tree width among all its tree decompositions.
If a graph has tree width w and we are given the corresponding tree decomposition, then the
problem can be solved in O(nd
w+1) time. Hence, CSPs with constraint graphs of bounded
tree width are solvable in polynomial time. Unfortunately, ﬁnding the decomposition with
minimal tree width is NP-hard, but there are heuristic methods that work well in practice.
So far, we have looked at the structure of the constraint graph. There can be important
structure in the values of variables as well. Consider the map-coloring problem withn colors.
For every consistent solution, there is actually a set of n! solutions formed by permuting the
color names. For example, on the Australia map we know thatWA,NT ,a n dSA must all have
different colors, but there are 3! = 6 ways to assign the three colors to these three regions.
This is called value symmetry . We would like to reduce the search space by a factor ofVALUE SYMMETRY
n! by breaking the symmetry. We do this by introducing a symmetry-breaking constraint.
SYMMETRY -
BREAKING
CONSTRAINT
For our example, we might impose an arbitrary ordering constraint, NT < SA < WA,t h a t
requires the three values to be in alphabetical order. This constraint ensures that only one of
the n! solutions is possible:{NT = blue,SA = green,WA = red}.
For map coloring, it was easy to ﬁnd a constraint that eliminates the symmetry, and
in general it is possible to ﬁnd constraints that eliminate all but one symmetric solution in
polynomial time, but it is NP-hard to eliminate all symmetry among intermediate sets of
values during search. In practice, breaking value symmetry has proved to be important and
effective on a wide range of problems.
Section 6.6. Summary 227
6.6 S UMMARY
•Constraint satisfaction problems (CSPs) represent a state with a set of variable/value
pairs and represent the conditions for a solution by a set of constraints on the variables.
Many important real-world problems can be described as CSPs.
•A number of inference techniques use the constraints to infer which variable/value pairs
are consistent and which are not. These include node, arc, path, and k-consistency.
•Backtracking search, a form of depth-ﬁrst search, is commonly used for solving CSPs.
Inference can be interwoven with search.
•The minimum-remaining-values and degree heuristics are domain-independent meth-
ods for deciding which variable to choose next in a backtracking search. The least-
constraining-value heuristic helps in deciding which value to try ﬁrst for a given
variable. Backtracking occurs when no legal assignment can be found for a variable.
Conﬂict-directed backjumping backtracks directly to the source of the problem.
•Local search using the min-conﬂicts heuristic has also been applied to constraint satis-
faction problems with great success.
•The complexity of solving a CSP is strongly related to the structure of its constraint
graph. Tree-structured problems can be solved in linear time. Cutset conditioning can
reduce a general CSP to a tree-structured one and is quite efﬁcient if a small cutset can
be found. Tree decompositiontechniques transform the CSP into a tree of subproblems
and are efﬁcient if the tree width of the constraint graph is small.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
The earliest work related to constraint satisfaction dealt largely with numerical constraints.
Equational constraints with integer domains were studied by the Indian mathematician Brah-
magupta in the seventh century; they are often calledDiophantine equations, after the GreekDIOPHANTINE
EQUA TIONS
mathematician Diophantus (c. 200–284), who actually considered the domain of positive ra-
tionals. Systematic methods for solving linear equations by variable elimination were studied
by Gauss (1829); the solution of linear inequality constraints goes back to Fourier (1827).
Finite-domain constraint satisfaction problems also have a long history. For example,
graph coloring (of which map coloring is a special case) is an old problem in mathematics.
GRAPH COLORING
The four-color conjecture (that every planar graph can be colored with four or fewer colors)
was ﬁrst made by Francis Guthrie, a student of De Morgan, in 1852. It resisted solution—
despite several published claims to the contrary—until a proof was devised by Appel and
Haken (1977) (see the book F our Colors Sufﬁce(Wilson, 2004)). Purists were disappointed
that part of the proof relied on a computer, so Georges Gonthier (2008), using the C
OQ
theorem prover, derived a formal proof that Appel and Haken’s proof was correct.
Speciﬁc classes of constraint satisfaction problems occur throughout the history of
computer science. One of the most inﬂuential early examples was the S KETCHPAD sys-
228 Chapter 6. Constraint Satisfaction Problems
tem (Sutherland, 1963), which solved geometric constraints in diagrams and was the fore-
runner of modern drawing programs and CAD tools. The identiﬁcation of CSPs as a general
class is due to Ugo Montanari (1974). The reduction of higher-order CSPs to purely binary
CSPs with auxiliary variables (see Exercise 6.6) is due originally to the 19th-century logician
Charles Sanders Peirce. It was introduced into the CSP literature by Dechter (1990b) and
was elaborated by Bacchus and van Beek (1998). CSPs with preferences among solutions are
studied widely in the optimization literature; see Bistarelli et al. (1997) for a generalization
of the CSP framework to allow for preferences. The bucket-elimination algorithm (Dechter,
1999) can also be applied to optimization problems.
Constraint propagation methods were popularized by Waltz’s (1975) success on poly-
hedral line-labeling problems for computer vision. Waltz showed that, in many problems,
propagation completely eliminates the need for backtracking. Montanari (1974) introduced
the notion of constraint networks and propagation by path consistency. Alan Mackworth
(1977) proposed the AC-3 algorithm for enforcing arc consistency as well as the general idea
of combining backtracking with some degree of consistency enforcement. AC-4, a more
efﬁcient arc-consistency algorithm, was developed by Mohr and Henderson (1986). Soon af-
ter Mackworth’s paper appeared, researchers began experimenting with the tradeoff between
the cost of consistency enforcement and the beneﬁts in terms of search reduction. Haralick
and Elliot (1980) favored the minimal forward-checking algorithm described by McGregor
(1979), whereas Gaschnig (1979) suggested full arc-consistency checking after each vari-
able assignment—an algorithm later called MAC by Sabin and Freuder (1994). The latter
paper provides somewhat convincing evidence that, on harder CSPs, full arc-consistency
checking pays off. Freuder (1978, 1982) investigated the notion of k-consistency and its
relationship to the complexity of solving CSPs. Apt (1999) describes a generic algorithmic
framework within which consistency propagation algorithms can be analyzed, and Bessi`ere
(2006) presents a current survey.
Special methods for handling higher-order or global constraints were developed ﬁrst
within the context of constraint logic programming . Marriott and Stuckey (1998) provide
excellent coverage of research in this area. The Alldiﬀ constraint was studied by Regin
(1994), Stergiou and Walsh (1999), and van Hoeve (2001). Bounds constraints were incorpo-
rated into constraint logic programming by Van Hentenryck et al. (1998). A survey of global
constraints is provided by van Hoeve and Katriel (2006).
Sudoku has become the most widely known CSP and was described as such by Simonis
(2005). Agerbeck and Hansen (2008) describe some of the strategies and show that Sudoku
on an n
2 × n2 board is in the class of NP-hard problems. Reeson et al. (2007) show an
interactive solver based on CSP techniques.
The idea of backtracking search goes back to Golomb and Baumert (1965), and its
application to constraint satisfaction is due to Bitner and Reingold (1975), although they trace
the basic algorithm back to the 19th century. Bitner and Reingold also introduced the MRV
heuristic, which they called the most-constrained-variable heuristic. Brelaz (1979) used the
degree heuristic as a tiebreaker after applying the MRV heuristic. The resulting algorithm,
despite its simplicity, is still the best method for k-coloring arbitrary graphs. Haralick and
Elliot (1980) proposed the least-constraining-value heuristic.
Bibliographical and Historical Notes 229
The basic backjumping method is due to John Gaschnig (1977, 1979). Kondrak and
van Beek (1997) showed that this algorithm is essentially subsumed by forward checking.
Conﬂict-directed backjumping was devised by Prosser (1993). The most general and pow-
erful form of intelligent backtracking was actually developed very early on by Stallman and
Sussman (1977). Their technique of dependency-directed backtracking led to the develop-
DEPENDENCY -
DIRECTED
BACKTRACKING
ment of truth maintenance systems (Doyle, 1979), which we discuss in Section 12.6.2. The
connection between the two areas is analyzed by de Kleer (1989).
The work of Stallman and Sussman also introduced the idea of constraint learning,
in which partial results obtained by search can be saved and reused later in the search. The
idea was formalized Dechter (1990a). Backmarking (Gaschnig, 1979) is a particularly sim-
BACKMARKING
ple method in which consistent and inconsistent pairwise assignments are saved and used
to avoid rechecking constraints. Backmarking can be combined with conﬂict-directed back-
jumping; Kondrak and van Beek (1997) present a hybrid algorithm that provably subsumes
either method taken separately. The method of dynamic backtracking (Ginsberg, 1993) re-
DYNAMIC
BACKTRACKING
tains successful partial assignments from later subsets of variables when backtracking over
an earlier choice that does not invalidate the later success.
Empirical studies of several randomized backtracking methods were done by Gomes
et al. (2000) and Gomes and Selman (2001). Van Beek (2006) surveys backtracking.
Local search in constraint satisfaction problems was popularized by the work of Kirk-
patrick et al. (1983) on simulated annealing (see Chapter 4), which is widely used for schedul-
ing problems. The min-conﬂicts heuristic was ﬁrst proposed by Gu (1989) and was developed
independently by Minton et al. (1992). Sosic and Gu (1994) showed how it could be applied
to solve the 3,000,000 queens problem in less than a minute. The astounding success of
local search using min-conﬂicts on the n-queens problem led to a reappraisal of the nature
and prevalence of “easy” and “hard” problems. Peter Cheeseman et al. (1991) explored the
difﬁculty of randomly generated CSPs and discovered that almost all such problems either
are trivially easy or have no solutions. Only if the parameters of the problem generator are
set in a certain narrow range, within which roughly half of the problems are solvable, do we
ﬁnd “hard” problem instances. We discuss this phenomenon further in Chapter 7. Konolige
(1994) showed that local search is inferior to backtracking search on problems with a certain
degree of local structure; this led to work that combined local search and inference, such as
that by Pinkas and Dechter (1995). Hoos and Tsang (2006) survey local search techniques.
Work relating the structure and complexity of CSPs originates with Freuder (1985), who
showed that search on arc consistent trees works without any backtracking. A similar result,
with extensions to acyclic hypergraphs, was developed in the database community (Beeri
et al. , 1983). Bayardo and Miranker (1994) present an algorithm for tree-structured CSPs
that runs in linear time without any preprocessing.
Since those papers were published, there has been a great deal of progress in developing
more general results relating the complexity of solving a CSP to the structure of its constraint
graph. The notion of tree width was introduced by the graph theorists Robertson and Seymour
(1986). Dechter and Pearl (1987, 1989), building on the work of Freuder, applied a related
notion (which they called induced width) to constraint satisfaction problems and developed
the tree decomposition approach sketched in Section 6.5. Drawing on this work and on results
230 Chapter 6. Constraint Satisfaction Problems
from database theory, Gottlobet al. (1999a, 1999b) developed a notion,hypertree width,t h a t
is based on the characterization of the CSP as a hypergraph. In addition to showing that any
CSP with hypertree width w can be solved in time O(n
w+1 log n), they also showed that
hypertree width subsumes all previously deﬁned measures of “width” in the sense that there
are cases where the hypertree width is bounded and the other measures are unbounded.
Interest in look-back approaches to backtracking was rekindled by the work of Bayardo
and Schrag (1997), whose R
ELSAT algorithm combined constraint learning and backjumping
and was shown to outperform many other algorithms of the time. This led to AND/OR
search algorithms applicable to both CSPs and probabilistic reasoning (Dechter and Ma-
teescu, 2007). Brown et al. (1988) introduce the idea of symmetry breaking in CSPs, and
Gent et al. (2006) give a recent survey.
The ﬁeld of distributed constraint satisfaction looks at solving CSPs when there is a
DISTRIBUTED
CONSTRAINT
SA TISFACTION
collection of agents, each of which controls a subset of the constraint variables. There have
been annual workshops on this problem since 2000, and good coverage elsewhere (Collin
et al., 1999; Pearce et al., 2008; Shoham and Leyton-Brown, 2009).
Comparing CSP algorithms is mostly an empirical science: few theoretical results show
that one algorithm dominates another on all problems; instead, we need to run experiments
to see which algorithms perform better on typical instances of problems. As Hooker (1995)
points out, we need to be careful to distinguish between competitive testing—as occurs in
competitions among algorithms based on run time—and scientiﬁc testing, whose goal is to
identify the properties of an algorithm that determine its efﬁcacy on a class of problems.
The recent textbooks by Apt (2003) and Dechter (2003), and the collection by Rossi
et al. (2006) are excellent resources on constraint processing. There are several good earlier
surveys, including those by Kumar (1992), Dechter and Frost (2002), and Bartak (2001); and
the encyclopedia articles by Dechter (1992) and Mackworth (1992). Pearson and Jeavons
(1997) survey tractable classes of CSPs, covering both structural decomposition methods
and methods that rely on properties of the domains or constraints themselves. Kondrak and
van Beek (1997) give an analytical survey of backtracking search algorithms, and Bacchus
and van Run (1995) give a more empirical survey. Constraint programming is covered in the
books by Apt (2003) and Fruhwirth and Abdennadher (2003). Several interesting applications
are described in the collection edited by Freuder and Mackworth (1994). Papers on constraint
satisfaction appear regularly inArtiﬁcial Intelligence and in the specialist journalConstraints.
The primary conference venue is the International Conference on Principles and Practice of
Constraint Programming, often called CP.
EXERCISES
6.1 How many solutions are there for the map-coloring problem in Figure 6.1? How many
solutions if four colors are allowed? Two colors?
6.2 Consider the problem of placing k knights on an n× n chessboard such that no two
knights are attacking each other, where k is given and k≤n2.
Exercises 231
a. Choose a CSP formulation. In your formulation, what are the variables?
b. What are the possible values of each variable?
c. What sets of variables are constrained, and how?
d. Now consider the problem of putting as many knights as possible on the board with-
out any attacks. Explain how to solve this with local search by deﬁning appropriate
ACTIONS and RESULT functions and a sensible objective function.
6.3 Consider the problem of constructing (not solving) crossword puzzles: 5 ﬁtting words
into a rectangular grid. The grid, which is given as part of the problem, speciﬁes which
squares are blank and which are shaded. Assume that a list of words (i.e., a dictionary)
is provided and that the task is to ﬁll in the blank squares by using any subset of the list.
Formulate this problem precisely in two ways:
a. As a general search problem. Choose an appropriate search algorithm and specify a
heuristic function. Is it better to ﬁll in blanks one letter at a time or one word at a time?
b. As a constraint satisfaction problem. Should the variables be words or letters?
Which formulation do you think will be better? Why?
6.4 Give precise formulations for each of the following as constraint satisfaction problems:
a. Rectilinear ﬂoor-planning: ﬁnd non-overlapping places in a large rectangle for a number
of smaller rectangles.
b. Class scheduling: There is a ﬁxed number of professors and classrooms, a list of classes
to be offered, and a list of possible time slots for classes. Each professor has a set of
classes that he or she can teach.
c. Hamiltonian tour: given a network of cities connected by roads, choose an order to visit
all cities in a country without repeating any.
6.5 Solve the cryptarithmetic problem in Figure 6.2 by hand, using the strategy of back-
tracking with forward checking and the MRV and least-constraining-value heuristics.
6.6 Show how a single ternary constraint such as “ A + B = C” can be turned into three
binary constraints by using an auxiliary variable. You may assume ﬁnite domains. ( Hint:
Consider a new variable that takes on values that are pairs of other values, and consider
constraints such as “X is the ﬁrst element of the pair Y .”) Next, show how constraints with
more than three variables can be treated similarly. Finally, show how unary constraints can be
eliminated by altering the domains of variables. This completes the demonstration that any
CSP can be transformed into a CSP with only binary constraints.
6.7 Consider the following logic puzzle: In ﬁve houses, each with a different color, live ﬁve
persons of different nationalities, each of whom prefers a different brand of candy, a different
drink, and a different pet. Given the following facts, the questions to answer are “Where does
the zebra live, and in which house do they drink water?”
5 Ginsberg et al. (1990) discuss several methods for constructing crossword puzzles. Littman et al. (1999) tackle
the harder problem of solving them.
232 Chapter 6. Constraint Satisfaction Problems
The Englishman lives in the red house.
The Spaniard owns the dog.
The Norwegian lives in the ﬁrst house on the left.
The green house is immediately to the right of the ivory house.
The man who eats Hershey bars lives in the house next to the man with the fox.
Kit Kats are eaten in the yellow house.
The Norwegian lives next to the blue house.
The Smarties eater owns snails.
The Snickers eater drinks orange juice.
The Ukrainian drinks tea.
The Japanese eats Milky Ways.
Kit Kats are eaten in a house next to the house where the horse is kept.
Coffee is drunk in the green house.
Milk is drunk in the middle house.
Discuss different representations of this problem as a CSP. Why would one prefer one repre-
sentation over another?
6.8 Consider the graph with 8 nodes A
1, A2, A3, A4, H, T , F1, F2. Ai is connected to
Ai+1 for all i, each Ai is connected to H, H is connected to T ,a n dT is connected to each
Fi. Find a 3-coloring of this graph by hand using the following strategy: backtracking with
conﬂict-directed backjumping, the variable order A1, H, A4, F1, A2, F2, A3, T ,a n dt h e
value order R, G, B.
6.9 Explain why it is a good heuristic to choose the variable that ismost constrained but the
value that is least constraining in a CSP search.
6.10 Generate random instances of map-coloring problems as follows: scatter n points on
the unit square; select a point X at random, connect X by a straight line to the nearest point
Y such that X is not already connected to Y and the line crosses no other line; repeat the
previous step until no more connections are possible. The points represent regions on the
map and the lines connect neighbors. Now try to ﬁnd k-colorings of each map, for both
k =3 and k =4 , using min-conﬂicts, backtracking, backtracking with forward checking, and
backtracking with MAC. Construct a table of average run times for each algorithm for values
of n up to the largest you can manage. Comment on your results.
6.11 Use the AC-3 algorithm to show that arc consistency can detect the inconsistency of
the partial assignment{WA=green,V =red} for the problem shown in Figure 6.1.
6.12 What is the worst-case complexity of running AC-3 on a tree-structured CSP?
6.13 AC-3 puts back on the queue every arc (X
k,Xi) whenever any value is deleted from
the domain of Xi, even if each value ofXk is consistent with several remaining values ofXi.
Suppose that, for every arc (Xk,Xi), we keep track of the number of remaining values of Xi
that are consistent with each value of Xk. Explain how to update these numbers efﬁciently
and hence show that arc consistency can be enforced in total time O(n2d2).
Exercises 233
6.14 The TREE -CSP-S OLVER (Figure 6.10) makes arcs consistent starting at the leaves and
working backwards towards the root. Why does it do that? What would happen if it went in
the opposite direction?
6.15 We introduced Sudoku as a CSP to be solved by search over partial assignments be-
cause that is the way people generally undertake solving Sudoku problems. It is also possible,
of course, to attack these problems with local search over complete assignments. How well
would a local solver using the min-conﬂicts heuristic do on Sudoku problems?
6.16 Deﬁne in your own words the terms constraint, backtracking search, arc consistency,
backjumping, min-conﬂicts, and cycle cutset.
6.17 Suppose that a graph is known to have a cycle cutset of no more thank nodes. Describe
a simple algorithm for ﬁnding a minimal cycle cutset whose run time is not much more than
O(nk) for a CSP withn variables. Search the literature for methods for ﬁnding approximately
minimal cycle cutsets in time that is polynomial in the size of the cutset. Does the existence
of such algorithms make the cycle cutset method practical?


--- BOOK CHAPTER: 7_Logical_Agents ---

7 LOGICAL AGENTS
In which we design agents that can form representations of a complex world, use a
process of inference to derive new representations about the world, and use these
new representations to deduce what to do.
Humans, it seems, know things; and what they know helps them do things. These are
not empty statements. They make strong claims about how the intelligence of humans is
achieved—not by purely reﬂex mechanisms but by processes of reasoning that operate on
REASONING
internal representations of knowledge. In AI, this approach to intelligence is embodied inREPRESENTA TION
knowledge-based agents.KNOWLEDGE-BASED
AGENTS
The problem-solving agents of Chapters 3 and 4 know things, but only in a very limited,
inﬂexible sense. For example, the transition model for the 8-puzzle—knowledge of what the
actions do—is hidden inside the domain-speciﬁc code of the R
ESULT function. It can be
used to predict the outcome of actions but not to deduce that two tiles cannot occupy the
same space or that states with odd parity cannot be reached from states with even parity. The
atomic representations used by problem-solving agents are also very limiting. In a partially
observable environment, an agent’s only choice for representing what it knows about the
current state is to list all possible concrete states—a hopeless prospect in large environments.
Chapter 6 introduced the idea of representing states as assignments of values to vari-
ables; this is a step in the right direction, enabling some parts of the agent to work in a
domain-independent way and allowing for more efﬁcient algorithms. In this chapter and
those that follow, we take this step to its logical conclusion, so to speak—we develop logic
LOGIC
as a general class of representations to support knowledge-based agents. Such agents can
combine and recombine information to suit myriad purposes. Often, this process can be quite
far removed from the needs of the moment—as when a mathematician proves a theorem or
an astronomer calculates the earth’s life expectancy. Knowledge-based agents can accept new
tasks in the form of explicitly described goals; they can achieve competence quickly by being
told or learning new knowledge about the environment; and they can adapt to changes in the
environment by updating the relevant knowledge.
We begin in Section 7.1 with the overall agent design. Section 7.2 introduces a sim-
ple new environment, the wumpus world, and illustrates the operation of a knowledge-based
agent without going into any technical detail. Then we explain the general principles oflogic
234
Section 7.1. Knowledge-Based Agents 235
in Section 7.3 and the speciﬁcs of propositional logic in Section 7.4. While less expressive
than ﬁrst-order logic (Chapter 8), propositional logic illustrates all the basic concepts of
logic; it also comes with well-developed inference technologies, which we describe in sec-
tions 7.5 and 7.6. Finally, Section 7.7 combines the concept of knowledge-based agents with
the technology of propositional logic to build some simple agents for the wumpus world.
7.1 K NOWLEDGE -BASED AGENTS
The central component of a knowledge-based agent is its knowledge base, or KB. A knowl-KNOWLEDGE BASE
edge base is a set of sentences. (Here “sentence” is used as a technical term. It is relatedSENTENCE
but not identical to the sentences of English and other natural languages.) Each sentence is
expressed in a language called a knowledge representation language and represents some
KNOWLEDGE
REPRESENTA TION
LANGUAGE
assertion about the world. Sometimes we dignify a sentence with the name axiom, when theAXIOM
sentence is taken as given without being derived from other sentences.
There must be a way to add new sentences to the knowledge base and a way to query
what is known. The standard names for these operations are T ELL and A SK, respectively.
Both operations may involve inference—that is, deriving new sentences from old. InferenceINFERENCE
must obey the requirement that when one ASKs a question of the knowledge base, the answer
should follow from what has been told (or T ELL ed) to the knowledge base previously. Later
in this chapter, we will be more precise about the crucial word “follow.” For now, take it to
mean that the inference process should not make things up as it goes along.
Figure 7.1 shows the outline of a knowledge-based agent program. Like all our agents,
it takes a percept as input and returns an action. The agent maintains a knowledge base, KB,
which may initially contain some background knowledge.
BACKGROUND
KNOWLEDGE
Each time the agent program is called, it does three things. First, it T ELL s the knowl-
edge base what it perceives. Second, it A SKs the knowledge base what action it should
perform. In the process of answering this query, extensive reasoning may be done about
the current state of the world, about the outcomes of possible action sequences, and so on.
Third, the agent program T
ELL s the knowledge base which action was chosen, and the agent
executes the action.
The details of the representation language are hidden inside three functions that imple-
ment the interface between the sensors and actuators on one side and the core representation
and reasoning system on the other. M
AKE -PERCEPT -SENTENCE constructs a sentence as-
serting that the agent perceived the given percept at the given time. MAKE -ACTION -QUERY
constructs a sentence that asks what action should be done at the current time. Finally,
MAKE -ACTION -SENTENCE constructs a sentence asserting that the chosen action was ex-
ecuted. The details of the inference mechanisms are hidden inside T ELL and A SK. Later
sections will reveal these details.
The agent in Figure 7.1 appears quite similar to the agents with internal state described
in Chapter 2. Because of the deﬁnitions of T ELL and A SK, however, the knowledge-based
agent is not an arbitrary program for calculating actions. It is amenable to a description at
236 Chapter 7. Logical Agents
function KB-A GENT (percept) returns an action
persistent: KB, a knowledge base
t, a counter, initially 0, indicating time
TELL (KB,M AKE -PERCEPT -SENTENCE (percept,t))
action←ASK(KB,M AKE -ACTION -QUERY (t))
TELL (KB,M AKE -ACTION -SENTENCE (action,t))
t←t +1
return action
Figure 7.1 A generic knowledge-based agent. Given a percept, the agent adds the percept
to its knowledge base, asks the knowledge base for the best action, and tells the knowledge
base that it has in fact taken that action.
the knowledge level, where we need specify only what the agent knows and what its goalsKNOWLEDGE LEVEL
are, in order to ﬁx its behavior. For example, an automated taxi might have the goal of
taking a passenger from San Francisco to Marin County and might know that the Golden
Gate Bridge is the only link between the two locations. Then we can expect it to cross the
Golden Gate Bridge because it knows that that will achieve its goal . Notice that this analysis
is independent of how the taxi works at the implementation level. It doesn’t matter whether
IMPLEMENTA TION
LEVEL
its geographical knowledge is implemented as linked lists or pixel maps, or whether it reasons
by manipulating strings of symbols stored in registers or by propagating noisy signals in a
network of neurons.
A knowledge-based agent can be built simply by T
ELL ing it what it needs to know.
Starting with an empty knowledge base, the agent designer can T ELL sentences one by one
until the agent knows how to operate in its environment. This is called the declarative ap-DECLARA TIVE
proach to system building. In contrast, the procedural approach encodes desired behaviors
directly as program code. In the 1970s and 1980s, advocates of the two approaches engaged
in heated debates. We now understand that a successful agent often combines both declarative
and procedural elements in its design, and that declarative knowledge can often be compiled
into more efﬁcient procedural code.
We can also provide a knowledge-based agent with mechanisms that allow it to learn
for itself. These mechanisms, which are discussed in Chapter 18, create general knowledge
about the environment from a series of percepts. A learning agent can be fully autonomous.
7.2 T HE WUMPUS WORLD
In this section we describe an environment in which knowledge-based agents can show their
worth. The wumpus world is a cave consisting of rooms connected by passageways. Lurking
WUMPUS WORLD
somewhere in the cave is the terrible wumpus, a beast that eats anyone who enters its room.
The wumpus can be shot by an agent, but the agent has only one arrow. Some rooms contain
Section 7.2. The Wumpus World 237
bottomless pits that will trap anyone who wanders into these rooms (except for the wumpus,
which is too big to fall in). The only mitigating feature of this bleak environment is the
possibility of ﬁnding a heap of gold. Although the wumpus world is rather tame by modern
computer game standards, it illustrates some important points about intelligence.
A sample wumpus world is shown in Figure 7.2. The precise deﬁnition of the task
environment is given, as suggested in Section 2.3, by the PEAS description:
•Performance measure: +1000 for climbing out of the cave with the gold, –1000 for
falling into a pit or being eaten by the wumpus, –1 for each action taken and –10 for
using up the arrow. The game ends either when the agent dies or when the agent climbs
out of the cave.
•Environment:A 4× 4 grid of rooms. The agent always starts in the square labeled
[1,1], facing to the right. The locations of the gold and the wumpus are chosen ran-
domly, with a uniform distribution, from the squares other than the start square. In
addition, each square other than the start can be a pit, with probability 0.2.
•Actuators: The agent can move Fo r w a rd, TurnLeft by 90
◦,o r TurnRight by 90◦.T h e
agent dies a miserable death if it enters a square containing a pit or a live wumpus. (It
is safe, albeit smelly, to enter a square with a dead wumpus.) If an agent tries to move
forward and bumps into a wall, then the agent does not move. The action Grab can be
used to pick up the gold if it is in the same square as the agent. The action Shoot can
be used to ﬁre an arrow in a straight line in the direction the agent is facing. The arrow
continues until it either hits (and hence kills) the wumpus or hits a wall. The agent has
only one arrow, so only the ﬁrst Shoot action has any effect. Finally, the action Climb
can be used to climb out of the cave, but only from square [1,1].
•Sensors: The agent has ﬁve sensors, each of which gives a single bit of information:
– In the square containing the wumpus and in the directly (not diagonally) adjacent
squares, the agent will perceive a Stench.
– In the squares directly adjacent to a pit, the agent will perceive a Breeze.
– In the square where the gold is, the agent will perceive a Glitter.
– When an agent walks into a wall, it will perceive a Bump.
– When the wumpus is killed, it emits a woeful Scream that can be perceived any-
where in the cave.
The percepts will be given to the agent program in the form of a list of ﬁve symbols;
for example, if there is a stench and a breeze, but no glitter, bump, or scream, the agent
program will get [Stench,Breeze,None,None,None].
We can characterize the wumpus environment along the various dimensions given in Chap-
ter 2. Clearly, it is discrete, static, and single-agent. (The wumpus doesn’t move, fortunately.)
It is sequential, because rewards may come only after many actions are taken. It is partially
observable, because some aspects of the state are not directly perceivable: the agent’s lo-
cation, the wumpus’s state of health, and the availability of an arrow. As for the locations
of the pits and the wumpus: we could treat them as unobserved parts of the state that hap-
pen to be immutable—in which case, the transition model for the environment is completely
238 Chapter 7. Logical Agents
PIT
1234
1
2
3
4
START
Stench
Stench
Breeze
Gold
PIT
PIT
Breeze
Breeze
Breeze
Breeze
Breeze
Stench
Figure 7.2 A typical wumpus world. The agent is in the bottom left corner, facing right.
known; or we could say that the transition model itself is unknown because the agent doesn’t
know which Fo r w a rdactions are fatal—in which case, discovering the locations of pits and
wumpus completes the agent’s knowledge of the transition model.
For an agent in the environment, the main challenge is its initial ignorance of the con-
ﬁguration of the environment; overcoming this ignorance seems to require logical reasoning.
In most instances of the wumpus world, it is possible for the agent to retrieve the gold safely.
Occasionally, the agent must choose between going home empty-handed and risking death to
ﬁnd the gold. About 21% of the environments are utterly unfair, because the gold is in a pit
or surrounded by pits.
Let us watch a knowledge-based wumpus agent exploring the environment shown in
Figure 7.2. We use an informal knowledge representation language consisting of writing
down symbols in a grid (as in Figures 7.3 and 7.4).
The agent’s initial knowledge base contains the rules of the environment, as described
previously; in particular, it knows that it is in [1,1] and that [1,1] is a safe square; we denote
that with an “A” and “OK,” respectively, in square [1,1].
The ﬁrst percept is [None,None,None,None,None], from which the agent can con-
clude that its neighboring squares, [1,2] and [2,1], are free of dangers—they are OK. Fig-
ure 7.3(a) shows the agent’s state of knowledge at this point.
A cautious agent will move only into a square that it knows to be OK. Let us suppose
the agent decides to move forward to [2,1]. The agent perceives a breeze (denoted by “B”) in
[2,1], so there must be a pit in a neighboring square. The pit cannot be in [1,1], by the rules of
the game, so there must be a pit in [2,2] or [3,1] or both. The notation “P?” in Figure 7.3(b)
indicates a possible pit in those squares. At this point, there is only one known square that is
OK and that has not yet been visited. So the prudent agent will turn around, go back to [1,1],
and then proceed to [1,2].
The agent perceives a stench in [1,2], resulting in the state of knowledge shown in
Figure 7.4(a). The stench in [1,2] means that there must be a wumpus nearby. But the
Section 7.2. The Wumpus World 239
A
B
G
P
S
W
 = Agent
 = Breeze
 = Glitter, Gold
 = Pit
 = Stench
 = Wumpus
OK = Safe square
V  = Visited
A
OK
 1,1  2,1  3,1  4,1
 1,2  2,2  3,2  4,2
 1,3  2,3  3,3  4,3
 1,4  2,4  3,4  4,4
OKOK
B
P?
P?A
OK OK
OK
 1,1  2,1  3,1  4,1
 1,2  2,2  3,2  4,2
 1,3  2,3  3,3  4,3
 1,4  2,4  3,4  4,4
V
(a) (b)
Figure 7.3 The ﬁrst step taken by the agent in the wumpus world. (a) The initial sit-
uation, after percept [None,None,None,None,None]. (b) After one move, with percept
[None,Breeze,None,None,None].
BB P!
A
OK OK
OK
 1,1  2,1  3,1  4,1
 1,2  2,2  3,2  4,2
 1,3  2,3  3,3  4,3
 1,4  2,4  3,4  4,4
V
OK
W!
V
P!
A
OK OK
OK
 1,1  2,1  3,1  4,1
 1,2  2,2  3,2  4,2
 1,3  2,3  3,3  4,3
 1,4  2,4  3,4  4,4
V
S
OK
W!
V
VV
B
SG
P?
P?
(b)(a)
S
A
B
G
P
S
W
 = Agent
 = Breeze
 = Glitter, Gold
 = Pit
 = Stench
 = Wumpus
OK = Safe square
V  = Visited
Figure 7.4 Two later stages in the progress of the agent. (a) After the third move,
with percept [Stench,None,None,None,None]. (b) After the ﬁfth move, with percept
[Stench,Breeze,Glitter,None,None].
wumpus cannot be in [1,1], by the rules of the game, and it cannot be in [2,2] (or the agent
would have detected a stench when it was in [2,1]). Therefore, the agent can infer that the
wumpus is in [1,3]. The notation W! indicates this inference. Moreover, the lack of a breeze
in [1,2] implies that there is no pit in [2,2]. Yet the agent has already inferred that there must
be a pit in either [2,2] or [3,1], so this means it must be in [3,1]. This is a fairly difﬁcult
inference, because it combines knowledge gained at different times in different places and
relies on the lack of a percept to make one crucial step.
240 Chapter 7. Logical Agents
The agent has now proved to itself that there is neither a pit nor a wumpus in [2,2], so it
is OK to move there. We do not show the agent’s state of knowledge at [2,2]; we just assume
that the agent turns and moves to [2,3], giving us Figure 7.4(b). In [2,3], the agent detects a
glitter, so it should grab the gold and then return home.
Note that in each case for which the agent draws a conclusion from the available in-
formation, that conclusion is guaranteed to be correct if the available information is correct.
This is a fundamental property of logical reasoning. In the rest of this chapter, we describe
how to build logical agents that can represent information and draw conclusions such as those
described in the preceding paragraphs.
7.3 L OGIC
This section summarizes the fundamental concepts of logical representation and reasoning.
These beautiful ideas are independent of any of logic’s particular forms. We therefore post-
pone the technical details of those forms until the next section, using instead the familiar
example of ordinary arithmetic.
In Section 7.1, we said that knowledge bases consist of sentences. These sentences
are expressed according to the syntax of the representation language, which speciﬁes all the
SYNTAX
sentences that are well formed. The notion of syntax is clear enough in ordinary arithmetic:
“x + y =4 ” is a well-formed sentence, whereas “x4y+= ” is not.
A logic must also deﬁne the semantics or meaning of sentences. The semantics deﬁnesSEMANTICS
the truth of each sentence with respect to each possible world. For example, the semanticsTRUTH
POSSIBLE WORLD for arithmetic speciﬁes that the sentence “ x + y =4 ” is true in a world where x is 2 and y
is 2, but false in a world where x is 1 and y is 1. In standard logics, every sentence must be
either true or false in each possible world—there is no “in between.”1
When we need to be precise, we use the term model in place of “possible world.”MODEL
Whereas possible worlds might be thought of as (potentially) real environments that the agent
might or might not be in, models are mathematical abstractions, each of which simply ﬁxes
the truth or falsehood of every relevant sentence. Informally, we may think of a possible world
as, for example, havingx men and y women sitting at a table playing bridge, and the sentence
x + y =4 is true when there are four people in total. Formally, the possible models are just
all possible assignments of real numbers to the variablesx and y. Each such assignment ﬁxes
the truth of any sentence of arithmetic whose variables are x and y. If a sentence α is true in
model m, we say that m satisﬁes α or sometimes m is a model of α. We use the notation
SA TISFACTION
M(α) to mean the set of all models of α.
Now that we have a notion of truth, we are ready to talk about logical reasoning. This
involves the relation of logical entailment between sentences—the idea that a sentence fol-ENTAILMENT
lows logically from another sentence. In mathematical notation, we write
α|= β
1 Fuzzy logic, discussed in Chapter 14, allows for degrees of truth.
Section 7.3. Logic 241
123
1
2 PIT
123
1
2 PIT
123
1
2 PIT PIT
PIT
123
1
2 PIT
PIT
123
1
2
PIT
123
1
2 PIT
PIT
123
1
2 PIT PIT
123
1
2
KB α 1
Breeze
Breeze
Breeze
Breeze
Breeze
Breeze
Breeze
Breeze
123
1
2 PIT
123
1
2 PIT
123
1
2 PIT PIT
PIT
123
1
2 PIT
PIT
123
1
2
PIT
123
1
2 PIT
PIT
123
1
2 PIT PIT
123
1
2
KB
Breeze
α 2
Breeze
Breeze
Breeze
Breeze
Breeze
Breeze
Breeze
(a) (b)
Figure 7.5 Possible models for the presence of pits in squares [1,2], [2,2], and [3,1]. The
KB corresponding to the observations of nothing in [1,1] and a breeze in [2,1] is shown by
the solid line. (a) Dotted line shows models of α1 (no pit in [1,2]). (b) Dotted line shows
models of α2 (no pit in [2,2]).
to mean that the sentenceα entails the sentenceβ. The formal deﬁnition of entailment is this:
α|= β if and only if, in every model in whichα is true, β is also true. Using the notation just
introduced, we can write
α|= β if and only if M(α)⊆M(β) .
(Note the direction of the⊆here: if α|= β,t h e nα is a stronger assertion than β: it rules out
more possible worlds.) The relation of entailment is familiar from arithmetic; we are happy
with the idea that the sentence x =0 entails the sentence xy =0 . Obviously, in any model
where x is zero, it is the case that xy is zero (regardless of the value of y).
We can apply the same kind of analysis to the wumpus-world reasoning example given
in the preceding section. Consider the situation in Figure 7.3(b): the agent has detected
nothing in [1,1] and a breeze in [2,1]. These percepts, combined with the agent’s knowledge
of the rules of the wumpus world, constitute the KB. The agent is interested (among other
things) in whether the adjacent squares [1,2], [2,2], and [3,1] contain pits. Each of the three
squares might or might not contain a pit, so (for the purposes of this example) there are23 =8
possible models. These eight models are shown in Figure 7.5.2
The KB can be thought of as a set of sentences or as a single sentence that asserts all
the individual sentences. The KB is false in models that contradict what the agent knows—
for example, the KB is false in any model in which [1,2] contains a pit, because there is
no breeze in [1,1]. There are in fact just three models in which the KB is true, and these are
2 Although the ﬁgure shows the models as partial wumpus worlds, they are really nothing more than assignments
of true and false to the sentences “there is a pit in [1,2]” etc. Models, in the mathematical sense, do not need to
have ’orrible ’airy wumpuses in them.
242 Chapter 7. Logical Agents
shown surrounded by a solid line in Figure 7.5. Now let us consider two possible conclusions:
α1 = “There is no pit in [1,2].”
α2 = “There is no pit in [2,2].”
We have surrounded the models of α1 and α2 with dotted lines in Figures 7.5(a) and 7.5(b),
respectively. By inspection, we see the following:
in every model in which KB is true, α1 is also true.
Hence, KB|= α1: there is no pit in [1,2]. We can also see that
in some models in which KB is true, α2 is false.
Hence, KB̸|= α2: the agent cannot conclude that there is no pit in [2,2]. (Nor can it conclude
that there is a pit in [2,2].)3
The preceding example not only illustrates entailment but also shows how the deﬁnition
of entailment can be applied to derive conclusions—that is, to carry out logical inference.LOGICAL INFERENCE
The inference algorithm illustrated in Figure 7.5 is called model checking, because it enu-MODEL CHECKING
merates all possible models to check that α is true in all models in which KB is true, that is,
that M(KB)⊆M(α).
In understanding entailment and inference, it might help to think of the set of all conse-
quences of KB as a haystack and of α as a needle. Entailment is like the needle being in the
haystack; inference is like ﬁnding it. This distinction is embodied in some formal notation: if
an inference algorithm i can derive α from KB, we write
KB⊢
i α,
which is pronounced “α is derived from KB by i”o r“ i derives α from KB.”
An inference algorithm that derives only entailed sentences is called sound or truth-SOUND
preserving. Soundness is a highly desirable property. An unsound inference procedure es-TRUTH-PRESERVING
sentially makes things up as it goes along—it announces the discovery of nonexistent needles.
It is easy to see that model checking, when it is applicable,4 is a sound procedure.
The property of completeness is also desirable: an inference algorithm is complete ifCOMPLETENESS
it can derive any sentence that is entailed. For real haystacks, which are ﬁnite in extent,
it seems obvious that a systematic examination can always decide whether the needle is in
the haystack. For many knowledge bases, however, the haystack of consequences is inﬁnite,
and completeness becomes an important issue.
5 Fortunately, there are complete inference
procedures for logics that are sufﬁciently expressive to handle many knowledge bases.
We have described a reasoning process whose conclusions are guaranteed to be true
in any world in which the premises are true; in particular, if KB is true in the real world,
then any sentence α derived from KB by a sound inference procedure is also true in the real
world. So, while an inference process operates on “syntax”—internal physical conﬁgurations
such as bits in registers or patterns of electrical blips in brains—the process corresponds
3 The agent can calculate the probability that there is a pit in [2,2]; Chapter 13 shows how.
4 Model checking works if the space of models is ﬁnite—for example, in wumpus worlds of ﬁxed size. For
arithmetic, on the other hand, the space of models is inﬁnite : even if we restrict ourselves to the integers, there
are inﬁnitely many pairs of values for x and y in the sentence x + y =4 .
5 Compare with the case of inﬁnite search spaces in Chapter 3, where depth-ﬁrst search is not complete.
Section 7.4. Propositional Logic: A Very Simple Logic 243
Follows
Sentences Sentence
Entails
Semantics
Semantics
Representation
World
Aspects of the
    real world
Aspect of the
   real world
Figure 7.6 Sentences are physical conﬁgurations of the agent, and reasoning is a process
of constructing new physical conﬁgurations from old ones. Logical reasoning should en-
sure that the new conﬁgurations represent aspects of the world that actually follow from the
aspects that the old conﬁgurations represent.
to the real-world relationship whereby some aspect of the real world is the case 6 by virtue
of other aspects of the real world being the case. This correspondence between world and
representation is illustrated in Figure 7.6.
The ﬁnal issue to consider is grounding—the connection between logical reasoning
GROUNDING
processes and the real environment in which the agent exists. In particular, how do we know
that KB is true in the real world? (After all, KB is just “syntax” inside the agent’s head.)
This is a philosophical question about which many, many books have been written. (See
Chapter 26.) A simple answer is that the agent’s sensors create the connection. For example,
our wumpus-world agent has a smell sensor. The agent program creates a suitable sentence
whenever there is a smell. Then, whenever that sentence is in the knowledge base, it is
true in the real world. Thus, the meaning and truth of percept sentences are deﬁned by the
processes of sensing and sentence construction that produce them. What about the rest of the
agent’s knowledge, such as its belief that wumpuses cause smells in adjacent squares? This
is not a direct representation of a single percept, but a general rule—derived, perhaps, from
perceptual experience but not identical to a statement of that experience. General rules like
this are produced by a sentence construction process called learning, which is the subject
of Part V. Learning is fallible. It could be the case that wumpuses cause smells except on
February 29 in leap years, which is when they take their baths. Thus, KB may not be true in
the real world, but with good learning procedures, there is reason for optimism.
7.4 P ROPOSITIONAL LOGIC :AV ERY SIMPLE LOGIC
We now present a simple but powerful logic calledpropositional logic. We cover the syntaxPROPOSITIONAL
LOGIC
of propositional logic and its semantics—the way in which the truth of sentences is deter-
mined. Then we look at entailment—the relation between a sentence and another sentence
that follows from it—and see how this leads to a simple algorithm for logical inference. Ev-
erything takes place, of course, in the wumpus world.
6 As Wittgenstein (1922) put it in his famous Tractatus: “The world is everything that is the case.”
244 Chapter 7. Logical Agents
7.4.1 Syntax
The syntax of propositional logic deﬁnes the allowable sentences. The atomic sentencesA TOMIC SENTENCES
consist of a single proposition symbol. Each such symbol stands for a proposition that canPROPOSITION
SYMBOL
be true or false. We use symbols that start with an uppercase letter and may contain other
letters or subscripts, for example: P , Q, R, W1,3 and North. The names are arbitrary but
are often chosen to have some mnemonic value—we use W1,3 to stand for the proposition
that the wumpus is in [1,3]. (Remember that symbols such as W1,3 are atomic, i.e., W ,1 ,
and 3 are not meaningful parts of the symbol.) There are two proposition symbols with ﬁxed
meanings: True is the always-true proposition and False is the always-false proposition.
Complex sentences are constructed from simpler sentences, using parentheses and logical
COMPLEX
SENTENCES
connectives. There are ﬁve connectives in common use:LOGICAL
CONNECTIVES
¬ (not). A sentence such as ¬W1,3 is called the negation of W1,3.A literal is either anNEGA TION
LITERAL atomic sentence (a positive literal) or a negated atomic sentence (a negative literal).
∧(and). A sentence whose main connective is ∧,s u c ha sW1,3 ∧P3,1, is called a con-
junction; its parts are the conjuncts.( T h e∧looks like an “A” for “And.”)CONJUNCTION
∨(or). A sentence using∨,s u c ha s(W1,3∧P3,1)∨W2,2,i sa disjunction of the disjunctsDISJUNCTION
(W1,3∧P3,1) and W2,2. (Historically, the∨comes from the Latin “vel,” which means
“or.” For most people, it is easier to remember∨as an upside-down∧.)
⇒(implies). A sentence such as (W1,3∧P3,1) ⇒¬W2,2 is called an implication (or con-IMPLICA TION
ditional). Its premise or antecedent is (W1,3∧P3,1), and its conclusion or consequentPREMISE
CONCLUSION is¬W2,2. Implications are also known as rules or if–then statements. The implication
RULES symbol is sometimes written in other books as⊃or→.
⇔(if and only if). The sentence W1,3 ⇔¬W2,2 is a biconditional. Some other booksBICONDITIONAL
write this as≡.
Sentence → AtomicSentence| ComplexSentence
AtomicSentence → True| False| P| Q| R| ...
ComplexSentence → ( Sentence )| [ Sentence ]
|¬ Sentence
| Sentence∧Sentence
| Sentence∨Sentence
| Sentence ⇒ Sentence
| Sentence ⇔ Sentence
OPERATOR PRECEDENCE : ¬,∧,∨,⇒,⇔
Figure 7.7 A BNF (Backus–Naur Form) grammar of sentences in propositional logic,
along with operator precedences, from highest to lowest.

Section 7.4. Propositional Logic: A Very Simple Logic 245
Figure 7.7 gives a formal grammar of propositional logic; see page 1060 if you are not
familiar with the BNF notation. The BNF grammar by itself is ambiguous; a sentence with
several operators can be parsed by the grammar in multiple ways. To eliminate the ambiguity
we deﬁne a precedence for each operator. The “not” operator (¬) has the highest precedence,
which means that in the sentence ¬A∧B the¬ binds most tightly, giving us the equivalent
of (¬A)∧B rather than¬(A∧B). (The notation for ordinary arithmetic is the same:−2+4
is 2, not –6.) When in doubt, use parentheses to make sure of the right interpretation. Square
brackets mean the same thing as parentheses; the choice of square brackets or parentheses is
solely to make it easier for a human to read a sentence.
7.4.2 Semantics
Having speciﬁed the syntax of propositional logic, we now specify its semantics. The se-
mantics deﬁnes the rules for determining the truth of a sentence with respect to a particular
model. In propositional logic, a model simply ﬁxes the truth value—true or false—for ev-
TRUTH VALUE
ery proposition symbol. For example, if the sentences in the knowledge base make use of the
proposition symbols P1,2, P2,2,a n dP3,1, then one possible model is
m1 ={P1,2 =false,P 2,2 =false,P 3,1 =true} .
With three proposition symbols, there are 23 =8 possible models—exactly those depicted
in Figure 7.5. Notice, however, that the models are purely mathematical objects with no
necessary connection to wumpus worlds. P
1,2 is just a symbol; it might mean “there is a pit
in [1,2]” or “I’m in Paris today and tomorrow.”
The semantics for propositional logic must specify how to compute the truth value of
any sentence, given a model. This is done recursively. All sentences are constructed from
atomic sentences and the ﬁve connectives; therefore, we need to specify how to compute the
truth of atomic sentences and how to compute the truth of sentences formed with each of the
ﬁve connectives. Atomic sentences are easy:
•True is true in every model and False is false in every model.
•The truth value of every other proposition symbol must be speciﬁed directly in the
model. For example, in the model m
1 given earlier, P1,2 is false.
For complex sentences, we have ﬁve rules, which hold for any subsentences P and Q in any
model m (here “iff” means “if and only if”):
•¬P is true iff P is false in m.
•P∧Q is true iff both P and Q are true in m.
•P∨Q is true iff either P or Q is true in m.
•P ⇒Q is true unless P is true and Q is false in m.
•P ⇔Q is true iff P and Q are both true or both false in m.
The rules can also be expressed with truth tables that specify the truth value of a complexTRUTH TABLE
sentence for each possible assignment of truth values to its components. Truth tables for the
ﬁve connectives are given in Figure 7.8. From these tables, the truth value of any sentence s
can be computed with respect to any modelm by a simple recursive evaluation. For example,
246 Chapter 7. Logical Agents
P
 Q
 ¬P
 P∧Q
 P∨Q
 P ⇒Q
 P ⇔Q
false
 false
 true
 false
 false
 true
 true
false
 true
 true
 false
 true
 true
 false
true
 false
 false
 false
 true
 false
 false
true
 true
 false
 true
 true
 true
 true
Figure 7.8 Truth tables for the ﬁve logical connectives. To use the table to compute, for
example, the value of P∨Q when P is true and Q is false, ﬁrst look on the left for the row
where P is true and Q is false (the third row). Then look in that row under theP∨Q column
to see the result: true.
the sentence¬P1,2 ∧(P2,2 ∨P3,1), evaluated in m1,g i v e strue∧(false∨true)= true∧
true =true. Exercise 7.3 asks you to write the algorithm PL-T RUE ?(s,m), which computes
the truth value of a propositional logic sentence s in a model m.
The truth tables for “and,” “or,” and “not” are in close accord with our intuitions about
the English words. The main point of possible confusion is that P∨Q is true when P is true
or Q is true or both. A different connective, called “exclusive or” (“xor” for short), yields
false when both disjuncts are true. 7 There is no consensus on the symbol for exclusive or;
some choices are ˙∨or̸= or⊕.
The truth table for⇒may not quite ﬁt one’s intuitive understanding of “P implies Q”
or “if P then Q.” For one thing, propositional logic does not require any relation ofcausation
or relevance between P and Q. The sentence “5 is odd implies Tokyo is the capital of Japan”
is a true sentence of propositional logic (under the normal interpretation), even though it is
a decidedly odd sentence of English. Another point of confusion is that any implication is
true whenever its antecedent is false. For example, “5 is even implies Sam is smart” is true,
regardless of whether Sam is smart. This seems bizarre, but it makes sense if you think of
“P ⇒Q” as saying, “If P is true, then I am claiming that Q is true. Otherwise I am making
no claim.” The only way for this sentence to be false is if P is true but Q is false.
The biconditional, P ⇔Q, is true whenever both P ⇒Q and Q ⇒P are true. In
English, this is often written as “P if and only if Q.” Many of the rules of the wumpus world
are best written using ⇔. For example, a square is breezy if a neighboring square has a pit,
and a square is breezy only if a neighboring square has a pit. So we need a biconditional,
B
1,1 ⇔(P1,2∨P2,1) ,
where B1,1 means that there is a breeze in [1,1].
7.4.3 A simple knowledge base
Now that we have deﬁned the semantics for propositional logic, we can construct a knowledge
base for the wumpus world. We focus ﬁrst on the immutable aspects of the wumpus world,
leaving the mutable aspects for a later section. For now, we need the following symbols for
each [x, y] location:
7 Latin has a separate word, aut, for exclusive or.
Section 7.4. Propositional Logic: A Very Simple Logic 247
Px,y is true if there is a pit in [x, y].
Wx,y is true if there is a wumpus in [x, y], dead or alive.
Bx,y is true if the agent perceives a breeze in [x, y].
Sx,y is true if the agent perceives a stench in [x, y].
The sentences we write will sufﬁce to derive ¬P1,2 (there is no pit in [1,2]), as was done
informally in Section 7.3. We label each sentence Ri so that we can refer to them:
•There is no pit in [1,1]:
R1 : ¬P1,1 .
•A square is breezy if and only if there is a pit in a neighboring square. This has to be
stated for each square; for now, we include just the relevant squares:
R2 : B1,1 ⇔ (P1,2∨P2,1) .
R3 : B2,1 ⇔ (P1,1∨P2,2∨P3,1) .
•The preceding sentences are true in all wumpus worlds. Now we include the breeze
percepts for the ﬁrst two squares visited in the speciﬁc world the agent is in, leading up
to the situation in Figure 7.3(b).
R
4 : ¬B1,1 .
R5 : B2,1 .
7.4.4 A simple inference procedure
Our goal now is to decide whether KB |= α for some sentence α. For example, is ¬P1,2
entailed by our KB? Our ﬁrst algorithm for inference is a model-checking approach that is a
direct implementation of the deﬁnition of entailment: enumerate the models, and check that
α is true in every model in which KB is true. Models are assignments of true or false to
every proposition symbol. Returning to our wumpus-world example, the relevant proposi-
tion symbols are B
1,1, B2,1, P1,1, P1,2, P2,1, P2,2,a n dP3,1. With seven symbols, there are
27 = 128possible models; in three of these, KB is true (Figure 7.9). In those three models,
¬P1,2 is true, hence there is no pit in [1,2]. On the other hand, P2,2 is true in two of the three
models and false in one, so we cannot yet tell whether there is a pit in [2,2].
Figure 7.9 reproduces in a more precise form the reasoning illustrated in Figure 7.5. A
general algorithm for deciding entailment in propositional logic is shown in Figure 7.10. Like
the B
ACKTRACKING -SEARCH algorithm on page 215, TT-E NTAILS ? performs a recursive
enumeration of a ﬁnite space of assignments to symbols. The algorithm is sound because it
implements directly the deﬁnition of entailment, and complete because it works for any KB
and α and always terminates—there are only ﬁnitely many models to examine.
Of course, “ﬁnitely many” is not always the same as “few.” If KB and α contain n
symbols in all, then there are 2n models. Thus, the time complexity of the algorithm is
O(2n). (The space complexity is only O(n) because the enumeration is depth-ﬁrst.) Later in
this chapter we show algorithms that are much more efﬁcient in many cases. Unfortunately,
propositional entailment is co-NP-complete (i.e., probably no easier than NP-complete—see
Appendix A), so every known inference algorithm for propositional logic has a worst-case
complexity that is exponential in the size of the input.
248 Chapter 7. Logical Agents
B1,1
 B2,1
 P1,1
 P1,2
 P2,1
 P2,2
 P3,1
 R1
 R2
 R3
 R4
 R5
 KB
false
 false
 false
 false
 false
 false
 false
 true
 true
 true
 true
 false
 false
false
 false
 false
 false
 false
 false
 true
 true
 true
 false
 true
 false
 false
...
...
...
...
...
...
...
...
...
...
...
...
...
false
 true
 false
 false
 false
 false
 false
 true
 true
 false
 true
 true
 false
false
 true
 false
 false
 false
 false
 true
 true
 true
 true
 true
 true
 true
false
 true
 false
 false
 false
 true
 false
 true
 true
 true
 true
 true
 true
false
 true
 false
 false
 false
 true
 true
 true
 true
 true
 true
 true
 true
false
 true
 false
 false
 true
 false
 false
 true
 false
 false
 true
 true
 false
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
true
 true
 true
 true
 true
 true
 true
 false
 true
 true
 false
 true
 false
Figure 7.9 A truth table constructed for the knowledge base given in the text. KB is true
if R1 through R5 are true, which occurs in just 3 of the 128 rows (the ones underlined in the
right-hand column). In all 3 rows, P1,2 is false, so there is no pit in [1,2]. On the other hand,
there might (or might not) be a pit in [2,2].
function TT-ENTAILS ?(KB,α) returns true or false
inputs: KB, the knowledge base, a sentence in propositional logic
α, the query, a sentence in propositional logic
symbols←a list of the proposition symbols in KB and α
return TT-CHECK -ALL(KB,α,symbols,{} )
function TT-CHECK -ALL(KB,α,symbols,model) returns true or false
if EMPTY ?(symbols) then
if PL-T RUE ?(KB,model) then return PL-T RUE ?(α,model)
else return true // when KB is false, always return true
else do
P←FIRST (symbols)
rest←REST (symbols)
return (TT-C HECK -ALL(KB,α,rest,model∪{P = true})
and
TT-CHECK -ALL(KB,α,rest,model ∪{P = false}))
Figure 7.10 A truth-table enumeration algorithm for deciding propositional entailment.
(TT stands for truth table.) PL-T RUE ? returns true if a sentence holds within a model. The
variable model represents a partial model—an assignment to some of the symbols. The key-
word “and” is used here as a logical operation on its two arguments, returning true or false.

Section 7.5. Propositional Theorem Proving 249
(α∧β) ≡(β∧α) commutativity of∧
(α∨β) ≡(β∨α) commutativity of∨
((α∧β)∧γ) ≡(α∧(β∧γ)) associativity of∧
((α∨β)∨γ) ≡(α∨(β∨γ)) associativity of∨
¬(¬α) ≡α double-negation elimination
(α ⇒β) ≡(¬β ⇒¬α) contraposition
(α ⇒β) ≡(¬α∨β) implication elimination
(α ⇔β) ≡((α ⇒β)∧(β ⇒α)) biconditional elimination
¬(α∧β) ≡(¬α∨¬β) De Morgan
¬(α∨β) ≡(¬α∧¬β) De Morgan
(α∧(β∨γ)) ≡((α∧β)∨(α∧γ)) distributivity of ∧over∨
(α∨(β∧γ)) ≡((α∨β)∧(α∨γ)) distributivity of ∨over∧
Figure 7.11 Standard logical equivalences. The symbols α, β,a n dγ stand for arbitrary
sentences of propositional logic.
7.5 P ROPOSITIONAL THEOREM PROVING
So far, we have shown how to determine entailment bymodel checking: enumerating models
and showing that the sentence must hold in all models. In this section, we show how entail-
ment can be done by theorem proving—applying rules of inference directly to the sentences
THEOREM PROVING
in our knowledge base to construct a proof of the desired sentence without consulting models.
If the number of models is large but the length of the proof is short, then theorem proving can
be more efﬁcient than model checking.
Before we plunge into the details of theorem-proving algorithms, we will need some
additional concepts related to entailment. The ﬁrst concept is logical equivalence: two sen-LOGICAL
EQUIVALENCE
tences α and β are logically equivalent if they are true in the same set of models. We write
this as α≡β. For example, we can easily show (using truth tables) that P∧Q and Q∧P
are logically equivalent; other equivalences are shown in Figure 7.11. These equivalences
play much the same role in logic as arithmetic identities do in ordinary mathematics. An
alternative deﬁnition of equivalence is as follows: any two sentences α and β are equivalent
only if each of them entails the other:
α≡β if and only if α|= β and β|= α.
The second concept we will need isvalidity. A sentence is valid if it is true inall models. For
VALIDITY
example, the sentence P∨¬P is valid. Valid sentences are also known astautologies—theyTAUTOLOGY
are necessarily true. Because the sentence True is true in all models, every valid sentence
is logically equivalent to True. What good are valid sentences? From our deﬁnition of
entailment, we can derive the deduction theorem, which was known to the ancient Greeks:DEDUCTION
THEOREM
F or any sentencesα and β, α|= β if and only if the sentence (α⇒β) is valid.
(Exercise 7.5 asks for a proof.) Hence, we can decide if α|= β by checking that (α⇒β) is
true in every model—which is essentially what the inference algorithm in Figure 7.10 does—
250 Chapter 7. Logical Agents
or by proving that (α⇒β) is equivalent to True. Conversely, the deduction theorem states
that every valid implication sentence describes a legitimate inference.
The ﬁnal concept we will need is satisﬁability. A sentence is satisﬁable if it is trueSA TISFIABILITY
in, or satisﬁed by, some model. For example, the knowledge base given earlier, ( R1 ∧R2∧
R3 ∧R4 ∧R5), is satisﬁable because there are three models in which it is true, as shown
in Figure 7.9. Satisﬁability can be checked by enumerating the possible models until one is
found that satisﬁes the sentence. The problem of determining the satisﬁability of sentences
in propositional logic—the SA Tproblem—was the ﬁrst problem proved to be NP-complete.
SA T
Many problems in computer science are really satisﬁability problems. For example, all the
constraint satisfaction problems in Chapter 6 ask whether the constraints are satisﬁable by
some assignment.
Validity and satisﬁability are of course connected: α is valid iff ¬α is unsatisﬁable;
contrapositively,α is satisﬁable iff¬α is not valid. We also have the following useful result:
α|= β if and only if the sentence (α∧¬β) is unsatisﬁable.
Proving β from α by checking the unsatisﬁability of (α∧¬β) corresponds exactly to the
standard mathematical proof technique of reductio ad absurdum (literally, “reduction to anREDUCTIO AD
ABSURDUM
absurd thing”). It is also called proof byrefutation or proof bycontradiction. One assumes aREFUT A TION
CONTRADICTION sentence β to be false and shows that this leads to a contradiction with known axiomsα.T h i s
contradiction is exactly what is meant by saying that the sentence (α∧¬β) is unsatisﬁable.
7.5.1 Inference and proofs
This section covers inference rules that can be applied to derive a proof—a chain of conclu-INFERENCE RULES
PROOF sions that leads to the desired goal. The best-known rule is called Modus Ponens (Latin for
MODUS PONENS mode that afﬁrms) and is written
α ⇒β, α
β .
The notation means that, whenever any sentences of the form α⇒β and α are given, then
the sentence β can be inferred. For example, if(WumpusAhead∧WumpusAlive)⇒Shoot
and (WumpusAhead∧WumpusAlive) are given, then Shoot can be inferred.
Another useful inference rule isAnd-Elimination, which says that, from a conjunction,AND-ELIMINA TION
any of the conjuncts can be inferred:
α∧β
α .
For example, from (WumpusAhead∧WumpusAlive), WumpusAlive can be inferred.
By considering the possible truth values of α and β, one can show easily that Modus
Ponens and And-Elimination are sound once and for all. These rules can then be used in
any particular instances where they apply, generating sound inferences without the need for
enumerating models.
All of the logical equivalences in Figure 7.11 can be used as inference rules. For exam-
ple, the equivalence for biconditional elimination yields the two inference rules
α ⇔β
(α ⇒β)∧(β ⇒α) and (α ⇒β)∧(β ⇒α)
α ⇔β .
Section 7.5. Propositional Theorem Proving 251
Not all inference rules work in both directions like this. For example, we cannot run Modus
Ponens in the opposite direction to obtain α⇒β and α from β.
Let us see how these inference rules and equivalences can be used in the wumpus world.
We start with the knowledge base containing R1 through R5 and show how to prove¬P1,2,
that is, there is no pit in [1,2]. First, we apply biconditional elimination to R2 to obtain
R6 :( B1,1 ⇒(P1,2∨P2,1)) ∧((P1,2∨P2,1) ⇒B1,1) .
Then we apply And-Elimination to R6 to obtain
R7 :( ( P1,2∨P2,1) ⇒B1,1) .
Logical equivalence for contrapositives gives
R8 :( ¬B1,1 ⇒¬(P1,2∨P2,1)) .
Now we can apply Modus Ponens with R8 and the percept R4 (i.e.,¬B1,1), to obtain
R9 : ¬(P1,2∨P2,1) .
Finally, we apply De Morgan’s rule, giving the conclusion
R10 : ¬P1,2∧¬P2,1 .
That is, neither [1,2] nor [2,1] contains a pit.
We found this proof by hand, but we can apply any of the search algorithms in Chapter 3
to ﬁnd a sequence of steps that constitutes a proof. We just need to deﬁne a proof problem as
follows:
•I
NITIAL STATE: the initial knowledge base.
•ACTIONS : the set of actions consists of all the inference rules applied to all the sen-
tences that match the top half of the inference rule.
•RESULT : the result of an action is to add the sentence in the bottom half of the inference
rule.
•GOAL: the goal is a state that contains the sentence we are trying to prove.
Thus, searching for proofs is an alternative to enumerating models. In many practical cases
ﬁnding a proof can be more efﬁcient because the proof can ignore irrelevant propositions, no
matter how many of them there are. For example, the proof given earlier leading to¬P1,2∧
¬P2,1 does not mention the propositions B2,1, P1,1, P2,2,o r P3,1. They can be ignored
because the goal proposition, P1,2, appears only in sentence R2; the other propositions in R2
appear only in R4 and R2;s o R1, R3,a n dR5 have no bearing on the proof. The same would
hold even if we added a million more sentences to the knowledge base; the simple truth-table
algorithm, on the other hand, would be overwhelmed by the exponential explosion of models.
One ﬁnal property of logical systems is monotonicity, which says that the set of en-
MONOTONICITY
tailed sentences can only increase as information is added to the knowledge base. 8 For any
sentences α and β,
if KB|= α then KB∧β|= α.
8 Nonmonotonic logics, which violate the monotonicity property, capture a common property of human rea-
soning: changing one’s mind. They are discussed in Section 12.6.
252 Chapter 7. Logical Agents
For example, suppose the knowledge base contains the additional assertionβ stating that there
are exactly eight pits in the world. This knowledge might help the agent drawadditional con-
clusions, but it cannot invalidate any conclusion α already inferred—such as the conclusion
that there is no pit in [1,2]. Monotonicity means that inference rules can be applied whenever
suitable premises are found in the knowledge base—the conclusion of the rule must follow
regardless of what else is in the knowledge base .
7.5.2 Proof by resolution
We have argued that the inference rules covered so far are sound, but we have not discussed
the question of completeness for the inference algorithms that use them. Search algorithms
such as iterative deepening search (page 89) are complete in the sense that they will ﬁnd
any reachable goal, but if the available inference rules are inadequate, then the goal is not
reachable—no proof exists that uses only those inference rules. For example, if we removed
the biconditional elimination rule, the proof in the preceding section would not go through.
The current section introduces a single inference rule, resolution, that yields a complete
inference algorithm when coupled with any complete search algorithm.
We begin by using a simple version of the resolution rule in the wumpus world. Let us
consider the steps leading up to Figure 7.4(a): the agent returns from [2,1] to [1,1] and then
goes to [1,2], where it perceives a stench, but no breeze. We add the following facts to the
knowledge base:
R
11 : ¬B1,2 .
R12 : B1,2 ⇔(P1,1∨P2,2∨P1,3) .
By the same process that led to R10 earlier, we can now derive the absence of pits in [2,2]
and [1,3] (remember that [1,1] is already known to be pitless):
R13 : ¬P2,2 .
R14 : ¬P1,3 .
We can also apply biconditional elimination to R3, followed by Modus Ponens with R5,t o
obtain the fact that there is a pit in [1,1], [2,2], or [3,1]:
R15 : P1,1∨P2,2∨P3,1 .
Now comes the ﬁrst application of the resolution rule: the literal ¬P2,2 in R13 resolves with
the literal P2,2 in R15 to give the resolventRESOLVENT
R16 : P1,1∨P3,1 .
In English; if there’s a pit in one of [1,1], [2,2], and [3,1] and it’s not in [2,2], then it’s in [1,1]
or [3,1]. Similarly, the literal¬P1,1 in R1 resolves with the literal P1,1 in R16 to give
R17 : P3,1 .
In English: if there’s a pit in [1,1] or [3,1] and it’s not in [1,1], then it’s in [3,1]. These last
two inference steps are examples of the unit resolution inference rule,UNIT RESOLUTION
ℓ1∨···∨ℓk,m
ℓ1∨···∨ℓi−1∨ℓi+1∨···∨ℓk
,
where each ℓ is a literal and ℓi and m are complementary literals (i.e., one is the negationCOMPLEMENTARY
LITERALS
Section 7.5. Propositional Theorem Proving 253
of the other). Thus, the unit resolution rule takes a clause—a disjunction of literals—and aCLAUSE
literal and produces a new clause. Note that a single literal can be viewed as a disjunction of
one literal, also known as a unit clause.UNIT CLAUSE
The unit resolution rule can be generalized to the full resolution rule,RESOLUTION
ℓ1∨···∨ℓk,m 1∨···∨mn
ℓ1∨···∨ℓi−1∨ℓi+1∨···∨ℓk ∨m1∨···∨mj−1∨mj+1∨···∨mn
,
where ℓi and mj are complementary literals. This says that resolution takes two clauses and
produces a new clause containing all the literals of the two original clauses except the two
complementary literals. For example, we have
P1,1∨P3,1, ¬P1,1∨¬P2,2
P3,1∨¬P2,2
.
There is one more technical aspect of the resolution rule: the resulting clause should contain
only one copy of each literal.9 The removal of multiple copies of literals is called factoring.FACTORING
For example, if we resolve (A∨B) with (A∨¬B), we obtain (A∨A), which is reduced to
just A.
The soundness of the resolution rule can be seen easily by considering the literalℓi that
is complementary to literal mj in the other clause. If ℓi is true, then mj is false, and hence
m1∨···∨mj−1∨mj+1∨···∨mn must be true, because m1∨···∨mn is given. If ℓi is
false, then ℓ1∨···∨ℓi−1∨ℓi+1∨···∨ℓk must be true because ℓ1∨···∨ℓk is given. Now
ℓi is either true or false, so one or other of these conclusions holds—exactly as the resolution
rule states.
What is more surprising about the resolution rule is that it forms the basis for a family
of complete inference procedures. A resolution-based theorem prover can, for any sentences
α and β in propositional logic, decide whether α |= β. The next two subsections explain
how resolution accomplishes this.
Conjunctive normal form
The resolution rule applies only to clauses (that is, disjunctions of literals), so it would seem
to be relevant only to knowledge bases and queries consisting of clauses. How, then, can
it lead to a complete inference procedure for all of propositional logic? The answer is that
every sentence of propositional logic is logically equivalent to a conjunction of clauses. A
sentence expressed as a conjunction of clauses is said to be in conjunctive normal form orCONJUNCTIVE
NORMAL FORM
CNF (see Figure 7.14). We now describe a procedure for converting to CNF. We illustrate
the procedure by converting the sentence B1,1 ⇔(P1,2 ∨P2,1) into CNF. The steps are as
follows:
1. Eliminate ⇔, replacing α⇔β with (α ⇒β)∧(β ⇒α).
(B1,1 ⇒(P1,2∨P2,1))∧((P1,2∨P2,1) ⇒B1,1) .
2. Eliminate ⇒, replacing α⇒β with¬α∨β:
(¬B1,1∨P1,2∨P2,1)∧(¬(P1,2∨P2,1)∨B1,1) .
9 If a clause is viewed as a set of literals, then this restriction is automatically respected. Using set notation for
clauses makes the resolution rule much cleaner, at the cost of introducing additional notation.
254 Chapter 7. Logical Agents
3. CNF requires ¬ to appear only in literals, so we “move ¬ inwards” by repeated appli-
cation of the following equivalences from Figure 7.11:
¬(¬α)≡α (double-negation elimination)
¬(α∧β)≡(¬α∨¬β) (De Morgan)
¬(α∨β)≡(¬α∧¬β) (De Morgan)
In the example, we require just one application of the last rule:
(¬B1,1∨P1,2∨P2,1)∧((¬P1,2∧¬P2,1)∨B1,1) .
4. Now we have a sentence containing nested ∧and∨operators applied to literals. We
apply the distributivity law from Figure 7.11, distributing∨over∧wherever possible.
(¬B1,1∨P1,2∨P2,1)∧(¬P1,2∨B1,1)∧(¬P2,1∨B1,1) .
The original sentence is now in CNF, as a conjunction of three clauses. It is much harder to
read, but it can be used as input to a resolution procedure.
A resolution algorithm
Inference procedures based on resolution work by using the principle of proof by contradic-
tion introduced on page 250. That is, to show that KB |= α, we show that (KB∧¬α) is
unsatisﬁable. We do this by proving a contradiction.
A resolution algorithm is shown in Figure 7.12. First, (KB ∧¬α) is converted into
CNF. Then, the resolution rule is applied to the resulting clauses. Each pair that contains
complementary literals is resolved to produce a new clause, which is added to the set if it is
not already present. The process continues until one of two things happens:
•there are no new clauses that can be added, in which case KB does not entail α;o r ,
•two clauses resolve to yield the empty clause, in which case KB entails α.
The empty clause—a disjunction of no disjuncts—is equivalent toFalse because a disjunction
is true only if at least one of its disjuncts is true. Another way to see that an empty clause
represents a contradiction is to observe that it arises only from resolving two complementary
unit clauses such as P and¬P .
We can apply the resolution procedure to a very simple inference in the wumpus world.
When the agent is in [1,1], there is no breeze, so there can be no pits in neighboring squares.
The relevant knowledge base is
KB = R
2∧R4 =( B1,1 ⇔(P1,2∨P2,1))∧¬B1,1
and we wish to prove α which is, say, ¬P1,2. When we convert (KB∧¬α) into CNF, we
obtain the clauses shown at the top of Figure 7.13. The second row of the ﬁgure shows
clauses obtained by resolving pairs in the ﬁrst row. Then, when P1,2 is resolved with¬P1,2,
we obtain the empty clause, shown as a small square. Inspection of Figure 7.13 reveals that
many resolution steps are pointless. For example, the clauseB1,1∨¬B1,1∨P1,2 is equivalent
to True∨P1,2 which is equivalent to True. Deducing that True is true is not very helpful.
Therefore, any clause in which two complementary literals appear can be discarded.
Section 7.5. Propositional Theorem Proving 255
function PL-R ESOLUTION (KB,α) returns true or false
inputs: KB, the knowledge base, a sentence in propositional logic
α, the query, a sentence in propositional logic
clauses←the set of clauses in the CNF representation of KB∧¬α
new←{}
loop do
for each pair of clauses Ci, Cj in clauses do
resolvents←PL-R ESOLVE (Ci,Cj )
if resolvents contains the empty clause then return true
new←new∪resolvents
if new⊆clauses then return false
clauses←clauses∪new
Figure 7.12 A simple resolution algorithm f or propositional logic. The function
PL-R ESOLVE returns the set of all possible clauses obtained by resolving its two inputs.
¬P2,1     B1,1 ¬B1,1    P1,2     P2,1 ¬P1,2    B1,1 ¬B1,1 P1,2
¬P2,1 ¬P1,2P1,2    P2,1     ¬P2,1 ¬B1,1     P2,1     B1,1 P1,2    P2,1    ¬P1,2¬B1,1    P1,2     B1,1
^ ^ ^
^^ ^ ^ ^ ^ ^ ^
^
Figure 7.13 Partial application of PL-R ESOLUTION to a simple inference in the wumpus
world.¬P1,2 is shown to follow from the ﬁrst four clauses in the top row.
Completeness of resolution
To conclude our discussion of resolution, we now show why PL-R ESOLUTION is complete.
To do this, we introduce the resolution closure RC(S) of a set of clauses S, which is the setRESOLUTION
CLOSURE
of all clauses derivable by repeated application of the resolution rule to clauses in S or their
derivatives. The resolution closure is what PL-R ESOLUTION computes as the ﬁnal value of
the variableclauses. It is easy to see thatRC(S) must be ﬁnite, because there are only ﬁnitely
many distinct clauses that can be constructed out of the symbolsP1,...,P k that appear in S.
(Notice that this would not be true without the factoring step that removes multiple copies of
literals.) Hence, PL-R ESOLUTION always terminates.
The completeness theorem for resolution in propositional logic is called the ground
resolution theorem:
GROUND
RESOLUTION
THEOREM
If a set of clauses is unsatisﬁable, then the resolution closure of those clauses
contains the empty clause.
This theorem is proved by demonstrating its contrapositive: if the closure RC(S) does not
256 Chapter 7. Logical Agents
contain the empty clause, then S is satisﬁable. In fact, we can construct a model for S with
suitable truth values for P1,...,P k. The construction procedure is as follows:
For i from 1 to k,
– If a clause inRC(S) contains the literal¬Pi and all its other literals are false under
the assignment chosen for P1,...,P i−1, then assign false to Pi.
– Otherwise, assign true to Pi.
This assignment to P1,...,P k is a model of S. To see this, assume the opposite—that, at
some stage i in the sequence, assigning symbol Pi causes some clause C to become false.
For this to happen, it must be the case that all the other literals in C must already have been
falsiﬁed by assignments to P1,...,P i−1. Thus, C must now look like either (false∨false∨
··· false∨Pi) or like(false∨false∨···false∨¬Pi). If just one of these two is inRC(S),t h e n
the algorithm will assign the appropriate truth value to Pi to make C true, so C can only be
falsiﬁed if both of these clauses are inRC(S).N o w ,s i n c eRC(S) is closed under resolution,
it will contain the resolvent of these two clauses, and that resolvent will have all of its literals
already falsiﬁed by the assignments to P
1,...,P i−1. This contradicts our assumption that
the ﬁrst falsiﬁed clause appears at stage i. Hence, we have proved that the construction never
falsiﬁes a clause in RC(S); that is, it produces a model of RC(S) and thus a model of S
itself (since S is contained in RC(S)).
7.5.3 Horn clauses and deﬁnite clauses
The completeness of resolution makes it a very important inference method. In many practical
situations, however, the full power of resolution is not needed. Some real-world knowledge
bases satisfy certain restrictions on the form of sentences they contain, which enables them
to use a more restricted and efﬁcient inference algorithm.
One such restricted form is the deﬁnite clause , which is a disjunction of literals of
DEFINITE CLAUSE
which exactly one is positive. For example, the clause (¬L1,1∨¬Breeze∨B1,1) is a deﬁnite
clause, whereas (¬B1,1∨P1,2∨P2,1) is not.
Slightly more general is the Horn clause, which is a disjunction of literals of which atHORN CLAUSE
most one is positive . So all deﬁnite clauses are Horn clauses, as are clauses with no positive
literals; these are calledgoal clauses. Horn clauses are closed under resolution: if you resolveGOAL CLAUSES
two Horn clauses, you get back a Horn clause.
Knowledge bases containing only deﬁnite clauses are interesting for three reasons:
1. Every deﬁnite clause can be written as an implication whose premise is a conjunction
of positive literals and whose conclusion is a single positive literal. (See Exercise 7.13.)
For example, the deﬁnite clause (¬L1,1 ∨¬Breeze∨B1,1) can be written as the im-
plication (L1,1 ∧Breeze) ⇒B1,1. In the implication form, the sentence is easier to
understand: it says that if the agent is in [1,1] and there is a breeze, then [1,1] is breezy.
In Horn form, the premise is called the body and the conclusion is called the head.ABODY
HEAD sentence consisting of a single positive literal, such as L1,1, is called a fact. It too can
FACT be written in implication form as True⇒L1,1, but it is simpler to write just L1,1.
Section 7.5. Propositional Theorem Proving 257
CNFSentence → Clause1 ∧···∧Clausen
Clause → Literal1 ∨···∨Literalm
Literal → Symbol|¬ Symbol
Symbol → P | Q| R| ...
HornClauseForm → DeﬁniteClauseForm| GoalClauseForm
DeﬁniteClauseForm → (Symbol1 ∧···∧Symboll) ⇒Symbol
GoalClauseForm → (Symbol1 ∧···∧Symboll) ⇒False
Figure 7.14 A grammar for conjunctive normal form, Horn clauses, and deﬁnite clauses.
A clause such as A∧B ⇒C is still a deﬁnite clause when it is written as ¬A∨¬B∨C,
but only the former is considered the canonical form for deﬁnite clauses. One more class is
the k-CNF sentence, which is a CNF sentence where each clause has at most k literals.
2. Inference with Horn clauses can be done through theforward-chaining and backward-FORWARD-CHAINING
chaining algorithms, which we explain next. Both of these algorithms are natural,BACKWARD-
CHAINING
in that the inference steps are obvious and easy for humans to follow. This type of
inference is the basis for logic programming, which is discussed in Chapter 9.
3. Deciding entailment with Horn clauses can be done in time that is linear in the size of
the knowledge base—a pleasant surprise.
7.5.4 Forward and backward chaining
The forward-chaining algorithm PL-FC-E NTAILS ?(KB,q) determines if a single proposi-
tion symbol q—the query—is entailed by a knowledge base of deﬁnite clauses. It begins
from known facts (positive literals) in the knowledge base. If all the premises of an implica-
tion are known, then its conclusion is added to the set of known facts. For example, if L1,1
and Breeze are known and (L1,1∧Breeze)⇒B1,1 is in the knowledge base, then B1,1 can
be added. This process continues until the query q is added or until no further inferences can
be made. The detailed algorithm is shown in Figure 7.15; the main point to remember is that
it runs in linear time.
The best way to understand the algorithm is through an example and a picture. Fig-
ure 7.16(a) shows a simple knowledge base of Horn clauses with A and B as known facts.
Figure 7.16(b) shows the same knowledge base drawn as an AND–OR graph (see Chap-
ter 4). In AND–OR graphs, multiple links joined by an arc indicate a conjunction—every
link must be proved—while multiple links without an arc indicate a disjunction—any link
can be proved. It is easy to see how forward chaining works in the graph. The known leaves
(here, A and B) are set, and inference propagates up the graph as far as possible. Wher-
ever a conjunction appears, the propagation waits until all the conjuncts are known before
proceeding. The reader is encouraged to work through the example in detail.
258 Chapter 7. Logical Agents
function PL-FC-E NTAILS ?(KB,q) returns true or false
inputs: KB, the knowledge base, a set of propositional deﬁnite clauses
q, the query, a proposition symbol
count←at a b l e ,w h e r ecount[c] is the number of symbols in c’s premise
inferred←at a b l e ,w h e r einferred[s] is initially false for all symbols
agenda←a queue of symbols, initially symbols known to be true in KB
while agenda is not empty do
p←POP(agenda)
if p = q then return true
if inferred[p]= false then
inferred[p]←true
for each clause c in KB where p is in c.PREMISE do
decrement count[c]
if count[c]=0 then add c.CONCLUSION to agenda
return false
Figure 7.15 The forward-chaining algorithm for propositional logic. The agenda keeps
track of symbols known to be true but not yet “processed.” The count table keeps track of
how many premises of each implication are as yet unknown. Whenever a new symbolp from
the agenda is processed, the count is reduced by one for each implication in whose premise
p appears (easily identiﬁed in constant time with appropriate indexing.) If a count reaches
zero, all the premises of the implication are known, so its conclusion can be added to the
agenda. Finally, we need to keep track of which symbols have been processed; a symbol that
is already in the set of inferred symbols need not be added to the agenda again. This avoids
redundant work and prevents loops caused by implications such as P ⇒Q and Q⇒P .
It is easy to see that forward chaining is sound: every inference is essentially an appli-
cation of Modus Ponens. Forward chaining is also complete: every entailed atomic sentence
will be derived. The easiest way to see this is to consider the ﬁnal state of the inferred table
(after the algorithm reaches a ﬁxed point where no new inferences are possible). The tableFIXED POINT
contains true for each symbol inferred during the process, and false for all other symbols.
We can view the table as a logical model; moreover,every deﬁnite clause in the original KB is
true in this model. To see this, assume the opposite, namely that some clausea1∧...∧ak ⇒b
is false in the model. Then a1 ∧... ∧ak must be true in the model and b must be false in
the model. But this contradicts our assumption that the algorithm has reached a ﬁxed point!
We can conclude, therefore, that the set of atomic sentences inferred at the ﬁxed point deﬁnes
a model of the original KB. Furthermore, any atomic sentence q that is entailed by the KB
must be true in all its models and in this model in particular. Hence, every entailed atomic
sentence q must be inferred by the algorithm.
Forward chaining is an example of the general concept of data-driven reasoning—that
DA T A-DRIVEN
is, reasoning in which the focus of attention starts with the known data. It can be used within
an agent to derive conclusions from incoming percepts, often without a speciﬁc query in
mind. For example, the wumpus agent might T
ELL its percepts to the knowledge base using
Section 7.6. Effective Propositional Model Checking 259
P ⇒Q
L∧M ⇒P
B∧L ⇒M
A∧P ⇒L
A∧B ⇒L
A
B
Q
P
M
L
BA
(a) (b)
Figure 7.16 (a) A set of Horn clauses. (b) The corresponding AND –OR graph.
an incremental forward-chaining algorithm in which new facts can be added to the agenda to
initiate new inferences. In humans, a certain amount of data-driven reasoning occurs as new
information arrives. For example, if I am indoors and hear rain starting to fall, it might occur
to me that the picnic will be canceled. Yet it will probably not occur to me that the seventeenth
petal on the largest rose in my neighbor’s garden will get wet; humans keep forward chaining
under careful control, lest they be swamped with irrelevant consequences.
The backward-chaining algorithm, as its name suggests, works backward from the
query. If the query q is known to be true, then no work is needed. Otherwise, the algorithm
ﬁnds those implications in the knowledge base whose conclusion is q. If all the premises of
one of those implications can be proved true (by backward chaining), then q is true. When
applied to the query Q in Figure 7.16, it works back down the graph until it reaches a set of
known facts, A and B, that forms the basis for a proof. The algorithm is essentially identical
to the A
ND-OR-GRAPH -SEARCH algorithm in Figure 4.11. As with forward chaining, an
efﬁcient implementation runs in linear time.
Backward chaining is a form of goal-directed reasoning. It is useful for answeringGOAL-DIRECTED
REASONING
speciﬁc questions such as “What shall I do now?” and “Where are my keys?” Often, the cost
of backward chaining is much less than linear in the size of the knowledge base, because the
process touches only relevant facts.
7.6 E FFECTIVE PROPOSITIONAL MODEL CHECKING
In this section, we describe two families of efﬁcient algorithms for general propositional
inference based on model checking: One approach based on backtracking search, and one
on local hill-climbing search. These algorithms are part of the “technology” of propositional
logic. This section can be skimmed on a ﬁrst reading of the chapter.
260 Chapter 7. Logical Agents
The algorithms we describe are for checking satisﬁability: the SAT problem. (As noted
earlier, testing entailment, α |= β, can be done by testing unsatisﬁability of α∧¬β.) We
have already noted the connection between ﬁnding a satisfying model for a logical sentence
and ﬁnding a solution for a constraint satisfaction problem, so it is perhaps not surprising that
the two families of algorithms closely resemble the backtracking algorithms of Section 6.3
and the local search algorithms of Section 6.4. They are, however, extremely important in
their own right because so many combinatorial problems in computer science can be reduced
to checking the satisﬁability of a propositional sentence. Any improvement in satisﬁability
algorithms has huge consequences for our ability to handle complexity in general.
7.6.1 A complete backtracking algorithm
The ﬁrst algorithm we consider is often called the Davis–Putnam algorithm, after the sem-DAVIS–PUTNAM
ALGORITHM
inal paper by Martin Davis and Hilary Putnam (1960). The algorithm is in fact the version
described by Davis, Logemann, and Loveland (1962), so we will call it DPLL after the ini-
tials of all four authors. DPLL takes as input a sentence in conjunctive normal form—a set
of clauses. Like B
ACKTRACKING -SEARCH and TT-E NTAILS ?, it is essentially a recursive,
depth-ﬁrst enumeration of possible models. It embodies three improvements over the simple
scheme of TT-E
NTAILS ?:
•Early termination: The algorithm detects whether the sentence must be true or false,
even with a partially completed model. A clause is true if any literal is true, even if
the other literals do not yet have truth values; hence, the sentence as a whole could be
judged true even before the model is complete. For example, the sentence (A∨B)∧
(A∨C) is true if A is true, regardless of the values of B and C. Similarly, a sentence
is false if any clause is false, which occurs when each of its literals is false. Again, this
can occur long before the model is complete. Early termination avoids examination of
entire subtrees in the search space.
•Pure symbol heuristic:A pure symbol is a symbol that always appears with the samePURE SYMBOL
“sign” in all clauses. For example, in the three clauses (A∨¬B), (¬B∨¬C),a n d
(C∨A), the symbol A is pure because only the positive literal appears, B is pure
because only the negative literal appears, and C is impure. It is easy to see that if
a sentence has a model, then it has a model with the pure symbols assigned so as to
make their literals true, because doing so can never make a clause false. Note that, in
determining the purity of a symbol, the algorithm can ignore clauses that are already
known to be true in the model constructed so far. For example, if the model contains
B =false, then the clause (¬B∨¬C) is already true, and in the remaining clauses C
appears only as a positive literal; therefore C becomes pure.
•Unit clause heuristic :A unit clause was deﬁned earlier as a clause with just one lit-
eral. In the context of DPLL, it also means clauses in which all literals but one are
already assigned false by the model. For example, if the model contains B =true,
then (¬B∨¬C) simpliﬁes to ¬C, which is a unit clause. Obviously, for this clause
to be true, C must be set to false. The unit clause heuristic assigns all such symbols
before branching on the remainder. One important consequence of the heuristic is that
Section 7.6. Effective Propositional Model Checking 261
function DPLL-S ATISFIABLE ?(s) returns true or false
inputs: s, a sentence in propositional logic
clauses←the set of clauses in the CNF representation of s
symbols←a list of the proposition symbols in s
return DPLL(clauses,symbols,{} )
function DPLL(clauses,symbols,model) returns true or false
if every clause in clauses is true in model then return true
if some clause in clauses is false in model then return false
P,value←FIND -PURE -SYMBOL (symbols,clauses,model)
if P is non-null then return DPLL(clauses,symbols – P,model∪{P=value})
P,value←FIND -UNIT-CLAUSE (clauses,model)
if P is non-null then return DPLL(clauses,symbols – P,model∪{P=value})
P←FIRST (symbols); rest←REST (symbols)
return DPLL(clauses,rest,model∪{P=true}) or
DPLL(clauses,rest,model∪{P=false}))
Figure 7.17 The DPLL algorithm for checking satisﬁability of a sentence in propositional
logic. The ideas behind F IND -PURE -SYMBOL and F IND -UNIT-CLAUSE are described in
the text; each returns a symbol (or null) and the truth value to assign to that symbol. Like
TT-ENTAILS ?, DPLL operates over partial models.
any attempt to prove (by refutation) a literal that is already in the knowledge base will
succeed immediately (Exercise 7.23). Notice also that assigning one unit clause can
create another unit clause—for example, when C is set to false, (C∨A) becomes a
unit clause, causing true to be assigned to A. This “cascade” of forced assignments
is called unit propagation. It resembles the process of forward chaining with deﬁniteUNIT PROP AGA TION
clauses, and indeed, if the CNF expression contains only deﬁnite clauses then DPLL
essentially replicates forward chaining. (See Exercise 7.24.)
The DPLL algorithm is shown in Figure 7.17, which gives the the essential skeleton of the
search process.
What Figure 7.17 does not show are the tricks that enable SAT solvers to scale up to
large problems. It is interesting that most of these tricks are in fact rather general, and we
have seen them before in other guises:
1. Component analysis (as seen with Tasmania in CSPs): As DPLL assigns truth values
to variables, the set of clauses may become separated into disjoint subsets, called com-
ponents, that share no unassigned variables. Given an efﬁcient way to detect when this
occurs, a solver can gain considerable speed by working on each component separately.
2. Variable and value ordering (as seen in Section 6.3.1 for CSPs): Our simple imple-
mentation of DPLL uses an arbitrary variable ordering and always tries the value true
before false.T h e degree heuristic (see page 216) suggests choosing the variable that
appears most frequently over all remaining clauses.
262 Chapter 7. Logical Agents
3. Intelligent backtracking (as seen in Section 6.3 for CSPs): Many problems that can-
not be solved in hours of run time with chronological backtracking can be solved in
seconds with intelligent backtracking that backs up all the way to the relevant point of
conﬂict. All SAT solvers that do intelligent backtracking use some form of conﬂict
clause learning to record conﬂicts so that they won’t be repeated later in the search.
Usually a limited-size set of conﬂicts is kept, and rarely used ones are dropped.
4. Random restarts (as seen on page 124 for hill-climbing): Sometimes a run appears not
to be making progress. In this case, we can start over from the top of the search tree,
rather than trying to continue. After restarting, different random choices (in variable
and value selection) are made. Clauses that are learned in the ﬁrst run are retained after
the restart and can help prune the search space. Restarting does not guarantee that a
solution will be found faster, but it does reduce the variance on the time to solution.
5. Clever indexing (as seen in many algorithms): The speedup methods used in DPLL
itself, as well as the tricks used in modern solvers, require fast indexing of such things
as “the set of clauses in which variable X
i appears as a positive literal.” This task is
complicated by the fact that the algorithms are interested only in the clauses that have
not yet been satisﬁed by previous assignments to variables, so the indexing structures
must be updated dynamically as the computation proceeds.
With these enhancements, modern solvers can handle problems with tens of millions of vari-
ables. They have revolutionized areas such as hardware veriﬁcation and security protocol
veriﬁcation, which previously required laborious, hand-guided proofs.
7.6.2 Local search algorithms
We have seen several local search algorithms so far in this book, including HILL -CLIMBING
(page 122) and S IMULATED -ANNEALING (page 126). These algorithms can be applied di-
rectly to satisﬁability problems, provided that we choose the right evaluation function. Be-
cause the goal is to ﬁnd an assignment that satisﬁes every clause, an evaluation function that
counts the number of unsatisﬁed clauses will do the job. In fact, this is exactly the measure
used by the M
IN-CONFLICTS algorithm for CSPs (page 221). All these algorithms take steps
in the space of complete assignments, ﬂipping the truth value of one symbol at a time. The
space usually contains many local minima, to escape from which various forms of random-
ness are required. In recent years, there has been a great deal of experimentation to ﬁnd a
good balance between greediness and randomness.
One of the simplest and most effective algorithms to emerge from all this work is called
W
ALK SAT (Figure 7.18). On every iteration, the algorithm picks an unsatisﬁed clause and
picks a symbol in the clause to ﬂip. It chooses randomly between two ways to pick which
symbol to ﬂip: (1) a “min-conﬂicts” step that minimizes the number of unsatisﬁed clauses in
the new state and (2) a “random walk” step that picks the symbol randomly.
When W
ALK SAT returns a model, the input sentence is indeed satisﬁable, but when
it returns failure, there are two possible causes: either the sentence is unsatisﬁable or we
need to give the algorithm more time. If we set max
 ﬂips=∞and p> 0,W ALK SAT will
eventually return a model (if one exists), because the random-walk steps will eventually hit
Section 7.6. Effective Propositional Model Checking 263
function WALK SAT(clauses,p,max
 ﬂips) returns a satisfying model or failure
inputs: clauses, a set of clauses in propositional logic
p, the probability of choosing to do a “random walk” move, typically around 0.5
max
 ﬂips, number of ﬂips allowed before giving up
model←a random assignment of true/false to the symbols in clauses
for i =1 to max
 ﬂips do
if model satisﬁes clauses then return model
clause←a randomly selected clause from clauses that is false in model
with probability p ﬂip the value in model of a randomly selected symbol fromclause
else ﬂip whichever symbol in clause maximizes the number of satisﬁed clauses
return failure
Figure 7.18 The WALK SAT algorithm for checking satisﬁability by randomly ﬂipping
the values of variables. Many versions of the algorithm exist.
upon the solution. Alas, if max
 ﬂips is inﬁnity and the sentence is unsatisﬁable, then the
algorithm never terminates!
For this reason, WALK SAT is most useful when we expect a solution to exist—for ex-
ample, the problems discussed in Chapters 3 and 6 usually have solutions. On the other hand,
WALK SAT cannot always detect unsatisﬁability, which is required for deciding entailment.
For example, an agent cannot reliably use WALK SAT to prove that a square is safe in the
wumpus world. Instead, it can say, “I thought about it for an hour and couldn’t come up with
a possible world in which the square isn’t safe.” This may be a good empirical indicator that
the square is safe, but it’s certainly not a proof.
7.6.3 The landscape of random SAT problems
Some SAT problems are harder than others. Easy problems can be solved by any old algo-
rithm, but because we know that SAT is NP-complete, at least some problem instances must
require exponential run time. In Chapter 6, we saw some surprising discoveries about certain
kinds of problems. For example, the n-queens problem—thought to be quite tricky for back-
tracking search algorithms—turned out to be trivially easy for local search methods, such as
min-conﬂicts. This is because solutions are very densely distributed in the space of assign-
ments, and any initial assignment is guaranteed to have a solution nearby. Thus, n-queens is
easy because it is underconstrained.
UNDERCONSTRAINED
When we look at satisﬁability problems in conjunctive normal form, an undercon-
strained problem is one with relatively few clauses constraining the variables. For example,
here is a randomly generated 3-CNF sentence with ﬁve symbols and ﬁve clauses:
(¬D∨¬B∨C)∧(B∨¬A∨¬C)∧(¬C∨¬B∨E)
∧(E∨¬D∨B)∧(B∨E∨¬C) .
Sixteen of the 32 possible assignments are models of this sentence, so, on average, it would
take just two random guesses to ﬁnd a model. This is an easy satisﬁability problem, as are
264 Chapter 7. Logical Agents
most such underconstrained problems. On the other hand, an overconstrained problem has
many clauses relative to the number of variables and is likely to have no solutions.
To go beyond these basic intuitions, we must deﬁne exactly how random sentences
are generated. The notation CNF k(m, n) denotes a k-CNF sentence with m clauses and n
symbols, where the clauses are chosen uniformly, independently, and without replacement
from among all clauses with k different literals, which are positive or negative at random. (A
symbol may not appear twice in a clause, nor may a clause appear twice in a sentence.)
Given a source of random sentences, we can measure the probability of satisﬁability.
Figure 7.19(a) plots the probability for CNF 3(m,50), that is, sentences with 50 variables
and 3 literals per clause, as a function of the clause/symbol ratio, m/n. As we expect, for
small m/n the probability of satisﬁability is close to 1, and at large m/n the probability
is close to 0. The probability drops fairly sharply around m/n =4.3. Empirically, we ﬁnd
that the “cliff” stays in roughly the same place (for k =3 ) and gets sharper and sharper as n
increases. Theoretically, the satisﬁability threshold conjecture says that for every k ≥3,
SA TISFIABILITY
THRESHOLD
CONJECTURE
there is a threshold ratio rk such that, as n goes to inﬁnity, the probability thatCNF k(n,rn )
is satisﬁable becomes 1 for all values of r below the threshold, and 0 for all values above.
The conjecture remains unproven.
0
0.2
0.4
0.6
0.8
1
0 1 2 3 4 5 6 7 8
P(satisfiable)
Clause/symbol ratio m/n
0
200
400
600
800
1000
1200
1400
1600
1800
2000
0 1 2 3 4 5 6 7 8
Runtime
Clause/symbol ratio m/n
DPLL
WalkSAT
(a) (b)
Figure 7.19 (a) Graph showing the probability that a random 3-CNF sentence withn =50
symbols is satisﬁable, as a function of the clause/symbol ratiom/n. (b) Graph of the median
run time (measured in number of recursive calls to DPLL, a good proxy) on random 3-CNF
sentences. The most difﬁcult problems have a clause/symbol ratio of about 4.3.
Now that we have a good idea where the satisﬁable and unsatisﬁable problems are, the
next question is, where are the hard problems? It turns out that they are also often at the
threshold value. Figure 7.19(b) shows that 50-symbol problems at the threshold value of 4.3
are about 20 times more difﬁcult to solve than those at a ratio of 3.3. The underconstrained
problems are easiest to solve (because it is so easy to guess a solution); the overconstrained
problems are not as easy as the underconstrained, but still are much easier than the ones right
at the threshold.
Section 7.7. Agents Based on Propositional Logic 265
7.7 A GENTS BASED ON PROPOSITIONAL LOGIC
In this section, we bring together what we have learned so far in order to construct wumpus
world agents that use propositional logic. The ﬁrst step is to enable the agent to deduce, to the
extent possible, the state of the world given its percept history. This requires writing down a
complete logical model of the effects of actions. We also show how the agent can keep track of
the world efﬁciently without going back into the percept history for each inference. Finally,
we show how the agent can use logical inference to construct plans that are guaranteed to
achieve its goals.
7.7.1 The current state of the world
As stated at the beginning of the chapter, a logical agent operates by deducing what to do
from a knowledge base of sentences about the world. The knowledge base is composed of
axioms—general knowledge about how the world works—and percept sentences obtained
from the agent’s experience in a particular world. In this section, we focus on the problem of
deducing the current state of the wumpus world—where am I, is that square safe, and so on.
We began collecting axioms in Section 7.4.3. The agent knows that the starting square
contains no pit (¬P
1,1) and no wumpus (¬W1,1). Furthermore, for each square, it knows that
the square is breezy if and only if a neighboring square has a pit; and a square is smelly if and
only if a neighboring square has a wumpus. Thus, we include a large collection of sentences
of the following form:
B1,1 ⇔(P1,2∨P2,1)
S1,1 ⇔(W1,2∨W2,1)
···
The agent also knows that there is exactly one wumpus. This is expressed in two parts. First,
we have to say that there is at least one wumpus:
W1,1∨W1,2∨···∨W4,3∨W4,4 .
Then, we have to say that there is at most one wumpus. For each pair of locations, we add a
sentence saying that at least one of them must be wumpus-free:
¬W1,1∨¬W1,2
¬W1,1∨¬W1,3
···
¬W4,3∨¬W4,4 .
So far, so good. Now let’s consider the agent’s percepts. If there is currently a stench, one
might suppose that a proposition Stench should be added to the knowledge base. This is not
quite right, however: if there was no stench at the previous time step, then¬Stench would al-
ready be asserted, and the new assertion would simply result in a contradiction. The problem
is solved when we realize that a percept asserts something only about the current time. Thus,
if the time step (as supplied to M
AKE -PERCEPT -SENTENCE in Figure 7.1) is 4, then we add
266 Chapter 7. Logical Agents
Stench4 to the knowledge base, rather than Stench—neatly avoiding any contradiction with
¬Stench3. The same goes for the breeze, bump, glitter, and scream percepts.
The idea of associating propositions with time steps extends to any aspect of the world
that changes over time. For example, the initial knowledge base includesL0
1,1—the agent is in
square [1,1] at time 0—as well as FacingEast0, HaveArrow0,a n dWumpusAlive0.W eu s e
the word ﬂuent (from the Latin ﬂuens, ﬂowing) to refer an aspect of the world that changes.FLUENT
“Fluent” is a synonym for “state variable,” in the sense described in the discussion of factored
representations in Section 2.4.7 on page 57. Symbols associated with permanent aspects of
the world do not need a time superscript and are sometimes called atemporal variables.
A TEMPORAL
VARIABLE
We can connect stench and breeze percepts directly to the properties of the squares
where they are experienced through the location ﬂuent as follows. 10 For any time step t and
any square [x, y], we assert
Lt
x,y ⇒(Breezet ⇔Bx,y)
Lt
x,y ⇒(Stencht ⇔Sx,y) .
Now, of course, we need axioms that allow the agent to keep track of ﬂuents such as Lt
x,y.
These ﬂuents change as the result of actions taken by the agent, so, in the terminology of
Chapter 3, we need to write down the transition model of the wumpus world as a set of
logical sentences.
First, we need proposition symbols for the occurrences of actions. As with percepts,
these symbols are indexed by time; thus,Forward
0 means that the agent executes theFo r w a rd
action at time 0. By convention, the percept for a given time step happens ﬁrst, followed by
the action for that time step, followed by a transition to the next time step.
To describe how the world changes, we can try writing effect axioms that specify theEFFECT AXIOM
outcome of an action at the next time step. For example, if the agent is at location[1,1] facing
east at time 0 and goes Forward, the result is that the agent is in square [2,1] and no longer
is in [1,1]:
L0
1,1∧FacingEast0∧Forward0 ⇒(L1
2,1∧¬L1
1,1) . (7.1)
We would need one such sentence for each possible time step, for each of the 16 squares,
and each of the four orientations. We would also need similar sentences for the other actions:
Grab, Shoot, Climb, TurnLeft,a n dTurnRight.
Let us suppose that the agent does decide to move Fo r w a rdat time 0 and asserts this
fact into its knowledge base. Given the effect axiom in Equation (7.1), combined with the
initial assertions about the state at time 0, the agent can now deduce that it is in [2,1].T h a t
is, A
SK(KB,L1
2,1)= true. So far, so good. Unfortunately, the news elsewhere is less good:
if we A SK(KB,HaveArrow1), the answer is false, that is, the agent cannot prove it still
has the arrow; nor can it prove it doesn’t have it! The information has been lost because the
effect axiom fails to state what remains unchanged as the result of an action. The need to do
this gives rise to the frame problem.11 One possible solution to the frame problem wouldFRAME PROBLEM
10 Section 7.4.3 conveniently glossed over this requirement.
11 The name “frame problem” comes from “frame of reference” in physics—the assumed stationary background
with respect to which motion is measured. It also has an analogy to the frames of a movie, in which normally
most of the background stays constant while changes occur in the foreground.
Section 7.7. Agents Based on Propositional Logic 267
be to add frame axioms explicitly asserting all the propositions that remain the same. ForFRAME AXIOM
example, for each time t we would have
Forwardt ⇒(HaveArrowt ⇔HaveArrowt+1)
Forwardt ⇒(WumpusAlivet ⇔WumpusAlivet+1)
···
where we explicitly mention every proposition that stays unchanged from time t to time
t +1 under the action Forward. Although the agent now knows that it still has the arrow
after moving forward and that the wumpus hasn’t died or come back to life, the proliferation
of frame axioms seems remarkably inefﬁcient. In a world with m different actions and n
ﬂuents, the set of frame axioms will be of size O(mn). This speciﬁc manifestation of the
frame problem is sometimes called the representational frame problem . Historically, the
REPRESENTA TIONAL
FRAME PROBLEM
problem was a signiﬁcant one for AI researchers; we explore it further in the notes at the end
of the chapter.
The representational frame problem is signiﬁcant because the real world has very many
ﬂuents, to put it mildly. Fortunately for us humans, each action typically changes no more
than some small number k of those ﬂuents—the world exhibits locality. Solving the repre-
LOCALITY
sentational frame problem requires deﬁning the transition model with a set of axioms of size
O(mk) rather than size O(mn). There is also an inferential frame problem : the problemINFERENTIAL FRAME
PROBLEM
of projecting forward the results of a t step plan of action in time O(kt) rather than O(nt).
The solution to the problem involves changing one’s focus from writing axioms about
actions to writing axioms about ﬂuents. Thus, for each ﬂuent F , we will have an axiom that
deﬁnes the truth value ofFt+1 in terms of ﬂuents (includingF itself) at timet and the actions
that may have occurred at timet. Now, the truth value ofFt+1 can be set in one of two ways:
either the action at time t causes F to be true at t +1 ,o r F was already true at time t and the
action at timet does not cause it to be false. An axiom of this form is called asuccessor-state
axiom and has this schema:SUCCESSOR-STA TE
AXIOM
Ft+1 ⇔ActionCausesFt∨(Ft∧¬ActionCausesNotFt) .
One of the simplest successor-state axioms is the one for HaveArrow. Because there is no
action for reloading, the ActionCausesFt part goes away and we are left with
HaveArrowt+1 ⇔(HaveArrowt∧¬Shoott) . (7.2)
For the agent’s location, the successor-state axioms are more elaborate. For example, Lt+1
1,1
is true if either (a) the agent moved Fo r w a rdfrom [1,2] when facing south, or from [2,1]
when facing west; or (b) Lt
1,1 was already true and the action did not cause movement (either
because the action was not Forward or because the action bumped into a wall). Written out
in propositional logic, this becomes
Lt+1
1,1 ⇔ (Lt
1,1∧(¬Forwardt∨Bumpt+1))
∨(Lt
1,2∧(Southt∧Forwardt)) (7.3)
∨(Lt
2,1∧(Westt∧Forwardt)) .
Exercise 7.26 asks you to write out axioms for the remaining wumpus world ﬂuents.
268 Chapter 7. Logical Agents
Given a complete set of successor-state axioms and the other axioms listed at the begin-
ning of this section, the agent will be able to ASK and answer any answerable question about
the current state of the world. For example, in Section 7.2 the initial sequence of percepts and
actions is
¬Stench
0∧¬Breeze0∧¬Glitter0∧¬Bump0∧¬Scream0 ; Forward0
¬Stench1∧Breeze1∧¬Glitter1∧¬Bump1∧¬Scream1 ; TurnRight1
¬Stench2∧Breeze2∧¬Glitter2∧¬Bump2∧¬Scream2 ; TurnRight2
¬Stench3∧Breeze3∧¬Glitter3∧¬Bump3∧¬Scream3 ; Forward3
¬Stench4∧¬Breeze4∧¬Glitter4∧¬Bump4∧¬Scream4 ; TurnRight4
¬Stench5∧¬Breeze5∧¬Glitter5∧¬Bump5∧¬Scream5 ; Forward5
Stench6∧¬Breeze6∧¬Glitter6∧¬Bump6∧¬Scream6
At this point, we have A SK(KB,L6
1,2)= true, so the agent knows where it is. Moreover,
ASK(KB,W1,3)= true and ASK(KB,P3,1)= true, so the agent has found the wumpus and
one of the pits. The most important question for the agent is whether a square is OK to move
into, that is, the square contains no pit nor live wumpus. It’s convenient to add axioms for
this, having the form
OK
t
x,y ⇔¬Px,y ∧¬(Wx,y ∧WumpusAlivet) .
Finally, ASK(KB,OK6
2,2)= true, so the square [2,2] is OK to move into. In fact, given a
sound and complete inference algorithm such as DPLL, the agent can answer any answerable
question about which squares are OK—and can do so in just a few milliseconds for small-to-
medium wumpus worlds.
Solving the representational and inferential frame problems is a big step forward, but
a pernicious problem remains: we need to conﬁrm that all the necessary preconditions of an
action hold for it to have its intended effect. We said that theForward action moves the agent
ahead unless there is a wall in the way, but there are many other unusual exceptions that could
cause the action to fail: the agent might trip and fall, be stricken with a heart attack, be carried
away by giant bats, etc. Specifying all these exceptions is called the qualiﬁcation problem.
QUALIFICA TION
PROBLEM
There is no complete solution within logic; system designers have to use good judgment in
deciding how detailed they want to be in specifying their model, and what details they want
to leave out. We will see in Chapter 13 that probability theory allows us to summarize all the
exceptions without explicitly naming them.
7.7.2 A hybrid agent
The ability to deduce various aspects of the state of the world can be combined fairly straight-
forwardly with condition–action rules and with problem-solving algorithms from Chapters 3
and 4 to produce a hybrid agent for the wumpus world. Figure 7.20 shows one possible way
HYBRID AGENT
to do this. The agent program maintains and updates a knowledge base as well as a current
plan. The initial knowledge base contains the atemporal axioms—those that don’t depend
on t, such as the axiom relating the breeziness of squares to the presence of pits. At each
time step, the new percept sentence is added along with all the axioms that depend ont,s u c h
Section 7.7. Agents Based on Propositional Logic 269
as the successor-state axioms. (The next section explains why the agent doesn’t need axioms
for future time steps.) Then, the agent uses logical inference, by A SKing questions of the
knowledge base, to work out which squares are safe and which have yet to be visited.
The main body of the agent program constructs a plan based on a decreasing priority of
goals. First, if there is a glitter, the program constructs a plan to grab the gold, follow a route
back to the initial location, and climb out of the cave. Otherwise, if there is no current plan,
the program plans a route to the closest safe square that it has not visited yet, making sure
the route goes through only safe squares. Route planning is done with A
∗ search, not with
ASK. If there are no safe squares to explore, the next step—if the agent still has an arrow—is
to try to make a safe square by shooting at one of the possible wumpus locations. These are
determined by asking where A
SK(KB,¬Wx,y) is false—that is, where it is not known that
there is not a wumpus. The function P LAN -SHOT (not shown) uses PLAN -ROUTE to plan a
sequence of actions that will line up this shot. If this fails, the program looks for a square to
explore that is not provably unsafe—that is, a square for which A
SK(KB,¬OKt
x,y) returns
false. If there is no such square, then the mission is impossible and the agent retreats to [1,1]
and climbs out of the cave.
7.7.3 Logical state estimation
The agent program in Figure 7.20 works quite well, but it has one major weakness: as time
goes by, the computational expense involved in the calls to A
SK goes up and up. This happens
mainly because the required inferences have to go back further and further in time and involve
more and more proposition symbols. Obviously, this is unsustainable—we cannot have an
agent whose time to process each percept grows in proportion to the length of its life! What
we really need is a constant update time—that is, independent of t. The obvious answer is to
save, or cache, the results of inference, so that the inference process at the next time step canCACHING
build on the results of earlier steps instead of having to start again from scratch.
As we saw in Section 4.4, the past history of percepts and all their ramiﬁcations can
be replaced by the belief state—that is, some representation of the set of all possible current
states of the world.12 The process of updating the belief state as new percepts arrive is called
state estimation. Whereas in Section 4.4 the belief state was an explicit list of states, here
we can use a logical sentence involving the proposition symbols associated with the current
time step, as well as the atemporal symbols. For example, the logical sentence
WumpusAlive
1∧L1
2,1∧B2,1∧(P3,1∨P2,2) (7.4)
represents the set of all states at time 1 in which the wumpus is alive, the agent is at [2,1],
that square is breezy, and there is a pit in [3,1] or [2,2] or both.
Maintaining an exact belief state as a logical formula turns out not to be easy. If there
are n ﬂuent symbols for timet, then there are2n possible states—that is, assignments of truth
values to those symbols. Now, the set of belief states is the powerset (set of all subsets) of the
set of physical states. There are 2
n physical states, hence 22n
belief states. Even if we used
the most compact possible encoding of logical formulas, with each belief state represented
12 We can think of the percept history itself as a representation of the belief state, but one that makes inference
increasingly expensive as the history gets longer.
270 Chapter 7. Logical Agents
function HYBRID -WUMPUS -AGENT (percept) returns an action
inputs: percept, a list, [stench,breeze,glitter,bump,scream]
persistent: KB, a knowledge base, initially the atemporal “wumpus physics”
t, a counter, initially 0, indicating time
plan, an action sequence, initially empty
TELL (KB,M AKE -PERCEPT -SENTENCE (percept,t))
TELL the KB the temporal “physics” sentences for time t
safe←{[x,y]: ASK(KB,OKt
x,y)= true}
if ASK(KB,Glittert)= true then
plan←[Grab]+P LAN -ROUTE (current,{[1,1]},safe)+[ Climb]
if plan is empty then
unvisited←{[x,y]: ASK(KB,Lt′
x,y)= false for all t′ ≤t}
plan←PLAN -ROUTE (current,unvisited∩safe,safe)
if plan is empty and ASK(KB,HaveArrowt)= true then
possible
 wumpus←{[x,y]: ASK(KB,¬ Wx,y)= false}
plan←PLAN -SHOT(current,possible
 wumpus,safe)
if plan is empty then // no choice but to take a risk
not
 unsafe←{[x,y]: ASK(KB,¬ OKt
x,y)= false}
plan←PLAN -ROUTE (current,unvisited∩not
 unsafe,safe)
if plan is empty then
plan←PLAN -ROUTE (current,{[1, 1]},safe)+ [Climb]
action←POP(plan)
TELL (KB,M AKE -ACTION -SENTENCE (action,t))
t←t +1
return action
function PLAN -ROUTE (current,goals,allowed) returns an action sequence
inputs: current, the agent’s current position
goals, a set of squares; try to plan a route to one of them
allowed, a set of squares that can form part of the route
problem←ROUTE -PROBLEM (current,goals,allowed)
return A*-G RAPH -SEARCH (problem)
Figure 7.20 A hybrid agent program for the wumpus world. It uses a propositional knowl-
edge base to infer the state of the world, and a combination of problem-solving search and
domain-speciﬁc code to decide what actions to take.
by a unique binary number, we would need numbers with log2(22n
)=2 n bits to label the
current belief state. That is, exact state estimation may require logical formulas whose size is
exponential in the number of symbols.
One very common and natural scheme for approximate state estimation is to represent
belief states as conjunctions of literals, that is, 1-CNF formulas. To do this, the agent program
simply tries to prove X
t and¬Xt for each symbol Xt (as well as each atemporal symbol
whose truth value is not yet known), given the belief state at t−1. The conjunction of
Section 7.7. Agents Based on Propositional Logic 271
Figure 7.21 Depiction of a 1-CNF belief state (bold outline) as a simply representable,
conservative approximation to the exact (wi ggly) belief state (shaded region with dashed
outline). Each possible world is shown as a circle; the shaded ones are consistent with all the
percepts.
provable literals becomes the new belief state, and the previous belief state is discarded.
It is important to understand that this scheme may lose some information as time goes
along. For example, if the sentence in Equation (7.4) were the true belief state, then neither
P
3,1 nor P2,2 would be provable individually and neither would appear in the 1-CNF belief
state. (Exercise 7.27 explores one possible solution to this problem.) On the other hand,
because every literal in the 1-CNF belief state is proved from the previous belief state, and
the initial belief state is a true assertion, we know that entire 1-CNF belief state must be
true. Thus, the set of possible states represented by the 1-CNF belief state includes all states
that are in fact possible given the full percept history. As illustrated in Figure 7.21, the 1-
CNF belief state acts as a simple outer envelope, orconservative approximation, around theCONSERVA TIVE
APPROXIMA TION
exact belief state. We see this idea of conservative approximations to complicated sets as a
recurring theme in many areas of AI.
7.7.4 Making plans by propositional inference
The agent in Figure 7.20 uses logical inference to determine which squares are safe, but uses
A
∗ search to make plans. In this section, we show how to make plans by logical inference.
The basic idea is very simple:
1. Construct a sentence that includes
(a) Init0, a collection of assertions about the initial state;
(b) Transition1,..., Transitiont, the successor-state axioms for all possible actions
at each time up to some maximum time t;
(c) the assertion that the goal is achieved at time t: HaveGoldt∧ClimbedOutt.
272 Chapter 7. Logical Agents
2. Present the whole sentence to a SAT solver. If the solver ﬁnds a satisfying model, then
the goal is achievable; if the sentence is unsatisﬁable, then the planning problem is
impossible.
3. Assuming a model is found, extract from the model those variables that represent ac-
tions and are assigned true. Together they represent a plan to achieve the goals.
A propositional planning procedure, SATP
LAN , is shown in Figure 7.22. It implements the
basic idea just given, with one twist. Because the agent does not know how many steps it
will take to reach the goal, the algorithm tries each possible number of steps t,u pt os o m e
maximum conceivable plan lengthT
max. In this way, it is guaranteed to ﬁnd the shortest plan
if one exists. Because of the way SATP LAN searches for a solution, this approach cannot
be used in a partially observable environment; SATP LAN would just set the unobservable
variables to the values it needs to create a solution.
function SATPLAN (init, transition, goal,T max) returns solution or failure
inputs: init, transition, goal, constitute a description of the problem
T max, an upper limit for plan length
for t =0 to T max do
cnf←TRANSLATE -TO-SAT(init, transition, goal,t)
model←SAT-SOLVER (cnf )
if model is not null then
return EXTRACT -SOLUTION (model)
return failure
Figure 7.22 The SATP LAN algorithm. The planning problem is translated into a CNF
sentence in which the goal is asserted to hold at a ﬁxed time step t and axioms are included
for each time step up tot. If the satisﬁability algorithm ﬁnds a model, then a plan is extracted
by looking at those proposition symbols t hat refer to actions and are assigned true in the
model. If no model exists, then the process is repeated with the goal moved one step later.
The key step in using SATP LAN is the construction of the knowledge base. It might
seem, on casual inspection, that the wumpus world axioms in Section 7.7.1 sufﬁce for steps
1(a) and 1(b) above. There is, however, a signiﬁcant difference between the requirements for
entailment (as tested by A
SK) and those for satisﬁability. Consider, for example, the agent’s
location, initially [1,1], and suppose the agent’s unambitious goal is to be in [2,1] at time 1.
The initial knowledge base containsL0
1,1 and the goal is L1
2,1.U s i n gASK, we can prove L1
2,1
if Fo r w a rd0 is asserted, and, reassuringly, we cannot prove L1
2,1 if, say, Shoot0 is asserted
instead. Now, SATP LAN will ﬁnd the plan [Fo r w a rd0]; so far, so good. Unfortunately,
SATPLAN also ﬁnds the plan [Shoot0]. How could this be? To ﬁnd out, we inspect the model
that SATP LAN constructs: it includes the assignment L0
2,1, that is, the agent can be in [2,1]
at time 1 by being there at time 0 and shooting. One might ask, “Didn’t we say the agent is in
[1,1] at time 0?” Yes, we did, but we didn’t tell the agent that it can’t be in two places at once!
For entailment, L
0
2,1 is unknown and cannot, therefore, be used in a proof; for satisﬁability,
Section 7.7. Agents Based on Propositional Logic 273
on the other hand, L0
2,1 is unknown and can, therefore, be set to whatever value helps to
make the goal true. For this reason, SATPLAN is a good debugging tool for knowledge bases
because it reveals places where knowledge is missing. In this particular case, we can ﬁx the
knowledge base by asserting that, at each time step, the agent is in exactly one location, using
a collection of sentences similar to those used to assert the existence of exactly one wumpus.
Alternatively, we can assert¬L
0
x,y for all locations other than[1,1]; the successor-state axiom
for location takes care of subsequent time steps. The same ﬁxes also work to make sure the
agent has only one orientation.
SATP
LAN has more surprises in store, however. The ﬁrst is that it ﬁnds models with
impossible actions, such as shooting with no arrow. To understand why, we need to look more
carefully at what the successor-state axioms (such as Equation (7.3)) say about actions whose
preconditions are not satisﬁed. The axiomsdo predict correctly that nothing will happen when
such an action is executed (see Exercise 10.14), but they do not say that the action cannot be
executed! To avoid generating plans with illegal actions, we must add precondition axioms
PRECONDITION
AXIOMS
stating that an action occurrence requires the preconditions to be satisﬁed.13 For example, we
need to say, for each time t,t h a t
Shoott ⇒HaveArrowt .
This ensures that if a plan selects the Shoot action at any time, it must be the case that the
agent has an arrow at that time.
SATPLAN ’s second surprise is the creation of plans with multiple simultaneous actions.
For example, it may come up with a model in which both Forward0 and Shoot0 are true,
which is not allowed. To eliminate this problem, we introduce action exclusion axioms:f o rACTION EXCLUSION
AXIOM
every pair of actions At
i and At
j we add the axiom
¬At
i ∨¬At
j .
It might be pointed out that walking forward and shooting at the same time is not so hard to
do, whereas, say, shooting and grabbing at the same time is rather impractical. By imposing
action exclusion axioms only on pairs of actions that really do interfere with each other, we
can allow for plans that include multiple simultaneous actions—and because SATP
LAN ﬁnds
the shortest legal plan, we can be sure that it will take advantage of this capability.
To summarize, SATP LAN ﬁnds models for a sentence containing the initial state, the
goal, the successor-state axioms, the precondition axioms, and the action exclusion axioms.
It can be shown that this collection of axioms is sufﬁcient, in the sense that there are no
longer any spurious “solutions.” Any model satisfying the propositional sentence will be a
valid plan for the original problem. Modern SAT-solving technology makes the approach
quite practical. For example, a DPLL-style solver has no difﬁculty in generating the 11-step
solution for the wumpus world instance shown in Figure 7.2.
This section has described a declarative approach to agent construction: the agent works
by a combination of asserting sentences in the knowledge base and performing logical infer-
ence. This approach has some weaknesses hidden in phrases such as “for each time t”a n d
13 Notice that the addition of precondition axioms means that we need not include preconditions for actions in
the successor-state axioms.
274 Chapter 7. Logical Agents
“for each square [x, y].” For any practical agent, these phrases have to be implemented by
code that generates instances of the general sentence schema automatically for insertion into
the knowledge base. For a wumpus world of reasonable size—one comparable to a smallish
computer game—we might need a 100× 100 board and 1000 time steps, leading to knowl-
edge bases with tens or hundreds of millions of sentences. Not only does this become rather
impractical, but it also illustrates a deeper problem: we know something about the wum-
pus world—namely, that the “physics” works the same way across all squares and all time
steps—that we cannot express directly in the language of propositional logic. To solve this
problem, we need a more expressive language, one in which phrases like “for each time t”
and “for each square [x, y]” can be written in a natural way. First-order logic, described in
Chapter 8, is such a language; in ﬁrst-order logic a wumpus world of any size and duration
can be described in about ten sentences rather than ten million or ten trillion.
7.8 S UMMARY
We have introduced knowledge-based agents and have shown how to deﬁne a logic with
which such agents can reason about the world. The main points are as follows:
•Intelligent agents need knowledge about the world in order to reach good decisions.
•Knowledge is contained in agents in the form of sentences in a knowledge represen-
tation language that are stored in a knowledge base.
•A knowledge-based agent is composed of a knowledge base and an inference mecha-
nism. It operates by storing sentences about the world in its knowledge base, using the
inference mechanism to infer new sentences, and using these sentences to decide what
action to take.
•A representation language is deﬁned by its syntax, which speciﬁes the structure of
sentences, and its semantics, which deﬁnes the truth of each sentence in each possible
world or model.
•The relationship of entailment between sentences is crucial to our understanding of
reasoning. A sentence α entails another sentence β if β is true in all worlds where
α is true. Equivalent deﬁnitions include the validity of the sentence α ⇒β and the
unsatisﬁability of the sentence α∧¬β.
•Inference is the process of deriving new sentences from old ones.Sound inference algo-
rithms derive only sentences that are entailed; complete algorithms derive all sentences
that are entailed.
•Propositional logic is a simple language consisting ofproposition symbols and logical
connectives. It can handle propositions that are known true, known false, or completely
unknown.
•The set of possible models, given a ﬁxed propositional vocabulary, is ﬁnite, so en-
tailment can be checked by enumerating models. Efﬁcient model-checking inference
algorithms for propositional logic include backtracking and local search methods and
can often solve large problems quickly.
Bibliographical and Historical Notes 275
•Inference rules are patterns of sound inference that can be used to ﬁnd proofs. The
resolution rule yields a complete inference algorithm for knowledge bases that are
expressed in conjunctive normal form . Forward chaining and backward chaining
are very natural reasoning algorithms for knowledge bases in Horn form.
•Local search methods such as W ALK SAT can be used to ﬁnd solutions. Such algo-
rithms are sound but not complete.
•Logical state estimation involves maintaining a logical sentence that describes the set
of possible states consistent with the observation history. Each update step requires
inference using the transition model of the environment, which is built fromsuccessor-
state axioms that specify how each ﬂuent changes.
•Decisions within a logical agent can be made by SAT solving: ﬁnding possible models
specifying future action sequences that reach the goal. This approach works only for
fully observable or sensorless environments.
•Propositional logic does not scale to environments of unbounded size because it lacks
the expressive power to deal concisely with time, space, and universal patterns of rela-
tionships among objects.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
John McCarthy’s paper “Programs with Common Sense” (McCarthy, 1958, 1968) promul-
gated the notion of agents that use logical reasoning to mediate between percepts and actions.
It also raised the ﬂag of declarativism, pointing out that telling an agent what it needs to know
is an elegant way to build software. Allen Newell’s (1982) article “The Knowledge Level”
makes the case that rational agents can be described and analyzed at an abstract level deﬁned
by the knowledge they possess rather than the programs they run. The declarative and proce-
dural approaches to AI are analyzed in depth by Boden (1977). The debate was revived by,
among others, Brooks (1991) and Nilsson (1991), and continues to this day (Shaparau et al.,
2008). Meanwhile, the declarative approach has spread into other areas of computer science
such as networking (Loo et al., 2006).
Logic itself had its origins in ancient Greek philosophy and mathematics. Various log-
ical principles—principles connecting the syntactic structure of sentences with their truth
and falsity, with their meaning, or with the validity of arguments in which they ﬁgure—are
scattered in the works of Plato. The ﬁrst known systematic study of logic was carried out
by Aristotle, whose work was assembled by his students after his death in 322
B.C.a sa
treatise called the Organon. Aristotle’s syllogisms were what we would now call inferenceSYLLOGISM
rules. Although the syllogisms included elements of both propositional and ﬁrst-order logic,
the system as a whole lacked the compositional properties required to handle sentences of
arbitrary complexity.
The closely related Megarian and Stoic schools (originating in the ﬁfth century B.C.
and continuing for several centuries thereafter) began the systematic study of the basic logical
connectives. The use of truth tables for deﬁning connectives is due to Philo of Megara. The
276 Chapter 7. Logical Agents
Stoics took ﬁve basic inference rules as valid without proof, including the rule we now call
Modus Ponens. They derived a number of other rules from these ﬁve, using, among other
principles, the deduction theorem (page 249) and were much clearer about the notion of
proof than was Aristotle. A good account of the history of Megarian and Stoic logic is given
by Benson Mates (1953).
The idea of reducing logical inference to a purely mechanical process applied to a for-
mal language is due to Wilhelm Leibniz (1646–1716), although he had limited success in im-
plementing the ideas. George Boole (1847) introduced the ﬁrst comprehensive and workable
system of formal logic in his book The Mathematical Analysis of Logic . Boole’s logic was
closely modeled on the ordinary algebra of real numbers and used substitution of logically
equivalent expressions as its primary inference method. Although Boole’s system still fell
short of full propositional logic, it was close enough that other mathematicians could quickly
ﬁll in the gaps. Schr¨oder (1877) described conjunctive normal form, while Horn form was
introduced much later by Alfred Horn (1951). The ﬁrst comprehensive exposition of modern
propositional logic (and ﬁrst-order logic) is found in Gottlob Frege’s (1879) Begriffschrift
(“Concept Writing” or “Conceptual Notation”).
The ﬁrst mechanical device to carry out logical inferences was constructed by the third
Earl of Stanhope (1753–1816). The Stanhope Demonstrator could handle syllogisms and
certain inferences in the theory of probability. William Stanley Jevons, one of those who
improved upon and extended Boole’s work, constructed his “logical piano” in 1869 to per-
form inferences in Boolean logic. An entertaining and instructive history of these and other
early mechanical devices for reasoning is given by Martin Gardner (1968). The ﬁrst pub-
lished computer program for logical inference was the Logic Theorist of Newell, Shaw,
and Simon (1957). This program was intended to model human thought processes. Mar-
tin Davis (1957) had actually designed a program that came up with a proof in 1954, but the
Logic Theorist’s results were published slightly earlier.
Truth tables as a method of testing validity or unsatisﬁability in propositional logic were
introduced independently by Emil Post (1921) and Ludwig Wittgenstein (1922). In the 1930s,
a great deal of progress was made on inference methods for ﬁrst-order logic. In particular,
G¨odel (1930) showed that a complete procedure for inference in ﬁrst-order logic could be
obtained via a reduction to propositional logic, using Herbrand’s theorem (Herbrand, 1930).
We take up this history again in Chapter 9; the important point here is that the development
of efﬁcient propositional algorithms in the 1960s was motivated largely by the interest of
mathematicians in an effective theorem prover for ﬁrst-order logic. The Davis–Putnam algo-
rithm (Davis and Putnam, 1960) was the ﬁrst effective algorithm for propositional resolution
but was in most cases much less efﬁcient than the DPLL backtracking algorithm introduced
two years later (1962). The full resolution rule and a proof of its completeness appeared in a
seminal paper by J. A. Robinson (1965), which also showed how to do ﬁrst-order reasoning
without resort to propositional techniques.
Stephen Cook (1971) showed that deciding satisﬁability of a sentence in propositional
logic (the SAT problem) is NP-complete. Since deciding entailment is equivalent to decid-
ing unsatisﬁability, it is co-NP-complete. Many subsets of propositional logic are known for
which the satisﬁability problem is polynomially solvable; Horn clauses are one such subset.
Bibliographical and Historical Notes 277
The linear-time forward-chaining algorithm for Horn clauses is due to Dowling and Gallier
(1984), who describe their algorithm as a dataﬂow process similar to the propagation of sig-
nals in a circuit.
Early theoretical investigations showed that DPLL has polynomial average-case com-
plexity for certain natural distributions of problems. This potentially exciting fact became
less exciting when Franco and Paull (1983) showed that the same problems could be solved
in constant time simply by guessing random assignments. The random-generation method
described in the chapter produces much harder problems. Motivated by the empirical success
of local search on these problems, Koutsoupias and Papadimitriou (1992) showed that a sim-
ple hill-climbing algorithm can solve almost all satisﬁability problem instances very quickly,
suggesting that hard problems are rare. Moreover, Sch¨oning (1999) exhibited a randomized
hill-climbing algorithm whose worst-case expected run time on 3-SAT problems (that is, sat-
isﬁability of 3-CNF sentences) is O(1.333
n)—still exponential, but substantially faster than
previous worst-case bounds. The current record is O(1.324n) (Iwama and Tamaki, 2004).
Achlioptas et al. (2004) and Alekhnovich et al. (2005) exhibit families of 3-SAT instances
for which all known DPLL-like algorithms require exponential running time.
On the practical side, efﬁciency gains in propositional solvers have been marked. Given
ten minutes of computing time, the original DPLL algorithm in 1962 could only solve prob-
lems with no more than 10 or 15 variables. By 1995 the S ATZ solver (Li and Anbulagan,
1997) could handle 1,000 variables, thanks to optimized data structures for indexing vari-
ables. Two crucial contributions were the watched literal indexing technique of Zhang and
Stickel (1996), which makes unit propagation very efﬁcient, and the introduction of clause
(i.e., constraint) learning techniques from the CSP community by Bayardo and Schrag (1997).
Using these ideas, and spurred by the prospect of solving industrial-scale circuit veriﬁcation
problems, Moskewicz et al. (2001) developed the C
HAFF solver, which could handle prob-
lems with millions of variables. Beginning in 2002, SAT competitions have been held reg-
ularly; most of the winning entries have either been descendants of C
HAFF or have used the
same general approach. RS AT (Pipatsrisawat and Darwiche, 2007), the 2007 winner, falls in
the latter category. Also noteworthy is MINI SAT (Een and S¨orensson, 2003), an open-source
implementation available at http://minisat.se that is designed to be easily modiﬁed
and improved. The current landscape of solvers is surveyed by Gomes et al. (2008).
Local search algorithms for satisﬁability were tried by various authors throughout the
1980s; all of the algorithms were based on the idea of minimizing the number of unsatisﬁed
clauses (Hansen and Jaumard, 1990). A particularly effective algorithm was developed by
Gu (1989) and independently by Selman et al. (1992), who called it GSAT and showed that
it was capable of solving a wide range of very hard problems very quickly. The W
ALK SAT
algorithm described in the chapter is due to Selman et al. (1996).
The “phase transition” in satisﬁability of random k-SAT problems was ﬁrst observed
by Simon and Dubois (1989) and has given rise to a great deal of theoretical and empirical
research—due, in part, to the obvious connection to phase transition phenomena in statistical
physics. Cheeseman et al. (1991) observed phase transitions in several CSPs and conjecture
that all NP-hard problems have a phase transition. Crawford and Auton (1993) located the
3-SAT transition at a clause/variable ratio of around 4.26, noting that this coincides with a
278 Chapter 7. Logical Agents
sharp peak in the run time of their SAT solver. Cook and Mitchell (1997) provide an excellent
summary of the early literature on the problem.
The current state of theoretical understanding is summarized by Achlioptas (2009).
The satisﬁability threshold conjecture states that, for each k, there is a sharp satisﬁability
SA TISFIABILITY
THRESHOLD
CONJECTURE
threshold rk, such that as the number of variables n→∞, instances below the threshold are
satisﬁable with probability 1, while those above the threshold are unsatisﬁable with proba-
bility 1. The conjecture was not quite proved by Friedgut (1999): a sharp threshold exists but
its location might depend on n even as n→∞. Despite signiﬁcant progress in asymptotic
analysis of the threshold location for large k (Achlioptas and Peres, 2004; Achlioptas et al.,
2007), all that can be proved for k =3 is that it lies in the range [3.52,4.51]. Current theory
suggests that a peak in the run time of a SAT solver is not necessarily related to the satisﬁa-
bility threshold, but instead to a phase transition in the solution distribution and structure of
SAT instances. Empirical results due to Coarfa et al. (2003) support this view. In fact, al-
gorithms such as survey propagation (Parisi and Zecchina, 2002; Maneva et al., 2007) take
SURVEY
PROP AGA TION
advantage of special properties of random SAT instances near the satisﬁability threshold and
greatly outperform general SAT solvers on such instances.
The best sources for information on satisﬁability, both theoretical and practical, are the
Handbook of Satisﬁability (Biere et al., 2009) and the regular International Conferences on
Theory and Applications of Satisﬁability Testing , known as SAT.
The idea of building agents with propositional logic can be traced back to the seminal
paper of McCulloch and Pitts (1943), which initiated the ﬁeld of neural networks. Con-
trary to popular supposition, the paper was concerned with the implementation of a Boolean
circuit-based agent design in the brain. Circuit-based agents, which perform computation by
propagating signals in hardware circuits rather than running algorithms in general-purpose
computers, have received little attention in AI, however. The most notable exception is the
work of Stan Rosenschein (Rosenschein, 1985; Kaelbling and Rosenschein, 1990), who de-
veloped ways to compile circuit-based agents from declarative descriptions of the task envi-
ronment. (Rosenschein’s approach is described at some length in the second edition of this
book.) The work of Rod Brooks (1986, 1989) demonstrates the effectiveness of circuit-based
designs for controlling robots—a topic we take up in Chapter 25. Brooks (1991) argues
that circuit-based designs are all that is needed for AI—that representation and reasoning
are cumbersome, expensive, and unnecessary. In our view, neither approach is sufﬁcient by
itself. Williams et al. (2003) show how a hybrid agent design not too different from our
wumpus agent has been used to control NASA spacecraft, planning sequences of actions and
diagnosing and recovering from faults.
The general problem of keeping track of a partially observable environment was intro-
duced for state-based representations in Chapter 4. Its instantiation for propositional repre-
sentations was studied by Amir and Russell (2003), who identiﬁed several classes of envi-
ronments that admit efﬁcient state-estimation algorithms and showed that for several other
classes the problem is intractable. The temporal-projection problem, which involves deter-
TEMPORAL-
PROJECTION
mining what propositions hold true after an action sequence is executed, can be seen as a
special case of state estimation with empty percepts. Many authors have studied this problem
because of its importance in planning; some important hardness results were established by
Exercises 279
Liberatore (1997). The idea of representing a belief state with propositions can be traced to
Wittgenstein (1922).
Logical state estimation, of course, requires a logical representation of the effects of
actions—a key problem in AI since the late 1950s. The dominant proposal has been the sit-
uation calculus formalism (McCarthy, 1963), which is couched within ﬁrst-order logic. We
discuss situation calculus, and various extensions and alternatives, in Chapters 10 and 12. The
approach taken in this chapter—using temporal indices on propositional variables—is more
restrictive but has the beneﬁt of simplicity. The general approach embodied in the SATP
LAN
algorithm was proposed by Kautz and Selman (1992). Later generations of SATP LAN were
able to take advantage of the advances in SAT solvers, described earlier, and remain among
the most effective ways of solving difﬁcult problems (Kautz, 2006).
The frame problem was ﬁrst recognized by McCarthy and Hayes (1969). Many re-
searchers considered the problem unsolvable within ﬁrst-order logic, and it spurred a great
deal of research into nonmonotonic logics. Philosophers from Dreyfus (1972) to Crockett
(1994) have cited the frame problem as one symptom of the inevitable failure of the entire
AI enterprise. The solution of the frame problem with successor-state axioms is due to Ray
Reiter (1991). Thielscher (1999) identiﬁes the inferential frame problem as a separate idea
and provides a solution. In retrospect, one can see that Rosenschein’s (1985) agents were
using circuits that implemented successor-state axioms, but Rosenschein did not notice that
the frame problem was thereby largely solved. Foo (2001) explains why the discrete-event
control theory models typically used by engineers do not have to explicitly deal with the
frame problem: because they are dealing with prediction and control, not with explanation
and reasoning about counterfactual situations.
Modern propositional solvers have wide applicability in industrial applications. The ap-
plication of propositional inference in the synthesis of computer hardware is now a standard
technique having many large-scale deployments (Nowick et al., 1993). The SATMC satisﬁ-
ability checker was used to detect a previously unknown vulnerability in a Web browser user
sign-on protocol (Armando et al., 2008).
The wumpus world was invented by Gregory Yob (1975). Ironically, Yob developed it
because he was bored with games played on a rectangular grid: the topology of his original
wumpus world was a dodecahedron, and we put it back in the boring old grid. Michael
Genesereth was the ﬁrst to suggest that the wumpus world be used as an agent testbed.
EXERCISES
7.1 Suppose the agent has progressed to the point shown in Figure 7.4(a), page 239, having
perceived nothing in [1,1], a breeze in [2,1], and a stench in [1,2], and is now concerned with
the contents of [1,3], [2,2], and [3,1]. Each of these can contain a pit, and at most one can
contain a wumpus. Following the example of Figure 7.5, construct the set of possible worlds.
(You should ﬁnd 32 of them.) Mark the worlds in which the KB is true and those in which
280 Chapter 7. Logical Agents
each of the following sentences is true:
α2 = “There is no pit in [2,2].”
α3 = “There is a wumpus in [1,3].”
Hence show that KB|= α2 and KB|= α3.
7.2 (Adapted from Barwise and Etchemendy (1993).) Given the following, can you prove
that the unicorn is mythical? How about magical? Horned?
If the unicorn is mythical, then it is immortal, but if it is not mythical, then it is a
mortal mammal. If the unicorn is either immortal or a mammal, then it is horned.
The unicorn is magical if it is horned.
7.3 Consider the problem of deciding whether a propositional logic sentence is true in a
given model.
a. Write a recursive algorithm PL-T
RUE ?(s,m) that returns true if and only if the sen-
tence s is true in the model m (where m assigns a truth value for every symbol in s).
The algorithm should run in time linear in the size of the sentence. (Alternatively, use a
version of this function from the online code repository.)
b. Give three examples of sentences that can be determined to be true or false in a partial
model that does not specify a truth value for some of the symbols.
c. Show that the truth value (if any) of a sentence in a partial model cannot be determined
efﬁciently in general.
d. Modify your PL-T
RUE ? algorithm so that it can sometimes judge truth from partial
models, while retaining its recursive structure and linear run time. Give three examples
of sentences whose truth in a partial model is not detected by your algorithm.
e. Investigate whether the modiﬁed algorithm makes TT-E
NTAILS ? more efﬁcient.
7.4 Which of the following are correct?
a. False|= True.
b. True|= False.
c. (A∧B)|=( A ⇔B).
d. A ⇔B|= A∨B.
e. A ⇔B|=¬A∨B.
f. (A∧B) ⇒C|=( A ⇒C)∨(B ⇒C).
g. (C∨(¬A∧¬B))≡((A ⇒C)∧(B ⇒C)).
h. (A∨B)∧(¬C∨¬D∨E)|=( A∨B).
i. (A∨B)∧(¬C∨¬D∨E)|=( A∨B)∧(¬D∨E).
j. (A∨B)∧¬(A ⇒B) is satisﬁable.
k. (A ⇔B)∧(¬A∨B) is satisﬁable.
l. (A ⇔B) ⇔C has the same number of models as (A ⇔B) for any ﬁxed set of
proposition symbols that includes A, B, C.
Exercises 281
7.5 Prove each of the following assertions:
a. α is valid if and only if True|= α.
b.F o r a n yα, False|= α.
c. α|= β if and only if the sentence (α ⇒β) is valid.
d. α≡β if and only if the sentence (α ⇔β) is valid.
e. α|= β if and only if the sentence (α∧¬β) is unsatisﬁable.
7.6 Prove, or ﬁnd a counterexample to, each of the following assertions:
a.I f α|= γ or β|= γ (or both) then (α∧β)|= γ
b.I f α|=( β∧γ) then α|= β and α|= γ.
c.I f α|=( β∨γ) then α|= β or α|= γ (or both).
7.7 Consider a vocabulary with only four propositions,A, B, C,a n dD. How many models
are there for the following sentences?
a. B∨C.
b. ¬A∨¬B∨¬C∨¬D.
c. (A ⇒B)∧A∧¬B∧C∧D.
7.8 We have deﬁned four binary logical connectives.
a. Are there any others that might be useful?
b. How many binary connectives can there be?
c. Why are some of them not very useful?
7.9 Using a method of your choice, verify each of the equivalences in Figure 7.11 (page 249).
7.10 Decide whether each of the following sentences is valid, unsatisﬁable, or neither. Ver-
ify your decisions using truth tables or the equivalence rules of Figure 7.11 (page 249).
a. Smoke ⇒Smoke
b. Smoke ⇒Fire
c. (Smoke ⇒Fire) ⇒(¬Smoke ⇒¬Fire)
d. Smoke∨Fire∨¬Fire
e. ((Smoke∧Heat) ⇒Fire) ⇔((Smoke ⇒Fire)∨(Heat ⇒Fire))
f. (Smoke ⇒Fire) ⇒((Smoke∧Heat) ⇒Fire)
g. Big∨Dumb∨(Big ⇒Dumb)
7.11 Any propositional logic sentence is logically equivalent to the assertion that each pos-
sible world in which it would be false is not the case. From this observation, prove that any
sentence can be written in CNF.
7.12 Use resolution to prove the sentence¬A∧¬B from the clauses in Exercise 7.20.
7.13 This exercise looks into the relationship between clauses and implication sentences.
282 Chapter 7. Logical Agents
a. Show that the clause (¬P1∨···∨¬Pm∨Q) is logically equivalent to the implication
sentence (P1∧···∧Pm)⇒Q.
b. Show that every clause (regardless of the number of positive literals) can be written in
the form (P1 ∧···∧Pm)⇒(Q1 ∨···∨Qn),w h e r et h eP sa n dQs are proposition
symbols. A knowledge base consisting of such sentences is in implicative normal
form or Kowalski form (Kowalski, 1979).IMPLICA TIVE
NORMAL FORM
c. Write down the full resolution rule for sentences in implicative normal form.
7.14 According to some political pundits, a person who is radical ( R) is electable ( E)i f
he/she is conservative (C), but otherwise is not electable.
a. Which of the following are correct representations of this assertion?
(i) (R∧E) ⇐⇒C
(ii) R ⇒(E ⇐⇒C)
(iii) R ⇒((C ⇒E)∨¬E)
b. Which of the sentences in (a) can be expressed in Horn form?
7.15 This question considers representing satisﬁability (SAT) problems as CSPs.
a. Draw the constraint graph corresponding to the SAT problem
(¬X1∨X2)∧(¬X2∨X3)∧... ∧(¬Xn−1∨Xn)
for the particular case n =5 .
b. How many solutions are there for this general SAT problem as a function of n?
c. Suppose we apply B ACKTRACKING -SEARCH (page 215) to ﬁnd all solutions to a SAT
CSP of the type given in (a). (To ﬁnd all solutions to a CSP, we simply modify the
basic algorithm so it continues searching after each solution is found.) Assume that
variables are ordered X
1,...,X n and false is ordered before true. How much time
will the algorithm take to terminate? (Write an O(·) expression as a function of n.)
d. We know that SAT problems in Horn form can be solved in linear time by forward
chaining (unit propagation). We also know that every tree-structured binary CSP with
discrete, ﬁnite domains can be solved in time linear in the number of variables (Sec-
tion 6.5). Are these two facts connected? Discuss.
7.16 Explain why every nonempty propositional clause, by itself, is satisﬁable. Prove rig-
orously that every set of ﬁve 3-SAT clauses is satisﬁable, provided that each clause mentions
exactly three distinct variables. What is the smallest set of such clauses that is unsatisﬁable?
Construct such a set.
7.17 A propositional 2-CNF expression is a conjunction of clauses, each containingexactly
2 literals, e.g.,
(A∨B)∧(¬A∨C)∧(¬B∨D)∧(¬C∨G)∧(¬D∨G) .
a. Prove using resolution that the above sentence entails G.
Exercises 283
b. Two clauses are semantically distinct if they are not logically equivalent. How many
semantically distinct 2-CNF clauses can be constructed from n proposition symbols?
c. Using your answer to (b), prove that propositional resolution always terminates in time
polynomial in n given a 2-CNF sentence containing no more than n distinct symbols.
d. Explain why your argument in (c) does not apply to 3-CNF.
7.18 Consider the following sentence:
[(Food ⇒Party)∨(Drinks ⇒Party)] ⇒[(Food∧Drinks) ⇒Party] .
a. Determine, using enumeration, whether this sentence is valid, satisﬁable (but not valid),
or unsatisﬁable.
b. Convert the left-hand and right-hand sides of the main implication into CNF, showing
each step, and explain how the results conﬁrm your answer to (a).
c. Prove your answer to (a) using resolution.
7.19 A sentence is indisjunctive normal form (DNF) if it is the disjunction of conjunctionsDISJUNCTIVE
NORMAL FORM
of literals. For example, the sentence (A∧B∧¬C)∨(¬A∧C)∨(B∧¬C) is in DNF.
a. Any propositional logic sentence is logically equivalent to the assertion that some pos-
sible world in which it would be true is in fact the case. From this observation, prove
that any sentence can be written in DNF.
b. Construct an algorithm that converts any sentence in propositional logic into DNF.
(Hint: The algorithm is similar to the algorithm for conversion to CNF given in Sec-
tion 7.5.2.)
c. Construct a simple algorithm that takes as input a sentence in DNF and returns a satis-
fying assignment if one exists, or reports that no satisfying assignment exists.
d. Apply the algorithms in (b) and (c) to the following set of sentences:
A ⇒B
B ⇒C
C ⇒¬A.
e. Since the algorithm in (b) is very similar to the algorithm for conversion to CNF, and
since the algorithm in (c) is much simpler than any algorithm for solving a set of sen-
tences in CNF, why is this technique not used in automated reasoning?
7.20 Convert the following set of sentences to clausal form.
S1: A ⇔(B∨E).
S2: E ⇒D.
S3: C∧F ⇒¬B.
S4: E ⇒B.
S5: B ⇒F .
S6: B ⇒C
Give a trace of the execution of DPLL on the conjunction of these clauses.
284 Chapter 7. Logical Agents
7.21 Is a randomly generated 4-CNF sentence with n symbols and m clauses more or less
likely to be solvable than a randomly generated 3-CNF sentence with n symbols and m
clauses? Explain.
7.22 Minesweeper, the well-known computer game, is closely related to the wumpus world.
A minesweeper world is a rectangular grid of N squares with M invisible mines scattered
among them. Any square may be probed by the agent; instant death follows if a mine is
probed. Minesweeper indicates the presence of mines by revealing, in each probed square,
the number of mines that are directly or diagonally adjacent. The goal is to probe every
unmined square.
a.L e t X
i,j be true iff square [i, j] contains a mine. Write down the assertion that exactly
two mines are adjacent to [1,1] as a sentence involving some logical combination of
X
i,j propositions.
b. Generalize your assertion from (a) by explaining how to construct a CNF sentence
asserting that k of n neighbors contain mines.
c. Explain precisely how an agent can use DPLL to prove that a given square does (or
does not) contain a mine, ignoring the global constraint that there are exactly M mines
in all.
d. Suppose that the global constraint is constructed from your method from part (b). How
does the number of clauses depend on M and N? Suggest a way to modify DPLL so
that the global constraint does not need to be represented explicitly.
e. Are any conclusions derived by the method in part (c) invalidated when the global
constraint is taken into account?
f. Give examples of conﬁgurations of probe values that induce long-range dependencies
such that the contents of a given unprobed square would give information about the
contents of a far-distant square. (Hint: consider an N× 1 board.)
7.23 How long does it take to prove KB |= α using DPLL when α is a literal already
contained in KB? Explain.
7.24 Trace the behavior of DPLL on the knowledge base in Figure 7.16 when trying to
prove Q, and compare this behavior with that of the forward-chaining algorithm.
7.25 Write a successor-state axiom for the Locked predicate, which applies to doors, as-
suming the only actions available are Lock and Unlock.
7.26 Section 7.7.1 provides some of the successor-state axioms required for the wumpus
world. Write down axioms for all remaining ﬂuent symbols.
7.27 Modify the H YBRID -WUMPUS -AGENT to use the 1-CNF logical state estimation
method described on page 271. We noted on that page that such an agent will not be able
to acquire, maintain, and use more complex beliefs such as the disjunction P3,1∨P2,2. Sug-
gest a method for overcoming this problem by deﬁning additional proposition symbols, and
try it out in the wumpus world. Does it improve the performance of the agent?


END_INSTRUCTION
