
START_INSTRUCTION
You are an expert AI tutor creating a comprehensive study guide.
Your task is to rewrite and expand the "Lesson Notes" provided below into a cohesive, readable study text.

1. **Structure:** Follow the exact structure and order of the topics in the "Lesson Notes". Do not reorder them.
2. **Enrichment:** Use the "Textbook Context" provided to expand on every bullet point in the notes.
    - If the notes mention a concept (e.g., "Rationality"), define it precisely using the textbook.
    - If the notes list algorithms (e.g., "A*", "Simulated Annealing"), explain how they work, their complexity, and pros/cons using the textbook details.
    - Add examples from the textbook where appropriate.
3. **Tone:** Academic but accessible study material. Not just bullet points—write paragraphs where necessary to explain complex ideas.
4. **Language:** The output must be in **Czech** (since the lesson notes are in Czech), but you can keep standard English AI terminology (like "Overfitting") in parentheses.

--- LESSON NOTES (Skeleton) ---
Úvod, historie

Umělé = člověkem vytvořený artefakt (umělá hmota, umělý sníh, umělý kloub…)
• existuje nějaká přirozená věc, kterou je možno duplikovat
• existuje záměr člověka vytvořit duplikát oné přirozené věci
• došlo k provedení záměru
Inteligence:
všeobecná schopnost individua vědomě orientovat vlastní myšlení na nové požadavky, je 
to všeobecná duchovní schopnost přizpůsobit se novým životním úkolům a podmínkám. 
(W. Stern)
•
vnitřně členitá a zároveň globální schopnost individua účelně jednat, rozumně myslet a 
efektivně se vyrovnávat se svým okolím. (D. Wechsler)
•
schopnost zpracovávat informace. Informacemi je třeba chápat všechny dojmy, které 
člověk vnímá. (J. P. Guilford)
•
Druhy inteligence
Abstraktní
○ schopnost řešit dobře definované akademické problémy
○ jednoznačná odpověď
•
Praktická
○ schopnost řešit problémy každodenního života
○ nejednoznačné zadání i řešení
•
Sociální
○ Schopnost pohybovat se v sociálním prostředí
•
• Emoční
Silná vs Slabá AI
Silná - Povaha mysli je algoritmická, přičemž není podstatné, v jakém médiu (mozek, 
počítač) jsou algoritmy implementovány
•
• Slabá - Modelování dílčích projevů mysli (např. schopnosti usuzovat nebo řešit problémy).
Klasická vs Distribuovaná vs Nová AI
• Klasická - chápe inteligenci jako atribut jedné mysli
• Distribuovaná - chápe inteligenci jako produkt sociálních interakcí více myslí
• Nová - chápe inteligenci jako emergentní výsledek činnosti primitivních entit

--- TEXTBOOK CONTEXT (Source Material) ---


--- BOOK CHAPTER: 1_Introduction ---

1 INTRODUCTION
In which we try to explain why we consider artiﬁcial intelligence to be a subject
most worthy of study, and in which we try to decide what exactly it is, this being a
good thing to decide before embarking.
We call ourselves Homo sapiens—man the wise—because our intelligence is so importantINTELLIGENCE
to us. For thousands of years, we have tried to understand how we think; that is, how a mere
handful of matter can perceive, understand, predict, and manipulate a world far larger and
more complicated than itself. The ﬁeld of artiﬁcial intelligence, or AI, goes further still: it
ARTIFICIAL
INTELLIGENCE
attempts not just to understand but also to build intelligent entities.
AI is one of the newest ﬁelds in science and engineering. Work started in earnest soon
after World War II, and the name itself was coined in 1956. Along with molecular biology,
AI is regularly cited as the “ﬁeld I would most like to be in” by scientists in other disciplines.
A student in physics might reasonably feel that all the good ideas have already been taken by
Galileo, Newton, Einstein, and the rest. AI, on the other hand, still has openings for several
full-time Einsteins and Edisons.
AI currently encompasses a huge variety of subﬁelds, ranging from the general (learning
and perception) to the speciﬁc, such as playing chess, proving mathematical theorems, writing
poetry, driving a car on a crowded street, and diagnosing diseases. AI is relevant to any
intellectual task; it is truly a universal ﬁeld.
1.1 W HA T IS AI?
We have claimed that AI is exciting, but we have not said what it is. In Figure 1.1 we see
eight deﬁnitions of AI, laid out along two dimensions. The deﬁnitions on top are concerned
with thought processes and reasoning, whereas the ones on the bottom addressbehavior.T h e
deﬁnitions on the left measure success in terms of ﬁdelity to human performance, whereas
the ones on the right measure against an ideal performance measure, called rationality.A
RA TIONALITY
system is rational if it does the “right thing,” given what it knows.
Historically, all four approaches to AI have been followed, each by different people
with different methods. A human-centered approach must be in part an empirical science, in-
1
2 Chapter 1. Introduction
Thinking Humanly
 Thinking Rationally
“The exciting new effort to make comput-
ers think ... machines with minds ,i nt h e
full and literal sense.” (Haugeland, 1985)
“The study of mental faculties through the
use of computational models.”
(Charniak and McDermott, 1985)
“[The automation of] activities that we
associate with human thinking, activities
such as decision-making, problem solv-
ing, learning ... ” (Bellman, 1978)
“The study of the computations that make
it possible to perceive, reason, and act.”
(Winston, 1992)
Acting Humanly
 Acting Rationally
“The art of creating machines that per-
form functions that require intelligence
when performed by people.” (Kurzweil,
1990)
“Computational Intelligence is the study
of the design of intelligent agents.” (Poole
et al., 1998)
“The study of how to make computers do
things at which, at the moment, people are
better.” (Rich and Knight, 1991)
“AI . . . is concerned with intelligent be-
havior in artifacts.” (Nilsson, 1998)
Figure 1.1 Some deﬁnitions of artiﬁcial intelligence, organized into four categories.
volving observations and hypotheses about human behavior. A rationalist1 approach involves
a combination of mathematics and engineering. The various group have both disparaged and
helped each other. Let us look at the four approaches in more detail.
1.1.1 Acting humanly: The Turing Test approach
The Turing Test, proposed by Alan Turing (1950), was designed to provide a satisfactoryTURING TEST
operational deﬁnition of intelligence. A computer passes the test if a human interrogator, after
posing some written questions, cannot tell whether the written responses come from a person
or from a computer. Chapter 26 discusses the details of the test and whether a computer would
really be intelligent if it passed. For now, we note that programming a computer to pass a
rigorously applied test provides plenty to work on. The computer would need to possess the
following capabilities:
•natural language processing to enable it to communicate successfully in English;
NA TURAL LANGUAGE
PROCESSING
•knowledge representation to store what it knows or hears;KNOWLEDGE
REPRESENTA TION
•automated reasoning to use the stored information to answer questions and to drawAUTOMA TED
REASONING
new conclusions;
•machine learning to adapt to new circumstances and to detect and extrapolate patterns.MACHINE LEARNING
1 By distinguishing between human and rational behavior, we are not suggesting that humans are necessarily
“irrational” in the sense of “emotionally unstable” or “insane.” One merely need note that we are not perfect:
not all chess players are grandmasters; and, unfortunately, not everyone gets an A on the exam. Some systematic
errors in human reasoning are cataloged by Kahneman et al. (1982).
Section 1.1. What Is AI? 3
Turing’s test deliberately avoided direct physical interaction between the interrogator and the
computer, because physical simulation of a person is unnecessary for intelligence. However,
the so-called total Turing Test includes a video signal so that the interrogator can test theTOT AL TURING TEST
subject’s perceptual abilities, as well as the opportunity for the interrogator to pass physical
objects “through the hatch.” To pass the total Turing Test, the computer will need
•computer vision to perceive objects, andCOMPUTER VISION
•robotics to manipulate objects and move about.ROBOTICS
These six disciplines compose most of AI, and Turing deserves credit for designing a test
that remains relevant 60 years later. Yet AI researchers have devoted little effort to passing
the Turing Test, believing that it is more important to study the underlying principles of in-
telligence than to duplicate an exemplar. The quest for “artiﬁcial ﬂight” succeeded when the
Wright brothers and others stopped imitating birds and started using wind tunnels and learn-
ing about aerodynamics. Aeronautical engineering texts do not deﬁne the goal of their ﬁeld
as making “machines that ﬂy so exactly like pigeons that they can fool even other pigeons.”
1.1.2 Thinking humanly: The cognitive modeling approach
If we are going to say that a given program thinks like a human, we must have some way of
determining how humans think. We need to get inside the actual workings of human minds.
There are three ways to do this: through introspection—trying to catch our own thoughts as
they go by; through psychological experiments—observing a person in action; and through
brain imaging—observing the brain in action. Once we have a sufﬁciently precise theory of
the mind, it becomes possible to express the theory as a computer program. If the program’s
input–output behavior matches corresponding human behavior, that is evidence that some of
the program’s mechanisms could also be operating in humans. For example, Allen Newell
and Herbert Simon, who developed GPS, the “General Problem Solver” (Newell and Simon,
1961), were not content merely to have their program solve problems correctly. They were
more concerned with comparing the trace of its reasoning steps to traces of human subjects
solving the same problems. The interdisciplinary ﬁeld of cognitive science brings together
COGNITIVE SCIENCE
computer models from AI and experimental techniques from psychology to construct precise
and testable theories of the human mind.
Cognitive science is a fascinating ﬁeld in itself, worthy of several textbooks and at least
one encyclopedia (Wilson and Keil, 1999). We will occasionally comment on similarities or
differences between AI techniques and human cognition. Real cognitive science, however, is
necessarily based on experimental investigation of actual humans or animals. We will leave
that for other books, as we assume the reader has only a computer for experimentation.
In the early days of AI there was often confusion between the approaches: an author
would argue that an algorithm performs well on a task and that it is therefore a good model
of human performance, or vice versa. Modern authors separate the two kinds of claims;
this distinction has allowed both AI and cognitive science to develop more rapidly. The two
ﬁelds continue to fertilize each other, most notably in computer vision, which incorporates
neurophysiological evidence into computational models.
4 Chapter 1. Introduction
1.1.3 Thinking rationally: The “laws of thought” approach
The Greek philosopher Aristotle was one of the ﬁrst to attempt to codify “right thinking,” that
is, irrefutable reasoning processes. His syllogisms provided patterns for argument structuresSYLLOGISM
that always yielded correct conclusions when given correct premises—for example, “Socrates
is a man; all men are mortal; therefore, Socrates is mortal.” These laws of thought were
supposed to govern the operation of the mind; their study initiated the ﬁeld called logic.
LOGIC
Logicians in the 19th century developed a precise notation for statements about all kinds
of objects in the world and the relations among them. (Contrast this with ordinary arithmetic
notation, which provides only for statements about numbers.) By 1965, programs existed
that could, in principle, solve any solvable problem described in logical notation. (Although
if no solution exists, the program might loop forever.) The so-called logicist tradition within
LOGICIST
artiﬁcial intelligence hopes to build on such programs to create intelligent systems.
There are two main obstacles to this approach. First, it is not easy to take informal
knowledge and state it in the formal terms required by logical notation, particularly when
the knowledge is less than 100% certain. Second, there is a big difference between solving
a problem “in principle” and solving it in practice. Even problems with just a few hundred
facts can exhaust the computational resources of any computer unless it has some guidance
as to which reasoning steps to try ﬁrst. Although both of these obstacles apply to any attempt
to build computational reasoning systems, they appeared ﬁrst in the logicist tradition.
1.1.4 Acting rationally: The rational agent approach
An agent is just something that acts ( agent comes from the Latin agere, to do). Of course,AGENT
all computer programs do something, but computer agents are expected to do more: operate
autonomously, perceive their environment, persist over a prolonged time period, adapt to
change, and create and pursue goals. A rational agent is one that acts so as to achieve the
RA TIONAL AGENT
best outcome or, when there is uncertainty, the best expected outcome.
In the “laws of thought” approach to AI, the emphasis was on correct inferences. Mak-
ing correct inferences is sometimes part of being a rational agent, because one way to act
rationally is to reason logically to the conclusion that a given action will achieve one’s goals
and then to act on that conclusion. On the other hand, correct inference is not all of ration-
ality; in some situations, there is no provably correct thing to do, but something must still be
done. There are also ways of acting rationally that cannot be said to involve inference. For
example, recoiling from a hot stove is a reﬂex action that is usually more successful than a
slower action taken after careful deliberation.
All the skills needed for the Turing Test also allow an agent to act rationally. Knowledge
representation and reasoning enable agents to reach good decisions. We need to be able to
generate comprehensible sentences in natural language to get by in a complex society. We
need learning not only for erudition, but also because it improves our ability to generate
effective behavior.
The rational-agent approach has two advantages over the other approaches. First, it
is more general than the “laws of thought” approach because correct inference is just one
of several possible mechanisms for achieving rationality. Second, it is more amenable to
Section 1.2. The Foundations of Artiﬁcial Intelligence 5
scientiﬁc development than are approaches based on human behavior or human thought. The
standard of rationality is mathematically well deﬁned and completely general, and can be
“unpacked” to generate agent designs that provably achieve it. Human behavior, on the other
hand, is well adapted for one speciﬁc environment and is deﬁned by, well, the sum total
of all the things that humans do. This book therefore concentrates on general principles
of rational agents and on components for constructing them. We will see that despite the
apparent simplicity with which the problem can be stated, an enormous variety of issues
come up when we try to solve it. Chapter 2 outlines some of these issues in more detail.
One important point to keep in mind: We will see before too long that achieving perfect
rationality—always doing the right thing—is not feasible in complicated environments. The
computational demands are just too high. For most of the book, however, we will adopt the
working hypothesis that perfect rationality is a good starting point for analysis. It simpliﬁes
the problem and provides the appropriate setting for most of the foundational material in
the ﬁeld. Chapters 5 and 17 deal explicitly with the issue of limited rationality —acting
LIMITED
RA TIONALITY
appropriately when there is not enough time to do all the computations one might like.
1.2 T HE FOUNDATIONS OF ARTIFICIAL INTELLIGENCE
In this section, we provide a brief history of the disciplines that contributed ideas, viewpoints,
and techniques to AI. Like any history, this one is forced to concentrate on a small number
of people, events, and ideas and to ignore others that also were important. We organize the
history around a series of questions. We certainly would not wish to give the impression that
these questions are the only ones the disciplines address or that the disciplines have all been
working toward AI as their ultimate fruition.
1.2.1 Philosophy
•Can formal rules be used to draw valid conclusions?
•How does the mind arise from a physical brain?
•Where does knowledge come from?
•How does knowledge lead to action?
Aristotle (384–322 B.C.), whose bust appears on the front cover of this book, was the ﬁrst
to formulate a precise set of laws governing the rational part of the mind. He developed an
informal system of syllogisms for proper reasoning, which in principle allowed one to gener-
ate conclusions mechanically, given initial premises. Much later, Ramon Lull (d. 1315) had
the idea that useful reasoning could actually be carried out by a mechanical artifact. Thomas
Hobbes (1588–1679) proposed that reasoning was like numerical computation, that “we add
and subtract in our silent thoughts.” The automation of computation itself was already well
under way. Around 1500, Leonardo da Vinci (1452–1519) designed but did not build a me-
chanical calculator; recent reconstructions have shown the design to be functional. The ﬁrst
known calculating machine was constructed around 1623 by the German scientist Wilhelm
Schickard (1592–1635), although the Pascaline, built in 1642 by Blaise Pascal (1623–1662),
6 Chapter 1. Introduction
is more famous. Pascal wrote that “the arithmetical machine produces effects which appear
nearer to thought than all the actions of animals.” Gottfried Wilhelm Leibniz (1646–1716)
built a mechanical device intended to carry out operations on concepts rather than numbers,
but its scope was rather limited. Leibniz did surpass Pascal by building a calculator that
could add, subtract, multiply, and take roots, whereas the Pascaline could only add and sub-
tract. Some speculated that machines might not just do calculations but actually be able to
think and act on their own. In his 1651 book Leviathan, Thomas Hobbes suggested the idea
of an “artiﬁcial animal,” arguing “For what is the heart but a spring; and the nerves, but so
many strings; and the joints, but so many wheels.”
It’s one thing to say that the mind operates, at least in part, according to logical rules, and
to build physical systems that emulate some of those rules; it’s another to say that the mind
itself is such a physical system. Ren´e Descartes (1596–1650) gave the ﬁrst clear discussion
of the distinction between mind and matter and of the problems that arise. One problem with
a purely physical conception of the mind is that it seems to leave little room for free will:
if the mind is governed entirely by physical laws, then it has no more free will than a rock
“deciding” to fall toward the center of the earth. Descartes was a strong advocate of the power
of reasoning in understanding the world, a philosophy now called rationalism, and one that
RA TIONALISM
counts Aristotle and Leibnitz as members. But Descartes was also a proponent of dualism.DUALISM
He held that there is a part of the human mind (or soul or spirit) that is outside of nature,
exempt from physical laws. Animals, on the other hand, did not possess this dual quality;
they could be treated as machines. An alternative to dualism is materialism, which holdsMA TERIALISM
that the brain’s operation according to the laws of physics constitutes the mind. Free will is
simply the way that the perception of available choices appears to the choosing entity.
Given a physical mind that manipulates knowledge, the next problem is to establish
the source of knowledge. The empiricism movement, starting with Francis Bacon’s (1561–EMPIRICISM
1626) Novum Organum,2 is characterized by a dictum of John Locke (1632–1704): “Nothing
is in the understanding, which was not ﬁrst in the senses.” David Hume’s (1711–1776) A
Treatise of Human Nature (Hume, 1739) proposed what is now known as the principle of
induction: that general rules are acquired by exposure to repeated associations between theirINDUCTION
elements. Building on the work of Ludwig Wittgenstein (1889–1951) and Bertrand Russell
(1872–1970), the famous Vienna Circle, led by Rudolf Carnap (1891–1970), developed the
doctrine of logical positivism. This doctrine holds that all knowledge can be characterized by
LOGICAL POSITIVISM
logical theories connected, ultimately, to observation sentences that correspond to sensoryOBSERVA TION
SENTENCES
inputs; thus logical positivism combines rationalism and empiricism.3 The conﬁrmation the-
ory of Carnap and Carl Hempel (1905–1997) attempted to analyze the acquisition of knowl-CONFIRMA TION
THEORY
edge from experience. Carnap’s book The Logical Structure of the World (1928) deﬁned an
explicit computational procedure for extracting knowledge from elementary experiences. It
was probably the ﬁrst theory of mind as a computational process.
2 The Novum Organum is an update of Aristotle’s Organon, or instrument of thought. Thus Aristotle can be
seen as both an empiricist and a rationalist.
3 In this picture, all meaningful statements can be veriﬁed or falsiﬁed either by experimentation or by analysis
of the meaning of the words. Because this rules out most of metaphysics, as was the intention, logical positivism
was unpopular in some circles.
Section 1.2. The Foundations of Artiﬁcial Intelligence 7
The ﬁnal element in the philosophical picture of the mind is the connection between
knowledge and action. This question is vital to AI because intelligence requires action as well
as reasoning. Moreover, only by understanding how actions are justiﬁed can we understand
how to build an agent whose actions are justiﬁable (or rational). Aristotle argued (inDe Motu
Animalium) that actions are justiﬁed by a logical connection between goals and knowledge of
the action’s outcome (the last part of this extract also appears on the front cover of this book,
in the original Greek):
But how does it happen that thinking is sometimes accompanied by action and sometimes
not, sometimes by motion, and sometimes not? It looks as if almost the same thing
happens as in the case of reasoning and making inferences about unchanging objects. But
in that case the end is a speculative proposition ... whereas here the conclusion which
results from the two premises is an action. ... I need covering; a cloak is a covering. I
need a cloak. What I need, I have to make; I need a cloak. I have to make a cloak. And
the conclusion, the “I have to make a cloak,” is an action.
In the Nicomachean Ethics (Book III. 3, 1112b), Aristotle further elaborates on this topic,
suggesting an algorithm:
We deliberate not about ends, but about means. For a doctor does not deliberate whether
he shall heal, nor an orator whether he shall persuade, ... They assume the end and
consider how and by what means it is attained, and if it seems easily and best produced
thereby; while if it is achieved by one means only they consider how it will be achieved
by this and by what means this will be achieved, till they come to the ﬁrst cause, ... and
what is last in the order of analysis seems to be ﬁrst in the order of becoming. And if we
come on an impossibility, we give up the sear ch, e.g., if we need money and this cannot
be got; but if a thing appears possible we try to do it.
Aristotle’s algorithm was implemented 2300 years later by Newell and Simon in their GPS
program. We would now call it a regression planning system (see Chapter 10).
Goal-based analysis is useful, but does not say what to do when several actions will
achieve the goal or when no action will achieve it completely. Antoine Arnauld (1612–1694)
correctly described a quantitative formula for deciding what action to take in cases like this
(see Chapter 16). John Stuart Mill’s (1806–1873) book Utilitarianism (Mill, 1863) promoted
the idea of rational decision criteria in all spheres of human activity. The more formal theory
of decisions is discussed in the following section.
1.2.2 Mathematics
•What are the formal rules to draw valid conclusions?
•What can be computed?
•How do we reason with uncertain information?
Philosophers staked out some of the fundamental ideas of AI, but the leap to a formal science
required a level of mathematical formalization in three fundamental areas: logic, computa-
tion, and probability.
The idea of formal logic can be traced back to the philosophers of ancient Greece, but
its mathematical development really began with the work of George Boole (1815–1864), who
8 Chapter 1. Introduction
worked out the details of propositional, or Boolean, logic (Boole, 1847). In 1879, Gottlob
Frege (1848–1925) extended Boole’s logic to include objects and relations, creating the ﬁrst-
order logic that is used today.
4 Alfred Tarski (1902–1983) introduced a theory of reference
that shows how to relate the objects in a logic to objects in the real world.
The next step was to determine the limits of what could be done with logic and com-
putation. The ﬁrst nontrivial algorithm is thought to be Euclid’s algorithm for computingALGORITHM
greatest common divisors. The word algorithm (and the idea of studying them) comes from
al-Khowarazmi, a Persian mathematician of the 9th century, whose writings also introduced
Arabic numerals and algebra to Europe. Boole and others discussed algorithms for logical
deduction, and, by the late 19th century, efforts were under way to formalize general mathe-
matical reasoning as logical deduction. In 1930, Kurt G¨odel (1906–1978) showed that there
exists an effective procedure to prove any true statement in the ﬁrst-order logic of Frege and
Russell, but that ﬁrst-order logic could not capture the principle of mathematical induction
needed to characterize the natural numbers. In 1931, G¨ odel showed that limits on deduc-
tion do exist. His incompleteness theorem showed that in any formal theory as strong as
INCOMPLETENESS
THEOREM
Peano arithmetic (the elementary theory of natural numbers), there are true statements that
are undecidable in the sense that they have no proof within the theory.
This fundamental result can also be interpreted as showing that some functions on the
integers cannot be represented by an algorithm—that is, they cannot be computed. This
motivated Alan Turing (1912–1954) to try to characterize exactly which functions are com-
putable—capable of being computed. This notion is actually slightly problematic because
COMPUT ABLE
the notion of a computation or effective procedure really cannot be given a formal deﬁnition.
However, the Church–Turing thesis, which states that the Turing machine (Turing, 1936) is
capable of computing any computable function, is generally accepted as providing a sufﬁcient
deﬁnition. Turing also showed that there were some functions that no Turing machine can
compute. For example, no machine can tell in general whether a given program will return
an answer on a given input or run forever.
Although decidability and computability are important to an understanding of computa-
tion, the notion of tractability has had an even greater impact. Roughly speaking, a problem
TRACT ABILITY
is called intractable if the time required to solve instances of the problem grows exponentially
with the size of the instances. The distinction between polynomial and exponential growth
in complexity was ﬁrst emphasized in the mid-1960s (Cobham, 1964; Edmonds, 1965). It is
important because exponential growth means that even moderately large instances cannot be
solved in any reasonable time. Therefore, one should strive to divide the overall problem of
generating intelligent behavior into tractable subproblems rather than intractable ones.
How can one recognize an intractable problem? The theory of NP-completeness,p i o -
NP-COMPLETENESS
neered by Steven Cook (1971) and Richard Karp (1972), provides a method. Cook and Karp
showed the existence of large classes of canonical combinatorial search and reasoning prob-
lems that are NP-complete. Any problem class to which the class of NP-complete problems
can be reduced is likely to be intractable. (Although it has not been proved that NP-complete
4 Frege’s proposed notation for ﬁrst-order logic—an ar cane combination of textual and geometric features—
never became popular.
Section 1.2. The Foundations of Artiﬁcial Intelligence 9
problems are necessarily intractable, most theoreticians believe it.) These results contrast
with the optimism with which the popular press greeted the ﬁrst computers—“Electronic
Super-Brains” that were “Faster than Einstein!” Despite the increasing speed of computers,
careful use of resources will characterize intelligent systems. Put crudely, the world is an
extremely large problem instance! Work in AI has helped explain why some instances of
NP-complete problems are hard, yet others are easy (Cheeseman et al., 1991).
Besides logic and computation, the third great contribution of mathematics to AI is the
theory of probability. The Italian Gerolamo Cardano (1501–1576) ﬁrst framed the idea of
PROBABILITY
probability, describing it in terms of the possible outcomes of gambling events. In 1654,
Blaise Pascal (1623–1662), in a letter to Pierre Fermat (1601–1665), showed how to pre-
dict the future of an unﬁnished gambling game and assign average payoffs to the gamblers.
Probability quickly became an invaluable part of all the quantitative sciences, helping to deal
with uncertain measurements and incomplete theories. James Bernoulli (1654–1705), Pierre
Laplace (1749–1827), and others advanced the theory and introduced new statistical meth-
ods. Thomas Bayes (1702–1761), who appears on the front cover of this book, proposed
a rule for updating probabilities in the light of new evidence. Bayes’ rule underlies most
modern approaches to uncertain reasoning in AI systems.
1.2.3 Economics
•How should we make decisions so as to maximize payoff?
•How should we do this when others may not go along?
•How should we do this when the payoff may be far in the future?
The science of economics got its start in 1776, when Scottish philosopher Adam Smith
(1723–1790) published An Inquiry into the Nature and Causes of the Wealth of Nations .
While the ancient Greeks and others had made contributions to economic thought, Smith was
the ﬁrst to treat it as a science, using the idea that economies can be thought of as consist-
ing of individual agents maximizing their own economic well-being. Most people think of
economics as being about money, but economists will say that they are really studying how
people make choices that lead to preferred outcomes. When McDonald’s offers a hamburger
for a dollar, they are asserting that they would prefer the dollar and hoping that customers will
prefer the hamburger. The mathematical treatment of “preferred outcomes” or utility was
UTILITY
ﬁrst formalized by L´eon Walras (pronounced “Valrasse”) (1834-1910) and was improved by
Frank Ramsey (1931) and later by John von Neumann and Oskar Morgenstern in their book
The Theory of Games and Economic Behavior (1944).
Decision theory, which combines probability theory with utility theory, provides a for-
DECISION THEORY
mal and complete framework for decisions (economic or otherwise) made under uncertainty—
that is, in cases where probabilistic descriptions appropriately capture the decision maker’s
environment. This is suitable for “large” economies where each agent need pay no attention
to the actions of other agents as individuals. For “small” economies, the situation is much
more like a game: the actions of one player can signiﬁcantly affect the utility of another
(either positively or negatively). V on Neumann and Morgenstern’s development of game
theory (see also Luce and Raiffa, 1957) included the surprising result that, for some games,
GAME THEORY
10 Chapter 1. Introduction
a rational agent should adopt policies that are (or least appear to be) randomized. Unlike de-
cision theory, game theory does not offer an unambiguous prescription for selecting actions.
For the most part, economists did not address the third question listed above, namely,
how to make rational decisions when payoffs from actions are not immediate but instead re-
sult from several actions taken in sequence. This topic was pursued in the ﬁeld of operations
research, which emerged in World War II from efforts in Britain to optimize radar installa-
OPERA TIONS
RESEARCH
tions, and later found civilian applications in complex management decisions. The work of
Richard Bellman (1957) formalized a class of sequential decision problems called Markov
decision processes, which we study in Chapters 17 and 21.
Work in economics and operations research has contributed much to our notion of ra-
tional agents, yet for many years AI research developed along entirely separate paths. One
reason was the apparent complexity of making rational decisions. The pioneering AI re-
searcher Herbert Simon (1916–2001) won the Nobel Prize in economics in 1978 for his early
work showing that models based on satisﬁcing—making decisions that are “good enough,”
SA TISFICING
rather than laboriously calculating an optimal decision—gave a better description of actual
human behavior (Simon, 1947). Since the 1990s, there has been a resurgence of interest in
decision-theoretic techniques for agent systems (Wellman, 1995).
1.2.4 Neuroscience
•How do brains process information?
Neuroscience is the study of the nervous system, particularly the brain. Although the exactNEUROSCIENCE
way in which the brain enables thought is one of the great mysteries of science, the fact that it
does enable thought has been appreciated for thousands of years because of the evidence that
strong blows to the head can lead to mental incapacitation. It has also long been known that
human brains are somehow different; in about 335
B.C. Aristotle wrote, “Of all the animals,
man has the largest brain in proportion to his size.” 5 Still, it was not until the middle of the
18th century that the brain was widely recognized as the seat of consciousness. Before then,
candidate locations included the heart and the spleen.
Paul Broca’s (1824–1880) study of aphasia (speech deﬁcit) in brain-damaged patients
in 1861 demonstrated the existence of localized areas of the brain responsible for speciﬁc
cognitive functions. In particular, he showed that speech production was localized to the
portion of the left hemisphere now called Broca’s area. 6 By that time, it was known that
the brain consisted of nerve cells, or neurons, but it was not until 1873 that Camillo GolgiNEURON
(1843–1926) developed a staining technique allowing the observation of individual neurons
in the brain (see Figure 1.2). This technique was used by Santiago Ramon y Cajal (1852–
1934) in his pioneering studies of the brain’s neuronal structures.
7 Nicolas Rashevsky (1936,
1938) was the ﬁrst to apply mathematical models to the study of the nervous sytem.
5 Since then, it has been discovered that the tree shrew (Scandentia) has a higher ratio of brain to body mass.
6 Many cite Alexander Hood (1824) as a possible prior source.
7 Golgi persisted in his belief that the brain’s functions were carried out primarily in a continuous medium in
which neurons were embedded, whereas Cajal propounded t he “neuronal doctrine.” The two shared the Nobel
prize in 1906 but gave mutually antagonistic acceptance speeches.
Section 1.2. The Foundations of Artiﬁcial Intelligence 11
Axon
Cell body or Soma
Nucleus
Dendrite
Synapses
Axonal arborization
Axon from another cell
Synapse
Figure 1.2 The parts of a nerve cell or neuron. Each neuron consists of a cell body,
or soma, that contains a cell nucleus. Branching out from the cell body are a number of
ﬁbers called dendrites and a single long ﬁber called the axon. The axon stretches out for a
long distance, much longer than the scale in this diagram indicates. Typically, an axon is
1 cm long (100 times the diameter of the cell body), but can reach up to 1 meter. A neuron
makes connections with 10 to 100,000 other neurons at junctions called synapses. Signals are
propagated from neuron to neuron by a complicated electrochemical reaction. The signals
control brain activity in the short term and also enable long-term changes in the connectivity
of neurons. These mechanisms are thought to form the basis for learning in the brain. Most
information processing goes on in the cerebral cortex, the outer layer of the brain. The basic
organizational unit appears to be a column of tissue about 0.5 mm in diameter, containing
about 20,000 neurons and extending the full depth of the cortex about 4 mm in humans).
We now have some data on the mapping between areas of the brain and the parts of the
body that they control or from which they receive sensory input. Such mappings are able to
change radically over the course of a few weeks, and some animals seem to have multiple
maps. Moreover, we do not fully understand how other areas can take over functions when
one area is damaged. There is almost no theory on how an individual memory is stored.
The measurement of intact brain activity began in 1929 with the invention by Hans
Berger of the electroencephalograph (EEG). The recent development of functional magnetic
resonance imaging (fMRI) (Ogawa et al. , 1990; Cabeza and Nyberg, 2001) is giving neu-
roscientists unprecedentedly detailed images of brain activity, enabling measurements that
correspond in interesting ways to ongoing cognitive processes. These are augmented by
advances in single-cell recording of neuron activity. Individual neurons can be stimulated
electrically, chemically, or even optically (Han and Boyden, 2007), allowing neuronal input–
output relationships to be mapped. Despite these advances, we are still a long way from
understanding how cognitive processes actually work.
The truly amazing conclusion is that a collection of simple cells can lead to thought,
action, and consciousness or, in the pithy words of John Searle (1992), brains cause minds .
12 Chapter 1. Introduction
Supercomputer
 Personal Computer
 Human Brain
Computational units
 104 CPUs, 1012 transistors
 4C P U s ,109 transistors
 1011 neurons
Storage units
 1014 bits RAM
 1011 bits RAM
 1011 neurons
1015 bits disk
 1013 bits disk
 1014 synapses
Cycle time
 10−9 sec
 10−9 sec
 10−3 sec
Operations/sec
 1015
 1010
 1017
Memory updates/sec
 1014
 1010
 1014
Figure 1.3 A crude comparison of the raw computational resources available to the IBM
BLUE GENE supercomputer, a typical personal computer of 2008, and the human brain. The
brain’s numbers are essentially ﬁxed, whereas the supercomputer’s numbers have been in-
creasing by a factor of 10 every 5 years or so, allowing it to achieve rough parity with the
brain. The personal computer lags behind on all metrics except cycle time.
The only real alternative theory is mysticism: that minds operate in some mystical realm that
is beyond physical science.
Brains and digital computers have somewhat different properties. Figure 1.3 shows that
computers have a cycle time that is a million times faster than a brain. The brain makes up
for that with far more storage and interconnection than even a high-end personal computer,
although the largest supercomputers have a capacity that is similar to the brain’s. (It should
be noted, however, that the brain does not seem to use all of its neurons simultaneously.)
Futurists make much of these numbers, pointing to an approaching singularity at which
SINGULARITY
computers reach a superhuman level of performance (Vinge, 1993; Kurzweil, 2005), but the
raw comparisons are not especially informative. Even with a computer of virtually unlimited
capacity, we still would not know how to achieve the brain’s level of intelligence.
1.2.5 Psychology
•How do humans and animals think and act?
The origins of scientiﬁc psychology are usually traced to the work of the German physi-
cist Hermann von Helmholtz (1821–1894) and his student Wilhelm Wundt (1832–1920).
Helmholtz applied the scientiﬁc method to the study of human vision, and his Handbook
of Physiological Optics is even now described as “the single most important treatise on the
physics and physiology of human vision” (Nalwa, 1993, p.15). In 1879, Wundt opened the
ﬁrst laboratory of experimental psychology, at the University of Leipzig. Wundt insisted
on carefully controlled experiments in which his workers would perform a perceptual or as-
sociative task while introspecting on their thought processes. The careful controls went a
long way toward making psychology a science, but the subjective nature of the data made
it unlikely that an experimenter would ever disconﬁrm his or her own theories. Biologists
studying animal behavior, on the other hand, lacked introspective data and developed an ob-
jective methodology, as described by H. S. Jennings (1906) in his inﬂuential workBehavior of
the Lower Organisms. Applying this viewpoint to humans, the behaviorism movement, led
BEHAVIORISM
by John Watson (1878–1958), rejectedany theory involving mental processes on the grounds
Section 1.2. The Foundations of Artiﬁcial Intelligence 13
that introspection could not provide reliable evidence. Behaviorists insisted on studying only
objective measures of the percepts (or stimulus) given to an animal and its resulting actions
(or response). Behaviorism discovered a lot about rats and pigeons but had less success at
understanding humans.
Cognitive psychology , which views the brain as an information-processing device,COGNITIVE
PSYCHOLOGY
can be traced back at least to the works of William James (1842–1910). Helmholtz also
insisted that perception involved a form of unconscious logical inference. The cognitive
viewpoint was largely eclipsed by behaviorism in the United States, but at Cambridge’s Ap-
plied Psychology Unit, directed by Frederic Bartlett (1886–1969), cognitive modeling was
able to ﬂourish. The Nature of Explanation , by Bartlett’s student and successor Kenneth
Craik (1943), forcefully reestablished the legitimacy of such “mental” terms as beliefs and
goals, arguing that they are just as scientiﬁc as, say, using pressure and temperature to talk
about gases, despite their being made of molecules that have neither. Craik speciﬁed the
three key steps of a knowledge-based agent: (1) the stimulus must be translated into an inter-
nal representation, (2) the representation is manipulated by cognitive processes to derive new
internal representations, and (3) these are in turn retranslated back into action. He clearly
explained why this was a good design for an agent:
If the organism carries a “small-scale model” of external reality and of its own possible
actions within its head, it is able to try out various alternatives, conclude which is the best
of them, react to future situations before they arise, utilize the knowledge of past events
in dealing with the present and future, and in every way to react in a much fuller, safer,
and more competent manner to the emergencies which face it. (Craik, 1943)
After Craik’s death in a bicycle accident in 1945, his work was continued by Donald Broad-
bent, whose book Perception and Communication (1958) was one of the ﬁrst works to model
psychological phenomena as information processing. Meanwhile, in the United States, the
development of computer modeling led to the creation of the ﬁeld of cognitive science.T h e
ﬁeld can be said to have started at a workshop in September 1956 at MIT. (We shall see that
this is just two months after the conference at which AI itself was “born.”) At the workshop,
George Miller presented The Magic Number Seven, Noam Chomsky presented Three Models
of Language, and Allen Newell and Herbert Simon presented The Logic Theory Machine .
These three inﬂuential papers showed how computer models could be used to address the
psychology of memory, language, and logical thinking, respectively. It is now a common
(although far from universal) view among psychologists that “a cognitive theory should be
like a computer program” (Anderson, 1980); that is, it should describe a detailed information-
processing mechanism whereby some cognitive function might be implemented.
1.2.6 Computer engineering
•How can we build an efﬁcient computer?
For artiﬁcial intelligence to succeed, we need two things: intelligence and an artifact. The
computer has been the artifact of choice. The modern digital electronic computer was in-
vented independently and almost simultaneously by scientists in three countries embattled in
14 Chapter 1. Introduction
World War II. The ﬁrst operational computer was the electromechanical Heath Robinson, 8
built in 1940 by Alan Turing’s team for a single purpose: deciphering German messages. In
1943, the same group developed the Colossus, a powerful general-purpose machine based
on vacuum tubes.
9 The ﬁrst operational programmable computer was the Z-3, the inven-
tion of Konrad Zuse in Germany in 1941. Zuse also invented ﬂoating-point numbers and the
ﬁrst high-level programming language, Plankalk¨ul. The ﬁrst electronic computer, the ABC,
was assembled by John Atanasoff and his student Clifford Berry between 1940 and 1942
at Iowa State University. Atanasoff’s research received little support or recognition; it was
the ENIAC, developed as part of a secret military project at the University of Pennsylvania
by a team including John Mauchly and John Eckert, that proved to be the most inﬂuential
forerunner of modern computers.
Since that time, each generation of computer hardware has brought an increase in speed
and capacity and a decrease in price. Performance doubled every 18 months or so until around
2005, when power dissipation problems led manufacturers to start multiplying the number of
CPU cores rather than the clock speed. Current expectations are that future increases in power
will come from massive parallelism—a curious convergence with the properties of the brain.
Of course, there were calculating devices before the electronic computer. The earliest
automated machines, dating from the 17th century, were discussed on page 6. The ﬁrst pro-
grammable machine was a loom, devised in 1805 by Joseph Marie Jacquard (1752–1834),
that used punched cards to store instructions for the pattern to be woven. In the mid-19th
century, Charles Babbage (1792–1871) designed two machines, neither of which he com-
pleted. The Difference Engine was intended to compute mathematical tables for engineering
and scientiﬁc projects. It was ﬁnally built and shown to work in 1991 at the Science Museum
in London (Swade, 2000). Babbage’s Analytical Engine was far more ambitious: it included
addressable memory, stored programs, and conditional jumps and was the ﬁrst artifact capa-
ble of universal computation. Babbage’s colleague Ada Lovelace, daughter of the poet Lord
Byron, was perhaps the world’s ﬁrst programmer. (The programming language Ada is named
after her.) She wrote programs for the unﬁnished Analytical Engine and even speculated that
the machine could play chess or compose music.
AI also owes a debt to the software side of computer science, which has supplied the
operating systems, programming languages, and tools needed to write modern programs (and
papers about them). But this is one area where the debt has been repaid: work in AI has pio-
neered many ideas that have made their way back to mainstream computer science, including
time sharing, interactive interpreters, personal computers with windows and mice, rapid de-
velopment environments, the linked list data type, automatic storage management, and key
concepts of symbolic, functional, declarative, and object-oriented programming.
8 Heath Robinson was a cartoonist famous for his depictions of whimsical and absurdly complicated contrap-
tions for everyday tasks such as buttering toast.
9 In the postwar period, Turing wanted to use these com puters for AI research—for example, one of the ﬁrst
chess programs (Turing et al., 1953). His efforts were blocked by the British government.
Section 1.2. The Foundations of Artiﬁcial Intelligence 15
1.2.7 Control theory and cybernetics
•How can artifacts operate under their own control?
Ktesibios of Alexandria (c. 250 B.C.) built the ﬁrst self-controlling machine: a water clock
with a regulator that maintained a constant ﬂow rate. This invention changed the deﬁnition
of what an artifact could do. Previously, only living things could modify their behavior in
response to changes in the environment. Other examples of self-regulating feedback control
systems include the steam engine governor, created by James Watt (1736–1819), and the
thermostat, invented by Cornelis Drebbel (1572–1633), who also invented the submarine.
The mathematical theory of stable feedback systems was developed in the 19th century.
The central ﬁgure in the creation of what is now called control theory was Norbert
CONTROL THEORY
Wiener (1894–1964). Wiener was a brilliant mathematician who worked with Bertrand Rus-
sell, among others, before developing an interest in biological and mechanical control systems
and their connection to cognition. Like Craik (who also used control systems as psychological
models), Wiener and his colleagues Arturo Rosenblueth and Julian Bigelow challenged the
behaviorist orthodoxy (Rosenblueth et al., 1943). They viewed purposive behavior as aris-
ing from a regulatory mechanism trying to minimize “error”—the difference between current
state and goal state. In the late 1940s, Wiener, along with Warren McCulloch, Walter Pitts,
and John von Neumann, organized a series of inﬂuential conferences that explored the new
mathematical and computational models of cognition. Wiener’s book Cybernetics (1948) be-
CYBERNETICS
came a bestseller and awoke the public to the possibility of artiﬁcially intelligent machines.
Meanwhile, in Britain, W. Ross Ashby (Ashby, 1940) pioneered similar ideas. Ashby, Alan
Turing, Grey Walter, and others formed the Ratio Club for “those who had Wiener’s ideas
before Wiener’s book appeared.” Ashby’s Design for a Brain (1948, 1952) elaborated on his
idea that intelligence could be created by the use of homeostatic devices containing appro-HOMEOST A TIC
priate feedback loops to achieve stable adaptive behavior.
Modern control theory, especially the branch known as stochastic optimal control, has
as its goal the design of systems that maximize anobjective function over time. This roughlyOBJECTIVE
FUNCTION
matches our view of AI: designing systems that behave optimally. Why, then, are AI and
control theory two different ﬁelds, despite the close connections among their founders? The
answer lies in the close coupling between the mathematical techniques that were familiar to
the participants and the corresponding sets of problems that were encompassed in each world
view. Calculus and matrix algebra, the tools of control theory, lend themselves to systems that
are describable by ﬁxed sets of continuous variables, whereas AI was founded in part as a way
to escape from the these perceived limitations. The tools of logical inference and computation
allowed AI researchers to consider problems such as language, vision, and planning that fell
completely outside the control theorist’s purview.
1.2.8 Linguistics
•How does language relate to thought?
In 1957, B. F. Skinner published V erbal Behavior. This was a comprehensive, detailed ac-
count of the behaviorist approach to language learning, written by the foremost expert in
16 Chapter 1. Introduction
the ﬁeld. But curiously, a review of the book became as well known as the book itself, and
served to almost kill off interest in behaviorism. The author of the review was the linguist
Noam Chomsky, who had just published a book on his own theory, Syntactic Structures.
Chomsky pointed out that the behaviorist theory did not address the notion of creativity in
language—it did not explain how a child could understand and make up sentences that he or
she had never heard before. Chomsky’s theory—based on syntactic models going back to the
Indian linguist Panini (c. 350
B.C.)—could explain this, and unlike previous theories, it was
formal enough that it could in principle be programmed.
Modern linguistics and AI, then, were “born” at about the same time, and grew up
together, intersecting in a hybrid ﬁeld called computational linguistics or natural languageCOMPUT A TIONAL
LINGUISTICS
processing. The problem of understanding language soon turned out to be considerably more
complex than it seemed in 1957. Understanding language requires an understanding of the
subject matter and context, not just an understanding of the structure of sentences. This might
seem obvious, but it was not widely appreciated until the 1960s. Much of the early work in
knowledge representation (the study of how to put knowledge into a form that a computer
can reason with) was tied to language and informed by research in linguistics, which was
connected in turn to decades of work on the philosophical analysis of language.
1.3 T HE HISTORY OF ARTIFICIAL INTELLIGENCE
With the background material behind us, we are ready to cover the development of AI itself.
1.3.1 The gestation of artiﬁcial intelligence (1943–1955)
The ﬁrst work that is now generally recognized as AI was done by Warren McCulloch and
Walter Pitts (1943). They drew on three sources: knowledge of the basic physiology and
function of neurons in the brain; a formal analysis of propositional logic due to Russell and
Whitehead; and Turing’s theory of computation. They proposed a model of artiﬁcial neurons
in which each neuron is characterized as being “on” or “off,” with a switch to “on” occurring
in response to stimulation by a sufﬁcient number of neighboring neurons. The state of a
neuron was conceived of as “factually equivalent to a proposition which proposed its adequate
stimulus.” They showed, for example, that any computable function could be computed by
some network of connected neurons, and that all the logical connectives (and, or, not, etc.)
could be implemented by simple net structures. McCulloch and Pitts also suggested that
suitably deﬁned networks could learn. Donald Hebb (1949) demonstrated a simple updating
rule for modifying the connection strengths between neurons. His rule, now called Hebbian
learning, remains an inﬂuential model to this day.
HEBBIAN LEARNING
Two undergraduate students at Harvard, Marvin Minsky and Dean Edmonds, built the
ﬁrst neural network computer in 1950. The S NARC , as it was called, used 3000 vacuum
tubes and a surplus automatic pilot mechanism from a B-24 bomber to simulate a network of
40 neurons. Later, at Princeton, Minsky studied universal computation in neural networks.
His Ph.D. committee was skeptical about whether this kind of work should be considered
Section 1.3. The History of Artiﬁcial Intelligence 17
mathematics, but von Neumann reportedly said, “If it isn’t now, it will be someday.” Minsky
was later to prove inﬂuential theorems showing the limitations of neural network research.
There were a number of early examples of work that can be characterized as AI, but
Alan Turing’s vision was perhaps the most inﬂuential. He gave lectures on the topic as early
as 1947 at the London Mathematical Society and articulated a persuasive agenda in his 1950
article “Computing Machinery and Intelligence.” Therein, he introduced the Turing Test,
machine learning, genetic algorithms, and reinforcement learning. He proposed the Child
Programme idea, explaining “Instead of trying to produce a programme to simulate the adult
mind, why not rather try to produce one which simulated the child’s?”
1.3.2 The birth of artiﬁcial intelligence (1956)
Princeton was home to another inﬂuential ﬁgure in AI, John McCarthy. After receiving his
PhD there in 1951 and working for two years as an instructor, McCarthy moved to Stan-
ford and then to Dartmouth College, which was to become the ofﬁcial birthplace of the ﬁeld.
McCarthy convinced Minsky, Claude Shannon, and Nathaniel Rochester to help him bring
together U.S. researchers interested in automata theory, neural nets, and the study of intel-
ligence. They organized a two-month workshop at Dartmouth in the summer of 1956. The
proposal states:
10
We propose that a 2 month, 10 man study of artiﬁcial intelligence be carried
out during the summer of 1956 at Dartmouth College in Hanover, New Hamp-
shire. The study is to proceed on the basis of the conjecture that every aspect of
learning or any other feature of intelligence can in principle be so precisely de-
scribed that a machine can be made to simulate it. An attempt will be made to ﬁnd
how to make machines use language, form abstractions and concepts, solve kinds
of problems now reserved for humans, and improve themselves. We think that a
signiﬁcant advance can be made in one or more of these problems if a carefully
selected group of scientists work on it together for a summer.
There were 10 attendees in all, including Trenchard More from Princeton, Arthur Samuel
from IBM, and Ray Solomonoff and Oliver Selfridge from MIT.
Two researchers from Carnegie Tech,
11 Allen Newell and Herbert Simon, rather stole
the show. Although the others had ideas and in some cases programs for particular appli-
cations such as checkers, Newell and Simon already had a reasoning program, the Logic
Theorist (LT), about which Simon claimed, “We have invented a computer program capable
of thinking non-numerically, and thereby solved the venerable mind–body problem.”
12 Soon
after the workshop, the program was able to prove most of the theorems in Chapter 2 of Rus-
10 This was the ﬁrst ofﬁcial usage of McCarthy’s termartiﬁcial intelligence. Perhaps “computational rationality”
would have been more precise and less threatening, but “AI” has stuck. At the 50th anniversary of the Dartmouth
conference, McCarthy stated that he resisted the terms “computer” or “computational” in deference to Norbert
Weiner, who was promoting analog cybernetic devices rather than digital computers.
11 Now Carnegie Mellon University (CMU).
12 Newell and Simon also invented a list-processing language, IPL, to write LT. They had no compiler and
translated it into machine code by hand. To avoid errors, they worked in parallel, calling out binary numbers to
each other as they wrote each instruction to make sure they agreed.
18 Chapter 1. Introduction
sell and Whitehead’s Principia Mathematica. Russell was reportedly delighted when Simon
showed him that the program had come up with a proof for one theorem that was shorter than
the one in Principia. The editors of the Journal of Symbolic Logic were less impressed; they
rejected a paper coauthored by Newell, Simon, and Logic Theorist.
The Dartmouth workshop did not lead to any new breakthroughs, but it did introduce
all the major ﬁgures to each other. For the next 20 years, the ﬁeld would be dominated by
these people and their students and colleagues at MIT, CMU, Stanford, and IBM.
Looking at the proposal for the Dartmouth workshop (McCarthy et al., 1955), we can
see why it was necessary for AI to become a separate ﬁeld. Why couldn’t all the work done
in AI have taken place under the name of control theory or operations research or decision
theory, which, after all, have objectives similar to those of AI? Or why isn’t AI a branch
of mathematics? The ﬁrst answer is that AI from the start embraced the idea of duplicating
human faculties such as creativity, self-improvement, and language use. None of the other
ﬁelds were addressing these issues. The second answer is methodology. AI is the only one
of these ﬁelds that is clearly a branch of computer science (although operations research does
share an emphasis on computer simulations), and AI is the only ﬁeld to attempt to build
machines that will function autonomously in complex, changing environments.
1.3.3 Early enthusiasm, great expectations (1952–1969)
The early years of AI were full of successes—in a limited way. Given the primitive comput-
ers and programming tools of the time and the fact that only a few years earlier computers
were seen as things that could do arithmetic and no more, it was astonishing whenever a com-
puter did anything remotely clever. The intellectual establishment, by and large, preferred to
believe that “a machine can never do X.” (See Chapter 26 for a long list of X’s gathered
by Turing.) AI researchers naturally responded by demonstrating one X after another. John
McCarthy referred to this period as the “Look, Ma, no hands!” era.
Newell and Simon’s early success was followed up with the General Problem Solver,
or GPS. Unlike Logic Theorist, this program was designed from the start to imitate human
problem-solving protocols. Within the limited class of puzzles it could handle, it turned out
that the order in which the program considered subgoals and possible actions was similar to
that in which humans approached the same problems. Thus, GPS was probably the ﬁrst pro-
gram to embody the “thinking humanly” approach. The success of GPS and subsequent pro-
grams as models of cognition led Newell and Simon (1976) to formulate the famousphysical
symbol system hypothesis, which states that “a physical symbol system has the necessary and
PHYSICAL SYMBOL
SYSTEM
sufﬁcient means for general intelligent action.” What they meant is that any system (human
or machine) exhibiting intelligence must operate by manipulating data structures composed
of symbols. We will see later that this hypothesis has been challenged from many directions.
At IBM, Nathaniel Rochester and his colleagues produced some of the ﬁrst AI pro-
grams. Herbert Gelernter (1959) constructed the Geometry Theorem Prover, which was
able to prove theorems that many students of mathematics would ﬁnd quite tricky. Starting
in 1952, Arthur Samuel wrote a series of programs for checkers (draughts) that eventually
learned to play at a strong amateur level. Along the way, he disproved the idea that comput-
Section 1.3. The History of Artiﬁcial Intelligence 19
ers can do only what they are told to: his program quickly learned to play a better game than
its creator. The program was demonstrated on television in February 1956, creating a strong
impression. Like Turing, Samuel had trouble ﬁnding computer time. Working at night, he
used machines that were still on the testing ﬂoor at IBM’s manufacturing plant. Chapter 5
covers game playing, and Chapter 21 explains the learning techniques used by Samuel.
John McCarthy moved from Dartmouth to MIT and there made three crucial contribu-
tions in one historic year: 1958. In MIT AI Lab Memo No. 1, McCarthy deﬁned the high-level
language Lisp, which was to become the dominant AI programming language for the next 30
LISP
years. With Lisp, McCarthy had the tool he needed, but access to scarce and expensive com-
puting resources was also a serious problem. In response, he and others at MIT invented time
sharing. Also in 1958, McCarthy published a paper entitled Programs with Common Sense ,
in which he described the Advice Taker, a hypothetical program that can be seen as the ﬁrst
complete AI system. Like the Logic Theorist and Geometry Theorem Prover, McCarthy’s
program was designed to use knowledge to search for solutions to problems. But unlike the
others, it was to embody general knowledge of the world. For example, he showed how
some simple axioms would enable the program to generate a plan to drive to the airport. The
program was also designed to accept new axioms in the normal course of operation, thereby
allowing it to achieve competence in new areas without being reprogrammed . The Advice
Taker thus embodied the central principles of knowledge representation and reasoning: that
it is useful to have a formal, explicit representation of the world and its workings and to be
able to manipulate that representation with deductive processes. It is remarkable how much
of the 1958 paper remains relevant today.
1958 also marked the year that Marvin Minsky moved to MIT. His initial collaboration
with McCarthy did not last, however. McCarthy stressed representation and reasoning in for-
mal logic, whereas Minsky was more interested in getting programs to work and eventually
developed an anti-logic outlook. In 1963, McCarthy started the AI lab at Stanford. His plan
to use logic to build the ultimate Advice Taker was advanced by J. A. Robinson’s discov-
ery in 1965 of the resolution method (a complete theorem-proving algorithm for ﬁrst-order
logic; see Chapter 9). Work at Stanford emphasized general-purpose methods for logical
reasoning. Applications of logic included Cordell Green’s question-answering and planning
systems (Green, 1969b) and the Shakey robotics project at the Stanford Research Institute
(SRI). The latter project, discussed further in Chapter 25, was the ﬁrst to demonstrate the
complete integration of logical reasoning and physical activity.
Minsky supervised a series of students who chose limited problems that appeared to
require intelligence to solve. These limited domains became known as microworlds. James
MICROWORLD
Slagle’s SAINT program (1963) was able to solve closed-form calculus integration problems
typical of ﬁrst-year college courses. Tom Evans’s A NALOGY program (1968) solved geo-
metric analogy problems that appear in IQ tests. Daniel Bobrow’s STUDENT program (1967)
solved algebra story problems, such as the following:
If the number of customers Tom gets is twice the square of 20 percent of the number
of advertisements he runs, and the number of advertisements he runs is 45, what is the
number of customers Tom gets?
20 Chapter 1. Introduction
Red
Green
Red
Green
Green
Blue
Blue
Red
Figure 1.4 A scene from the blocks world. SHRDLU (Winograd, 1972) has just completed
the command “Find a block which is taller than the one you are holding and put it in the box.”
The most famous microworld was the blocks world, which consists of a set of solid blocks
placed on a tabletop (or more often, a simulation of a tabletop), as shown in Figure 1.4.
A typical task in this world is to rearrange the blocks in a certain way, using a robot hand
that can pick up one block at a time. The blocks world was home to the vision project of
David Huffman (1971), the vision and constraint-propagation work of David Waltz (1975),
the learning theory of Patrick Winston (1970), the natural-language-understanding program
of Terry Winograd (1972), and the planner of Scott Fahlman (1974).
Early work building on the neural networks of McCulloch and Pitts also ﬂourished.
The work of Winograd and Cowan (1963) showed how a large number of elements could
collectively represent an individual concept, with a corresponding increase in robustness and
parallelism. Hebb’s learning methods were enhanced by Bernie Widrow (Widrow and Hoff,
1960; Widrow, 1962), who called his networks adalines, and by Frank Rosenblatt (1962)
with his perceptrons.T h e perceptron convergence theorem (Block et al., 1962) says that
the learning algorithm can adjust the connection strengths of a perceptron to match any input
data, provided such a match exists. These topics are covered in Chapter 20.
1.3.4 A dose of reality (1966–1973)
From the beginning, AI researchers were not shy about making predictions of their coming
successes. The following statement by Herbert Simon in 1957 is often quoted:
It is not my aim to surprise or shock you—but the simplest way I can summarize is to say
that there are now in the world machines that think, that learn and that create. Moreover,
Section 1.3. The History of Artiﬁcial Intelligence 21
their ability to do these things is going to increase rapidly until—in a visible future—the
range of problems they can handle will be coextensive with the range to which the human
mind has been applied.
Terms such as “visible future” can be interpreted in various ways, but Simon also made
more concrete predictions: that within 10 years a computer would be chess champion, and
a signiﬁcant mathematical theorem would be proved by machine. These predictions came
true (or approximately true) within 40 years rather than 10. Simon’s overconﬁdence was due
to the promising performance of early AI systems on simple examples. In almost all cases,
however, these early systems turned out to fail miserably when tried out on wider selections
of problems and on more difﬁcult problems.
The ﬁrst kind of difﬁculty arose because most early programs knew nothing of their
subject matter; they succeeded by means of simple syntactic manipulations. A typical story
occurred in early machine translation efforts, which were generously funded by the U.S. Na-
tional Research Council in an attempt to speed up the translation of Russian scientiﬁc papers
in the wake of the Sputnik launch in 1957. It was thought initially that simple syntactic trans-
formations based on the grammars of Russian and English, and word replacement from an
electronic dictionary, would sufﬁce to preserve the exact meanings of sentences. The fact is
that accurate translation requires background knowledge in order to resolve ambiguity and
establish the content of the sentence. The famous retranslation of “the spirit is willing but
the ﬂesh is weak” as “the vodka is good but the meat is rotten” illustrates the difﬁculties en-
countered. In 1966, a report by an advisory committee found that “there has been no machine
translation of general scientiﬁc text, and none is in immediate prospect.” All U.S. government
funding for academic translation projects was canceled. Today, machine translation is an im-
perfect but widely used tool for technical, commercial, government, and Internet documents.
The second kind of difﬁculty was the intractability of many of the problems that AI was
attempting to solve. Most of the early AI programs solved problems by trying out different
combinations of steps until the solution was found. This strategy worked initially because
microworlds contained very few objects and hence very few possible actions and very short
solution sequences. Before the theory of computational complexity was developed, it was
widely thought that “scaling up” to larger problems was simply a matter of faster hardware
and larger memories. The optimism that accompanied the development of resolution theorem
proving, for example, was soon dampened when researchers failed to prove theorems involv-
ing more than a few dozen facts. The fact that a program can ﬁnd a solution in principle does
not mean that the program contains any of the mechanisms needed to ﬁnd it in practice.
The illusion of unlimited computational power was not conﬁned to problem-solving
programs. Early experiments in machine evolution (now called genetic algorithms)( F r i e d -MACHINE EVOLUTION
GENETIC
ALGORITHM berg, 1958; Friedberg et al. , 1959) were based on the undoubtedly correct belief that by
making an appropriate series of small mutations to a machine-code program, one can gen-
erate a program with good performance for any particular task. The idea, then, was to try
random mutations with a selection process to preserve mutations that seemed useful. De-
spite thousands of hours of CPU time, almost no progress was demonstrated. Modern genetic
algorithms use better representations and have shown more success.
22 Chapter 1. Introduction
Failure to come to grips with the “combinatorial explosion” was one of the main criti-
cisms of AI contained in the Lighthill report (Lighthill, 1973), which formed the basis for the
decision by the British government to end support for AI research in all but two universities.
(Oral tradition paints a somewhat different and more colorful picture, with political ambitions
and personal animosities whose description is beside the point.)
A third difﬁculty arose because of some fundamental limitations on the basic structures
being used to generate intelligent behavior. For example, Minsky and Papert’s book Percep-
trons (1969) proved that, although perceptrons (a simple form of neural network) could be
shown to learn anything they were capable of representing, they could represent very little. In
particular, a two-input perceptron (restricted to be simpler than the form Rosenblatt originally
studied) could not be trained to recognize when its two inputs were different. Although their
results did not apply to more complex, multilayer networks, research funding for neural-net
research soon dwindled to almost nothing. Ironically, the new back-propagation learning al-
gorithms for multilayer networks that were to cause an enormous resurgence in neural-net
research in the late 1980s were actually discovered ﬁrst in 1969 (Bryson and Ho, 1969).
1.3.5 Knowledge-based systems: The key to power? (1969–1979)
The picture of problem solving that had arisen during the ﬁrst decade of AI research was of
a general-purpose search mechanism trying to string together elementary reasoning steps to
ﬁnd complete solutions. Such approaches have been calledweak methods because, although
WEAK METHOD
general, they do not scale up to large or difﬁcult problem instances. The alternative to weak
methods is to use more powerful, domain-speciﬁc knowledge that allows larger reasoning
steps and can more easily handle typically occurring cases in narrow areas of expertise. One
might say that to solve a hard problem, you have to almost know the answer already.
The D
ENDRAL program (Buchanan et al., 1969) was an early example of this approach.
It was developed at Stanford, where Ed Feigenbaum (a former student of Herbert Simon),
Bruce Buchanan (a philosopher turned computer scientist), and Joshua Lederberg (a Nobel
laureate geneticist) teamed up to solve the problem of inferring molecular structure from the
information provided by a mass spectrometer. The input to the program consists of the ele-
mentary formula of the molecule (e.g., C
6H13NO2) and the mass spectrum giving the masses
of the various fragments of the molecule generated when it is bombarded by an electron beam.
For example, the mass spectrum might contain a peak at m =1 5, corresponding to the mass
of a methyl (CH
3)f r a g m e n t .
The naive version of the program generated all possible structures consistent with the
formula, and then predicted what mass spectrum would be observed for each, comparing this
with the actual spectrum. As one might expect, this is intractable for even moderate-sized
molecules. The D
ENDRAL researchers consulted analytical chemists and found that they
worked by looking for well-known patterns of peaks in the spectrum that suggested common
substructures in the molecule. For example, the following rule is used to recognize a ketone
(C=O) subgroup (which weighs 28):
if there are two peaks at x1 and x2 such that
(a) x1 + x2 = M +2 8 (M is the mass of the whole molecule);
Section 1.3. The History of Artiﬁcial Intelligence 23
(b) x1 −28 is a high peak;
(c) x2 −28 is a high peak;
(d) At least one of x1 and x2 is high.
then there is a ketone subgroup
Recognizing that the molecule contains a particular substructure reduces the number of pos-
sible candidates enormously. DENDRAL was powerful because
All the relevant theoretical knowledge to solve these problems has been mapped over from
its general form in the [spectrum prediction component] (“ﬁrst principles”) to efﬁcient
special forms (“cookbook recipes”). (Feigenbaum et al., 1971)
The signiﬁcance of D ENDRAL was that it was the ﬁrst successful knowledge-intensive sys-
tem: its expertise derived from large numbers of special-purpose rules. Later systems also
incorporated the main theme of McCarthy’s Advice Taker approach—the clean separation of
the knowledge (in the form of rules) from the reasoning component.
With this lesson in mind, Feigenbaum and others at Stanford began the Heuristic Pro-
gramming Project (HPP) to investigate the extent to which the new methodology of expert
systems could be applied to other areas of human expertise. The next major effort was in
EXPERT SYSTEMS
the area of medical diagnosis. Feigenbaum, Buchanan, and Dr. Edward Shortliffe developed
MYCIN to diagnose blood infections. With about 450 rules, M YCIN was able to perform
as well as some experts, and considerably better than junior doctors. It also contained two
major differences from D
ENDRAL . First, unlike the D ENDRAL rules, no general theoretical
model existed from which the MYCIN rules could be deduced. They had to be acquired from
extensive interviewing of experts, who in turn acquired them from textbooks, other experts,
and direct experience of cases. Second, the rules had to reﬂect the uncertainty associated with
medical knowledge. M
YCIN incorporated a calculus of uncertainty called certainty factorsCERT AINTY FACTOR
(see Chapter 14), which seemed (at the time) to ﬁt well with how doctors assessed the impact
of evidence on the diagnosis.
The importance of domain knowledge was also apparent in the area of understanding
natural language. Although Winograd’s SHRDLU system for understanding natural language
had engendered a good deal of excitement, its dependence on syntactic analysis caused some
of the same problems as occurred in the early machine translation work. It was able to
overcome ambiguity and understand pronoun references, but this was mainly because it was
designed speciﬁcally for one area—the blocks world. Several researchers, including Eugene
Charniak, a fellow graduate student of Winograd’s at MIT, suggested that robust language
understanding would require general knowledge about the world and a general method for
using that knowledge.
At Yale, linguist-turned-AI-researcher Roger Schank emphasized this point, claiming,
“There is no such thing as syntax,” which upset a lot of linguists but did serve to start a useful
discussion. Schank and his students built a series of programs (Schank and Abelson, 1977;
Wilensky, 1978; Schank and Riesbeck, 1981; Dyer, 1983) that all had the task of under-
standing natural language. The emphasis, however, was less on language per se and more on
the problems of representing and reasoning with the knowledge required for language under-
standing. The problems included representing stereotypical situations (Cullingford, 1981),
24 Chapter 1. Introduction
describing human memory organization (Rieger, 1976; Kolodner, 1983), and understanding
plans and goals (Wilensky, 1983).
The widespread growth of applications to real-world problems caused a concurrent in-
crease in the demands for workable knowledge representation schemes. A large number
of different representation and reasoning languages were developed. Some were based on
logic—for example, the Prolog language became popular in Europe, and the P
LANNER fam-
ily in the United States. Others, following Minsky’s idea of frames (1975), adopted a moreFRAMES
structured approach, assembling facts about particular object and event types and arranging
the types into a large taxonomic hierarchy analogous to a biological taxonomy.
1.3.6 AI becomes an industry (1980–present)
The ﬁrst successful commercial expert system, R1, began operation at the Digital Equipment
Corporation (McDermott, 1982). The program helped conﬁgure orders for new computer
systems; by 1986, it was saving the company an estimated $40 million a year. By 1988,
DEC’s AI group had 40 expert systems deployed, with more on the way. DuPont had 100 in
use and 500 in development, saving an estimated $10 million a year. Nearly every major U.S.
corporation had its own AI group and was either using or investigating expert systems.
In 1981, the Japanese announced the “Fifth Generation” project, a 10-year plan to build
intelligent computers running Prolog. In response, the United States formed the Microelec-
tronics and Computer Technology Corporation (MCC) as a research consortium designed to
assure national competitiveness. In both cases, AI was part of a broad effort, including chip
design and human-interface research. In Britain, the Alvey report reinstated the funding that
was cut by the Lighthill report.
13 In all three countries, however, the projects never met their
ambitious goals.
Overall, the AI industry boomed from a few million dollars in 1980 to billions of dollars
in 1988, including hundreds of companies building expert systems, vision systems, robots,
and software and hardware specialized for these purposes. Soon after that came a period
called the “AI Winter,” in which many companies fell by the wayside as they failed to deliver
on extravagant promises.
1.3.7 The return of neural networks (1986–present)
In the mid-1980s at least four different groups reinvented the back-propagation learningBACK-PROP AGA TION
algorithm ﬁrst found in 1969 by Bryson and Ho. The algorithm was applied to many learn-
ing problems in computer science and psychology, and the widespread dissemination of the
results in the collection Parallel Distributed Processing (Rumelhart and McClelland, 1986)
caused great excitement.
These so-called connectionist models of intelligent systems were seen by some as di-
CONNECTIONIST
rect competitors both to the symbolic models promoted by Newell and Simon and to the
logicist approach of McCarthy and others (Smolensky, 1988). It might seem obvious that
at some level humans manipulate symbols—in fact, Terrence Deacon’s book The Symbolic
13 To save embarrassment, a new ﬁeld called IKBS (Intelligent Knowledge-Based Systems) was invented because
Artiﬁcial Intelligence had been ofﬁcially canceled.
Section 1.3. The History of Artiﬁcial Intelligence 25
Species (1997) suggests that this is the deﬁning characteristic of humans—but the most ar-
dent connectionists questioned whether symbol manipulation had any real explanatory role in
detailed models of cognition. This question remains unanswered, but the current view is that
connectionist and symbolic approaches are complementary, not competing. As occurred with
the separation of AI and cognitive science, modern neural network research has bifurcated
into two ﬁelds, one concerned with creating effective network architectures and algorithms
and understanding their mathematical properties, the other concerned with careful modeling
of the empirical properties of actual neurons and ensembles of neurons.
1.3.8 AI adopts the scientiﬁc method (1987–present)
Recent years have seen a revolution in both the content and the methodology of work in
artiﬁcial intelligence.
14 It is now more common to build on existing theories than to propose
brand-new ones, to base claims on rigorous theorems or hard experimental evidence rather
than on intuition, and to show relevance to real-world applications rather than toy examples.
AI was founded in part as a rebellion against the limitations of existing ﬁelds like control
theory and statistics, but now it is embracing those ﬁelds. As David McAllester (1998) put it:
In the early period of AI it seemed plausible that new forms of symbolic computation,
e.g., frames and semantic networks, made much of classical theory obsolete. This led to
a form of isolationism in which AI became largely separated from the rest of computer
science. This isolationism is currently being abandoned. There is a recognition that
machine learning should not be isolated from information theory, that uncertain reasoning
should not be isolated from stochastic modeling, that search should not be isolated from
classical optimization and control, and that automated reasoning should not be isolated
from formal methods and static analysis.
In terms of methodology, AI has ﬁnally come ﬁrmly under the scientiﬁc method. To be ac-
cepted, hypotheses must be subjected to rigorous empirical experiments, and the results must
be analyzed statistically for their importance (Cohen, 1995). It is now possible to replicate
experiments by using shared repositories of test data and code.
The ﬁeld of speech recognition illustrates the pattern. In the 1970s, a wide variety of
different architectures and approaches were tried. Many of these were rather ad hoc and
fragile, and were demonstrated on only a few specially selected examples. In recent years,
approaches based on hidden Markov models (HMMs) have come to dominate the area. Two
HIDDEN MARKOV
MODELS
aspects of HMMs are relevant. First, they are based on a rigorous mathematical theory. This
has allowed speech researchers to build on several decades of mathematical results developed
in other ﬁelds. Second, they are generated by a process of training on a large corpus of
real speech data. This ensures that the performance is robust, and in rigorous blind tests the
HMMs have been improving their scores steadily. Speech technology and the related ﬁeld of
handwritten character recognition are already making the transition to widespread industrial
14 Some have characterized this change as a victory of the neats—those who think that AI theories should be
grounded in mathematical rigor—over the scrufﬁes—those who would rather try out lots of ideas, write some
programs, and then assess what seems to be working. Both approaches are important. A shift toward neatness
implies that the ﬁeld has reached a level of stability and maturity. Whether that stability will be disrupted by a
new scruffy idea is another question.
26 Chapter 1. Introduction
and consumer applications. Note that there is no scientiﬁc claim that humans use HMMs to
recognize speech; rather, HMMs provide a mathematical framework for understanding the
problem and support the engineering claim that they work well in practice.
Machine translation follows the same course as speech recognition. In the 1950s there
was initial enthusiasm for an approach based on sequences of words, with models learned
according to the principles of information theory. That approach fell out of favor in the
1960s, but returned in the late 1990s and now dominates the ﬁeld.
Neural networks also ﬁt this trend. Much of the work on neural nets in the 1980s was
done in an attempt to scope out what could be done and to learn how neural nets differ from
“traditional” techniques. Using improved methodology and theoretical frameworks, the ﬁeld
arrived at an understanding in which neural nets can now be compared with corresponding
techniques from statistics, pattern recognition, and machine learning, and the most promising
technique can be applied to each application. As a result of these developments, so-called
data mining technology has spawned a vigorous new industry.
DA T A MINING
Judea Pearl’s (1988) Probabilistic Reasoning in Intelligent Systems led to a new accep-
tance of probability and decision theory in AI, following a resurgence of interest epitomized
by Peter Cheeseman’s (1985) article “In Defense of Probability.” The Bayesian network
BAYESIAN NETWORK
formalism was invented to allow efﬁcient representation of, and rigorous reasoning with,
uncertain knowledge. This approach largely overcomes many problems of the probabilistic
reasoning systems of the 1960s and 1970s; it now dominates AI research on uncertain reason-
ing and expert systems. The approach allows for learning from experience, and it combines
the best of classical AI and neural nets. Work by Judea Pearl (1982a) and by Eric Horvitz and
David Heckerman (Horvitz and Heckerman, 1986; Horvitz et al., 1986) promoted the idea of
normative expert systems: ones that act rationally according to the laws of decision theory
and do not try to imitate the thought steps of human experts. The Windows
TM operating sys-
tem includes several normative diagnostic expert systems for correcting problems. Chapters
13 to 16 cover this area.
Similar gentle revolutions have occurred in robotics, computer vision, and knowledge
representation. A better understanding of the problems and their complexity properties, com-
bined with increased mathematical sophistication, has led to workable research agendas and
robust methods. Although increased formalization and specialization led ﬁelds such as vision
and robotics to become somewhat isolated from “mainstream” AI in the 1990s, this trend has
reversed in recent years as tools from machine learning in particular have proved effective for
many problems. The process of reintegration is already yielding signiﬁcant beneﬁts
1.3.9 The emergence of intelligent agents (1995–present)
Perhaps encouraged by the progress in solving the subproblems of AI, researchers have also
started to look at the “whole agent” problem again. The work of Allen Newell, John Laird,
and Paul Rosenbloom on S
OAR (Newell, 1990; Laird et al., 1987) is the best-known example
of a complete agent architecture. One of the most important environments for intelligent
agents is the Internet. AI systems have become so common in Web-based applications that
the “-bot” sufﬁx has entered everyday language. Moreover, AI technologies underlie many
Section 1.3. The History of Artiﬁcial Intelligence 27
Internet tools, such as search engines, recommender systems, and Web site aggregators.
One consequence of trying to build complete agents is the realization that the previously
isolated subﬁelds of AI might need to be reorganized somewhat when their results are to be
tied together. In particular, it is now widely appreciated that sensory systems (vision, sonar,
speech recognition, etc.) cannot deliver perfectly reliable information about the environment.
Hence, reasoning and planning systems must be able to handle uncertainty. A second major
consequence of the agent perspective is that AI has been drawn into much closer contact
with other ﬁelds, such as control theory and economics, that also deal with agents. Recent
progress in the control of robotic cars has derived from a mixture of approaches ranging from
better sensors, control-theoretic integration of sensing, localization and mapping, as well as
a degree of high-level planning.
Despite these successes, some inﬂuential founders of AI, including John McCarthy
(2007), Marvin Minsky (2007), Nils Nilsson (1995, 2005) and Patrick Winston (Beal and
Winston, 2009), have expressed discontent with the progress of AI. They think that AI should
put less emphasis on creating ever-improved versions of applications that are good at a spe-
ciﬁc task, such as driving a car, playing chess, or recognizing speech. Instead, they believe
AI should return to its roots of striving for, in Simon’s words, “machines that think, that learn
and that create.” They call the effort human-level AI or HLAI; their ﬁrst symposium was in
HUMAN-LEVEL AI
2004 (Minsky et al., 2004). The effort will require very large knowledge bases; Hendleret al.
(1995) discuss where these knowledge bases might come from.
A related idea is the subﬁeld of Artiﬁcial General Intelligence or AGI (Goertzel andARTIFICIAL GENERAL
INTELLIGENCE
Pennachin, 2007), which held its ﬁrst conference and organized theJournal of Artiﬁcial Gen-
eral Intelligence in 2008. AGI looks for a universal algorithm for learning and acting in
any environment, and has its roots in the work of Ray Solomonoff (1964), one of the atten-
dees of the original 1956 Dartmouth conference. Guaranteeing that what we create is really
Friendly AI is also a concern (Yudkowsky, 2008; Omohundro, 2008), one we will return to
FRIENDL Y AI
in Chapter 26.
1.3.10 The availability of very large data sets (2001–present)
Throughout the 60-year history of computer science, the emphasis has been on the algorithm
as the main subject of study. But some recent work in AI suggests that for many problems, it
makes more sense to worry about the data and be less picky about what algorithm to apply.
This is true because of the increasing availability of very large data sources: for example,
trillions of words of English and billions of images from the Web (Kilgarriff and Grefenstette,
2006); or billions of base pairs of genomic sequences (Collins et al., 2003).
One inﬂuential paper in this line was Yarowsky’s (1995) work on word-sense disam-
biguation: given the use of the word “plant” in a sentence, does that refer to ﬂora or factory?
Previous approaches to the problem had relied on human-labeled examples combined with
machine learning algorithms. Yarowsky showed that the task can be done, with accuracy
above 96%, with no labeled examples at all. Instead, given a very large corpus of unanno-
tated text and just the dictionary deﬁnitions of the two senses—“works, industrial plant” and
“ﬂora, plant life”—one can label examples in the corpus, and from there bootstrap to learn
28 Chapter 1. Introduction
new patterns that help label new examples. Banko and Brill (2001) show that techniques
like this perform even better as the amount of available text goes from a million words to a
billion and that the increase in performance from using more data exceeds any difference in
algorithm choice; a mediocre algorithm with 100 million words of unlabeled training data
outperforms the best known algorithm with 1 million words.
As another example, Hays and Efros (2007) discuss the problem of ﬁlling in holes in a
photograph. Suppose you use Photoshop to mask out an ex-friend from a group photo, but
now you need to ﬁll in the masked area with something that matches the background. Hays
and Efros deﬁned an algorithm that searches through a collection of photos to ﬁnd something
that will match. They found the performance of their algorithm was poor when they used
a collection of only ten thousand photos, but crossed a threshold into excellent performance
when they grew the collection to two million photos.
Work like this suggests that the “knowledge bottleneck” in AI—the problem of how to
express all the knowledge that a system needs—may be solved in many applications by learn-
ing methods rather than hand-coded knowledge engineering, provided the learning algorithms
have enough data to go on (Halevy et al., 2009). Reporters have noticed the surge of new ap-
plications and have written that “AI Winter” may be yielding to a new Spring (Havenstein,
2005). As Kurzweil (2005) writes, “today, many thousands of AI applications are deeply
embedded in the infrastructure of every industry.”
1.4 T HE STATE OF THE ART
What can AI do today? A concise answer is difﬁcult because there are so many activities in
so many subﬁelds. Here we sample a few applications; others appear throughout the book.
Robotic vehicles: A driverless robotic car named S
TANLEY sped through the rough
terrain of the Mojave dessert at 22 mph, ﬁnishing the 132-mile course ﬁrst to win the 2005
DARPA Grand Challenge. STANLEY is a V olkswagen Touareg outﬁtted with cameras, radar,
and laser rangeﬁnders to sense the environment and onboard software to command the steer-
ing, braking, and acceleration (Thrun, 2006). The following year CMU’s B
OSS won the Ur-
ban Challenge, safely driving in trafﬁc through the streets of a closed Air Force base, obeying
trafﬁc rules and avoiding pedestrians and other vehicles.
Speech recognition: A traveler calling United Airlines to book a ﬂight can have the en-
tire conversation guided by an automated speech recognition and dialog management system.
Autonomous planning and scheduling: A hundred million miles from Earth, NASA’s
Remote Agent program became the ﬁrst on-board autonomous planning program to control
the scheduling of operations for a spacecraft (Jonsson et al., 2000). R
EMOTE AGENT gen-
erated plans from high-level goals speciﬁed from the ground and monitored the execution of
those plans—detecting, diagnosing, and recovering from problems as they occurred. Succes-
sor program MAPGEN (Al-Chang et al., 2004) plans the daily operations for NASA’s Mars
Exploration Rovers, and MEXAR2 (Cesta et al., 2007) did mission planning—both logistics
and science planning—for the European Space Agency’s Mars Express mission in 2008.
Section 1.5. Summary 29
Game playing:I B M ’ s DEEP BLUE became the ﬁrst computer program to defeat the
world champion in a chess match when it bested Garry Kasparov by a score of 3.5 to 2.5 in
an exhibition match (Goodman and Keene, 1997). Kasparov said that he felt a “new kind of
intelligence” across the board from him. Newsweek magazine described the match as “The
brain’s last stand.” The value of IBM’s stock increased by $18 billion. Human champions
studied Kasparov’s loss and were able to draw a few matches in subsequent years, but the
most recent human-computer matches have been won convincingly by the computer.
Spam ﬁghting: Each day, learning algorithms classify over a billion messages as spam,
saving the recipient from having to waste time deleting what, for many users, could comprise
80% or 90% of all messages, if not classiﬁed away by algorithms. Because the spammers are
continually updating their tactics, it is difﬁcult for a static programmed approach to keep up,
and learning algorithms work best (Sahami et al., 1998; Goodman and Heckerman, 2004).
Logistics planning : During the Persian Gulf crisis of 1991, U.S. forces deployed a
Dynamic Analysis and Replanning Tool, DART (Cross and Walker, 1994), to do automated
logistics planning and scheduling for transportation. This involved up to 50,000 vehicles,
cargo, and people at a time, and had to account for starting points, destinations, routes, and
conﬂict resolution among all parameters. The AI planning techniques generated in hours
a plan that would have taken weeks with older methods. The Defense Advanced Research
Project Agency (DARPA) stated that this single application more than paid back DARPA’s
30-year investment in AI.
Robotics: The iRobot Corporation has sold over two million Roomba robotic vacuum
cleaners for home use. The company also deploys the more rugged PackBot to Iraq and
Afghanistan, where it is used to handle hazardous materials, clear explosives, and identify
the location of snipers.
Machine Translation: A computer program automatically translates from Arabic to
English, allowing an English speaker to see the headline “Ardogan Conﬁrms That Turkey
Would Not Accept Any Pressure, Urging Them to Recognize Cyprus.” The program uses a
statistical model built from examples of Arabic-to-English translations and from examples of
English text totaling two trillion words (Brants et al., 2007). None of the computer scientists
on the team speak Arabic, but they do understand statistics and machine learning algorithms.
These are just a few examples of artiﬁcial intelligence systems that exist today. Not
magic or science ﬁction—but rather science, engineering, and mathematics, to which this
book provides an introduction.
1.5 S UMMARY
This chapter deﬁnes AI and establishes the cultural background against which it has devel-
oped. Some of the important points are as follows:
•Different people approach AI with different goals in mind. Two important questions to
ask are: Are you concerned with thinking or behavior? Do you want to model humans
or work from an ideal standard?
30 Chapter 1. Introduction
•In this book, we adopt the view that intelligence is concerned mainly with rational
action. Ideally, an intelligent agent takes the best possible action in a situation. We
study the problem of building agents that are intelligent in this sense.
•Philosophers (going back to 400 B.C.) made AI conceivable by considering the ideas
that the mind is in some ways like a machine, that it operates on knowledge encoded in
some internal language, and that thought can be used to choose what actions to take.
•Mathematicians provided the tools to manipulate statements of logical certainty as well
as uncertain, probabilistic statements. They also set the groundwork for understanding
computation and reasoning about algorithms.
•Economists formalized the problem of making decisions that maximize the expected
outcome to the decision maker.
•Neuroscientists discovered some facts about how the brain works and the ways in which
it is similar to and different from computers.
•Psychologists adopted the idea that humans and animals can be considered information-
processing machines. Linguists showed that language use ﬁts into this model.
•Computer engineers provided the ever-more-powerful machines that make AI applica-
tions possible.
•Control theory deals with designing devices that act optimally on the basis of feedback
from the environment. Initially, the mathematical tools of control theory were quite
different from AI, but the ﬁelds are coming closer together.
•The history of AI has had cycles of success, misplaced optimism, and resulting cutbacks
in enthusiasm and funding. There have also been cycles of introducing new creative
approaches and systematically reﬁning the best ones.
•AI has advanced more rapidly in the past decade because of greater use of the scientiﬁc
method in experimenting with and comparing approaches.
•Recent progress in understanding the theoretical basis for intelligence has gone hand in
hand with improvements in the capabilities of real systems. The subﬁelds of AI have
become more integrated, and AI has found common ground with other disciplines.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
The methodological status of artiﬁcial intelligence is investigated inThe Sciences of the Artiﬁ-
cial, by Herb Simon (1981), which discusses research areas concerned with complex artifacts.
It explains how AI can be viewed as both science and mathematics. Cohen (1995) gives an
overview of experimental methodology within AI.
The Turing Test (Turing, 1950) is discussed by Shieber (1994), who severely criticizes
the usefulness of its instantiation in the Loebner Prize competition, and by Ford and Hayes
(1995), who argue that the test itself is not helpful for AI. Bringsjord (2008) gives advice for
a Turing Test judge. Shieber (2004) and Epstein et al. (2008) collect a number of essays on
the Turing Test. Artiﬁcial Intelligence: The V ery Idea , by John Haugeland (1985), gives a
Exercises 31
readable account of the philosophical and practical problems of AI. Signiﬁcant early papers
in AI are anthologized in the collections by Webber and Nilsson (1981) and by Luger (1995).
The Encyclopedia of AI (Shapiro, 1992) contains survey articles on almost every topic in
AI, as does Wikipedia. These articles usually provide a good entry point into the research
literature on each topic. An insightful and comprehensive history of AI is given by Nils
Nillson (2009), one of the early pioneers of the ﬁeld.
The most recent work appears in the proceedings of the major AI conferences: the bi-
ennial International Joint Conference on AI (IJCAI), the annual European Conference on AI
(ECAI), and the National Conference on AI, more often known as AAAI, after its sponsoring
organization. The major journals for general AI are Artiﬁcial Intelligence, Computational
Intelligence,t h eIEEE Transactions on Pattern Analysis and Machine Intelligence , IEEE In-
telligent Systems, and the electronicJournal of Artiﬁcial Intelligence Research.T h e r ea r ea l s o
many conferences and journals devoted to speciﬁc areas, which we cover in the appropriate
chapters. The main professional societies for AI are the American Association for Artiﬁcial
Intelligence (AAAI), the ACM Sp ecial Interest Group in Artiﬁcial Intelligence (SIGART),
and the Society for Artiﬁcial Intelligence an d Simulation of Behaviour (AISB). AAAI’s AI
Magazine contains many topical and tutorial articles, and its Web site, aaai.org, contains
news, tutorials, and background information.
EXERCISES
These exercises are intended to stimulate discussion, and some might be set as term projects.
Alternatively, preliminary attempts can be made now, and these attempts can be reviewed
after the completion of the book.
1.1 Deﬁne in your own words: (a) intelligence, (b) artiﬁcial intelligence, (c) agent, (d)
rationality, (e) logical reasoning.
1.2 Read Turing’s original paper on AI (Turing, 1950). In the paper, he discusses several
objections to his proposed enterprise and his test for intelligence. Which objections still carry
weight? Are his refutations valid? Can you think of new objections arising from develop-
ments since he wrote the paper? In the paper, he predicts that, by the year 2000, a computer
will have a 30% chance of passing a ﬁve-minute Turing Test with an unskilled interrogator.
What chance do you think a computer would have today? In another 50 years?
1.3 Are reﬂex actions (such as ﬂinching from a hot stove) rational? Are they intelligent?
1.4 Suppose we extend Evans’s A
NALOGY program so that it can score 200 on a standard
IQ test. Would we then have a program more intelligent than a human? Explain.
1.5 The neural structure of the sea slug Aplysia has been widely studied (ﬁrst by Nobel
Laureate Eric Kandel) because it has only about 20,000 neurons, most of them large and
easily manipulated. Assuming that the cycle time for an Aplysia neuron is roughly the same
as for a human neuron, how does the computational power, in terms of memory updates per
second, compare with the high-end computer described in Figure 1.3?
32 Chapter 1. Introduction
1.6 How could introspection—reporting on one’s inner thoughts—be inaccurate? Could I
be wrong about what I’m thinking? Discuss.
1.7 To what extent are the following computer systems instances of artiﬁcial intelligence:
•Supermarket bar code scanners.
•Web search engines.
•V oice-activated telephone menus.
•Internet routing algorithms that respond dynamically to the state of the network.
1.8 Many of the computational models of cognitive activities that have been proposed in-
volve quite complex mathematical operations, such as convolving an image with a Gaussian
or ﬁnding a minimum of the entropy function. Most humans (and certainly all animals) never
learn this kind of mathematics at all, almost no one learns it before college, and almost no
one can compute the convolution of a function with a Gaussian in their head. What sense
does it make to say that the “vision system” is doing this kind of mathematics, whereas the
actual person has no idea how to do it?
1.9 Why would evolution tend to result in systems that act rationally? What goals are such
systems designed to achieve?
1.10 Is AI a science, or is it engineering? Or neither or both? Explain.
1.11 “Surely computers cannot be intelligent—they can do only what their programmers
tell them.” Is the latter statement true, and does it imply the former?
1.12 “Surely animals cannot be intelligent—they can do only what their genes tell them.”
Is the latter statement true, and does it imply the former?
1.13 “Surely animals, humans, and computers cannot be intelligent—they can do only what
their constituent atoms are told to do by the laws of physics.” Is the latter statement true, and
does it imply the former?
1.14 Examine the AI literature to discover whether the following tasks can currently be
solved by computers:
a. Playing a decent game of table tennis (Ping-Pong).
b. Driving in the center of Cairo, Egypt.
c. Driving in Victorville, California.
d. Buying a week’s worth of groceries at the market.
e. Buying a week’s worth of groceries on the Web.
f. Playing a decent game of bridge at a competitive level.
g. Discovering and proving new mathematical theorems.
h. Writing an intentionally funny story.
i. Giving competent legal advice in a specialized area of law.
j. Translating spoken English into spoken Swedish in real time.
k. Performing a complex surgical operation.
Exercises 33
For the currently infeasible tasks, try to ﬁnd out what the difﬁculties are and predict when, if
ever, they will be overcome.
1.15 Various subﬁelds of AI have held contests by deﬁning a standard task and inviting re-
searchers to do their best. Examples include the DARPA Grand Challenge for robotic cars,
The International Planning Competition, the Robocup robotic soccer league, the TREC infor-
mation retrieval event, and contests in machine translation, speech recognition. Investigate
ﬁve of these contests, and describe the progress made over the years. To what degree have the
contests advanced toe state of the art in AI? Do what degree do they hurt the ﬁeld by drawing
energy away from new ideas?


--- BOOK CHAPTER: 26_Philosophical_Foundations ---

26
PHILOSOPHICAL
FOUNDATIONS
In which we consider what it means to think and whether artifacts could and
should ever do so.
Philosophers have been around far longer than computers and have been trying to resolve
some questions that relate to AI: How do minds work? Is it possible for machines to act
intelligently in the way that people do, and if they did, would they have real, conscious
minds? What are the ethical implications of intelligent machines?
First, some terminology: the assertion that machines could actas if they were intelligent
is called the weak AI hypothesis by philosophers, and the assertion that machines that do so
WEAK AI
are actually thinking (not just simulating thinking) is called the strong AI hypothesis.STRONG AI
Most AI researchers take the weak AI hypothesis for granted, and don’t care about the
strong AI hypothesis—as long as their program works, they don’t care whether you call it a
simulation of intelligence or real intelligence. All AI researchers should be concerned with
the ethical implications of their work.
26.1 W EAK AI: C AN MACHINES ACT INTELLIGENTLY ?
The proposal for the 1956 summer workshop that deﬁned the ﬁeld of Artiﬁcial Intelligence
(McCarthy et al., 1955) made the assertion that “Every aspect of learning or any other feature
of intelligence can be so precisely described that a machine can be made to simulate it.” Thus,
AI was founded on the assumption that weak AI is possible. Others have asserted that weak
AI is impossible: “Artiﬁcial intelligence pursued within the cult of computationalism stands
not even a ghost of a chance of producing durable results” (Sayre, 1993).
Clearly, whether AI is impossible depends on how it is deﬁned. In Section 1.1, we de-
ﬁned AI as the quest for the best agent program on a given architecture. With this formulation,
AI is by deﬁnition possible: for any digital architecture with k bits of program storage there
are exactly 2
k agent programs, and all we have to do to ﬁnd the best one is enumerate and test
them all. This might not be feasible for large k, but philosophers deal with the theoretical,
not the practical.
1020
Section 26.1. Weak AI: Can Machines Act Intelligently? 1021
Our deﬁnition of AI works well for the engineering problem of ﬁnding a good agent,
given an architecture. Therefore, we’re tempted to end this section right now, answering the
title question in the afﬁrmative. But philosophers are interested in the problem of compar-
ing two architectures—human and machine. Furthermore, they have traditionally posed the
question not in terms of maximizing expected utility but rather as, “Can machines think?”
CAN MACHINES
THINK?
The computer scientist Edsger Dijkstra (1984) said that “The question of whether Ma-
chines Can Think ...i sa bout as relevant as the question of whether Submarines Can Swim .”CAN SUBMARINES
SWIM?
The American Heritage Dictionary’s ﬁrst deﬁnition of swim is “To move through water by
means of the limbs, ﬁns, or tail,” and most people agree that submarines, being limbless,
cannot swim. The dictionary also deﬁnes ﬂy as “To move through the air by means of wings
or winglike parts,” and most people agree that airplanes, having winglike parts, can ﬂy. How-
ever, neither the questions nor the answers have any relevance to the design or capabilities of
airplanes and submarines; rather they are about the usage of words in English. (The fact that
ships do swim in Russian only ampliﬁes this point.). The practical possibility of “thinking
machines” has been with us for only 50 years or so, not long enough for speakers of English to
settle on a meaning for the word “think”—does it require “a brain” or just “brain-like parts.”
Alan Turing, in his famous paper “Computing Machinery and Intelligence” (1950), sug-
gested that instead of asking whether machines can think, we should ask whether machines
can pass a behavioral intelligence test, which has come to be called theTuring Test. The test
TURING TEST
is for a program to have a conversation (via online typed messages) with an interrogator for
ﬁve minutes. The interrogator then has to guess if the conversation is with a program or a
person; the program passes the test if it fools the interrogator 30% of the time. Turing con-
jectured that, by the year 2000, a computer with a storage of 109 units could be programmed
well enough to pass the test. He was wrong—programs have yet to fool a sophisticated judge.
On the other hand, many people have been fooled when they didn’t know they might
be chatting with a computer. The E LIZA program and Internet chatbots such as M GONZ
(Humphrys, 2008) and N ATACHATA have fooled their correspondents repeatedly, and the
chatbot CYBER LOVER has attracted the attention of law enforcement because of its penchant
for tricking fellow chatters into divulging enough personal information that their identity can
be stolen. The Loebner Prize competition, held annually since 1991, is the longest-running
Turing Test-like contest. The competitions have led to better models of human typing errors.
Turing himself examined a wide variety of possible objections to the possibility of in-
telligent machines, including virtually all of those that have been raised in the half-century
since his paper appeared. We will look at some of them.
26.1.1 The argument from disability
The “argument from disability” makes the claim that “a machine can never do X.” As exam-
ples of X, Turing lists the following:
Be kind, resourceful, beautiful, friendly, have initiative, have a sense of humor, tell right
from wrong, make mistakes, fall in love, enjoy strawberries and cream, make someone
fall in love with it, learn from experience, use words properly, be the subject of its own
thought, have as much diversity of behavior as man, do something really new.
1022 Chapter 26. Philosophical Foundations
In retrospect, some of these are rather easy—we’re all familiar with computers that “make
mistakes.” We are also familiar with a century-old technology that has had a proven ability
to “make someone fall in love with it”—the teddy bear. Computer chess expert David Levy
predicts that by 2050 people will routinely fall in love with humanoid robots (Levy, 2007).
As for a robot falling in love, that is a common theme in ﬁction,
1 but there has been only lim-
ited speculation about whether it is in fact likely (Kim et al., 2007). Programs do play chess,
checkers and other games; inspect parts on assembly lines, steer cars and helicopters; diag-
nose diseases; and do hundreds of other tasks as well as or better than humans. Computers
have made small but signiﬁcant discoveries in astronomy, mathematics, chemistry, mineral-
ogy, biology, computer science, and other ﬁelds. Each of these required performance at the
level of a human expert.
Given what we now know about computers, it is not surprising that they do well at
combinatorial problems such as playing chess. But algorithms also perform at human levels
on tasks that seemingly involve human judgment, or as Turing put it, “learning from experi-
ence” and the ability to “tell right from wrong.” As far back as 1955, Paul Meehl (see also
Grove and Meehl, 1996) studied the decision-making processes of trained experts at subjec-
tive tasks such as predicting the success of a student in a training program or the recidivism
of a criminal. In 19 out of the 20 studies he looked at, Meehl found that simple statistical
learning algorithms (such as linear regression or naive Bayes) predict better than the experts.
The Educational Testing Service has used an automated program to grade millions of essay
questions on the GMAT exam since 1999. The program agrees with human graders 97% of
the time, about the same level that two human graders agree (Burstein et al., 2001).
It is clear that computers can do many things as well as or better than humans, including
things that people believe require great human insight and understanding. This does not mean,
of course, that computers use insight and understanding in performing these tasks—those are
not part of behavior, and we address such questions elsewhere—but the point is that one’s
ﬁrst guess about the mental processes required to produce a given behavior is often wrong. It
is also true, of course, that there are many tasks at which computers do not yet excel (to put
it mildly), including Turing’s task of carrying on an open-ended conversation.
26.1.2 The mathematical objection
It is well known, through the work of Turing (1936) and G¨ odel (1931), that certain math-
ematical questions are in principle unanswerable by particular formal systems. G¨ odel’s in-
completeness theorem (see Section 9.5) is the most famous example of this. Brieﬂy, for any
formal axiomatic system F powerful enough to do arithmetic, it is possible to construct a
so-called G¨odel sentence G(F) with the following properties:
•G(F) is a sentence of F , but cannot be proved within F .
•If F is consistent, then G(F) is true.
1 For example, the opera Copp´elia (1870), the novel Do Androids Dream of Electric Sheep? (1968), the movies
AI (2001) and Wal l - E(2008), and in song, Noel Coward’s 1955 version ofLet’s Do It: Let’s Fall in Love predicted
“probably we’ll live to see machines do it.” He didn’t.
Section 26.1. Weak AI: Can Machines Act Intelligently? 1023
Philosophers such as J. R. Lucas (1961) have claimed that this theorem shows that machines
are mentally inferior to humans, because machines are formal systems that are limited by the
incompleteness theorem—they cannot establish the truth of their own G¨odel sentence—while
humans have no such limitation. This claim has caused decades of controversy, spawning a
vast literature, including two books by the mathematician Sir Roger Penrose (1989, 1994)
that repeat the claim with some fresh twists (such as the hypothesis that humans are different
because their brains operate by quantum gravity). We will examine only three of the problems
with the claim.
First, G¨odel’s incompleteness theorem applies only to formal systems that are powerful
enough to do arithmetic. This includes Turing machines, and Lucas’s claim is in part based
on the assertion that computers are Turing machines. This is a good approximation, but is not
quite true. Turing machines are inﬁnite, whereas computers are ﬁnite, and any computer can
therefore be described as a (very large) system in propositional logic, which is not subject to
G¨odel’s incompleteness theorem. Second, an agent should not be too ashamed that it cannot
establish the truth of some sentence while other agents can. Consider the sentence
J. R. Lucas cannot consistently assert that this sentence is true.
If Lucas asserted this sentence, then he would be contradicting himself, so therefore Lucas
cannot consistently assert it, and hence it must be true. We have thus demonstrated that there
is a sentence that Lucas cannot consistently assert while other people (and machines) can. But
that does not make us think less of Lucas. To take another example, no human could compute
the sum of a billion 10 digit numbers in his or her lifetime, but a computer could do it in
seconds. Still, we do not see this as a fundamental limitation in the human’s ability to think.
Humans were behaving intelligently for thousands of years before they invented mathematics,
so it is unlikely that formal mathematical reasoning plays more than a peripheral role in what
it means to be intelligent.
Third, and most important, even if we grant that computers have limitations on what
they can prove, there is no evidence that humans are immune from those limitations. It is
all too easy to show rigorously that a formal system cannot do X, and then claim that hu-
mans can do X using their own informal method, without giving any evidence for this claim.
Indeed, it is impossible to prove that humans are not subject to G¨odel’s incompleteness theo-
rem, because any rigorous proof would require a formalization of the claimed unformalizable
human talent, and hence refute itself. So we are left with an appeal to intuition that humans
can somehow perform superhuman feats of mathematical insight. This appeal is expressed
with arguments such as “we must assume our own consistency, if thought is to be possible at
all” (Lucas, 1976). But if anything, humans are known to be inconsistent. This is certainly
true for everyday reasoning, but it is also true for careful mathematical thought. A famous
example is the four-color map problem. Alfred Kempe published a proof in 1879 that was
widely accepted and contributed to his election as a Fellow of the Royal Society. In 1890,
however, Percy Heawood pointed out a ﬂaw and the theorem remained unproved until 1977.
1024 Chapter 26. Philosophical Foundations
26.1.3 The argument from informality
One of the most inﬂuential and persistent criticisms of AI as an enterprise was raised by Tur-
ing as the “argument from informality of behavior.” Essentially, this is the claim that human
behavior is far too complex to be captured by any simple set of rules and that because com-
puters can do no more than follow a set of rules, they cannot generate behavior as intelligent
as that of humans. The inability to capture everything in a set of logical rules is called the
qualiﬁcation problem in AI.
QUALIFICA TION
PROBLEM
The principal proponent of this view has been the philosopher Hubert Dreyfus, who
has produced a series of inﬂuential critiques of artiﬁcial intelligence: What Computers Can’t
Do (1972), the sequel What Computers Still Can’t Do (1992), and, with his brother Stuart,
Mind Over Machine (1986).
The position they criticize came to be called “Good Old-Fashioned AI,” or GOFAI ,a
term coined by philosopher John Haugeland (1985). GOFAI is supposed to claim that all
intelligent behavior can be captured by a system that reasons logically from a set of facts and
rules describing the domain. It therefore corresponds to the simplest logical agent described
in Chapter 7. Dreyfus is correct in saying that logical agents are vulnerable to the qualiﬁcation
problem. As we saw in Chapter 13, probabilistic reasoning systems are more appropriate for
open-ended domains. The Dreyfus critique therefore is not addressed against computers per
se, but rather against one particular way of programming them. It is reasonable to suppose,
however, that a book called What First-Order Logical Rule-Based Systems Without Learning
Can’t Do might have had less impact.
Under Dreyfus’s view, human expertise does include knowledge of some rules, but only
as a “holistic context” or “background” within which humans operate. He gives the example
of appropriate social behavior in giving and receiving gifts: “Normally one simply responds
in the appropriate circumstances by giving an appropriate gift.” One apparently has “a direct
sense of how things are done and what to expect.” The same claim is made in the context of
chess playing: “A mere chess master might need to ﬁgure out what to do, but a grandmaster
just sees the board as demanding a certain move . . . the right response just pops into his or her
head.” It is certainly true that much of the thought processes of a present-giver or grandmaster
is done at a level that is not open to introspection by the conscious mind. But that does not
mean that the thought processes do not exist. The important question that Dreyfus does not
answer is how the right move gets into the grandmaster’s head. One is reminded of Daniel
Dennett’s (1984) comment,
It is rather as if philosophers were to proclaim themselves expert explainers of the meth-
ods of stage magicians, and then, when we ask how the magician does the sawing-the-
lady-in-half trick, they explain that it is really quite obvious: the magician doesn’t really
saw her in half; he simply makes it appear that he does. “But how does he do that?” we
ask. “Not our department,” say the philosophers.
Dreyfus and Dreyfus (1986) propose a ﬁve-stage process of acquiring expertise, beginning
with rule-based processing (of the sort proposed in GOFAI ) and ending with the ability to
select correct responses instantaneously. In making this proposal, Dreyfus and Dreyfus in
effect move from being AI critics to AI theorists—they propose a neural network architecture
Section 26.1. Weak AI: Can Machines Act Intelligently? 1025
organized into a vast “case library,” but point out several problems. Fortunately, all of their
problems have been addressed, some with partial success and some with total success. Their
problems include the following:
1. Good generalization from examples cannot be achieved without background knowl-
edge. They claim no one has any idea how to incorporate background knowledge into
the neural network learning process. In fact, we saw in Chapters 19 and 20 that there
are techniques for using prior knowledge in learning algorithms. Those techniques,
however, rely on the availability of knowledge in explicit form, something that Dreyfus
and Dreyfus strenuously deny. In our view, this is a good reason for a serious redesign
of current models of neural processing so that they can take advantage of previously
learned knowledge in the way that other learning algorithms do.
2. Neural network learning is a form of supervised learning (see Chapter 18), requiring
the prior identiﬁcation of relevant inputs and correct outputs. Therefore, they claim,
it cannot operate autonomously without the help of a human trainer. In fact, learning
without a teacher can be accomplished by unsupervised learning (Chapter 20) and
reinforcement learning (Chapter 21).
3. Learning algorithms do not perform well with many features, and if we pick a subset
of features, “there is no known way of adding new features should the current set prove
inadequate to account for the learned facts.” In fact, new methods such as support
vector machines handle large feature sets very well. With the introduction of large
Web-based data sets, many applications in areas such as language processing (Sha and
Pereira, 2003) and computer vision (Viola and Jones, 2002a) routinely handle millions
of features. We saw in Chapter 19 that there are also principled ways to generate new
features, although much more work is needed.
4. The brain is able to direct its sensors to seek relevant information and to process it
to extract aspects relevant to the current situation. But, Dreyfus and Dreyfus claim,
“Currently, no details of this mechanism are understood or even hypothesized in a way
that could guide AI research.” In fact, the ﬁeld of active vision, underpinned by the
theory of information value (Chapter 16), is concerned with exactly the problem of
directing sensors, and already some robots have incorporated the theoretical results
obtained. S
TANLEY ’s 132-mile trip through the desert (page 28) was made possible in
large part by an active sensing system of this kind.
In sum, many of the issues Dreyfus has focused on—background commonsense knowledge,
the qualiﬁcation problem, uncertainty, learning, compiled forms of decision making—are
indeed important issues, and have by now been incorporated into standard intelligent agent
design. In our view, this is evidence of AI’s progress, not of its impossibility.
One of Dreyfus’ strongest arguments is for situated agents rather than disembodied
logical inference engines. An agent whose understanding of “dog” comes only from a limited
set of logical sentences such as “ Dog(x) ⇒Mammal(x)” is at a disadvantage compared
to an agent that has watched dogs run, has played fetch with them, and has been licked by
one. As philosopher Andy Clark (1998) says, “Biological brains are ﬁrst and foremost the
control systems for biological bodies. Biological bodies move and act in rich real-world
1026 Chapter 26. Philosophical Foundations
surroundings.” To understand how human (or other animal) agents work, we have to consider
the whole agent, not just the agent program. Indeed, theembodied cognition approach claimsEMBODIED
COGNITION
that it makes no sense to consider the brain separately: cognition takes place within a body,
which is embedded in an environment. We need to study the system as a whole; the brain
augments its reasoning by referring to the environment, as the reader does in perceiving (and
creating) marks on paper to transfer knowledge. Under the embodied cognition program,
robotics, vision, and other sensors become central, not peripheral.
26.2 S TRONG AI: C AN MACHINES REALLY THINK ?
Many philosophers have claimed that a machine that passes the Turing Test would still not
be actually thinking, but would be only a simulation of thinking. Again, the objection was
foreseen by Turing. He cites a speech by Professor Geoffrey Jefferson (1949):
Not until a machine could write a sonnet or compose a concerto because of thoughts and
emotions felt, and not by the chance fall of symbols, could we agree that machine equals
brain—that is, not only write it but know that it had written it.
Turing calls this the argument from consciousness—the machine has to be aware of its own
mental states and actions. While consciousness is an important subject, Jefferson’s key point
actually relates to phenomenology, or the study of direct experience: the machine has to
actually feel emotions. Others focus on intentionality—that is, the question of whether the
machine’s purported beliefs, desires, and other representations are actually “about” some-
thing in the real world.
Turing’s response to the objection is interesting. He could have presented reasons that
machines can in fact be conscious (or have phenomenology, or have intentions). Instead, he
maintains that the question is just as ill-deﬁned as asking, “Can machines think?” Besides,
why should we insist on a higher standard for machines than we do for humans? After all,
in ordinary life we never have any direct evidence about the internal mental states of other
humans. Nevertheless, Turing says, “Instead of arguing continually over this point, it is usual
to have the polite convention that everyone thinks.”
Turing argues that Jefferson would be willing to extend the polite convention to ma-
chines if only he had experience with ones that act intelligently. He cites the following dialog,
which has become such a part of AI’s oral tradition that we simply have to include it:
HUMAN : In the ﬁrst line of your sonnet which reads “shall I compare thee to a summer’s
day,” would not a “spring day” do as well or better?
MACHINE : It wouldn’t scan.
HUMAN : How about “a winter’s day.” That would scan all right.
MACHINE : Yes, but nobody wants to be compared to a winter’s day.
HUMAN : Would you say Mr. Pickwick reminded you of Christmas?
MACHINE :I naw a y .
HUMAN : Yet Christmas is a winter’s day, and I do not think Mr. Pickwick would mind
the comparison.
Section 26.2. Strong AI: Can Machines Really Think? 1027
MACHINE : I don’t think you’re serious. By a winter’s day one means a typical winter’s
day, rather than a special one like Christmas.
One can easily imagine some future time in which such conversations with machines are
commonplace, and it becomes customary to make no linguistic distinction between “real”
and “artiﬁcial” thinking. A similar transition occurred in the years after 1848, when artiﬁcial
urea was synthesized for the ﬁrst time by Frederick W¨ohler. Prior to this event, organic and
inorganic chemistry were essentially disjoint enterprises and many thought that no process
could exist that would convert inorganic chemicals into organic material. Once the synthesis
was accomplished, chemists agreed that artiﬁcial urea was urea, because it had all the right
physical properties. Those who had posited an intrinsic property possessed by organic ma-
terial that inorganic material could never have were faced with the impossibility of devising
any test that could reveal the supposed deﬁciency of artiﬁcial urea.
For thinking, we have not yet reached our 1848 and there are those who believe that
artiﬁcial thinking, no matter how impressive, will never be real. For example, the philosopher
John Searle (1980) argues as follows:
No one supposes that a computer simulation of a storm will leave us all wet ... Why on
earth would anyone in his right mind suppose a computer simulation of mental processes
actually had mental processes? (pp. 37–38)
While it is easy to agree that computer simulations of storms do not make us wet, it is not
clear how to carry this analogy over to computer simulations of mental processes. After
all, a Hollywood simulation of a storm using sprinklers and wind machines does make the
actors wet, and a video game simulation of a storm does make the simulated characters wet.
Most people are comfortable saying that a computer simulation of addition is addition, and
of chess is chess. In fact, we typically speak of an implementation of addition or chess, not a
simulation. Are mental processes more like storms, or more like addition?
Turing’s answer—the polite convention—suggests that the issue will eventually go
away by itself once machines reach a certain level of sophistication. This would have the
effect of dissolving the difference between weak and strong AI. Against this, one may insist
that there is a factual issue at stake: humans do have real minds, and machines might or
might not. To address this factual issue, we need to understand how it is that humans have
real minds, not just bodies that generate neurophysiological processes. Philosophical efforts
to solve this mind–body problem are directly relevant to the question of whether machines
MIND–BODY
PROBLEM
could have real minds.
The mind–body problem was considered by the ancient Greek philosophers and by var-
ious schools of Hindu thought, but was ﬁrst analyzed in depth by the 17th-century French
philosopher and mathematician Ren´e Descartes. His Meditations on First Philosophy (1641)
considered the mind’s activity of thinking (a process with no spatial extent or material prop-
erties) and the physical processes of the body, concluding that the two must exist in separate
realms—what we would now call a dualist theory. The mind–body problem faced by du-
DUALISM
alists is the question of how the mind can control the body if the two are really separate.
Descartes speculated that the two might interact through the pineal gland, which simply begs
the question of how the mind controls the pineal gland.
1028 Chapter 26. Philosophical Foundations
The monist theory of mind, often called physicalism, avoids this problem by assertingMONISM
PHYSICALISM the mind is not separate from the body—that mental states are physical states. Most modern
philosophers of mind are physicalists of one form or another, and physicalism allows, at least
in principle, for the possibility of strong AI. The problem for physicalists is to explain how
physical states—in particular, the molecular conﬁgurations and electrochemical processes of
the brain—can simultaneously bemental states, such as being in pain, enjoying a hamburger,
MENTAL STA TES
knowing that one is riding a horse, or believing that Vienna is the capital of Austria.
26.2.1 Mental states and the brain in a vat
Physicalist philosophers have attempted to explicate what it means to say that a person—and,
by extension, a computer—is in a particular mental state. They have focused in particular on
intentional states. These are states, such as believing, knowing, desiring, fearing, and so on,
INTENTIONAL ST A TE
that refer to some aspect of the external world. For example, the knowledge that one is eating
a hamburger is a belief about the hamburger and what is happening to it.
If physicalism is correct, it must be the case that the proper description of a person’s
mental state is determined by that person’s brain state. Thus, if I am currently focused on
eating a hamburger in a mindful way, my instantaneous brain state is an instance of the class of
mental states “knowing that one is eating a hamburger.” Of course, the speciﬁc conﬁgurations
of all the atoms of my brain are not essential: there are many conﬁgurations of my brain, or
of other people’s brain, that would belong to the same class of mental states. The key point is
that the same brain state could not correspond to a fundamentally distinct mental state, such
as the knowledge that one is eating a banana.
The simplicity of this view is challenged by some simple thought experiments. Imag-
ine, if you will, that your brain was removed from your body at birth and placed in a mar-
velously engineered vat. The vat sustains your brain, allowing it to grow and develop. At the
same time, electronic signals are fed to your brain from a computer simulation of an entirely
ﬁctitious world, and motor signals from your brain are intercepted and used to modify the
simulation as appropriate.
2 In fact, the simulated life you live replicates exactly the life you
would have lived, had your brain not been placed in the vat, including simulated eating of
simulated hamburgers. Thus, you could have a brain state identical to that of someone who is
really eating a real hamburger, but it would be literally false to say that you have the mental
state “knowing that one is eating a hamburger.” You aren’t eating a hamburger, you have
never even experienced a hamburger, and you could not, therefore, have such a mental state.
This example seems to contradict the view that brain states determine mental states. One
way to resolve the dilemma is to say that the content of mental states can be interpreted from
two different points of view. The “ wide content” view interprets it from the point of view
WIDE CONTENT
of an omniscient outside observer with access to the whole situation, who can distinguish
differences in the world. Under this view, the content of mental states involves both the brain
state and the environment history. Narrow content, on the other hand, considers only theNARROW CONTENT
brain state. The narrow content of the brain states of a real hamburger-eater and a brain-in-a-
vat “hamburger”-“eater” is the same in both cases.
2 This situation may be familiar to those who have seen the 1999 ﬁlm The Matrix.
Section 26.2. Strong AI: Can Machines Really Think? 1029
Wide content is entirely appropriate if one’s goals are to ascribe mental states to others
who share one’s world, to predict their likely behavior and its effects, and so on. This is the
setting in which our ordinary language about mental content has evolved. On the other hand,
if one is concerned with the question of whether AI systems are really thinking and really
do have mental states, then narrow content is appropriate; it simply doesn’t make sense to
say that whether or not an AI system is really thinking depends on conditions outside that
system. Narrow content is also relevant if we are thinking about designing AI systems or
understanding their operation, because it is the narrow content of a brain state that determines
what will be the (narrow content of the) next brain state. This leads naturally to the idea that
what matters about a brain state—what makes it have one kind of mental content and not
another—is its functional role within the mental operation of the entity involved.
26.2.2 Functionalism and the brain replacement experiment
The theory of functionalism says that a mental state is any intermediate causal conditionFUNCTIONALISM
between input and output. Under functionalist theory, any two systems with isomorphic
causal processes would have the same mental states. Therefore, a computer program could
have the same mental states as a person. Of course, we have not yet said what “isomorphic”
really means, but the assumption is that there is some level of abstraction below which the
speciﬁc implementation does not matter.
The claims of functionalism are illustrated most clearly by the brain replacement ex-
periment. This thought experiment was introduced by the philosopher Clark Glymour and
was touched on by John Searle (1980), but is most commonly associated with roboticist Hans
Moravec (1988). It goes like this: Suppose neurophysiology has developed to the point where
the input–output behavior and connectivity of all the neurons in the human brain are perfectly
understood. Suppose further that we can build microscopic electronic devices that mimic this
behavior and can be smoothly interfaced to neural tissue. Lastly, suppose that some mirac-
ulous surgical technique can replace individual neurons with the corresponding electronic
devices without interrupting the operation of the brain as a whole. The experiment consists
of gradually replacing all the neurons in someone’s head with electronic devices.
We are concerned with both the external behavior and the internal experience of the
subject, during and after the operation. By the deﬁnition of the experiment, the subject’s
external behavior must remain unchanged compared with what would be observed if the
operation were not carried out.
3 Now although the presence or absence of consciousness
cannot easily be ascertained by a third party, the subject of the experiment ought at least to
be able to record any changes in his or her own conscious experience. Apparently, there is
a direct clash of intuitions as to what would happen. Moravec, a robotics researcher and
functionalist, is convinced his consciousness would remain unaffected. Searle, a philosopher
and biological naturalist, is equally convinced his consciousness would vanish:
You ﬁnd, to your total amazement, that you are indeed losing control of your external
behavior. You ﬁnd, for example, that when doctors test your vision, you hear them say
“We are holding up a red object in front of you; please tell us what you see.” You want
3 One can imagine using an identical “control” subject who is given a placebo operation, for comparison.
1030 Chapter 26. Philosophical Foundations
to cry out “I can’t see anything. I’m going totally blind.” But you hear your voice saying
in a way that is completely out of your control, “I see a red object in front of me.” ...
your conscious experience slowly shrinks to nothing, while your externally observable
behavior remains the same. (Searle, 1992)
One can do more than argue from intuition. First, note that, for the external behavior to re-
main the same while the subject gradually becomes unconscious, it must be the case that the
subject’s volition is removed instantaneously and totally; otherwise the shrinking of aware-
ness would be reﬂected in external behavior—“Help, I’m shrinking!” or words to that effect.
This instantaneous removal of volition as a result of gradual neuron-at-a-time replacement
seems an unlikely claim to have to make.
Second, consider what happens if we do ask the subject questions concerning his or
her conscious experience during the period when no real neurons remain. By the conditions
of the experiment, we will get responses such as “I feel ﬁne. I must say I’m a bit surprised
because I believed Searle’s argument.” Or we might poke the subject with a pointed stick and
observe the response, “Ouch, that hurt.” Now, in the normal course of affairs, the skeptic can
dismiss such outputs from AI programs as mere contrivances. Certainly, it is easy enough to
use a rule such as “If sensor 12 reads ‘High’ then output ‘Ouch.’ ” But the point here is that,
because we have replicated the functional properties of a normal human brain, we assume
that the electronic brain contains no such contrivances. Then we must have an explanation of
the manifestations of consciousness produced by the electronic brain that appeals only to the
functional properties of the neurons. And this explanation must also apply to the real brain,
which has the same functional properties. There are three possible conclusions:
1. The causal mechanisms of consciousness that generate these kinds of outputs in normal
brains are still operating in the electronic version, which is therefore conscious.
2. The conscious mental events in the normal brain have no causal connection to behavior,
and are missing from the electronic brain, which is therefore not conscious.
3. The experiment is impossible, and therefore speculation about it is meaningless.
Although we cannot rule out the second possibility, it reduces consciousness to what philoso-
phers call an epiphenomenal role—something that happens, but casts no shadow, as it were,
EPIPHENOMENON
on the observable world. Furthermore, if consciousness is indeed epiphenomenal, then it
cannot be the case that the subject says “Ouch” because it hurts—that is, because of the con-
scious experience of pain. Instead, the brain must contain a second, unconscious mechanism
that is responsible for the “Ouch.”
Patricia Churchland (1986) points out that the functionalist arguments that operate at
the level of the neuron can also operate at the level of any larger functional unit—a clump
of neurons, a mental module, a lobe, a hemisphere, or the whole brain. That means that if
you accept the notion that the brain replacement experiment shows that the replacement brain
is conscious, then you should also believe that consciousness is maintained when the entire
brain is replaced by a circuit that updates its state and maps from inputs to outputs via a huge
lookup table. This is disconcerting to many people (including Turing himself), who have
the intuition that lookup tables are not conscious—or at least, that the conscious experiences
generated during table lookup are not the same as those generated during the operation of a
Section 26.2. Strong AI: Can Machines Really Think? 1031
system that might be described (even in a simple-minded, computational sense) as accessing
and generating beliefs, introspections, goals, and so on.
26.2.3 Biological naturalism and the Chinese Room
A strong challenge to functionalism has been mounted by John Searle’s (1980) biological
naturalism, according to which mental states are high-level emergent features that are causedBIOLOGICAL
NA TURALISM
by low-level physical processes in the neurons , and it is the (unspeciﬁed) properties of the
neurons that matter. Thus, mental states cannot be duplicated just on the basis of some pro-
gram having the same functional structure with the same input–output behavior; we would
require that the program be running on an architecture with the same causal power as neurons.
To support his view, Searle describes a hypothetical system that is clearly running a program
and passes the Turing Test, but that equally clearly (according to Searle) does notunderstand
anything of its inputs and outputs. His conclusion is that running the appropriate program
(i.e., having the right outputs) is not a sufﬁcient condition for being a mind.
The system consists of a human, who understands only English, equipped with a rule
book, written in English, and various stacks of paper, some blank, some with indecipherable
inscriptions. (The human therefore plays the role of the CPU, the rule book is the program,
and the stacks of paper are the storage device.) The system is inside a room with a small
opening to the outside. Through the opening appear slips of paper with indecipherable sym-
bols. The human ﬁnds matching symbols in the rule book, and follows the instructions. The
instructions may include writing symbols on new slips of paper, ﬁnding symbols in the stacks,
rearranging the stacks, and so on. Eventually, the instructions will cause one or more symbols
to be transcribed onto a piece of paper that is passed back to the outside world.
So far, so good. But from the outside, we see a system that is taking input in the form
of Chinese sentences and generating answers in Chinese that are as “intelligent” as those
in the conversation imagined by Turing.
4 Searle then argues: the person in the room does
not understand Chinese (given). The rule book and the stacks of paper, being just pieces of
paper, do not understand Chinese. Therefore, there is no understanding of Chinese. Hence,
according to Searle, running the right program does not necessarily generate understanding.
Like Turing, Searle considered and attempted to rebuff a number of replies to his ar-
gument. Several commentators, including John McCarthy and Robert Wilensky, proposed
what Searle calls the systems reply. The objection is that asking if the human in the room
understands Chinese is analogous to asking if the CPU can take cube roots. In both cases,
the answer is no, and in both cases, according to the systems reply, the entire system does
have the capacity in question. Certainly, if one asks the Chinese Room whether it understands
Chinese, the answer would be afﬁrmative (in ﬂuent Chinese). By Turing’s polite convention,
this should be enough. Searle’s response is to reiterate the point that the understanding is not
in the human and cannot be in the paper, so there cannot be any understanding. He seems to
be relying on the argument that a property of the whole must reside in one of the parts. Yet
4 The fact that the stacks of paper might contain trillions of pages and the generation of answers would take
millions of years has no bearing on the logical structure of the argument. One aim of philosophical training is to
develop a ﬁnely honed sense of which objections are germane and which are not.
1032 Chapter 26. Philosophical Foundations
water is wet, even though neither H nor O2 is. The real claim made by Searle rests upon the
following four axioms (Searle, 1990):
1. Computer programs are formal (syntactic).
2. Human minds have mental contents (semantics).
3. Syntax by itself is neither constitutive of nor sufﬁcient for semantics.
4. Brains cause minds.
From the ﬁrst three axioms Searle concludes that programs are not sufﬁcient for minds. In
other words, an agent running a programmight be a mind, but it is notnecessarily am i n dj u s t
by virtue of running the program. From the fourth axiom he concludes “Any other system
capable of causing minds would have to have causal powers (at least) equivalent to those
of brains.” From there he infers that any artiﬁcial brain would have to duplicate the causal
powers of brains, not just run a particular program, and that human brains do not produce
mental phenomena solely by virtue of running a program.
The axioms are controversial. For example, axioms 1 and 2 rely on an unspeciﬁed
distinction between syntax and semantics that seems to be closely related to the distinction
between narrow and wide content. On the one hand, we can view computers as manipulating
syntactic symbols; on the other, we can view them as manipulating electric current, which
happens to be what brains mostly do (according to our current understanding). So it seems
we could equally say that brains are syntactic.
Assuming we are generous in interpreting the axioms, then the conclusion—that pro-
grams are not sufﬁcient for minds— does follow. But the conclusion is unsatisfactory—all
Searle has shown is that if you explicitly deny functionalism (that is what his axiom 3 does),
then you can’t necessarily conclude that non-brains are minds. This is reasonable enough—
almost tautological—so the whole argument comes down to whether axiom 3 can be ac-
cepted. According to Searle, the point of the Chinese Room argument is to provide intuitions
for axiom 3. The public reaction shows that the argument is acting as what Daniel Dennett
(1991) calls an intuition pump: it ampliﬁes one’s prior intuitions, so biological naturalists
INTUITION PUMP
are more convinced of their positions, and functionalists are convinced only that axiom 3 is
unsupported, or that in general Searle’s argument is unconvincing. The argument stirs up
combatants, but has done little to change anyone’s opinion. Searle remains undeterred, and
has recently started calling the Chinese Room a “refutation” of strong AI rather than just an
“argument” (Snell, 2008).
Even those who accept axiom 3, and thus accept Searle’s argument, have only their in-
tuitions to fall back on when deciding what entities are minds. The argument purports to show
that the Chinese Room is not a mind by virtue of running the program, but the argument says
nothing about how to decide whether the room (or a computer, some other type of machine,
or an alien) is a mind by virtue of some other reason. Searle himself says that some machines
do have minds: humans are biological machines with minds. According to Searle, human
brains may or may not be running something like an AI program, but if they are, that is not
the reason they are minds. It takes more to make a mind—according to Searle, something
equivalent to the causal powers of individual neurons. What these powers are is left unspec-
iﬁed. It should be noted, however, that neurons evolved to fulﬁll functional roles—creatures
Section 26.2. Strong AI: Can Machines Really Think? 1033
with neurons were learning and deciding long before consciousness appeared on the scene. It
would be a remarkable coincidence if such neurons just happened to generate consciousness
because of some causal powers that are irrelevant to their functional capabilities; after all, it
is the functional capabilities that dictate survival of the organism.
In the case of the Chinese Room, Searle relies on intuition, not proof: just look at the
room; what’s there to be a mind? But one could make the same argument about the brain:
just look at this collection of cells (or of atoms), blindly operating according to the laws of
biochemistry (or of physics)—what’s there to be a mind? Why can a hunk of brain be a mind
while a hunk of liver cannot? That remains the great mystery.
26.2.4 Consciousness, qualia, and the explanatory gap
Running through all the debates about strong AI—the elephant in the debating room, so
to speak—is the issue of consciousness. Consciousness is often broken down into aspects
CONSCIOUSNESS
such as understanding and self-awareness. The aspect we will focus on is that of subjective
experience: why it is that it feels like something to have certain brain states (e.g., while eating
a hamburger), whereas it presumably does not feel like anything to have other physical states
(e.g., while being a rock). The technical term for the intrinsic nature of experiences is qualiaQUALIA
(from the Latin word meaning, roughly, “such things”).
Qualia present a challenge for functionalist accounts of the mind because different
qualia could be involved in what are otherwise isomorphic causal processes. Consider, for
example, the inverted spectrum thought experiment, which the subjective experience of per-
INVERTED
SPECTRUM
son X when seeing red objects is the same experience that the rest of us experience when
seeing green objects, and vice versa.X still calls red objects “red,” stops for red trafﬁc lights,
and agrees that the redness of red trafﬁc lights is a more intense red than the redness of the
setting sun. Yet, X’s subjective experience is just different.
Qualia are challenging not just for functionalism but for all of science. Suppose, for the
sake of argument, that we have completed the process of scientiﬁc research on the brain—we
have found that neural process P
12 in neuron N177 transforms molecule A into molecule B,
and so on, and on. There is simply no currently accepted form of reasoning that would lead
from such ﬁndings to the conclusion that the entity owning those neurons has any particular
subjective experience. This explanatory gap has led some philosophers to conclude that
EXPLANA TORY GAP
humans are simply incapable of forming a proper understanding of their own consciousness.
Others, notably Daniel Dennett (1991), avoid the gap by denying the existence of qualia,
attributing them to a philosophical confusion.
Turing himself concedes that the question of consciousness is a difﬁcult one, but denies
that it has much relevance to the practice of AI: “I do not wish to give the impression that I
think there is no mystery about consciousness ... But I do not think these mysteries neces-
sarily need to be solved before we can answer the question with which we are concerned in
this paper.” We agree with Turing—we are interested in creating programs that behave intel-
ligently. The additional project of making them conscious is not one that we are equipped to
take on, nor one whose success we would be able to determine.
1034 Chapter 26. Philosophical Foundations
26.3 T HE ETHICS AND RISKS OF DEVELOPING ARTIFICIAL INTELLIGENCE
So far, we have concentrated on whether we can develop AI, but we must also consider
whether we should. If the effects of AI technology are more likely to be negative than positive,
then it would be the moral responsibility of workers in the ﬁeld to redirect their research.
Many new technologies have had unintended negative side effects: nuclear ﬁssion brought
Chernobyl and the threat of global destruction; the internal combustion engine brought air
pollution, global warming, and the paving-over of paradise. In a sense, automobiles are
robots that have conquered the world by making themselves indispensable.
All scientists and engineers face ethical considerations of how they should act on the
job, what projects should or should not be done, and how they should be handled. See the
handbook on the Ethics of Computing (Berleur and Brunnstein, 2001). AI, however, seems
to pose some fresh problems beyond that of, say, building bridges that don’t fall down:
•People might lose their jobs to automation.
•People might have too much (or too little) leisure time.
•People might lose their sense of being unique.
•AI systems might be used toward undesirable ends.
•The use of AI systems might result in a loss of accountability.
•The success of AI might mean the end of the human race.
We will look at each issue in turn.
People might lose their jobs to automation. The modern industrial economy has be-
come dependent on computers in general, and select AI programs in particular. For example,
much of the economy, especially in the United States, depends on the availability of con-
sumer credit. Credit card applications, charge approvals, and fraud detection are now done
by AI programs. One could say that thousands of workers have been displaced by these AI
programs, but in fact if you took away the AI programs these jobs would not exist, because
human labor would add an unacceptable cost to the transactions. So far, automation through
information technology in general and AI in particular has created more jobs than it has
eliminated, and has created more interesting, higher-paying jobs. Now that the canonical AI
program is an “intelligent agent” designed to assist a human, loss of jobs is less of a concern
than it was when AI focused on “expert systems” designed to replace humans. But some
researchers think that doing the complete job is the right goal for AI. In reﬂecting on the 25th
Anniversary of the AAAI, Nils Nilsson (2005) set as a challenge the creation of human-level
AI that could pass the employment test rather than the Turing Test—a robot that could learn
to do any one of a range of jobs. We may end up in a future where unemployment is high, but
even the unemployed serve as managers of their own cadre of robot workers.
People might have too much (or too little) leisure time. Alvin Tofﬂer wrote inFuture
Shock (1970), “The work week has been cut by 50 percent since the turn of the century. It
is not out of the way to predict that it will be slashed in half again by 2000.” Arthur C.
Clarke (1968b) wrote that people in 2001 might be “faced with a future of utter boredom,
where the main problem in life is deciding which of several hundred TV channels to select.”
Section 26.3. The Ethics and Risks of Developing Artiﬁcial Intelligence 1035
The only one of these predictions that has come close to panning out is the number of TV
channels. Instead, people working in knowledge-intensive industries have found themselves
part of an integrated computerized system that operates 24 hours a day; to keep up, they have
been forced to worklonger hours. In an industrial economy, rewards are roughly proportional
to the time invested; working 10% more would tend to mean a 10% increase in income. In
an information economy marked by high-bandwidth communication and easy replication of
intellectual property (what Frank and Cook (1996) call the “Winner-Take-All Society”), there
is a large reward for being slightly better than the competition; working 10% more could mean
a 100% increase in income. So there is increasing pressure on everyone to work harder. AI
increases the pace of technological innovation and thus contributes to this overall trend, but
AI also holds the promise of allowing us to take some time off and let our automated agents
handle things for a while. Tim Ferriss (2007) recommends using automation and outsourcing
to achieve a four-hour work week.
People might lose their sense of being unique. In Computer Power and Human Rea-
son, Weizenbaum (1976), the author of the E
LIZA program, points out some of the potential
threats that AI poses to society. One of Weizenbaum’s principal arguments is that AI research
makes possible the idea that humans are automata—an idea that results in a loss of autonomy
or even of humanity. We note that the idea has been around much longer than AI, going back
at least to L’Homme Machine (La Mettrie, 1748). Humanity has survived other setbacks to
our sense of uniqueness: De Revolutionibus Orbium Coelestium (Copernicus, 1543) moved
the Earth away from the center of the solar system, and Descent of Man (Darwin, 1871) put
Homo sapiens at the same level as other species. AI, if widely successful, may be at least as
threatening to the moral assumptions of 21st-century society as Darwin’s theory of evolution
was to those of the 19th century.
AI systems might be used toward undesirable ends. Advanced technologies have
often been used by the powerful to suppress their rivals. As the number theorist G. H. Hardy
wrote (Hardy, 1940), “A science is said to be useful if its development tends to accentuate the
existing inequalities in the distribution of wealth, or more directly promotes the destruction
of human life.” This holds for all sciences, AI being no exception. Autonomous AI systems
are now commonplace on the battleﬁeld; the U.S. military deployed over 5,000 autonomous
aircraft and 12,000 autonomous ground vehicles in Iraq (Singer, 2009). One moral theory
holds that military robots are like medieval armor taken to its logical extreme: no one would
have moral objections to a soldier wanting to wear a helmet when being attacked by large,
angry, axe-wielding enemies, and a teleoperated robot is like a very safe form of armor. On
the other hand, robotic weapons pose additional risks. To the extent that human decision
making is taken out of the ﬁring loop, robots may end up making decisions that lead to the
killing of innocent civilians. At a larger scale, the possession of powerful robots (like the
possession of sturdy helmets) may give a nation overconﬁdence, causing it to go to war more
recklessly than necessary. In most wars, at least one party is overconﬁdent in its military
abilities—otherwise the conﬂict would have been resolved peacefully.
Weizenbaum (1976) also pointed out that speech recognition technology could lead to
widespread wiretapping, and hence to a loss of civil liberties. He didn’t foresee a world with
terrorist threats that would change the balance of how much surveillance people are willing to
1036 Chapter 26. Philosophical Foundations
accept, but he did correctly recognize that AI has the potential to mass-produce surveillance.
His prediction has in part come true: the U.K. now has an extensive network of surveillance
cameras, and other countries routinely monitor Web trafﬁc and telephone calls. Some accept
that computerization leads to a loss of privacy—Sun Microsystems CEO Scott McNealy has
said “You have zero privacy anyway. Get over it.” David Brin (1998) argues that loss of
privacy is inevitable, and the way to combat the asymmetry of power of the state over the
individual is to make the surveillance accessible to all citizens. Etzioni (2004) argues for a
balancing of privacy and security; individual rights and community.
The use of AI systems might result in a loss of accountability. In the litigious atmo-
sphere that prevails in the United States, legal liability becomes an important issue. When a
physician relies on the judgment of a medical expert system for a diagnosis, who is at fault if
the diagnosis is wrong? Fortunately, due in part to the growing inﬂuence of decision-theoretic
methods in medicine, it is now accepted that negligence cannot be shown if the physician
performs medical procedures that have high expected utility, even if theactual result is catas-
trophic for the patient. The question should therefore be “Who is at fault if the diagnosis is
unreasonable?” So far, courts have held that medical expert systems play the same role as
medical textbooks and reference books; physicians are responsible for understanding the rea-
soning behind any decision and for using their own judgment in deciding whether to accept
the system’s recommendations. In designing medical expert systems as agents, therefore,
the actions should be thought of not as directly affecting the patient but as inﬂuencing the
physician’s behavior. If expert systems become reliably more accurate than human diagnosti-
cians, doctors might become legally liable if theydon’t use the recommendations of an expert
system. Atul Gawande (2002) explores this premise.
Similar issues are beginning to arise regarding the use of intelligent agents on the Inter-
net. Some progress has been made in incorporating constraints into intelligent agents so that
they cannot, for example, damage the ﬁles of other users (Weld and Etzioni, 1994). The prob-
lem is magniﬁed when money changes hands. If monetary transactions are made “on one’s
behalf” by an intelligent agent, is one liable for the debts incurred? Would it be possible for
an intelligent agent to have assets itself and to perform electronic trades on its own behalf?
So far, these questions do not seem to be well understood. To our knowledge, no program
has been granted legal status as an individual for the purposes of ﬁnancial transactions; at
present, it seems unreasonable to do so. Programs are also not considered to be “drivers”
for the purposes of enforcing trafﬁc regulations on real highways. In California law, at least,
there do not seem to be any legal sanctions to prevent an automated vehicle from exceeding
the speed limits, although the designer of the vehicle’s control mechanism would be liable in
the case of an accident. As with human reproductive technology, the law has yet to catch up
with the new developments.
The success of AI might mean the end of the human race. Almost any technology
has the potential to cause harm in the wrong hands, but with AI and robotics, we have the new
problem that the wrong hands might belong to the technology itself. Countless science ﬁction
stories have warned about robots or robot–human cyborgs running amok. Early examples
Section 26.3. The Ethics and Risks of Developing Artiﬁcial Intelligence 1037
include Mary Shelley’s Frankenstein, or the Modern Prometheus (1818)5 and Karel Capek’s
play R.U.R. (1921), in which robots conquer the world. In movies, we have The Terminator
(1984), which combines the cliches of robots-conquer-the-world with time travel, and The
Matrix (1999), which combines robots-conquer-the-world with brain-in-a-vat.
It seems that robots are the protagonists of so many conquer-the-world stories because
they represent the unknown, just like the witches and ghosts of tales from earlier eras, or the
Martians from The War of the Worlds (Wells, 1898). The question is whether an AI system
poses a bigger risk than traditional software. We will look at three sources of risk.
First, the AI system’s state estimation may be incorrect, causing it to do the wrong
thing. For example, an autonomous car might incorrectly estimate the position of a car in the
adjacent lane, leading to an accident that might kill the occupants. More seriously, a missile
defense system might erroneously detect an attack and launch a counterattack, leading to
the death of billions. These risks are not really risks of AI systems—in both cases the same
mistake could just as easily be made by a human as by a computer. The correct way to mitigate
these risks is to design a system with checks and balances so that a single state-estimation
error does not propagate through the system unchecked.
Second, specifying the right utility function for an AI system to maximize is not so
easy. For example, we might propose a utility function designed tominimize human suffering,
expressed as an additive reward function over time as in Chapter 17. Given the way humans
are, however, we’ll always ﬁnd a way to suffer even in paradise; so the optimal decision for
the AI system is to terminate the human race as soon as possible—no humans, no suffering.
With AI systems, then, we need to be very careful what we ask for, whereas humans would
have no trouble realizing that the proposed utility function cannot be taken literally. On the
other hand, computers need not be tainted by the irrational behaviors described in Chapter 16.
Humans sometimes use their intelligence in aggressive ways because humans have some
innately aggressive tendencies, due to natural selection. The machines we build need not be
innately aggressive, unless we decide to build them that way (or unless they emerge as the
end product of a mechanism design that encourages aggressive behavior). Fortunately, there
are techniques, such as apprenticeship learning, that allows us to specify a utility function by
example. One can hope that a robot that is smart enough to ﬁgure out how to terminate the
human race is also smart enough to ﬁgure out that that was not the intended utility function.
Third, the AI system’s learning function may cause it to evolve into a system with
unintended behavior. This scenario is the most serious, and is unique to AI systems, so we
will cover it in more depth. I. J. Good wrote (1965),
Let an ultraintelligent machine be deﬁned as a machine that can far surpass all theUL TRAINTELLIGENT
MACHINE
intellectual activities of any man however clever. Since the design of machines is one of
these intellectual activities, an ultraintelligent machine could design even better machines;
there would then unquestionably be an “intellig ence explosion,” and the intelligence of
man would be left far behind. Thus the ﬁrst ultraintelligent machine is the last invention
that man need ever make, provided that the machine is docile enough to tell us how to
keep it under control.
5 As a young man, Charles Babbage was inﬂuenced by reading Frankenstein.
1038 Chapter 26. Philosophical Foundations
The “intelligence explosion” has also been called the technological singularity by mathe-TECHNOLOGICAL
SINGULARITY
matics professor and science ﬁction author Vernor Vinge, who writes (1993), “Within thirty
years, we will have the technological means to create superhuman intelligence. Shortly after,
the human era will be ended.” Good and Vinge (and many others) correctly note that the curve
of technological progress (on many measures) is growing exponentially at present (consider
Moore’s Law). However, it is a leap to extrapolate that the curve will continue to a singularity
of near-inﬁnite growth. So far, every other technology has followed an S-shaped curve, where
the exponential growth eventually tapers off. Sometimes new technologies step in when the
old ones plateau; sometimes we hit hard limits. With less than a century of high-technology
history to go on, it is difﬁcult to extrapolate hundreds of years ahead.
Note that the concept of ultraintelligent machines assumes that intelligence is an es-
pecially important attribute, and if you have enough of it, all problems can be solved. But
we know there are limits on computability and computational complexity. If the problem
of deﬁning ultraintelligent machines (or even approximations to them) happens to fall in the
class of, say, NEXPTIME-complete problems, and if there are no heuristic shortcuts, then
even exponential progress in technology won’t help—the speed of light puts a strict upper
bound on how much computing can be done; problems beyond that limit will not be solved.
We still don’t know where those upper bounds are.
Vinge is concerned about the coming singularity, but some computer scientists and
futurists relish it. Hans Moravec (2000) encourages us to give every advantage to our “mind
children,” the robots we create, which may surpass us in intelligence. There is even a new
word—transhumanism—for the active social movement that looks forward to this future in
TRANSHUMANISM
which humans are merged with—or replaced by—robotic and biotech inventions. Sufﬁce it
to say that such issues present a challenge for most moral theorists, who take the preservation
of human life and the human species to be a good thing. Ray Kurzweil is currently the most
visible advocate for the singularity view, writing in The Singularity is Near (2005):
The Singularity will allow us to transcend these limitations of our biological bodies and
brain. We will gain power over our fates. Our mortality will be in our own hands. We
will be able to live as long as we want (a subtly different statement from saying we will
live forever). We will fully understand human thinking and will vastly extend and expand
its reach. By the end of this century, the nonbiological portion of our intelligence will be
trillions of trillions of times more powerful than unaided human intelligence.
Kurzweil also notes the potential dangers, writing “But the Singularity will also amplify the
ability to act on our destructive inclinations, so its full story has not yet been written.”
If ultraintelligent machines are a possibility, we humans would do well to make sure
that we design their predecessors in such a way that they design themselves to treat us well.
Science ﬁction writer Isaac Asimov (1942) was the ﬁrst to address this issue, with his three
laws of robotics:
1. A robot may not injure a human being or, through inaction, allow a human being to
come to harm.
2. A robot must obey orders given to it by human beings, except where such orders would
conﬂict with the First Law.
Section 26.3. The Ethics and Risks of Developing Artiﬁcial Intelligence 1039
3. A robot must protect its own existence as long as such protection does not conﬂict with
the First or Second Law.
These laws seem reasonable, at least to us humans.6 But the trick is how to implement these
laws. In the Asimov story Roundabout a robot is sent to fetch some selenium. Later the
robot is found wandering in a circle around the selenium source. Every time it heads toward
the source, it senses a danger, and the third law causes it to veer away. But every time it
veers away, the danger recedes, and the power of the second law takes over, causing it to
veer back towards the selenium. The set of points that deﬁne the balancing point between
the two laws deﬁnes a circle. This suggests that the laws are not logical absolutes, but rather
are weighed against each other, with a higher weighting for the earlier laws. Asimov was
probably thinking of an architecture based on control theory—perhaps a linear combination
of factors—while today the most likely architecture would be a probabilistic reasoning agent
that reasons over probability distributions of outcomes, and maximizes utility as deﬁned by
the three laws. But presumably we don’t want our robots to prevent a human from crossing
the street because of the nonzero chance of harm. That means that the negative utility for
harm to a human must be much greater than for disobeying, but that each of the utilities is
ﬁnite, not inﬁnite.
Yudkowsky (2008) goes into more detail about how to design aFriendly AI. He asserts
FRIENDL Y AI
that friendliness (a desire not to harm humans) should be designed in from the start, but that
the designers should recognize both that their own designs may be ﬂawed, and that the robot
will learn and evolve over time. Thus the challenge is one of mechanism design—to deﬁne a
mechanism for evolving AI systems under a system of checks and balances, and to give the
systems utility functions that will remain friendly in the face of such changes.
We can’t just give a program a static utility function, because circumstances, and our de-
sired responses to circumstances, change over time. For example, if technology had allowed
us to design a super-powerful AI agent in 1800 and endow it with the prevailing morals of
the time, it would be ﬁghting today to reestablish slavery and abolish women’s right to vote.
On the other hand, if we build an AI agent today and tell it to evolve its utility function, how
can we assure that it won’t reason that “Humans think it is moral to kill annoying insects, in
part because insect brains are so primitive. But human brains are primitive compared to my
powers, so it must be moral for me to kill humans.”
Omohundro (2008) hypothesizes that even an innocuous chess program could pose a
risk to society. Similarly, Marvin Minsky once suggested that an AI program designed to
solve the Riemann Hypothesis might end up taking over all the resources of Earth to build
more powerful supercomputers to help achieve its goal. The moral is that even if you only
want your program to play chess or prove theorems, if you give it the capability to learn
and alter itself, you need safeguards. Omohundro concludes that “Social structures which
cause individuals to bear the cost of their negative externalities would go a long way toward
ensuring a stable and positive future,” This seems to be an excellent idea for society in general,
regardless of the possibility of ultraintelligent machines.
6 A robot might notice the inequity that a human is allowed to kill another in self-defense, but a robot is required
to sacriﬁce its own life to save a human.
1040 Chapter 26. Philosophical Foundations
We should note that the idea of safeguards against change in utility function is not a
new one. In the Odyssey, Homer (ca. 700 B.C.) described Ulysses’ encounter with the sirens,
whose song was so alluring it compelled sailors to cast themselves into the sea. Knowing it
would have that effect on him, Ulysses ordered his crew to bind him to the mast so that he
could not perform the self-destructive act. It is interesting to think how similar safeguards
could be built into AI systems.
Finally, let us consider the robot’s point of view. If robots become conscious, then to
treat them as mere “machines” (e.g., to take them apart) might be immoral. Science ﬁction
writers have addressed the issue of robot rights. The movie A.I. (Spielberg, 2001) was based
on a story by Brian Aldiss about an intelligent robot who was programmed to believe that
he was human and fails to understand his eventual abandonment by his owner–mother. The
story (and the movie) argue for the need for a civil rights movement for robots.
26.4 S UMMARY
This chapter has addressed the following issues:
•Philosophers use the term weak AI for the hypothesis that machines could possibly
behave intelligently, and strong AI for the hypothesis that such machines would count
as having actual minds (as opposed to simulated minds).
•Alan Turing rejected the question “Can machines think?” and replaced it with a be-
havioral test. He anticipated many objections to the possibility of thinking machines.
Few AI researchers pay attention to the Turing Test, preferring to concentrate on their
systems’ performance on practical tasks, rather than the ability to imitate humans.
•There is general agreement in modern times that mental states are brain states.
•Arguments for and against strong AI are inconclusive. Few mainstream AI researchers
believe that anything signiﬁcant hinges on the outcome of the debate.
•Consciousness remains a mystery.
•We identiﬁed six potential threats to society posed by AI and related technology. We
concluded that some of the threats are either unlikely or differ little from threats posed
by “unintelligent” technologies. One threat in particular is worthy of further consider-
ation: that ultraintelligent machines might lead to a future that is very different from
today—we may not like it, and at that point we may not have a choice. Such consid-
erations lead inevitably to the conclusion that we must weigh carefully, and soon, the
possible consequences of AI research.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Sources for the various responses to Turing’s 1950 paper and for the main critics of weak
AI were given in the chapter. Although it became fashionable in the post-neural-network era
Bibliographical and Historical Notes 1041
to deride symbolic approaches, not all philosophers are critical of GOFAI . Some are, in fact,
ardent advocates and even practitioners. Zenon Pylyshyn (1984) has argued that cognition
can best be understood through a computational model, not only in principle but also as a
way of conducting research at present, and has speciﬁcally rebutted Dreyfus’s criticisms of
the computational model of human cognition (Pylyshyn, 1974). Gilbert Harman (1983), in
analyzing belief revision, makes connections with AI research on truth maintenance systems.
Michael Bratman has applied his “belief-desire-intention” model of human psychology (Brat-
man, 1987) to AI research on planning (Bratman, 1992). At the extreme end of strong AI,
Aaron Sloman (1978, p. xiii) has even described as “racialist” the claim by Joseph Weizen-
baum (1976) that intelligent machines can never be regarded as persons.
Proponents of the importance of embodiment in cognition include the philosophers
Merleau-Ponty, whose Phenomenology of Perception (1945) stressed the importance of the
body and the subjective interpretation of reality afforded by our senses, and Heidegger, whose
Being and Time (1927) asked what it means to actually be an agent, and criticized all of the
history of philosophy for taking this notion for granted. In the computer age, Alva Noe (2009)
and Andy Clark (1998, 2008) propose that our brains form a rather minimal representation
of the world, use the world itself in a just-in-time basis to maintain the illusion of a detailed
internal model, use props in the world (such as paper and pencil as well as computers) to
increase the capabilities of the mind. Pfeifer et al. (2006) and Lakoff and Johnson (1999)
present arguments for how the body helps shape cognition.
The nature of the mind has been a standard topic of philosophical theorizing from an-
cient times to the present. In the Phaedo, Plato speciﬁcally considered and rejected the idea
that the mind could be an “attunement” or pattern of organization of the parts of the body, a
viewpoint that approximates the functionalist viewpoint in modern philosophy of mind. He
decided instead that the mind had to be an immortal, immaterial soul, separable from the
body and different in substance—the viewpoint of dualism. Aristotle distinguished a variety
of souls (Greekψυχη) in living things, some of which, at least, he described in a functionalist
manner. (See Nussbaum (1978) for more on Aristotle’s functionalism.)
Descartes is notorious for his dualistic view of the human mind, but ironically his histor-
ical inﬂuence was toward mechanism and physicalism. He explicitly conceived of animals as
automata, and he anticipated the Turing Test, writing “it is not conceivable [that a machine]
should produce different arrangements of words so as to give an appropriately meaningful
answer to whatever is said in its presence, as even the dullest of men can do” (Descartes,
1637). Descartes’s spirited defense of the animals-as-automata viewpoint actually had the
effect of making it easier to conceive of humans as automata as well, even though he himself
did not take this step. The book L’Homme Machine (La Mettrie, 1748) did explicitly argue
that humans are automata.
Modern analytic philosophy has typically accepted physicalism, but the variety of views
on the content of mental states is bewildering. The identiﬁcation of mental states with brain
states is usually attributed to Place (1956) and Smart (1959). The debate between narrow-
content and wide-content views of mental states was triggered by Hilary Putnam (1975), who
introduced so-called twin earths (rather than brain-in-a-vat, as we did in the chapter) as a
TWIN EARTHS
device to generate identical brain states with different (wide) content.
1042 Chapter 26. Philosophical Foundations
Functionalism is the philosophy of mind most naturally suggested by AI. The idea that
mental states correspond to classes of brain states deﬁned functionally is due to Putnam
(1960, 1967) and Lewis (1966, 1980). Perhaps the most forceful proponent of functional-
ism is Daniel Dennett, whose ambitiously titled work Consciousness Explained (Dennett,
1991) has attracted many attempted rebuttals. Metzinger (2009) argues there is no such thing
as an objective self, that consciousness is the subjective appearance of a world. The inverted
spectrum argument concerning qualia was introduced by John Locke (1690). Frank Jack-
son (1982) designed an inﬂuential thought experiment involving Mary, a color scientist who
has been brought up in an entirely black-and-white world. There’s Something About Mary
(Ludlow et al., 2004) collects several papers on this topic.
Functionalism has come under attack from authors who claim that they do not account
for the qualia or “what it’s like” aspect of mental states (Nagel, 1974). Searle has focused
instead on the alleged inability of functionalism to account for intentionality (Searle, 1980,
1984, 1992). Churchland and Churchland (1982) rebut both these types of criticism. The
Chinese Room has been debated endlessly (Searle, 1980, 1990; Preston and Bishop, 2002).
We’ll just mention here a related work: Terry Bisson’s (1990) science ﬁction story They’re
Made out of Meat , in which alien robotic explorers who visit earth are incredulous to ﬁnd
thinking human beings whose minds are made of meat. Presumably, the robotic alien equiv-
alent of Searle believes that he can think due to the special causal powers of robotic circuits;
causal powers that mere meat-brains do not possess.
Ethical issues in AI predate the existence of the ﬁeld itself. I. J. Good’s (1965) ul-
traintelligent machine idea was foreseen a hundred years earlier by Samuel Butler (1863).
Written four years after the publication of Darwin’s On the Origins of Species and at a time
when the most sophisticated machines were steam engines, Butler’s article onDarwin Among
the Machines envisioned “the ultimate development of mechanical consciousness” by natural
selection. The theme was reiterated by George Dyson (1998) in a book of the same title.
The philosophical literature on minds, brains, and related topics is large and difﬁcult to
read without training in the terminology and methods of argument employed. The Encyclo-
pedia of Philosophy (Edwards, 1967) is an impressively authoritative and very useful aid in
this process. The Cambridge Dictionary of Philosophy (Audi, 1999) is a shorter and more
accessible work, and the online Stanford Encyclopedia of Philosophy offers many excellent
articles and up-to-date references. The MIT Encyclopedia of Cognitive Science (Wilson and
Keil, 1999) covers the philosophy of mind as well as the biology and psychology of mind.
There are several general introductions to the philosophical “AI question” (Boden, 1990;
Haugeland, 1985; Copeland, 1993; McCorduck, 2004; Minsky, 2007). The Behavioral and
Brain Sciences, abbreviated BBS, is a major journal devoted to philosophical and scientiﬁc
debates about AI and neuroscience. Topics of ethics and responsibility in AI are covered in
the journals AI and Society and Journal of Artiﬁcial Intelligence and Law .
Exercises 1043
EXERCISES
26.1 Go through Turing’s list of alleged “disabilities” of machines, identifying which have
been achieved, which are achievable in principle by a program, and which are still problem-
atic because they require conscious mental states.
26.2 Find and analyze an account in the popular media of one or more of the arguments to
the effect that AI is impossible.
26.3 In the brain replacement argument, it is important to be able to restore the subject’s
brain to normal, such that its external behavior is as it would have been if the operation had
not taken place. Can the skeptic reasonably object that this would require updating those
neurophysiological properties of the neurons relating to conscious experience, as distinct
from those involved in the functional behavior of the neurons?
26.4 Suppose that a Prolog program containing many clauses about the rules of British
citizenship is compiled and run on an ordinary computer. Analyze the “brain states” of the
computer under wide and narrow content.
26.5 Alan Perlis (1982) wrote, “A year spent in artiﬁcial intelligence is enough to make one
believe in God”. He also wrote, in a letter to Philip Davis, that one of the central dreams of
computer science is that “through the performance of computers and their programs we will
remove all doubt that there is only a chemical distinction between the living and nonliving
world.” To what extent does the progress made so far in artiﬁcial intelligence shed light on
these issues? Suppose that at some future date, the AI endeavor has been completely success-
ful; that is, we have build intelligent agents capable of carrying out any human cognitive task
at human levels of ability. To what extent would that shed light on these issues?
26.6 Compare the social impact of artiﬁcial intelligence in the last ﬁfty years with the social
impact of the introduction of electric appliances and the internal combustion engine in the
ﬁfty years between 1890 and 1940.
26.7 I. J. Good claims that intelligence is the most important quality, and that building
ultraintelligent machines will change everything. A sentient cheetah counters that “Actually
speed is more important; if we could build ultrafast machines, that would change everything,”
and a sentient elephant claims “You’re both wrong; what we need is ultrastrong machines.”
What do you think of these arguments?
26.8 Analyze the potential threats from AI technology to society. What threats are most se-
rious, and how might they be combated? How do they compare to the potential beneﬁts?
26.9 How do the potential threats from AI technology compare with those from other com-
puter science technologies, and to bio-, nano-, and nuclear technologies?
26.10 Some critics object that AI is impossible, while others object that it is too possible
and that ultraintelligent machines pose a threat. Which of these objections do you think is
more likely? Would it be a contradiction for someone to hold both positions?


END_INSTRUCTION
