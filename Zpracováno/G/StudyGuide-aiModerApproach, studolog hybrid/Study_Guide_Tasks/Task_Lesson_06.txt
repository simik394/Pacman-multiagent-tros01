
START_INSTRUCTION
You are an expert AI tutor creating a comprehensive study guide.
Your task is to rewrite and expand the "Lesson Notes" provided below into a cohesive, readable study text.

1. **Structure:** Follow the exact structure and order of the topics in the "Lesson Notes". Do not reorder them.
2. **Enrichment:** Use the "Textbook Context" provided to expand on every bullet point in the notes.
    - If the notes mention a concept (e.g., "Rationality"), define it precisely using the textbook.
    - If the notes list algorithms (e.g., "A*", "Simulated Annealing"), explain how they work, their complexity, and pros/cons using the textbook details.
    - Add examples from the textbook where appropriate.
3. **Tone:** Academic but accessible study material. Not just bullet points—write paragraphs where necessary to explain complex ideas.
4. **Language:** The output must be in **Czech** (since the lesson notes are in Czech), but you can keep standard English AI terminology (like "Overfitting") in parentheses.

--- LESSON NOTES (Skeleton) ---
Plánování (a rozvrhování)

Plánování <--> rozvrhování <--> provádění
Plánování
• rozhodovací proces - stanovení cílů, výběr prostředků, způsob jejich dosažení
• Jaké akce jsou potřeba pro dosažení cílů
Rozvrhování
• proces rozdělení úloh na zdroje → minimalizace času vytíženosti zdrojů
• Rozvrhování jak zpracovat vzhledem k omezeným zdrojům a času
Úloha plánování
Je tvořena:
○ Počátečním stavem světa
○ Popisem akcí schopných měnit stav světa
○ Požadovaným stavem světa
•
Řešením je seznam (sekvence) akcí - plán
○ Bez ohledu na čas a zdroje
•
• Zabývá se kauzálními vztahy mezi akcemi a otázkou výběru akcí
Úloha rozvrhování
Je tvořena:
○ Skupinou částečně uspořádaných aktivit
○ Dostupnými zdroji
•
• Řešením je alokace aktivit na zdroje a v čase
• Soustředí se na alokaci naplánovaných úloh
Plánování → Rozvrhování
• plánovaní řeší hlavně kauzální vztahy mezi akcemi a otázkou výběru akcí
• rozvrhování řeší alokaci naplánovaných akcí v čase a prostoru
někdy je dobrý obě úlohy řešit najednou
○ třeba když existuje hodně plánů, ale jen málo z nich má přípustný rozvrh
•
Konceptuální model plánování
Systém Σ modelující stavy a přechody:
○ Množina stavů S
○ Množina akcí A
○ Množina událostí E
○ přechodová funkce y: SxAxE -> P(S)
•
Účelem plánování je zjistit jaké akce a na které stavy se mají aplikovat, abychom z dané situace 
dosáhli požadovaných cílů
•
Plánování jako booleovská splnitelnost
•
Plánování jako splňování podmínek
•
Plánování jako prohledávání
•
Plánování ve stavovém prostoru
• uzly odpovídají stavům
• hrany odpovídají přechodům mezi stavy pomocí akcí
• cílem je nalézt cestu z počátečního stavu do některého z koncových
Dopředné prohledávání ve stavovém prostoru
○ Začínáme v počátečním stavu a jdeme k některému stavu cílovému
Je potřeba umět:
▪ rozhodnout, zda je daný stav cílový nebo ne
▪ najít množinu akcí aplikovatelných na daný stav
▪ vypočítat stav, do kterého se dostaneme aplikací akce
○
•
Zpětné prohledávání ve stavovém prostoru
○ začínáme cílem a jdeme přes podcíle k počátečnímu stavu
Je potřeba umět:
▪ zjistit, zda daný stav odpovídá cíli
▪ pro daný cíl najít relevantní akce
○
•
▪ pro daný cíl najít relevantní akce
▪ vypočítat podcíl umožňující aplikovat danou akci
Plánování v prostoru plánů
• uzly odpovídají částečným (částečně instanciovaným) plánům
• hrany odpovídají přechodům mezi částečnými plány
• cílem je nalézt úplný plán 
Plánovací graf
• orientovaný vrstvený graf pro plánování ve stavovém prostoru
•
Plánovací algoritmus založený na plánovacím grafu.
Střídá se fáze expanze grafu a extrakce plánu
Expanze
Nejprve vytvoří plánovací graf až do vrstvy, kde jsou všechny cílové atomy a 
žádná dvojice se vzájemně nevylučuje
□
□ Pokud fáze extrakce neuspěje, přidá další vrstvu
▪
Extrakce
□ Z plánovacího grafu se pokusí vybrat plán vedoucí k cílovým atomům

--- TEXTBOOK CONTEXT (Source Material) ---


--- BOOK CHAPTER: 10_Classical_Planning ---

10 CLASSICAL PLANNING
In which we see how an agent can take advantage of the structure of a problem to
construct complex plans of action.
We have deﬁned AI as the study of rational action, which means that planning—devising a
plan of action to achieve one’s goals—is a critical part of AI. We have seen two examples
of planning agents so far: the search-based problem-solving agent of Chapter 3 and the hy-
brid logical agent of Chapter 7. In this chapter we introduce a representation for planning
problems that scales up to problems that could not be handled by those earlier approaches.
Section 10.1 develops an expressive yet carefully constrained language for representing
planning problems. Section 10.2 shows how forward and backward search algorithms can
take advantage of this representation, primarily through accurate heuristics that can be derived
automatically from the structure of the representation. (This is analogous to the way in which
effective domain-independent heuristics were constructed for constraint satisfaction problems
in Chapter 6.) Section 10.3 shows how a data structure called the planning graph can make the
search for a plan more efﬁcient. We then describe a few of the other approaches to planning,
and conclude by comparing the various approaches.
This chapter covers fully observable, deterministic, static environments with single
agents. Chapters 11 and 17 cover partially observable, stochastic, dynamic environments
with multiple agents.
10.1 D EFINITION OF CLASSICAL PLANNING
The problem-solving agent of Chapter 3 can ﬁnd sequences of actions that result in a goal
state. But it deals with atomic representations of states and thus needs good domain-speciﬁc
heuristics to perform well. The hybrid propositional logical agent of Chapter 7 can ﬁnd plans
without domain-speciﬁc heuristics because it uses domain-independent heuristics based on
the logical structure of the problem. But it relies on ground (variable-free) propositional
inference, which means that it may be swamped when there are many actions and states. For
example, in the wumpus world, the simple action of moving a step forward had to be repeated
for all four agent orientations, T time steps, and n
2 current locations.
366
Section 10.1. Deﬁnition of Classical Planning 367
In response to this, planning researchers have settled on a factored representation—
one in which a state of the world is represented by a collection of variables. We use a language
called PDDL, the Planning Domain Deﬁnition Language, that allows us to express all 4Tn
2PDDL
actions with one action schema. There have been several versions of PDDL; we select a
simple version and alter its syntax to be consistent with the rest of the book. 1 We now show
how PDDL describes the four things we need to deﬁne a search problem: the initial state, the
actions that are available in a state, the result of applying an action, and the goal test.
Each state is represented as a conjunction of ﬂuents that are ground, functionless atoms.
For example, Poor∧Unknown might represent the state of a hapless agent, and a state
in a package delivery problem might be At(Truck1,Melbourne)∧At(Truck2,Sydney).
Database semantics is used: the closed-world assumption means that any ﬂuents that are not
mentioned are false, and the unique names assumption means that Truck1 and Truck2 are
distinct. The following ﬂuents are not allowed in a state: At(x, y) (because it is non-ground),
¬Poor (because it is a negation), andAt(Father(Fred),Sydney) (because it uses a function
symbol). The representation of states is carefully designed so that a state can be treated
either as a conjunction of ﬂuents, which can be manipulated by logical inference, or as a set
of ﬂuents, which can be manipulated with set operations. The set semantics is sometimes
SET SEMANTICS
easier to deal with.
Actions are described by a set of action schemas that implicitly deﬁne the ACTIONS (s)
and RESULT (s,a) functions needed to do a problem-solving search. We saw in Chapter 7 that
any system for action description needs to solve the frame problem—to say what changes and
what stays the same as the result of the action. Classical planning concentrates on problems
where most actions leave most things unchanged. Think of a world consisting of a bunch of
objects on a ﬂat surface. The action of nudging an object causes that object to change its lo-
cation by a vectorΔ . A concise description of the action should mention onlyΔ ; it shouldn’t
have to mention all the objects that stay in place. PDDL does that by specifying the result of
an action in terms of what changes; everything that stays the same is left unmentioned.
A set of ground (variable-free) actions can be represented by a single action schema.
ACTION SCHEMA
The schema is a lifted representation—it lifts the level of reasoning from propositional logic
to a restricted subset of ﬁrst-order logic. For example, here is an action schema for ﬂying a
plane from one location to another:
Action(Fly(p,from,to),
P
RECOND :At(p,from)∧Plane(p)∧Airport(from)∧Airport(to)
EFFECT :¬At(p,from)∧At(p,to))
The schema consists of the action name, a list of all the variables used in the schema, a
precondition and an effect. Although we haven’t said yet how the action schema convertsPRECONDITION
EFFECT into logical sentences, think of the variables as being universally quantiﬁed. We are free to
choose whatever values we want to instantiate the variables. For example, here is one ground
1 PDDL was derived from the original S TRIPS planning language(Fikes and Nilsson, 1971). which is slightly
more restricted than PDDL: STRIPS preconditions and goals cannot contain negative literals.
368 Chapter 10. Classical Planning
action that results from substituting values for all the variables:
Action(Fly(P1,SFO,JFK),
PRECOND :At(P1,SFO)∧Plane(P1)∧Airport(SFO)∧Airport(JFK)
EFFECT :¬At(P1,SFO)∧At(P1,JFK))
The precondition and effect of an action are each conjunctions of literals (positive or negated
atomic sentences). The precondition deﬁnes the states in which the action can be executed,
and the effect deﬁnes the result of executing the action. An action a can be executed in state
s if s entails the precondition of a. Entailment can also be expressed with the set semantics:
s |= q iff every positive literal in q is in s and every negated literal in q is not. In formal
notation we say
(a∈A
CTIONS (s)) ⇔s|= PRECOND (a),
where any variables in a are universally quantiﬁed. For example,
∀p,from,to (Fly(p,from,to)∈ACTIONS (s)) ⇔
s|=( At(p,from)∧Plane(p)∧Airport(from)∧Airport(to))
We say that action a is applicable in state s if the preconditions are satisﬁed by s.W h e nAPPLICABLE
an action schema a contains variables, it may have multiple applicable instantiations. For
example, with the initial state deﬁned in Figure 10.1, the Fly action can be instantiated as
Fly(P1,SFO,JFK) or as Fly(P2,JFK,SFO), both of which are applicable in the initial
state. If an action a has v variables, then, in a domain withk unique names of objects, it takes
O(vk) time in the worst case to ﬁnd the applicable ground actions.
Sometimes we want topropositionalize a PDDL problem—replace each action schemaPROPOSITIONALIZE
with a set of ground actions and then use a propositional solver such as SATP LAN to ﬁnd a
solution. However, this is impractical when v and k are large.
The result of executing action a in state s is deﬁned as a state s′ which is represented
by the set of ﬂuents formed by starting with s, removing the ﬂuents that appear as negative
literals in the action’s effects (what we call the delete list or DEL(a)), and adding the ﬂuentsDELETE LIST
that are positive literals in the action’s effects (what we call the add list or ADD(a)):ADD LIST
RESULT (s,a)=( s−DEL(a))∪ADD(a) . (10.1)
For example, with the action Fly(P1,SFO,JFK), we would remove At(P1,SFO) and add
At(P1,JFK). It is a requirement of action schemas that any variable in the effect must also
appear in the precondition. That way, when the precondition is matched against the state s,
all the variables will be bound, and R ESULT (s,a) will therefore have only ground atoms. In
other words, ground states are closed under the RESULT operation.
Also note that the ﬂuents do not explicitly refer to time, as they did in Chapter 7. There
we needed superscripts for time, and successor-state axioms of the form
Ft+1 ⇔ActionCausesFt∨(Ft∧¬ActionCausesNotFt) .
In PDDL the times and states are implicit in the action schemas: the precondition always
refers to time t and the effect to time t +1 .
A set of action schemas serves as a deﬁnition of a planningdomain. A speciﬁc problem
within the domain is deﬁned with the addition of an initial state and a goal. The initial
Section 10.1. Deﬁnition of Classical Planning 369
Init(At(C1, SFO) ∧At(C2, JFK) ∧At(P1, SFO) ∧At(P2, JFK)
∧Cargo(C1) ∧Cargo(C2) ∧Plane(P1) ∧Plane(P2)
∧Airport(JFK) ∧Airport(SFO))
Goal(At(C1, JFK) ∧At(C2, SFO))
Action(Load(c, p, a ),
PRECOND : At(c, a) ∧At(p, a) ∧Cargo(c) ∧Plane(p) ∧Airport(a)
EFFECT :¬ At(c, a) ∧In(c, p))
Action(Unload(c, p, a ),
PRECOND : In(c, p) ∧At(p, a) ∧Cargo(c) ∧Plane(p) ∧Airport(a)
EFFECT : At(c, a) ∧¬In(c, p))
Action(Fly(p, from, to),
PRECOND : At(p, from) ∧Plane(p) ∧Airport(from) ∧Airport(to)
EFFECT :¬ At(p, from) ∧At(p, to))
Figure 10.1 A PDDL description of an air cargo transportation planning problem.
state is a conjunction of ground atoms. (As with all states, the closed-world assumption isINITIAL ST A TE
used, which means that any atoms that are not mentioned are false.) The goal is just like aGOAL
precondition: a conjunction of literals (positive or negative) that may contain variables, such
as At(p,SFO)∧Plane(p). Any variables are treated as existentially quantiﬁed, so this goal
is to have any plane at SFO. The problem is solved when we can ﬁnd a sequence of actions
that end in a state s that entails the goal. For example, the state Rich∧Famous∧Miserable
entails the goal Rich∧Famous, and the state Plane(Plane1)∧At(Plane1,SFO) entails
the goal At(p,SFO)∧Plane(p).
Now we have deﬁned planning as a search problem: we have an initial state, an ACTIONS
function, a R ESULT function, and a goal test. We’ll look at some example problems before
investigating efﬁcient search algorithms.
10.1.1 Example: Air cargo transport
Figure 10.1 shows an air cargo transport problem involving loading and unloading cargo and
ﬂying it from place to place. The problem can be deﬁned with three actions: Load, Unload,
and Fly. The actions affect two predicates: In(c, p) means that cargo c is inside plane p,a n d
At(x, a) means that object x (either plane or cargo) is at airport a. Note that some care must
be taken to make sure the At predicates are maintained properly. When a plane ﬂies from
one airport to another, all the cargo inside the plane goes with it. In ﬁrst-order logic it would
be easy to quantify over all objects that are inside the plane. But basic PDDL does not have
a universal quantiﬁer, so we need a different solution. The approach we use is to say that a
piece of cargo ceases to beAt anywhere when it isIn a plane; the cargo only becomesAt the
new airport when it is unloaded. So At really means “available for use at a given location.”
The following plan is a solution to the problem:
[Load(C
1,P1,SFO),Fly(P1,SFO,JFK),Unload(C1,P1,JFK),
Load(C2,P2,JFK),Fly(P2,JFK,SFO),Unload(C2,P2,SFO)] .
370 Chapter 10. Classical Planning
Finally, there is the problem of spurious actions such as Fly(P1,JFK,JFK), which should
be a no-op, but which has contradictory effects (according to the deﬁnition, the effect would
include At(P
1,JFK)∧¬At(P1,JFK)). It is common to ignore such problems, because
they seldom cause incorrect plans to be produced. The correct approach is to add inequality
preconditions saying that the from and to airports must be different; see another example of
this in Figure 10.3.
10.1.2 Example: The spare tire problem
Consider the problem of changing a ﬂat tire (Figure 10.2). The goal is to have a good spare
tire properly mounted onto the car’s axle, where the initial state has a ﬂat tire on the axle and
a good spare tire in the trunk. To keep it simple, our version of the problem is an abstract
one, with no sticky lug nuts or other complications. There are just four actions: removing the
spare from the trunk, removing the ﬂat tire from the axle, putting the spare on the axle, and
leaving the car unattended overnight. We assume that the car is parked in a particularly bad
neighborhood, so that the effect of leaving it overnight is that the tires disappear. A solution
to the problem is [Remove(Flat,Axle),Remove(Spare,Trunk),PutOn(Spare,Axle)].
Init(Tire(Flat) ∧Tire(Spare) ∧At(Flat,Axle) ∧At(Spare,Trunk))
Goal(At(Spare,Axle))
Action(Remove(obj,loc),
PRECOND : At(obj,loc)
EFFECT :¬ At(obj,loc) ∧At(obj,Ground))
Action(PutOn(t, Axle),
PRECOND : Tire(t) ∧At(t,Ground) ∧¬At(Flat,Axle)
EFFECT :¬ At(t,Ground) ∧At(t,Axle))
Action(LeaveOvernight,
PRECOND :
EFFECT :¬ At(Spare,Ground) ∧¬At(Spare,Axle) ∧¬At(Spare,Trunk)
∧¬At(Flat,Ground) ∧¬At(Flat,Axle) ∧¬At(Flat, Trunk))
Figure 10.2 The simple spare tire problem.
10.1.3 Example: The blocks world
One of the most famous planning domains is known as the blocks world . This domainBLOCKS WORLD
consists of a set of cube-shaped blocks sitting on a table. 2 The blocks can be stacked, but
only one block can ﬁt directly on top of another. A robot arm can pick up a block and move
it to another position, either on the table or on top of another block. The arm can pick up
only one block at a time, so it cannot pick up a block that has another one on it. The goal will
always be to build one or more stacks of blocks, speciﬁed in terms of what blocks are on top
2 The blocks world used in planning research is much simpler than SHRDLU ’s version, shown on page 20.
Section 10.1. Deﬁnition of Classical Planning 371
Init(On(A,Table) ∧On(B, Table) ∧On(C, A)
∧Block(A) ∧Block(B) ∧Block(C) ∧Clear(B) ∧Clear(C))
Goal(On(A, B) ∧On(B,C ))
Action(Move(b, x, y),
PRECOND : On(b, x) ∧Clear(b) ∧Clear(y) ∧Block(b) ∧Block(y) ∧
(b̸=x) ∧(b̸=y) ∧(x̸=y),
EFFECT : On(b, y) ∧Clear(x) ∧¬On(b, x) ∧¬Clear(y))
Action(MoveToTable(b, x),
PRECOND : On(b, x) ∧Clear(b) ∧Block(b) ∧(b̸=x),
EFFECT : On(b,Table) ∧Clear(x) ∧¬On(b, x))
Figure 10.3 A planning problem in the blocks world: building a three-block tower. One
solution is the sequence [MoveToTable(C, A),Move(B, Table,C ),Move(A,Table,B )].
Start State Goal State
B A
C
A
B
C
Figure 10.4 Diagram of the blocks-world problem in Figure 10.3.
of what other blocks. For example, a goal might be to get block A on B and block B on C
(see Figure 10.4).
We use On(b, x) to indicate that block b is on x,w h e r ex is either another block or the
table. The action for moving block b from the top of x to the top of y will be Move(b, x, y).
Now, one of the preconditions on movingb is that no other block be on it. In ﬁrst-order logic,
this would be ¬∃x On(x, b) or, alternatively,∀x¬On(x, b). Basic PDDL does not allow
quantiﬁers, so instead we introduce a predicate Clear(x) that is true when nothing is on x.
(The complete problem description is in Figure 10.3.)
The action Move moves a block b from x to y if both b and y are clear. After the move
is made, b is still clear but y is not. A ﬁrst attempt at the Move schema is
Action(Move(b, x, y),
PRECOND :On(b, x)∧Clear(b)∧Clear(y),
EFFECT :On(b, y)∧Clear(x)∧¬On(b, x)∧¬Clear(y)) .
Unfortunately, this does not maintain Clear properly when x or y is the table. When x is the
Table, this action has the effect Clear(Table), but the table should not become clear; and
when y =Table, it has the preconditionClear(Table), but the table does not have to be clear
372 Chapter 10. Classical Planning
for us to move a block onto it. To ﬁx this, we do two things. First, we introduce another
action to move a block b from x to the table:
Action(MoveToTable(b, x),
PRECOND :On(b, x)∧Clear(b),
EFFECT :On(b,Table)∧Clear(x)∧¬On(b, x)) .
Second, we take the interpretation of Clear(x) to be “there is a clear space on x to hold a
block.” Under this interpretation,Clear(Table) will always be true. The only problem is that
nothing prevents the planner from using Move(b, x,Table) instead of MoveToTable(b, x).
We could live with this problem—it will lead to a larger-than-necessary search space, but will
not lead to incorrect answers—or we could introduce the predicateBlock and add Block(b)∧
Block(y) to the precondition of Move.
10.1.4 The complexity of classical planning
In this subsection we consider the theoretical complexity of planning and distinguish two
decision problems. PlanSA Tis the question of whether there exists any plan that solves aPLANSA T
planning problem. Bounded PlanSA Tasks whether there is a solution of length k or less;BOUNDED PLANSA T
this can be used to ﬁnd an optimal plan.
The ﬁrst result is that both decision problems are decidable for classical planning. The
proof follows from the fact that the number of states is ﬁnite. But if we add function symbols
to the language, then the number of states becomes inﬁnite, and PlanSAT becomes only
semidecidable: an algorithm exists that will terminate with the correct answer for any solvable
problem, but may not terminate on unsolvable problems. The Bounded PlanSAT problem
remains decidable even in the presence of function symbols. For proofs of the assertions in
this section, see Ghallab et al. (2004).
Both PlanSAT and Bounded PlanSAT are in the complexity class PSPACE, a class that
is larger (and hence more difﬁcult) than NP and refers to problems that can be solved by a
deterministic Turing machine with a polynomial amount of space. Even if we make some
rather severe restrictions, the problems remain quite difﬁcult. For example, if we disallow
negative effects, both problems are still NP-hard. However, if we also disallow negative
preconditions, PlanSAT reduces to the class P.
These worst-case results may seem discouraging. We can take solace in the fact that
agents are usually not asked to ﬁnd plans for arbitrary worst-case problem instances, but
rather are asked for plans in speciﬁc domains (such as blocks-world problems withn blocks),
which can be much easier than the theoretical worst case. For many domains (including the
blocks world and the air cargo world), Bounded PlanSAT is NP-complete while PlanSAT is
in P; in other words, optimal planning is usually hard, but sub-optimal planning is sometimes
easy. To do well on easier-than-worst-case problems, we will need good search heuristics.
That’s the true advantage of the classical planning formalism: it has facilitated the develop-
ment of very accurate domain-independent heuristics, whereas systems based on successor-
state axioms in ﬁrst-order logic have had less success in coming up with good heuristics.
Section 10.2. Algorithms for Planning as State-Space Search 373
10.2 A LGORITHMS FOR PLANNING AS STATE-SPACE SEARCH
Now we turn our attention to planning algorithms. We saw how the description of a planning
problem deﬁnes a search problem: we can search from the initial state through the space
of states, looking for a goal. One of the nice advantages of the declarative representation of
action schemas is that we can also search backward from the goal, looking for the initial state.
Figure 10.5 compares forward and backward searches.
10.2.1 Forward (progression) state-space search
Now that we have shown how a planning problem maps into a search problem, we can solve
planning problems with any of the heuristic search algorithms from Chapter 3 or a local
search algorithm from Chapter 4 (provided we keep track of the actions used to reach the
goal). From the earliest days of planning research (around 1961) until around 1998 it was
assumed that forward state-space search was too inefﬁcient to be practical. It is not hard to
come up with reasons why.
First, forward search is prone to exploring irrelevant actions. Consider the noble task
of buying a copy of AI: A Modern Approach from an online bookseller. Suppose there is an
(a)
(b)
At(P1, A)
Fly(P1, A, B)
Fly(P2, A, B)
Fly(P1, A, B)
Fly(P2, A, B)
At(P2, A)
At(P1, B)
At(P2, A)
At(P1, A)
At(P2, B)
At(P1, B)
At(P2, B)
At(P1, B)
At(P2, A)
At(P1, A)
At(P2, B)
Figure 10.5 Two approaches to searching for a plan. (a) Forward (progression) search
through the space of states, starting in the initial state and using the problem’s actions to
search forward for a member of the set of goal states. (b) Backward (regression) search
through sets of relevant states, starting at the set of states representing the goal and using the
inverse of the actions to search backward for the initial state.

374 Chapter 10. Classical Planning
action schema Buy(isbn) with effect Own(isbn). ISBNs are 10 digits, so this action schema
represents 10 billion ground actions. An uninformed forward-search algorithm would have
to start enumerating these 10 billion actions to ﬁnd one that leads to the goal.
Second, planning problems often have large state spaces. Consider an air cargo problem
with 10 airports, where each airport has 5 planes and 20 pieces of cargo. The goal is to move
all the cargo at airport A to airport B. There is a simple solution to the problem: load the 20
pieces of cargo into one of the planes at A, ﬂy the plane to B, and unload the cargo. Finding
the solution can be difﬁcult because the average branching factor is huge: each of the 50
planes can ﬂy to 9 other airports, and each of the 200 packages can be either unloaded (if
it is loaded) or loaded into any plane at its airport (if it is unloaded). So in any state there
is a minimum of 450 actions (when all the packages are at airports with no planes) and a
maximum of 10,450 (when all packages and planes are at the same airport). On average, let’s
say there are about 2000 possible actions per state, so the search graph up to the depth of the
obvious solution has about 2000
41 nodes.
Clearly, even this relatively small problem instance is hopeless without an accurate
heuristic. Although many real-world applications of planning have relied on domain-speciﬁc
heuristics, it turns out (as we see in Section 10.2.3) that strong domain-independent heuristics
can be derived automatically; that is what makes forward search feasible.
10.2.2 Backward (regression) relevant-states search
In regression search we start at the goal and apply the actions backward until we ﬁnd a
sequence of steps that reaches the initial state. It is called relevant-states search because weRELEVANT -ST A TES
only consider actions that are relevant to the goal (or current state). As in belief-state search
(Section 4.4), there is a set of relevant states to consider at each step, not just a single state.
We start with the goal, which is a conjunction of literals forming a description of a set of
states—for example, the goal¬Poor∧Famous describes those states in whichPoor is false,
Famous is true, and any other ﬂuent can have any value. If there are n ground ﬂuents in a
domain, then there are 2n ground states (each ﬂuent can be true or false), but 3n descriptions
of sets of goal states (each ﬂuent can be positive, negative, or not mentioned).
In general, backward search works only when we know how to regress from a state
description to the predecessor state description. For example, it is hard to search backwards
for a solution to then-queens problem because there is no easy way to describe the states that
are one move away from the goal. Happily, the PDDL representation was designed to make
it easy to regress actions—if a domain can be expressed in PDDL, then we can do regression
search on it. Given a ground goal description g and a ground action a, the regression from g
over a gives us a state description g
′ deﬁned by
g′ =( g−ADD(a))∪Precond(a) .
That is, the effects that were added by the action need not have been true before, and also
the preconditions must have held before, or else the action could not have been executed.
Note that D EL(a) does not appear in the formula; that’s because while we know the ﬂuents
in DEL(a) are no longer true after the action, we don’t know whether or not they were true
before, so there’s nothing to be said about them.
Section 10.2. Algorithms for Planning as State-Space Search 375
To get the full advantage of backward search, we need to deal with partially uninstanti-
ated actions and states, not just ground ones. For example, suppose the goal is to deliver a spe-
ciﬁc piece of cargo to SFO: At(C
2,SFO). That suggests the action Unload(C2,p ′,SFO):
Action(Unload(C2,p ′,SFO),
PRECOND :In(C2,p ′)∧At(p′,SFO)∧Cargo(C2)∧Plane(p′)∧Airport(SFO)
EFFECT :At(C2,SFO)∧¬In(C2,p ′).
(Note that we have standardized variable names (changing p to p′ in this case) so that there
will be no confusion between variable names if we happen to use the same action schema
twice in a plan. The same approach was used in Chapter 9 for ﬁrst-order logical inference.)
This represents unloading the package from an unspeciﬁed plane at SFO; any plane will do,
but we need not say which one now. We can take advantage of the power of ﬁrst-order
representations: a single description summarizes the possibility of using any of the planes by
implicitly quantifying over p
′. The regressed state description is
g′ = In(C2,p ′)∧At(p′,SFO)∧Cargo(C2)∧Plane(p′)∧Airport(SFO).
The ﬁnal issue is deciding which actions are candidates to regress over. In the forward direc-
tion we chose actions that were applicable—those actions that could be the next step in the
plan. In backward search we want actions that are relevant—those actions that could be theRELEVANCE
last step in a plan leading up to the current goal state.
For an action to be relevant to a goal it obviously must contribute to the goal: at least
one of the action’s effects (either positive or negative) must unify with an element of the goal.
What is less obvious is that the action must not have any effect (positive or negative) that
negates an element of the goal. Now, if the goal is A∧B∧C and an action has the effect
A∧B∧¬C then there is a colloquial sense in which that action is very relevant to the goal—it
gets us two-thirds of the way there. But it is not relevant in the technical sense deﬁned here,
because this action could not be the ﬁnal step of a solution—we would always need at least
one more step to achieve C.
Given the goal At(C
2,SFO), several instantiations of Unload are relevant: we could
chose any speciﬁc plane to unload from, or we could leave the plane unspeciﬁed by using
the action Unload(C2,p ′,SFO). We can reduce the branching factor without ruling out any
solutions by always using the action formed by substituting the most general uniﬁer into the
(standardized) action schema.
As another example, consider the goal Own(0136042597), given an initial state with
10 billion ISBNs, and the single action schema
A = Action(Buy(i), PRECOND :ISBN(i), EFFECT :Own(i)).
As we mentioned before, forward search without a heuristic would have to start enumer-
ating the 10 billion ground Buy actions. But with backward search, we would unify the
goal Own(0136042597) with the (standardized) effect Own(i′), yielding the substitution
θ = {i′/0136042597}. Then we would regress over the action Subst(θ,A′) to yield the
predecessor state description ISBN (0136042597). This is part of, and thus entailed by, the
initial state, so we are done.
376 Chapter 10. Classical Planning
We can make this more formal. Assume a goal description g which contains a goal
literal gi and an action schema A that is standardized to produce A′.I f A′ has an effect literal
e′
j where Unify(gi,e ′
j)= θand where we deﬁne a′ = SUBST (θ,A′) and if there is no effect
in a′ that is the negation of a literal in g,t h e na′ is a relevant action towards g.
Backward search keeps the branching factor lower than forward search, for most prob-
lem domains. However, the fact that backward search uses state sets rather than individual
states makes it harder to come up with good heuristics. That is the main reason why the
majority of current systems favor forward search.
10.2.3 Heuristics for planning
Neither forward nor backward search is efﬁcient without a good heuristic function. Recall
from Chapter 3 that a heuristic function h(s) estimates the distance from a state s to the
goal and that if we can derive an admissible heuristic for this distance—one that does not
overestimate—then we can use A
∗ search to ﬁnd optimal solutions. An admissible heuristic
can be derived by deﬁning a relaxed problem that is easier to solve. The exact cost of a
solution to this easier problem then becomes the heuristic for the original problem.
By deﬁnition, there is no way to analyze an atomic state, and thus it it requires some
ingenuity by a human analyst to deﬁne good domain-speciﬁc heuristics for search problems
with atomic states. Planning uses a factored representation for states and action schemas.
That makes it possible to deﬁne good domain-independent heuristics and for programs to
automatically apply a good domain-independent heuristic for a given problem.
Think of a search problem as a graph where the nodes are states and the edges are
actions. The problem is to ﬁnd a path connecting the initial state to a goal state. There are
two ways we can relax this problem to make it easier: by adding more edges to the graph,
making it strictly easier to ﬁnd a path, or by grouping multiple nodes together, forming an
abstraction of the state space that has fewer states, and thus is easier to search.
We look ﬁrst at heuristics that add edges to the graph. For example, the ignore pre-
conditions heuristic drops all preconditions from actions. Every action becomes applicable
IGNORE
PRECONDITIONS
HEURISTIC
in every state, and any single goal ﬂuent can be achieved in one step (if there is an applica-
ble action—if not, the problem is impossible). This almost implies that the number of steps
required to solve the relaxed problem is the number of unsatisﬁed goals—almost but not
quite, because (1) some action may achieve multiple goals and (2) some actions may undo
the effects of others. For many problems an accurate heuristic is obtained by considering (1)
and ignoring (2). First, we relax the actions by removing all preconditions and all effects
except those that are literals in the goal. Then, we count the minimum number of actions
required such that the union of those actions’ effects satisﬁes the goal. This is an instance
of the set-cover problem. There is one minor irritation: the set-cover problem is NP-hard.
SET -COVER
PROBLEM
Fortunately a simple greedy algorithm is guaranteed to return a set covering whose size is
within a factor of log n of the true minimum covering, where n is the number of literals in
the goal. Unfortunately, the greedy algorithm loses the guarantee of admissibility.
It is also possible to ignore onlyselected preconditions of actions. Consider the sliding-
block puzzle (8-puzzle or 15-puzzle) from Section 3.2. We could encode this as a planning
Section 10.2. Algorithms for Planning as State-Space Search 377
problem involving tiles with a single schema Slide:
Action(Slide(t, s1,s2),
PRECOND :On(t, s1)∧Tile(t)∧Blank(s2)∧Adjacent(s1,s2)
EFFECT :On(t, s2)∧Blank(s1)∧¬On(t, s1)∧¬Blank(s2))
As we saw in Section 3.6, if we remove the preconditions Blank(s2)∧Adjacent(s1,s2)
then any tile can move in one action to any space and we get the number-of-misplaced-tiles
heuristic. If we remove Blank(s
2) then we get the Manhattan-distance heuristic. It is easy to
see how these heuristics could be derived automatically from the action schema description.
The ease of manipulating the schemas is the great advantage of the factored representation of
planning problems, as compared with the atomic representation of search problems.
Another possibility is the ignore delete lists heuristic. Assume for a moment that all
IGNORE DELETE
LISTS
goals and preconditions contain only positive literals3 We want to create a relaxed version of
the original problem that will be easier to solve, and where the length of the solution will serve
as a good heuristic. We can do that by removing the delete lists from all actions (i.e., removing
all negative literals from effects). That makes it possible to make monotonic progress towards
the goal—no action will ever undo progress made by another action. It turns out it is still NP-
hard to ﬁnd the optimal solution to this relaxed problem, but an approximate solution can be
found in polynomial time by hill-climbing. Figure 10.6 diagrams part of the state space for
two planning problems using the ignore-delete-lists heuristic. The dots represent states and
the edges actions, and the height of each dot above the bottom plane represents the heuristic
value. States on the bottom plane are solutions. In both these problems, there is a wide path
to the goal. There are no dead ends, so no need for backtracking; a simple hillclimbing search
will easily ﬁnd a solution to these problems (although it may not be an optimal solution).
The relaxed problems leave us with a simpliﬁed—but still expensive—planning prob-
lem just to calculate the value of the heuristic function. Many planning problems have 10
100
states or more, and relaxing the actions does nothing to reduce the number of states. There-
fore, we now look at relaxations that decrease the number of states by forming a state ab-
straction—a many-to-one mapping from states in the ground representation of the problemSTA TE ABSTRACTION
to the abstract representation.
The easiest form of state abstraction is to ignore some ﬂuents. For example, consider
an air cargo problem with 10 airports, 50 planes, and 200 pieces of cargo. Each plane can
be at one of 10 airports and each package can be either in one of the planes or unloaded at
one of the airports. So there are 50
10 × 20050+10 ≈10155 states. Now consider a particular
problem in that domain in which it happens that all the packages are at just 5 of the airports,
and all packages at a given airport have the same destination. Then a useful abstraction of the
problem is to drop all the At ﬂuents except for the ones involving one plane and one package
at each of the 5 airports. Now there are only 5
10 × 55+10 ≈1017 states. A solution in this
abstract state space will be shorter than a solution in the original space (and thus will be an
admissible heuristic), and the abstract solution is easy to extend to a solution to the original
problem (by adding additional Load and Unload actions).
3 Many problems are written with this convention. For problems that aren’t, replace every negative literal ¬P
in a goal or precondition with a new positive literal, P ′.
378 Chapter 10. Classical Planning
Figure 10.6 Two state spaces from planning problems with the ignore-delete-lists heuris-
tic. The height above the bottom plane is the heuristic score of a state; states on the bottom
plane are goals. There are no local minima, so search for the goal is straightforward. From
Hoffmann (2005).
A key idea in deﬁning heuristics is decomposition: dividing a problem into parts, solv-DECOMPOSITION
ing each part independently, and then combining the parts. The subgoal independence as-SUBGOAL
INDEPENDENCE
sumption is that the cost of solving a conjunction of subgoals is approximated by the sum
of the costs of solving each subgoal independently. The subgoal independence assumption
can be optimistic or pessimistic. It is optimistic when there are negative interactions between
the subplans for each subgoal—for example, when an action in one subplan deletes a goal
achieved by another subplan. It is pessimistic, and therefore inadmissible, when subplans
contain redundant actions—for instance, two actions that could be replaced by a single action
in the merged plan.
Suppose the goal is a set of ﬂuentsG, which we divide into disjoint subsetsG
1,...,G n.
We then ﬁnd plans P1,...,P n that solve the respective subgoals. What is an estimate of the
cost of the plan for achieving all ofG? We can think of eachCost(Pi) as a heuristic estimate,
and we know that if we combine estimates by taking their maximum value, we always get an
admissible heuristic. So max
i COST(Pi) is admissible, and sometimes it is exactly correct:
it could be that P1 serendipitously achieves all the Gi. But in most cases, in practice the
estimate is too low. Could we sum the costs instead? For many problems that is a reasonable
estimate, but it is not admissible. The best case is when we can determine that G
i and Gj are
independent. If the effects of Pi leave all the preconditions and goals ofPj unchanged, then
the estimate COST(Pi)+ COST(Pj) is admissible, and more accurate than the max estimate.
We show in Section 10.3.1 that planning graphs can help provide better heuristic estimates.
It is clear that there is great potential for cutting down the search space by forming ab-
stractions. The trick is choosing the right abstractions and using them in a way that makes
the total cost—deﬁning an abstraction, doing an abstract search, and mapping the abstraction
back to the original problem—less than the cost of solving the original problem. The tech-
Section 10.3. Planning Graphs 379
niques of pattern databases from Section 3.6.3 can be useful, because the cost of creating
the pattern database can be amortized over multiple problem instances.
An example of a system that makes use of effective heuristics is FF, or FAST FORW ARD
(Hoffmann, 2005), a forward state-space searcher that uses the ignore-delete-lists heuristic,
estimating the heuristic with the help of a planning graph (see Section 10.3). FF then uses
hill-climbing search (modiﬁed to keep track of the plan) with the heuristic to ﬁnd a solution.
When it hits a plateau or local maximum—when no action leads to a state with better heuristic
score—then FF uses iterative deepening search until it ﬁnds a state that is better, or it gives
up and restarts hill-climbing.
10.3 P LANNING GRAPHS
All of the heuristics we have suggested can suffer from inaccuracies. This section shows
how a special data structure called a planning graph can be used to give better heuristic
PLANNING GRAPH
estimates. These heuristics can be applied to any of the search techniques we have seen so
far. Alternatively, we can search for a solution over the space formed by the planning graph,
using an algorithm called GRAPHPLAN .
A planning problem asks if we can reach a goal state from the initial state. Suppose we
are given a tree of all possible actions from the initial state to successor states, and their suc-
cessors, and so on. If we indexed this tree appropriately, we could answer the planning ques-
tion “can we reach state G from state S
0” immediately, just by looking it up. Of course, the
tree is of exponential size, so this approach is impractical. A planning graph is polynomial-
size approximation to this tree that can be constructed quickly. The planning graph can’t
answer deﬁnitively whether G is reachable from S
0, but it can estimate how many steps it
takes to reach G. The estimate is always correct when it reports the goal is not reachable, and
it never overestimates the number of steps, so it is an admissible heuristic.
A planning graph is a directed graph organized into levels:ﬁ r s tal e v e lS0 for the initialLEVEL
state, consisting of nodes representing each ﬂuent that holds inS0;t h e nal e v e lA0 consisting
of nodes for each ground action that might be applicable in S0; then alternating levels Si
followed by Ai; until we reach a termination condition (to be discussed later).
Roughly speaking, Si contains all the literals that could hold at time i, depending on
the actions executed at preceding time steps. If it is possible that either P or¬P could hold,
then both will be represented in Si. Also roughly speaking, Ai contains all the actions that
could have their preconditions satisﬁed at time i. We say “roughly speaking” because the
planning graph records only a restricted subset of the possible negative interactions among
actions; therefore, a literal might show up at level S
j when actually it could not be true until
a later level, if at all. (A literal will never show up too late.) Despite the possible error, the
level j at which a literal ﬁrst appears is a good estimate of how difﬁcult it is to achieve the
literal from the initial state.
Planning graphs work only for propositional planning problems—ones with no vari-
ables. As we mentioned on page 368, it is straightforward to propositionalize a set of ac-
380 Chapter 10. Classical Planning
Init(Have(Cake))
Goal(Have(Cake) ∧Eaten(Cake))
Action(Eat(Cake)
PRECOND : Have(Cake)
EFFECT :¬ Have(Cake) ∧Eaten(Cake))
Action(Bake(Cake)
PRECOND :¬ Have(Cake)
EFFECT : Have(Cake))
Figure 10.7 The “have cake and eat cake too” problem.
Bake(Cake)
Eat(Cake)
Have(Cake)
S0 A0 S1 A1 S2
Have(Cake) Have(Cake) Have(Cake)
Have(Cake)
Eaten(Cake)
Eaten(Cake) Eaten(Cake)Eaten(Cake)
Eaten(Cake)
Eat(Cake)
¬
¬ ¬
¬
¬
Figure 10.8 The planning graph for the “have cake and eat cake too” problem up to level
S2. Rectangles indicate actions (small squares i ndicate persistence actions), and straight
lines indicate preconditions and effects. Mutex links are shown as curved gray lines. Not all
mutex links are shown, because the graph would be too cluttered. In general, if two literals
are mutex at Si, then the persistence actions for those literals will be mutex at Ai and we
need not draw that mutex link.
tion schemas. Despite the resulting increase in the size of the problem description, planning
graphs have proved to be effective tools for solving hard planning problems.
Figure 10.7 shows a simple planning problem, and Figure 10.8 shows its planning
graph. Each action at level Ai is connected to its preconditions at Si and its effects at Si+1.
So a literal appears because an action caused it, but we also want to say that a literal can
persist if no action negates it. This is represented by a persistence action (sometimes called
PERSISTENCE
ACTION
a no-op). For every literal C, we add to the problem a persistence action with preconditionC
and effect C.L e v e lA0 in Figure 10.8 shows one “real” action, Eat(Cake), along with two
persistence actions drawn as small square boxes.
Level A0 contains all the actions that could occur in state S0, but just as important it
records conﬂicts between actions that would prevent them from occurring together. The gray
lines in Figure 10.8 indicate mutual exclusion (or mutex) links. For example, Eat(Cake) is
MUTUAL EXCLUSION
MUTEX mutually exclusive with the persistence of either Have(Cake) or¬Eaten(Cake).W e s h a l l
see shortly how mutex links are computed.
Level S1 contains all the literals that could result from picking any subset of the actions
in A0, as well as mutex links (gray lines) indicating literals that could not appear together,
regardless of the choice of actions. For example, Have(Cake) and Eaten(Cake) are mutex:
Section 10.3. Planning Graphs 381
depending on the choice of actions in A0, either, but not both, could be the result. In other
words, S1 represents a belief state: a set of possible states. The members of this set are all
subsets of the literals such that there is no mutex link between any members of the subset.
We continue in this way, alternating between state levelSi and action level Ai until we
reach a point where two consecutive levels are identical. At this point, we say that the graph
has leveled off. The graph in Figure 10.8 levels off at S2.LEVELED OFF
What we end up with is a structure where everyAi level contains all the actions that are
applicable in Si, along with constraints saying that two actions cannot both be executed at the
same level. Every Si level contains all the literals that could result from any possible choice
of actions in Ai−1, along with constraints saying which pairs of literals are not possible.
It is important to note that the process of constructing the planning graph does not require
choosing among actions, which would entail combinatorial search. Instead, it just records the
impossibility of certain choices using mutex links.
We now deﬁne mutex links for both actions and literals. A mutex relation holds between
two actions at a given level if any of the following three conditions holds:
•Inconsistent effects: one action negates an effect of the other. For example,Eat(Cake)
and the persistence of Have(Cake) have inconsistent effects because they disagree on
the effect Have(Cake).
•Interference: one of the effects of one action is the negation of a precondition of the
other. For exampleEat(Cake) interferes with the persistence ofHave(Cake) by negat-
ing its precondition.
•Competing needs: one of the preconditions of one action is mutually exclusive with a
precondition of the other. For example,Bake(Cake) and Eat(Cake) are mutex because
they compete on the value of the Have(Cake) precondition.
A mutex relation holds between twoliterals at the same level if one is the negation of the other
or if each possible pair of actions that could achieve the two literals is mutually exclusive.
This condition is called inconsistent support. For example, Have(Cake) and Eaten(Cake)
are mutex in S
1 because the only way of achieving Have(Cake), the persistence action, is
mutex with the only way of achieving Eaten(Cake), namely Eat(Cake).I n S2 the two
literals are not mutex, because there are new ways of achieving them, such as Bake(Cake)
and the persistence of Eaten(Cake), that are not mutex.
A planning graph is polynomial in the size of the planning problem. For a planning
problem with l literals and a actions, each Si has no more than l nodes and l2 mutex links,
and each Ai has no more than a + l nodes (including the no-ops), (a + l)2 mutex links, and
2(al + l) precondition and effect links. Thus, an entire graph with n levels has a size of
O(n(a + l)2). The time to build the graph has the same complexity.
10.3.1 Planning graphs for heuristic estimation
A planning graph, once constructed, is a rich source of information about the problem. First,
if any goal literal fails to appear in the ﬁnal level of the graph, then the problem is unsolvable.
Second, we can estimate the cost of achieving any goal literal g
i from state s as the level at
which gi ﬁrst appears in the planning graph constructed from initial state s. We call this the
382 Chapter 10. Classical Planning
level cost of gi. In Figure 10.8, Have(Cake) has level cost 0 andEaten(Cake) has level costLEVEL COST
1. It is easy to show (Exercise 10.10) that these estimates are admissible for the individual
goals. The estimate might not always be accurate, however, because planning graphs allow
several actions at each level, whereas the heuristic counts just the level and not the number
of actions. For this reason, it is common to use a serial planning graph for computing
SERIAL PLANNING
GRAPH
heuristics. A serial graph insists that only one action can actually occur at any given time
step; this is done by adding mutex links between every pair of nonpersistence actions. Level
costs extracted from serial graphs are often quite reasonable estimates of actual costs.
To estimate the cost of a conjunction of goals, there are three simple approaches. The
max-level heuristic simply takes the maximum level cost of any of the goals; this is admissi-
MAX-LEVEL
ble, but not necessarily accurate.
The level sum heuristic, following the subgoal independence assumption, returns theLEVEL SUM
sum of the level costs of the goals; this can be inadmissible but works well in practice
for problems that are largely decomposable. It is much more accurate than the number-
of-unsatisﬁed-goals heuristic from Section 10.2. For our problem, the level-sum heuristic
estimate for the conjunctive goal Have(Cake)∧Eaten(Cake) will be 0+1=1 , whereas
the correct answer is 2, achieved by the plan [Eat(Cake),Bake(Cake)]. That doesn’t seem
so bad. A more serious error is that if Bake(Cake) were not in the set of actions, then the
estimate would still be 1, when in fact the conjunctive goal would be impossible.
Finally, the set-level heuristic ﬁnds the level at which all the literals in the conjunctive
SET -LEVEL
goal appear in the planning graph without any pair of them being mutually exclusive. This
heuristic gives the correct values of 2 for our original problem and inﬁnity for the problem
without Bake(Cake). It is admissible, it dominates the max-level heuristic, and it works
extremely well on tasks in which there is a good deal of interaction among subplans. It is not
perfect, of course; for example, it ignores interactions among three or more literals.
As a tool for generating accurate heuristics, we can view the planning graph as a relaxed
problem that is efﬁciently solvable. To understand the nature of the relaxed problem, we
need to understand exactly what it means for a literal g to appear at level S
i in the planning
graph. Ideally, we would like it to be a guarantee that there exists a plan with i action levels
that achieves g, and also that if g does not appear, there is no such plan. Unfortunately,
making that guarantee is as difﬁcult as solving the original planning problem. So the planning
graph makes the second half of the guarantee (if g does not appear, there is no plan), but
if g does appear, then all the planning graph promises is that there is a plan that possibly
achieves g and has no “obvious” ﬂaws. An obvious ﬂaw is deﬁned as a ﬂaw that can be
detected by considering two actions or two literals at a time—in other words, by looking at
the mutex relations. There could be more subtle ﬂaws involving three, four, or more actions,
but experience has shown that it is not worth the computational effort to keep track of these
possible ﬂaws. This is similar to a lesson learned from constraint satisfaction problems—that
it is often worthwhile to compute 2-consistency before searching for a solution, but less often
worthwhile to compute 3-consistency or higher. (See page 211.)
One example of an unsolvable problem that cannot be recognized as such by a planning
graph is the blocks-world problem where the goal is to get block A on B, B on C,a n dC on
A. This is an impossible goal; a tower with the bottom on top of the top. But a planning graph
Section 10.3. Planning Graphs 383
cannot detect the impossibility, because any two of the three subgoals are achievable. There
are no mutexes between any pair of literals, only between the three as a whole. To detect that
this problem is impossible, we would have to search over the planning graph.
10.3.2 The GRAPHPLAN algorithm
This subsection shows how to extract a plan directly from the planning graph, rather than just
using the graph to provide a heuristic. The G RAPHPLAN algorithm (Figure 10.9) repeatedly
adds a level to a planning graph with E XPAND -GRAPH . Once all the goals show up as non-
mutex in the graph, GRAPHPLAN calls EXTRACT -SOLUTION to search for a plan that solves
the problem. If that fails, it expands another level and tries again, terminating with failure
when there is no reason to go on.
function GRAPHPLAN (problem) returns solution or failure
graph←INITIAL -PLANNING -GRAPH (problem)
goals←CONJUNCTS (problem.GOAL )
nogoods←an empty hash table
for tl =0 to∞do
if goals all non-mutex in St of graph then
solution←EXTRACT -SOLUTION (graph, goals,N UMLEVELS (graph), nogoods)
if solution̸= failure then return solution
if graph and nogoods have both leveled off then return failure
graph←EXPAND -GRAPH (graph, problem)
Figure 10.9 The GRAPHPLAN algorithm. G RAPHPLAN calls EXPAND -GRAPH to add a
level until either a solution is found by EXTRACT -SOLUTION , or no solution is possible.
Let us now trace the operation of GRAPHPLAN on the spare tire problem from page 370.
The graph is shown in Figure 10.10. The ﬁrst line of G RAPHPLAN initializes the planning
graph to a one-level ( S0) graph representing the initial state. The positive ﬂuents from the
problem description’s initial state are shown, as are the relevant negative ﬂuents. Not shown
are the unchanging positive literals (such asTire(Spare)) and the irrelevant negative literals.
The goal At(Spare,Axle) is not present in S
0, so we need not call E XTRACT -SOLUTION —
we are certain that there is no solution yet. Instead, E XPAND -GRAPH adds into A0 the three
actions whose preconditions exist at levelS0 (i.e., all the actions exceptPutOn(Spare,Axle)),
along with persistence actions for all the literals inS0. The effects of the actions are added at
level S1.E XPAND -GRAPH then looks for mutex relations and adds them to the graph.
At(Spare,Axle) is still not present inS1, so again we do not call EXTRACT -SOLUTION .
We call EXPAND -GRAPH again, adding A1 and S1 and giving us the planning graph shown
in Figure 10.10. Now that we have the full complement of actions, it is worthwhile to look at
some of the examples of mutex relations and their causes:
•Inconsistent effects: Remove(Spare,Trunk) is mutex with LeaveOvernight because
one has the effect At(Spare,Ground) and the other has its negation.
384 Chapter 10. Classical Planning
S0 A1 S2
At(Spare,Trunk)
At(Spare,Trunk)
At(Flat,Axle)
At(Flat,Axle)
At(Spare,Axle)
At(Flat,Ground)
At(Flat,Ground)
At(Spare,Ground)
At(Spare,Ground)
At(Spare,Trunk)
At(Spare,Trunk)
At(Flat,Axle)
At(Flat,Axle)
At(Spare,Axle)
At(Flat,Ground)
At(Flat,Ground)
At(Spare,Ground)
At(Spare,Ground)
At(Spare,Axle)
At(Spare,Trunk)
At(Flat,Axle)
At(Spare,Axle)
At(Flat,Ground)
At(Spare,Ground)
PutOn(Spare,Axle)
LeaveOvernight
Remove(Flat,Axle)
Remove(Spare,Trunk)
Remove(Spare,Trunk)
Remove(Flat,Axle)
LeaveOvernight
¬
¬
¬
¬
¬
¬
¬
¬
¬
¬
¬
¬
¬
A0 S1
Figure 10.10 The planning graph for the spare tire problem after expansion to level S2.
Mutex links are shown as gray lines. Not all links are shown, because the graph would be too
cluttered if we showed them all. The solution is indicated by bold lines and outlines.
•Interference: Remove(Flat,Axle) is mutex withLeaveOvernight because one has the
precondition At(Flat,Axle) and the other has its negation as an effect.
•Competing needs: PutOn(Spare,Axle) is mutex with Remove(Flat,Axle) because
one has At(Flat,Axle) as a precondition and the other has its negation.
•Inconsistent support: At(Spare,Axle) is mutex withAt(Flat,Axle) in S2 because the
only way of achieving At(Spare,Axle) is by PutOn(Spare,Axle), and that is mutex
with the persistence action that is the only way of achieving At(Flat,Axle). Thus, the
mutex relations detect the immediate conﬂict that arises from trying to put two objects
in the same place at the same time.
This time, when we go back to the start of the loop, all the literals from the goal are present
in S
2, and none of them is mutex with any other. That means that a solution might exist,
and E XTRACT -SOLUTION will try to ﬁnd it. We can formulate E XTRACT -SOLUTION as a
Boolean constraint satisfaction problem (CSP) where the variables are the actions at each
level, the values for each variable arein or out of the plan, and the constraints are the mutexes
and the need to satisfy each goal and precondition.
Alternatively, we can deﬁne E
XTRACT -SOLUTION as a backward search problem, where
each state in the search contains a pointer to a level in the planning graph and a set of unsat-
isﬁed goals. We deﬁne this search problem as follows:
•The initial state is the last level of the planning graph, Sn, along with the set of goals
from the planning problem.
•The actions available in a state at level Si are to select any conﬂict-free subset of the
actions in Ai−1 whose effects cover the goals in the state. The resulting state has level
Si−1 and has as its set of goals the preconditions for the selected set of actions. By
“conﬂict free,” we mean a set of actions such that no two of them are mutex and no two
of their preconditions are mutex.
Section 10.3. Planning Graphs 385
•The goal is to reach a state at level S0 such that all the goals are satisﬁed.
•The cost of each action is 1.
For this particular problem, we start atS2 with the goalAt(Spare,Axle). The only choice we
have for achieving the goal set isPutOn(Spare,Axle). That brings us to a search state at S1
with goals At(Spare,Ground) and¬At(Flat,Axle). The former can be achieved only by
Remove(Spare,Trunk), and the latter by either Remove(Flat,Axle) or LeaveOvernight.
But LeaveOvernight is mutex withRemove(Spare,Trunk), so the only solution is to choose
Remove(Spare,Trunk) and Remove(Flat,Axle). That brings us to a search state atS0 with
the goals At(Spare,Trunk) and At(Flat,Axle). Both of these are present in the state, so
we have a solution: the actions Remove(Spare, Trunk) and Remove(Flat, Axle) in level
A0, followed by PutOn(Spare, Axle) in A1.
In the case where E XTRACT -SOLUTION fails to ﬁnd a solution for a set of goals at
a given level, we record the (level,goals) pair as a no-good, just as we did in constraint
learning for CSPs (page 220). Whenever EXTRACT -SOLUTION is called again with the same
level and goals, we can ﬁnd the recorded no-good and immediately return failure rather than
searching again. We see shortly that no-goods are also used in the termination test.
We know that planning is PSPACE-complete and that constructing the planning graph
takes polynomial time, so it must be the case that solution extraction is intractable in the worst
case. Therefore, we will need some heuristic guidance for choosing among actions during the
backward search. One approach that works well in practice is a greedy algorithm based on
the level cost of the literals. For any set of goals, we proceed in the following order:
1. Pick ﬁrst the literal with the highest level cost.
2. To achieve that literal, prefer actions with easier preconditions. That is, choose an action
such that the sum (or maximum) of the level costs of its preconditions is smallest.
10.3.3 T ermination of GRAPHPLAN
So far, we have skated over the question of termination. Here we show that GRAPHPLAN will
in fact terminate and return failure when there is no solution.
The ﬁrst thing to understand is why we can’t stop expanding the graph as soon as it has
leveled off. Consider an air cargo domain with one plane and n pieces of cargo at airport
A, all of which have airport B as their destination. In this version of the problem, only one
piece of cargo can ﬁt in the plane at a time. The graph will level off at level 4, reﬂecting the
fact that for any single piece of cargo, we can load it, ﬂy it, and unload it at the destination in
three steps. But that does not mean that a solution can be extracted from the graph at level 4;
in fact a solution will require 4n−1 steps: for each piece of cargo we load, ﬂy, and unload,
and for all but the last piece we need to ﬂy back to airport A to get the next piece.
How long do we have to keep expanding after the graph has leveled off? If the function
E
XTRACT -SOLUTION fails to ﬁnd a solution, then there must have been at least one set of
goals that were not achievable and were marked as a no-good. So if it is possible that there
might be fewer no-goods in the next level, then we should continue. As soon as the graph
itself and the no-goods have both leveled off, with no solution found, we can terminate with
failure because there is no possibility of a subsequent change that could add a solution.
386 Chapter 10. Classical Planning
Now all we have to do is prove that the graph and the no-goods will always level off. The
key to this proof is that certain properties of planning graphs are monotonically increasing or
decreasing. “X increases monotonically” means that the set of Xs at level i +1 is a superset
(not necessarily proper) of the set at level i. The properties are as follows:
•Literals increase monotonically: Once a literal appears at a given level, it will appear
at all subsequent levels. This is because of the persistence actions; once a literal shows
up, persistence actions cause it to stay forever.
•Actions increase monotonically: Once an action appears at a given level, it will appear
at all subsequent levels. This is a consequence of the monotonic increase of literals; if
the preconditions of an action appear at one level, they will appear at subsequent levels,
and thus so will the action.
•Mutexes decrease monotonically: If two actions are mutex at a given levelA
i, then they
will also be mutex for all previous levels at which they both appear. The same holds for
mutexes between literals. It might not always appear that way in the ﬁgures, because
the ﬁgures have a simpliﬁcation: they display neither literals that cannot hold at level
S
i nor actions that cannot be executed at level Ai. We can see that “mutexes decrease
monotonically” is true if you consider that these invisible literals and actions are mutex
with everything.
The proof can be handled by cases: if actions A and B are mutex at level A
i,i t
must be because of one of the three types of mutex. The ﬁrst two, inconsistent effects
and interference, are properties of the actions themselves, so if the actions are mutex
at A
i, they will be mutex at every level. The third case, competing needs, depends on
conditions at level Si: that level must contain a precondition of A that is mutex with
a precondition of B. Now, these two preconditions can be mutex if they are negations
of each other (in which case they would be mutex in every level) or if all actions for
achieving one are mutex with all actions for achieving the other. But we already know
that the available actions are increasing monotonically, so, by induction, the mutexes
must be decreasing.
•No-goods decrease monotonically: If a set of goals is not achievable at a given level,
then they are not achievable in anyprevious level. The proof is by contradiction: if they
were achievable at some previous level, then we could just add persistence actions to
make them achievable at a subsequent level.
Because the actions and literals increase monotonically and because there are only a ﬁnite
number of actions and literals, there must come a level that has the same number of actions
and literals as the previous level. Because mutexes and no-goods decrease, and because there
can never be fewer than zero mutexes or no-goods, there must come a level that has the
same number of mutexes and no-goods as the previous level. Once a graph has reached this
state, then if one of the goals is missing or is mutex with another goal, then we can stop the
G
RAPHPLAN algorithm and return failure. That concludes a sketch of the proof; for more
details see Ghallab et al. (2004).
Section 10.4. Other Classical Planning Approaches 387
Ye a r
 Track
 Winning Systems (approaches)
2008
 Optimal
 GAMER (model checking, bidirectional search)
2008
 Satisﬁcing
 LAMA (fast downward search with FF heuristic)
2006
 Optimal
 SATPLAN ,M AXPLAN (Boolean satisﬁability)
2006
 Satisﬁcing
 SGPLAN (forward search; partiti ons into independent subproblems)
2004
 Optimal
 SATPLAN (Boolean satisﬁability)
2004
 Satisﬁcing
 FAST DIAGONALLY DOWNWARD (forward search with causal graph)
2002
 Automated
 LPG (local search, planning graphs converted to CSPs)
2002
 Hand-coded
 TLPLAN (temporal action logic with control rules for forward search)
2000
 Automated
 FF (forward search)
2000
 Hand-coded
 TAL PLANNER (temporal action logic with control rules for forward search)
1998
 Automated
 IPP (planning graphs); HSP (forward search)
Figure 10.11 Some of the top-performing systems in the International Planning Compe-
tition. Each year there are various tracks: “ Optimal” means the planners must produce the
shortest possible plan, while “Satisﬁcing” means nonoptimal solutions are accepted. “Hand-
coded” means domain-speciﬁc heuristics are allowed; “Automated” means they are not.
10.4 O THER CLASSICAL PLANNING APPROACHES
Currently the most popular and effective approaches to fully automated planning are:
•Translating to a Boolean satisﬁability (SAT) problem
•Forward state-space search with carefully crafted heuristics (Section 10.2)
•Search using a planning graph (Section 10.3)
These three approaches are not the only ones tried in the 40-year history of automated plan-
ning. Figure 10.11 shows some of the top systems in the International Planning Competitions,
which have been held every even year since 1998. In this section we ﬁrst describe the transla-
tion to a satisﬁability problem and then describe three other inﬂuential approaches: planning
as ﬁrst-order logical deduction; as constraint satisfaction; and as plan reﬁnement.
10.4.1 Classical planning as Boolean satisﬁability
In Section 7.7.4 we saw how SATPLAN solves planning problems that are expressed in propo-
sitional logic. Here we show how to translate a PDDL description into a form that can be
processed by SATP
LAN . The translation is a series of straightforward steps:
•Propositionalize the actions: replace each action schema with a set of ground actions
formed by substituting constants for each of the variables. These ground actions are not
part of the translation, but will be used in subsequent steps.
•Deﬁne the initial state: assert F
0 for every ﬂuent F in the problem’s initial state, and
¬F for every ﬂuent not mentioned in the initial state.
•Propositionalize the goal: for every variable in the goal, replace the literals that contain
the variable with a disjunction over constants. For example, the goal of having blockA
388 Chapter 10. Classical Planning
on another block, On(A, x)∧Block(x) in a world with objects A, B and C, would be
replaced by the goal
(On(A, A)∧Block(A))∨(On(A, B)∧Block(B))∨(On(A, C)∧Block(C)).
•Add successor-state axioms: For each ﬂuent F , add an axiom of the form
Ft+1 ⇔ActionCausesFt∨(Ft∧¬ActionCausesNotFt) ,
where ActionCausesF is a disjunction of all the ground actions that have F in their
add list, and ActionCausesNotF is a disjunction of all the ground actions that have F
in their delete list.
•Add precondition axioms: For each ground action A, add the axiom At ⇒PRE(A)t,
that is, if an action is taken at time t, then the preconditions must have been true.
•Add action exclusion axioms: say that every action is distinct from every other action.
The resulting translation is in the form that we can hand to SATPLAN to ﬁnd a solution.
10.4.2 Planning as ﬁrst-order logical deduction: Situation calculus
PDDL is a language that carefully balances the expressiveness of the language with the com-
plexity of the algorithms that operate on it. But some problems remain difﬁcult to express in
PDDL. For example, we can’t express the goal “move all the cargo from A to B regardless
of how many pieces of cargo there are” in PDDL, but we can do it in ﬁrst-order logic, using a
universal quantiﬁer. Likewise, ﬁrst-order logic can concisely express global constraints such
as “no more than four robots can be in the same place at the same time.” PDDL can only say
this with repetitious preconditions on every possible action that involves a move.
The propositional logic representation of planning problems also has limitations, such
as the fact that the notion of time is tied directly to ﬂuents. For example, South
2 means
“the agent is facing south at time 2.” With that representation, there is no way to say “the
agent would be facing south at time 2 if it executed a right turn at time 1; otherwise it would
be facing east.” First-order logic lets us get around this limitation by replacing the notion
of linear time with a notion of branching situations, using a representation called situation
calculus that works like this:
SITUA TION
CALCULUS
•The initial state is called a situation.I f s is a situation and a is an action, thenSITUA TION
RESULT (s,a) is also a situation. There are no other situations. Thus, a situation cor-
responds to a sequence, or history, of actions. You can also think of a situation as the
result of applying the actions, but note that two situations are the same only if their start
and actions are the same: (R
ESULT (s,a)= RESULT (s′,a ′)) ⇔ (s = s′ ∧a = a′).
Some examples of actions and situations are shown in Figure 10.12.
•A function or relation that can vary from one situation to the next is aﬂuent. By conven-
tion, the situation s is always the last argument to the ﬂuent, for exampleAt(x, l, s) is a
relational ﬂuent that is true when objectx is at locationl in situations,a n dLocation is a
functional ﬂuent such thatLocation(x, s)= l holds in the same situations asAt(x, l, s).
•Each action’s preconditions are described with a possibility axiom that says when thePOSSIBILITY AXIOM
action can be taken. It has the form Φ(s) ⇒Poss(a, s) where Φ(s) is some formula
Section 10.4. Other Classical Planning Approaches 389
PIT
PIT
PIT
Gold
PIT
PIT
PIT
Gold
PIT
PIT
PIT
Gold
S0
Forward
Result(S0, Forward)
Result(Result(S0, Forward),
Turn(Right))
Turn(Right)
Figure 10.12 Situations as the results of actions in the wumpus world.
involving s that describes the preconditions. An example from the wumpus world says
that it is possible to shoot if the agent is alive and has an arrow:
Alive(Agent,s)∧Have(Agent,Arrow,s) ⇒Poss(Shoot,s)
•Each ﬂuent is described with a successor-state axiom that says what happens to the
ﬂuent, depending on what action is taken. This is similar to the approach we took for
propositional logic. The axiom has the form
Action is possible ⇒
(Fluent is true in result state ⇔Action’s effect made it true
∨It was true before and action left it alone ) .
For example, the axiom for the relational ﬂuent Holding says that the agent is holding
some gold g after executing a possible action if and only if the action was a Grab of g
or if the agent was already holding g and the action was not releasing it:
Poss(a, s) ⇒
(Holding(Agent,g, Result(a, s))⇔
a=Grab(g)∨(Holding(Agent,g,s )∧a̸= Release(g))) .
•We need unique action axioms so that the agent can deduce that, for example, a ̸=
UNIQUE ACTION
AXIOMS
Release(g). For each distinct pair of action names Ai and Aj we have an axiom that
says the actions are different:
Ai(x ,... )̸= Aj(y,... )
390 Chapter 10. Classical Planning
and for each action name Ai we have an axiom that says two uses of that action name
are equal if and only if all their arguments are equal:
Ai(x1,...,x n)= Ai(y1,...,y n) ⇔x1 = y1∧... ∧xn = yn .
•A solution is a situation (and hence a sequence of actions) that satisﬁes the goal.
Work in situation calculus has done a lot to deﬁne the formal semantics of planning and to
open up new areas of investigation. But so far there have not been any practical large-scale
planning programs based on logical deduction over the situation calculus. This is in part
because of the difﬁculty of doing efﬁcient inference in FOL, but is mainly because the ﬁeld
has not yet developed effective heuristics for planning with situation calculus.
10.4.3 Planning as constraint satisfaction
We have seen that constraint satisfaction has a lot in common with Boolean satisﬁability, and
we have seen that CSP techniques are effective for scheduling problems, so it is not surprising
that it is possible to encode a bounded planning problem (i.e., the problem of ﬁnding a plan of
length k) as a constraint satisfaction problem (CSP). The encoding is similar to the encoding
to a SAT problem (Section 10.4.1), with one important simpliﬁcation: at each time step we
need only a single variable, Action
t, whose domain is the set of possible actions. We no
longer need one variable for every action, and we don’t need the action exclusion axioms. It
is also possible to encode a planning graph into a CSP. This is the approach taken by GP-CSP
(Do and Kambhampati, 2003).
10.4.4 Planning as reﬁnement of partially ordered plans
All the approaches we have seen so far construct totally ordered plans consisting of a strictly
linear sequences of actions. This representation ignores the fact that many subproblems are
independent. A solution to an air cargo problem consists of a totally ordered sequence of
actions, yet if 30 packages are being loaded onto one plane in one airport and 50 packages are
being loaded onto another at another airport, it seems pointless to come up with a strict linear
ordering of 80 load actions; the two subsets of actions should be thought of independently.
An alternative is to represent plans as partially ordered structures: a plan is a set of
actions and a set of constraints of the form Before(ai,aj) saying that one action occurs
before another. In the bottom of Figure 10.13, we see a partially ordered plan that is a solution
to the spare tire problem. Actions are boxes and ordering constraints are arrows. Note that
Remove(Spare,Trunk) and Remove(Flat,Axle) can be done in either order as long as they
are both completed before the PutOn(Spare,Axle) action.
Partially ordered plans are created by a search through the space of plans rather than
through the state space. We start with the empty plan consisting of just the initial state and
the goal, with no actions in between, as in the top of Figure 10.13. The search procedure then
looks for a ﬂaw in the plan, and makes an addition to the plan to correct the ﬂaw (or if no
FLAW
correction can be made, the search backtracks and tries something else). A ﬂaw is anything
that keeps the partial plan from being a solution. For example, one ﬂaw in the empty plan is
that no action achievesAt(Spare,Axle). One way to correct the ﬂaw is to insert into the plan
Section 10.4. Other Classical Planning Approaches 391
FinishAt(Spare,Axle)Start
At(Flat,Axle)
At(Spare,Trunk)
(a)
Remove(Spare,Trunk)At(Spare,Trunk)
PutOn(Spare,Axle)
At(Spare,Ground)
At(Flat,Axle)
FinishAt(Spare,Axle)Start
At(Flat,Axle)
At(Spare,Trunk)
¬
(b)
Start
Remove(Spare,Trunk)At(Spare,Trunk)
Remove(Flat,Axle)At(Flat,Axle)
PutOn(Spare,Axle)
At(Spare,Ground)
At(Flat,Axle)
FinishAt(Spare,Axle)
At(Flat,Axle)
At(Spare,Trunk)
¬
(c)
Figure 10.13 (a) the tire problem expressed as an empty plan. (b) an incomplete partially
ordered plan for the tire problem. Boxes represent actions and arrows indicate that one action
must occur before another. (c) a complete partially-ordered solution.
the actionPutOn(Spare,Axle). Of course that introduces some new ﬂaws: the preconditions
of the new action are not achieved. The search keeps adding to the plan (backtracking if
necessary) until all ﬂaws are resolved, as in the bottom of Figure 10.13. At every step, we
make the least commitment possible to ﬁx the ﬂaw. For example, in adding the action
LEAST COMMITMENT
Remove(Spare,Trunk) we need to commit to having it occur before PutOn(Spare,Axle),
but we make no other commitment that places it before or after other actions. If there were a
variable in the action schema that could be left unbound, we would do so.
In the 1980s and 90s, partial-order planning was seen as the best way to handle plan-
ning problems with independent subproblems—after all, it was the only approach that ex-
plicitly represents independent branches of a plan. On the other hand, it has the disadvantage
of not having an explicit representation of states in the state-transition model. That makes
some computations cumbersome. By 2000, forward-search planners had developed excellent
heuristics that allowed them to efﬁciently discover the independent subproblems that partial-
order planning was designed for. As a result, partial-order planners are not competitive on
fully automated classical planning problems.
However, partial-order planning remains an important part of the ﬁeld. For some spe-
ciﬁc tasks, such as operations scheduling, partial-order planning with domain speciﬁc heuris-
tics is the technology of choice. Many of these systems use libraries of high-level plans, as
described in Section 11.2. Partial-order planning is also often used in domains where it is im-
portant for humans to understand the plans. Operational plans for spacecraft and Mars rovers
are generated by partial-order planners and are then checked by human operators before being
uploaded to the vehicles for execution. The plan reﬁnement approach makes it easier for the
humans to understand what the planning algorithms are doing and verify that they are correct.
392 Chapter 10. Classical Planning
10.5 A NALYSIS OF PLANNING APPROACHES
Planning combines the two major areas of AI we have covered so far: search and logic.A
planner can be seen either as a program that searches for a solution or as one that (construc-
tively) proves the existence of a solution. The cross-fertilization of ideas from the two areas
has led both to improvements in performance amounting to several orders of magnitude in
the last decade and to an increased use of planners in industrial applications. Unfortunately,
we do not yet have a clear understanding of which techniques work best on which kinds of
problems. Quite possibly, new techniques will emerge that dominate existing methods.
Planning is foremost an exercise in controlling combinatorial explosion. If there are n
propositions in a domain, then there are 2
n states. As we have seen, planning is PSPACE-
hard. Against such pessimism, the identiﬁcation of independent subproblems can be a pow-
erful weapon. In the best case—full decomposability of the problem—we get an exponential
speedup. Decomposability is destroyed, however, by negative interactions between actions.
GRAPHPLAN records mutexes to point out where the difﬁcult interactions are. SATPLAN rep-
resents a similar range of mutex relations, but does so by using the general CNF form rather
than a speciﬁc data structure. Forward search addresses the problem heuristically by trying
to ﬁnd patterns (subsets of propositions) that cover the independent subproblems. Since this
approach is heuristic, it can work even when the subproblems are not completely independent.
Sometimes it is possible to solve a problem efﬁciently by recognizing that negative
interactions can be ruled out. We say that a problem has serializable subgoals if there exists
SERIALIZABLE
SUBGOAL
an order of subgoals such that the planner can achieve them in that order without having to
undo any of the previously achieved subgoals. For example, in the blocks world, if the goal
is to build a tower (e.g., A on B, which in turn is on C, which in turn is on the Table,a si n
Figure 10.4 on page 371), then the subgoals are serializable bottom to top: if we ﬁrst achieve
C on Table, we will never have to undo it while we are achieving the other subgoals. A
planner that uses the bottom-to-top trick can solve any problem in the blocks world without
backtracking (although it might not always ﬁnd the shortest plan).
As a more complex example, for the Remote Agent planner that commanded NASA’s
Deep Space One spacecraft, it was determined that the propositions involved in command-
ing a spacecraft are serializable. This is perhaps not too surprising, because a spacecraft is
designed by its engineers to be as easy as possible to control (subject to other constraints).
Taking advantage of the serialized ordering of goals, the Remote Agent planner was able to
eliminate most of the search. This meant that it was fast enough to control the spacecraft in
real time, something previously considered impossible.
Planners such as G
RAPHPLAN ,S A T PLAN , and FF have moved the ﬁeld of planning
forward, by raising the level of performance of planning systems, by clarifying the repre-
sentational and combinatorial issues involved, and by the development of useful heuristics.
However, there is a question of how far these techniques will scale. It seems likely that further
progress on larger problems cannot rely only on factored and propositional representations,
and will require some kind of synthesis of ﬁrst-order and hierarchical representations with
the efﬁcient heuristics currently in use.
Section 10.6. Summary 393
10.6 S UMMARY
In this chapter, we deﬁned the problem of planning in deterministic, fully observable, static
environments. We described the PDDL representation for planning problems and several
algorithmic approaches for solving them. The points to remember:
•Planning systems are problem-solving algorithms that operate on explicit propositional
or relational representations of states and actions. These representations make possi-
ble the derivation of effective heuristics and the development of powerful and ﬂexible
algorithms for solving problems.
•PDDL, the Planning Domain Deﬁnition Language, describes the initial and goal states
as conjunctions of literals, and actions in terms of their preconditions and effects.
•State-space search can operate in the forward direction ( progression) or the backward
direction ( regression). Effective heuristics can be derived by subgoal independence
assumptions and by various relaxations of the planning problem.
•A planning graph can be constructed incrementally, starting from the initial state. Each
layer contains a superset of all the literals or actions that could occur at that time step
and encodes mutual exclusion (mutex) relations among literals or actions that cannot co-
occur. Planning graphs yield useful heuristics for state-space and partial-order planners
and can be used directly in the G
RAPHPLAN algorithm.
•Other approaches include ﬁrst-order deduction over situation calculus axioms; encoding
a planning problem as a Boolean satisﬁability problem or as a constraint satisfaction
problem; and explicitly searching through the space of partially ordered plans.
•Each of the major approaches to planning has its adherents, and there is as yet no con-
sensus on which is best. Competition and cross-fertilization among the approaches have
resulted in signiﬁcant gains in efﬁciency for planning systems.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
AI planning arose from investigations into state-space search, theorem proving, and control
theory and from the practical needs of robotics, scheduling, and other domains. S
TRIPS (Fikes
and Nilsson, 1971), the ﬁrst major planning system, illustrates the interaction of these inﬂu-
ences. S
TRIPS was designed as the planning component of the software for the Shakey robot
project at SRI. Its overall control structure was modeled on that of GPS, the General Problem
Solver (Newell and Simon, 1961), a state-space search system that used means–ends anal-
ysis. Bylander (1992) shows simple S
TRIPS planning to be PSPACE-complete. Fikes and
Nilsson (1993) give a historical retrospective on the S TRIPS project and its relationship to
more recent planning efforts.
The representation language used by S TRIPS has been far more inﬂuential than its al-
gorithmic approach; what we call the “classical” language is close to what S TRIPS used.
394 Chapter 10. Classical Planning
The Action Description Language, or ADL (Pednault, 1986), relaxed some of the S TRIPS
restrictions and made it possible to encode more realistic problems. Nebel (2000) explores
schemes for compiling ADL into S TRIPS . The Problem Domain Description Language, or
PDDL (Ghallab et al., 1998), was introduced as a computer-parsable, standardized syntax for
representing planning problems and has been used as the standard language for the Interna-
tional Planning Competition since 1998. There have been several extensions; the most recent
version, PDDL 3.0, includes plan constraints and preferences (Gerevini and Long, 2005).
Planners in the early 1970s generally considered totally ordered action sequences. Prob-
lem decomposition was achieved by computing a subplan for each subgoal and then stringing
the subplans together in some order. This approach, called linear planning by Sacerdoti
LINEAR PLANNING
(1975), was soon discovered to be incomplete. It cannot solve some very simple problems,
such as the Sussman anomaly (see Exercise 10.7), found by Allen Brown during experimen-
tation with the H
ACKER system (Sussman, 1975). A complete planner must allow for inter-
leaving of actions from different subplans within a single sequence. The notion of serializableINTERLEAVING
subgoals (Korf, 1987) corresponds exactly to the set of problems for which noninterleaved
planners are complete.
One solution to the interleaving problem was goal-regression planning, a technique in
which steps in a totally ordered plan are reordered so as to avoid conﬂict between subgoals.
This was introduced by Waldinger (1975) and also used by Warren’s (1974) W ARPLAN .
WARPLAN is also notable in that it was the ﬁrst planner to be written in a logic program-
ming language (Prolog) and is one of the best examples of the remarkable economy that can
sometimes be gained with logic programming: W
ARPLAN is only 100 lines of code, a small
fraction of the size of comparable planners of the time.
The ideas underlying partial-order planning include the detection of conﬂicts (Tate,
1975a) and the protection of achieved conditions from interference (Sussman, 1975). The
construction of partially ordered plans (then called task networks ) was pioneered by the
NOAH planner (Sacerdoti, 1975, 1977) and by Tate’s (1975b, 1977) NONLIN system.
Partial-order planning dominated the next 20 years of research, yet the ﬁrst clear for-
mal exposition was T WEAK (Chapman, 1987), a planner that was simple enough to allow
proofs of completeness and intractability (NP-hardness and undecidability) of various plan-
ning problems. Chapman’s work led to a straightforward description of a complete partial-
order planner (McAllester and Rosenblitt, 1991), then to the widely distributed implementa-
tions SNLP (Soderland and Weld, 1991) and UCPOP (Penberthy and Weld, 1992). Partial-
order planning fell out of favor in the late 1990s as faster methods emerged. Nguyen and
Kambhampati (2001) suggest that a reconsideration is merited: with accurate heuristics de-
rived from a planning graph, their R
EPOP planner scales up much better than G RAPHPLAN
in parallelizable domains and is competitive with the fastest state-space planners.
The resurgence of interest in state-space planning was pioneered by Drew McDer-
mott’s UNPOP program (1996), which was the ﬁrst to suggest the ignore-delete-list heuristic,
The name U NPOP was a reaction to the overwhelming concentration on partial-order plan-
ning at the time; McDermott suspected that other approaches were not getting the attention
they deserved. Bonet and Geffner’s Heuristic Search Planner (HSP) and its later deriva-
tives (Bonet and Geffner, 1999; Haslum et al., 2005; Haslum, 2006) were the ﬁrst to make
Bibliographical and Historical Notes 395
state-space search practical for large planning problems. HSP searches in the forward di-
rection while HSP R (Bonet and Geffner, 1999) searches backward. The most successful
state-space searcher to date is FF (Hoffmann, 2001; Hoffmann and Nebel, 2001; Hoffmann,
2005), winner of the AIPS 2000 planning competition. F
AST DOWNW ARD (Helmert, 2006)
is a forward state-space search planner that preprocesses the action schemas into an alter-
native representation which makes some of the constraints more explicit. FAST DOWNW ARD
(Helmert and Richter, 2004; Helmert, 2006) won the 2004 planning competition, and LAMA
(Richter and Westphal, 2008), a planner based on F AST DOWNW ARD with improved heuris-
tics, won the 2008 competition.
Bylander (1994) and Ghallab et al. (2004) discuss the computational complexity of
several variants of the planning problem. Helmert (2003) proves complexity bounds for many
of the standard benchmark problems, and Hoffmann (2005) analyzes the search space of the
ignore-delete-list heuristic. Heuristics for the set-covering problem are discussed by Caprara
et al. (1995) for scheduling operations of the Italian railway. Edelkamp (2009) and Haslum
et al. (2007) describe how to construct pattern databases for planning heuristics. As we
mentioned in Chapter 3, Felneret al. (2004) show encouraging results using pattern databases
for sliding blocks puzzles, which can be thought of as a planning domain, but Hoffmannet al.
(2006) show some limitations of abstraction for classical planning problems.
Avrim Blum and Merrick Furst (1995, 1997) revitalized the ﬁeld of planning with their
G
RAPHPLAN system, which was orders of magnitude faster than the partial-order planners of
the time. Other graph-planning systems, such as IPP (Koehler et al., 1997), S TAN (Fox and
Long, 1998), and SGP (Weldet al., 1998), soon followed. A data structure closely resembling
the planning graph had been developed slightly earlier by Ghallab and Laruelle (1994), whose
IXTET partial-order planner used it to derive accurate heuristics to guide searches. Nguyen
et al. (2001) thoroughly analyze heuristics derived from planning graphs. Our discussion of
planning graphs is based partly on this work and on lecture notes and articles by Subbarao
Kambhampati (Bryce and Kambhampati, 2007). As mentioned in the chapter, a planning
graph can be used in many different ways to guide the search for a solution. The winner
of the 2002 AIPS planning competition, LPG (Gerevini and Serina, 2002, 2003), searched
planning graphs using a local search technique inspired by W
ALK SAT.
The situation calculus approach to planning was introduced by John McCarthy (1963).
The version we show here was proposed by Ray Reiter (1991, 2001).
Kautz et al. (1996) investigated various ways to propositionalize action schemas, ﬁnd-
ing that the most compact forms did not necessarily lead to the fastest solution times. A
systematic analysis was carried out by Ernst et al. (1997), who also developed an auto-
matic “compiler” for generating propositional representations from PDDL problems. The
B
LACKBOX planner, which combines ideas from G RAPHPLAN and SATP LAN ,w a sd e v e l -
oped by Kautz and Selman (1998). CP LAN , a planner based on constraint satisfaction, was
described by van Beek and Chen (1999).
Most recently, there has been interest in the representation of plans as binary decision
diagrams, compact data structures for Boolean expressions widely studied in the hardwareBINARY DECISION
DIAGRAM
veriﬁcation community (Clarke and Grumberg, 1987; McMillan, 1993). There are techniques
for proving properties of binary decision diagrams, including the property of being a solution
396 Chapter 10. Classical Planning
to a planning problem. Cimatti et al. (1998) present a planner based on this approach. Other
representations have also been used; for example, V ossen et al. (2001) survey the use of
integer programming for planning.
The jury is still out, but there are now some interesting comparisons of the various
approaches to planning. Helmert (2001) analyzes several classes of planning problems, and
shows that constraint-based approaches such as GRAPHPLAN and SATPLAN are best for NP-
hard domains, while search-based approaches do better in domains where feasible solutions
can be found without backtracking. G
RAPHPLAN and SATP LAN have trouble in domains
with many objects because that means they must create many actions. In some cases the
problem can be delayed or avoided by generating the propositionalized actions dynamically,
only as needed, rather than instantiating them all before the search begins.
Readings in Planning (Allen et al., 1990) is a comprehensive anthology of early work
in the ﬁeld. Weld (1994, 1999) provides two excellent surveys of planning algorithms of
the 1990s. It is interesting to see the change in the ﬁve years between the two surveys:
the ﬁrst concentrates on partial-order planning, and the second introduces G
RAPHPLAN and
SATPLAN . Automated Planning (Ghallab et al., 2004) is an excellent textbook on all aspects
of planning. LaValle’s text Planning Algorithms (2006) covers both classical and stochastic
planning, with extensive coverage of robot motion planning.
Planning research has been central to AI since its inception, and papers on planning are
a staple of mainstream AI journals and conferences. There are also specialized conferences
such as the International Conference on AI Planning Systems, the International Workshop on
Planning and Scheduling for Space, and the European Conference on Planning.
EXERCISES
10.1 Describe the differences and similarities between problem solving and planning.
10.2 Given the action schemas and initial state from Figure 10.1, what are all the applicable
concrete instances of Fly(p,from,to) in the state described by
At(P1,JFK)∧At(P2,SFO)∧Plane(P1)∧Plane(P2)
∧Airport(JFK)∧Airport(SFO)?
10.3 The monkey-and-bananas problem is faced by a monkey in a laboratory with some
bananas hanging out of reach from the ceiling. A box is available that will enable the monkey
to reach the bananas if he climbs on it. Initially, the monkey is atA, the bananas atB,a n dt h e
box at C. The monkey and box have height Low, but if the monkey climbs onto the box he
will have height High, the same as the bananas. The actions available to the monkey include
Go from one place to another, Push an object from one place to another, ClimbUp onto or
ClimbDown from an object, and Grasp or Ungrasp an object. The result of a Grasp is that
the monkey holds the object if the monkey and object are in the same place at the same height.
a. Write down the initial state description.
Exercises 397
Room 4
Room 3
Room 2
Room 1
Door 1
Door 2
Door 3
Door 4
Box 1
Box 2Box 3
Shakey
Switch 1
Switch 2
Switch 3
Switch 4
Box 4
Corridor
Figure 10.14 Shakey’s world. Shakey can move between landmarks within a room, can
pass through the door between rooms, can climb climbable objects and push pushable objects,
and can ﬂip light switches.
b. Write the six action schemas.
c. Suppose the monkey wants to fool the scientists, who are off to tea, by grabbing the
bananas, but leaving the box in its original place. Write this as a general goal (i.e., not
assuming that the box is necessarily at C) in the language of situation calculus. Can this
goal be solved by a classical planning system?
d. Your schema for pushing is probably incorrect, because if the object is too heavy, its
position will remain the same when thePush schema is applied. Fix your action schema
to account for heavy objects.
10.4 The original S
TRIPS planner was designed to control Shakey the robot. Figure 10.14
shows a version of Shakey’s world consisting of four rooms lined up along a corridor, where
each room has a door and a light switch. The actions in Shakey’s world include moving from
place to place, pushing movable objects (such as boxes), climbing onto and down from rigid
398 Chapter 10. Classical Planning
objects (such as boxes), and turning light switches on and off. The robot itself could not climb
on a box or toggle a switch, but the planner was capable of ﬁnding and printing out plans that
were beyond the robot’s abilities. Shakey’s six actions are the following:
•Go(x, y, r), which requires that Shakey be At x and that x and y are locations In the
same room r. By convention a door between two rooms is in both of them.
•Push a box b from location x to location y within the same room: Push(b, x, y, r).Y o u
will need the predicate Box and constants for the boxes.
•Climb onto a box from position x: ClimbUp(x, b); climb down from a box to position
x: ClimbDown(b, x). We will need the predicate On and the constant Floor.
•Turn a light switch on or off: TurnOn(s,b); TurnOﬀ(s,b). To turn a light on or off,
Shakey must be on top of a box at the light switch’s location.
Write PDDL sentences for Shakey’s six actions and the initial state from Figure 10.14. Con-
struct a plan for Shakey to get Box
2 into Room2.
10.5 A ﬁnite Turing machine has a ﬁnite one-dimensional tape of cells, each cell containing
one of a ﬁnite number of symbols. One cell has a read and write head above it. There is a
ﬁnite set of states the machine can be in, one of which is the accept state. At each time step,
depending on the symbol on the cell under the head and the machine’s current state, there are
a set of actions we can choose from. Each action involves writing a symbol to the cell under
the head, transitioning the machine to a state, and optionally moving the head left or right.
The mapping that determines which actions are allowed is the Turing machine’s program.
Your goal is to control the machine into the accept state.
Represent the Turing machine acceptance problem as a planning problem. If you can
do this, it demonstrates that determining whether a planning problem has a solution is at least
as hard as the Turing acceptance problem, which is PSPACE-hard.
10.6 Explain why dropping negative effects from every action schema in a planning prob-
lem results in a relaxed problem.
10.7 Figure 10.4 (page 371) shows a blocks-world problem that is known as the Sussman
anomaly. The problem was considered anomalous because the noninterleaved planners of
SUSSMAN ANOMAL Y
the early 1970s could not solve it. Write a deﬁnition of the problem and solve it, either by
hand or with a planning program. A noninterleaved planner is a planner that, when given two
subgoals G1 and G2, produces either a plan for G1 concatenated with a plan for G2,o rv i c e
versa. Explain why a noninterleaved planner cannot solve this problem.
10.8 Prove that backward search with PDDL problems is complete.
10.9 Construct levels 0, 1, and 2 of the planning graph for the problem in Figure 10.1.
10.10 Prove the following assertions about planning graphs:
a. A literal that does not appear in the ﬁnal level of the graph cannot be achieved.
Exercises 399
b. The level cost of a literal in a serial graph is no greater than the actual cost of an optimal
plan for achieving it.
10.11 The set-level heuristic (see page 382) uses a planning graph to estimate the cost of
achieving a conjunctive goal from the current state. What relaxed problem is the set-level
heuristic the solution to?
10.12 Examine the deﬁnition of bidirectional search in Chapter 3.
a. Would bidirectional state-space search be a good idea for planning?
b. What about bidirectional search in the space of partial-order plans?
c. Devise a version of partial-order planning in which an action can be added to a plan if its
preconditions can be achieved by the effects of actions already in the plan. Explain how
to deal with conﬂicts and ordering constraints. Is the algorithm essentially identical to
forward state-space search?
10.13 We contrasted forward and backward state-space searchers with partial-order plan-
ners, saying that the latter is a plan-space searcher. Explain how forward and backward state-
space search can also be considered plan-space searchers, and say what the plan reﬁnement
operators are.
10.14 Up to now we have assumed that the plans we create always make sure that an action’s
preconditions are satisﬁed. Let us now investigate what propositional successor-state axioms
such as HaveArrow
t+1 ⇔ (HaveArrowt ∧¬Shoott) have to say about actions whose
preconditions are not satisﬁed.
a. Show that the axioms predict that nothing will happen when an action is executed in a
state where its preconditions are not satisﬁed.
b. Consider a plan p that contains the actions required to achieve a goal but also includes
illegal actions. Is it the case that
initial state∧successor-state axioms∧p|= goal ?
c. With ﬁrst-order successor-state axioms in situation calculus, is it possible to prove that
a plan containing illegal actions will achieve the goal?
10.15 Consider how to translate a set of action schemas into the successor-state axioms of
situation calculus.
a. Consider the schema for Fly(p,from,to). Write a logical deﬁnition for the predicate
Poss(Fly(p,from,to),s), which is true if the preconditions for Fly(p,from,to) are
satisﬁed in situation s.
b. Next, assuming that Fly(p,from,to) is the only action schema available to the agent,
write down a successor-state axiom for At(p, x, s) that captures the same information
as the action schema.
400 Chapter 10. Classical Planning
c. Now suppose there is an additional method of travel: Teleport(p,from,to).I t h a s
the additional precondition¬Warped(p) and the additional effectWarped(p). Explain
how the situation calculus knowledge base must be modiﬁed.
d. Finally, develop a general and precisely speciﬁed procedure for carrying out the trans-
lation from a set of action schemas to a set of successor-state axioms.
10.16 In the SATP LAN algorithm in Figure 7.22 (page 272), each call to the satisﬁabil-
ity algorithm asserts a goal gT ,w h e r eT ranges from 0 to Tmax. Suppose instead that the
satisﬁability algorithm is called only once, with the goal g0∨g1∨···∨gTmax .
a. Will this always return a plan if one exists with length less than or equal to Tmax?
b. Does this approach introduce any new spurious “solutions”?
c. Discuss how one might modify a satisﬁability algorithm such as W ALK SAT so that it
ﬁnds short solutions (if they exist) when given a disjunctive goal of this form.


--- BOOK CHAPTER: 4_Beyond_Classical_Search ---

4
BEYOND CLASSICAL
SEARCH
In which we relax the simplifying assumptions of the previous chapter , thereby
getting closer to the real world.
Chapter 3 addressed a single category of problems: observable, deterministic, known envi-
ronments where the solution is a sequence of actions. In this chapter, we look at what happens
when these assumptions are relaxed. We begin with a fairly simple case: Sections 4.1 and 4.2
cover algorithms that perform purely local search in the state space, evaluating and modify-
ing one or more current states rather than systematically exploring paths from an initial state.
These algorithms are suitable for problems in which all that matters is the solution state, not
the path cost to reach it. The family of local search algorithms includes methods inspired by
statistical physics (simulated annealing) and evolutionary biology (genetic algorithms).
Then, in Sections 4.3–4.4, we examine what happens when we relax the assumptions
of determinism and observability. The key idea is that if an agent cannot predict exactly what
percept it will receive, then it will need to consider what to do under each contingency that
its percepts may reveal. With partial observability, the agent will also need to keep track of
the states it might be in.
Finally, Section 4.5 investigates online search, in which the agent is faced with a state
space that is initially unknown and must be explored.
4.1 L OCAL SEARCH ALGORITHMS AND OPTIMIZA TION PROBLEMS
The search algorithms that we have seen so far are designed to explore search spaces sys-
tematically. This systematicity is achieved by keeping one or more paths in memory and by
recording which alternatives have been explored at each point along the path. When a goal is
found, the path to that goal also constitutes asolution to the problem. In many problems, how-
ever, the path to the goal is irrelevant. For example, in the 8-queens problem (see page 71),
what matters is the ﬁnal conﬁguration of queens, not the order in which they are added. The
same general property holds for many important applications such as integrated-circuit de-
sign, factory-ﬂoor layout, job-shop scheduling, automatic programming, telecommunications
network optimization, vehicle routing, and portfolio management.
120
Section 4.1. Local Search Algorithms and Optimization Problems 121
If the path to the goal does not matter, we might consider a different class of algo-
rithms, ones that do not worry about paths at all. Local search algorithms operate usingLOCAL SEARCH
a single current node (rather than multiple paths) and generally move only to neighborsCURRENT NODE
of that node. Typically, the paths followed by the search are not retained. Although local
search algorithms are not systematic, they have two key advantages: (1) they use very little
memory—usually a constant amount; and (2) they can often ﬁnd reasonable solutions in large
or inﬁnite (continuous) state spaces for which systematic algorithms are unsuitable.
In addition to ﬁnding goals, local search algorithms are useful for solving pure op-
timization problems, in which the aim is to ﬁnd the best state according to an objectiveOPTIMIZA TION
PROBLEM
function. Many optimization problems do not ﬁt the “standard” search model introduced inOBJECTIVE
FUNCTION
Chapter 3. For example, nature provides an objective function—reproductive ﬁtness—that
Darwinian evolution could be seen as attempting to optimize, but there is no “goal test” and
no “path cost” for this problem.
To understand local search, we ﬁnd it useful to consider the state-space landscape (as
STA TE-SP ACE
LANDSCAPE
in Figure 4.1). A landscape has both “location” (deﬁned by the state) and “elevation” (deﬁned
by the value of the heuristic cost function or objective function). If elevation corresponds to
cost, then the aim is to ﬁnd the lowest valley—a global minimum; if elevation corresponds
GLOBAL MINIMUM
to an objective function, then the aim is to ﬁnd the highest peak—a global maximum.( Y o uGLOBAL MAXIMUM
can convert from one to the other just by inserting a minus sign.) Local search algorithms
explore this landscape. A complete local search algorithm always ﬁnds a goal if one exists;
an optimal algorithm always ﬁnds a global minimum/maximum.
current
state
objective function
state space
global maximum
local maximum
“flat” local maximum
shoulder
Figure 4.1 A one-dimensional state-space landscape in which elevation corresponds to the
objective function. The aim is to ﬁnd the global maximum. Hill-climbing search modiﬁes
the current state to try to improve it, as shown by the arrow. The various topographic features
are deﬁned in the text.

122 Chapter 4. Beyond Classical Search
function HILL -CLIMBING (problem) returns a state that is a local maximum
current←MAKE -NODE (problem.INITIAL -STATE)
loop do
neighbor←a highest-valued successor of current
if neighbor.VALUE ≤current.VALUE then return current.STATE
current←neighbor
Figure 4.2 The hill-climbing search algorithm, which is the most basic local search tech-
nique. At each step the current node is replaced by the best neighbor; in this version, that
means the neighbor with the highest V
ALUE , but if a heuristic cost estimate h is used, we
would ﬁnd the neighbor with the lowest h.
4.1.1 Hill-climbing search
The hill-climbing search algorithm ( steepest-ascent version) is shown in Figure 4.2. It isHILL CLIMBING
STEEPEST ASCENT simply a loop that continually moves in the direction of increasing value—that is, uphill. It
terminates when it reaches a “peak” where no neighbor has a higher value. The algorithm
does not maintain a search tree, so the data structure for the current node need only record
the state and the value of the objective function. Hill climbing does not look ahead beyond
the immediate neighbors of the current state. This resembles trying to ﬁnd the top of Mount
Everest in a thick fog while suffering from amnesia.
To illustrate hill climbing, we will use the 8-queens problem introduced on page 71.
Local search algorithms typically use a complete-state formulation, where each state has
8 queens on the board, one per column. The successors of a state are all possible states
generated by moving a single queen to another square in the same column (so each state has
8× 7=5 6 successors). The heuristic cost function h is the number of pairs of queens that
are attacking each other, either directly or indirectly. The global minimum of this function
is zero, which occurs only at perfect solutions. Figure 4.3(a) shows a state with h=1 7.T h e
ﬁgure also shows the values of all its successors, with the best successors having h=1 2.
Hill-climbing algorithms typically choose randomly among the set of best successors if there
is more than one.
Hill climbing is sometimes calledgreedy local search because it grabs a good neighbor
GREEDY LOCAL
SEARCH
state without thinking ahead about where to go next. Although greed is considered one of the
seven deadly sins, it turns out that greedy algorithms often perform quite well. Hill climbing
often makes rapid progress toward a solution because it is usually quite easy to improve a bad
state. For example, from the state in Figure 4.3(a), it takes just ﬁve steps to reach the state
in Figure 4.3(b), which has h=1 and is very nearly a solution. Unfortunately, hill climbing
often gets stuck for the following reasons:
•Local maxima: a local maximum is a peak that is higher than each of its neighboring
LOCAL MAXIMUM
states but lower than the global maximum. Hill-climbing algorithms that reach the
vicinity of a local maximum will be drawn upward toward the peak but will then be
stuck with nowhere else to go. Figure 4.1 illustrates the problem schematically. More
Section 4.1. Local Search Algorithms and Optimization Problems 123
14
18
17
15
14
18
14
14
14
14
14
12
16
12
13
16
17
14
18
13
14
17
15
18
15
13
15
13
12
15
15
13
15
12
13
14
14
14
16
12
14
12
12
15
16
13
14
12
14
18
16
16
16
14
16
14
(a) (b)
Figure 4.3 (a) An 8-queens state with heuristic cost estimateh =17 , showing the value of
h for each possible successor obtained by moving a queen within its column. The best moves
are marked. (b) A local minimum in the 8-queens state space; the state has h =1 but every
successor has a higher cost.
concretely, the state in Figure 4.3(b) is a local maximum (i.e., a local minimum for the
cost h); every move of a single queen makes the situation worse.
•Ridges: a ridge is shown in Figure 4.4. Ridges result in a sequence of local maximaRIDGE
that is very difﬁcult for greedy algorithms to navigate.
•Plateaux: a plateau is a ﬂat area of the state-space landscape. It can be a ﬂat localPLA TEAU
maximum, from which no uphill exit exists, or a shoulder, from which progress isSHOULDER
possible. (See Figure 4.1.) A hill-climbing search might get lost on the plateau.
In each case, the algorithm reaches a point at which no progress is being made. Starting from
a randomly generated 8-queens state, steepest-ascent hill climbing gets stuck 86% of the time,
solving only 14% of problem instances. It works quickly, taking just 4 steps on average when
it succeeds and 3 when it gets stuck—not bad for a state space with 8
8 ≈17 million states.
The algorithm in Figure 4.2 halts if it reaches a plateau where the best successor has
the same value as the current state. Might it not be a good idea to keep going—to allow a
sideways move in the hope that the plateau is really a shoulder, as shown in Figure 4.1? The
SIDEWAYS MOVE
answer is usually yes, but we must take care. If we always allow sideways moves when there
are no uphill moves, an inﬁnite loop will occur whenever the algorithm reaches a ﬂat local
maximum that is not a shoulder. One common solution is to put a limit on the number of con-
secutive sideways moves allowed. For example, we could allow up to, say, 100 consecutive
sideways moves in the 8-queens problem. This raises the percentage of problem instances
solved by hill climbing from 14% to 94%. Success comes at a cost: the algorithm averages
roughly 21 steps for each successful instance and 64 for each failure.
124 Chapter 4. Beyond Classical Search
Figure 4.4 Illustration of why ridges cause difﬁculties for hill climbing. The grid of states
(dark circles) is superimposed on a ridge rising from left to right, creating a sequence of local
maxima that are not directly connected to each other. From each local maximum, all the
available actions point downhill.
Many variants of hill climbing have been invented.Stochastic hill climbing chooses atSTOCHASTIC HILL
CLIMBING
random from among the uphill moves; the probability of selection can vary with the steepness
of the uphill move. This usually converges more slowly than steepest ascent, but in some
state landscapes, it ﬁnds better solutions. First-choice hill climbing implements stochastic
FIRST -CHOICE HILL
CLIMBING
hill climbing by generating successors randomly until one is generated that is better than the
current state. This is a good strategy when a state has many (e.g., thousands) of successors.
The hill-climbing algorithms described so far are incomplete—they often fail to ﬁnd
a goal when one exists because they can get stuck on local maxima. Random-restart hill
climbing adopts the well-known adage, “If at ﬁrst you don’t succeed, try, try again.” It con-RANDOM-REST ART
HILL CLIMBING
ducts a series of hill-climbing searches from randomly generated initial states, 1 until a goal
is found. It is trivially complete with probability approaching 1, because it will eventually
generate a goal state as the initial state. If each hill-climbing search has a probability p of
success, then the expected number of restarts required is 1/p. For 8-queens instances with
no sideways moves allowed, p≈0.14, so we need roughly 7 iterations to ﬁnd a goal (6 fail-
ures and 1 success). The expected number of steps is the cost of one successful iteration plus
(1−p)/p times the cost of failure, or roughly 22 steps in all. When we allow sideways moves,
1/0.94≈1.06 iterations are needed on average and(1× 21)+ (0.06/0.94)× 64≈25 steps.
For 8-queens, then, random-restart hill climbing is very effective indeed. Even for three mil-
lion queens, the approach can ﬁnd solutions in under a minute.
2
1 Generating a random state from an implicitly speciﬁed state space can be a hard problem in itself.
2 Luby et al. (1993) prove that it is best, in some cases, to restarta randomized search algorithm after a particular,
ﬁxed amount of time and that this can be much more efﬁcient than letting each search continue indeﬁnitely.
Disallowing or limiting the number of sideways moves is an example of this idea.
Section 4.1. Local Search Algorithms and Optimization Problems 125
The success of hill climbing depends very much on the shape of the state-space land-
scape: if there are few local maxima and plateaux, random-restart hill climbing will ﬁnd a
good solution very quickly. On the other hand, many real problems have a landscape that
looks more like a widely scattered family of balding porcupines on a ﬂat ﬂoor, with miniature
porcupines living on the tip of each porcupine needle, ad inﬁnitum. NP-hard problems typi-
cally have an exponential number of local maxima to get stuck on. Despite this, a reasonably
good local maximum can often be found after a small number of restarts.
4.1.2 Simulated annealing
A hill-climbing algorithm that never makes “downhill” moves toward states with lower value
(or higher cost) is guaranteed to be incomplete, because it can get stuck on a local maxi-
mum. In contrast, a purely random walk—that is, moving to a successor chosen uniformly
at random from the set of successors—is complete but extremely inefﬁcient. Therefore, it
seems reasonable to try to combine hill climbing with a random walk in some way that yields
both efﬁciency and completeness. Simulated annealing is such an algorithm. In metallurgy,
SIMULA TED
ANNEALING
annealing is the process used to temper or harden metals and glass by heating them to a
high temperature and then gradually cooling them, thus allowing the material to reach a low-
energy crystalline state. To explain simulated annealing, we switch our point of view from
hill climbing to gradient descent (i.e., minimizing cost) and imagine the task of getting a
GRADIENT DESCENT
ping-pong ball into the deepest crevice in a bumpy surface. If we just let the ball roll, it will
come to rest at a local minimum. If we shake the surface, we can bounce the ball out of the
local minimum. The trick is to shake just hard enough to bounce the ball out of local min-
ima but not hard enough to dislodge it from the global minimum. The simulated-annealing
solution is to start by shaking hard (i.e., at a high temperature) and then gradually reduce the
intensity of the shaking (i.e., lower the temperature).
The innermost loop of the simulated-annealing algorithm (Figure 4.5) is quite similar to
hill climbing. Instead of picking the best move, however, it picks arandom move. If the move
improves the situation, it is always accepted. Otherwise, the algorithm accepts the move with
some probability less than 1. The probability decreases exponentially with the “badness” of
the move—the amount Δ E by which the evaluation is worsened. The probability also de-
creases as the “temperature” T goes down: “bad” moves are more likely to be allowed at the
start when T is high, and they become more unlikely as T decreases. If the schedule lowers
T slowly enough, the algorithm will ﬁnd a global optimum with probability approaching 1.
Simulated annealing was ﬁrst used extensively to solve VLSI layout problems in the
early 1980s. It has been applied widely to factory scheduling and other large-scale optimiza-
tion tasks. In Exercise 4.4, you are asked to compare its performance to that of random-restart
hill climbing on the 8-queens puzzle.
4.1.3 Local beam search
Keeping just one node in memory might seem to be an extreme reaction to the problem of
memory limitations. The local beam search algorithm
3 keeps track of k states rather thanLOCAL BEAM
SEARCH
3 Local beam search is an adaptation of beam search, which is a path-based algorithm.
126 Chapter 4. Beyond Classical Search
function SIMULATED -ANNEALING (problem,schedule) returns a solution state
inputs: problem, a problem
schedule, a mapping from time to “temperature”
current←MAKE -NODE (problem.INITIAL -STATE)
for t =1 to∞do
T←schedule(t)
if T =0 then return current
next←a randomly selected successor of current
ΔE←next.VALUE – current.VALUE
if ΔE > 0 then current←next
else current←next only with probability eΔ E/T
Figure 4.5 The simulated annealing algorithm, a version of stochastic hill climbing where
some downhill moves are allowed. Downhill moves are accepted readily early in the anneal-
ing schedule and then less often as time goes on. The schedule input determines the value of
the temperature T as a function of time.
just one. It begins with k randomly generated states. At each step, all the successors of all k
states are generated. If any one is a goal, the algorithm halts. Otherwise, it selects the k best
successors from the complete list and repeats.
At ﬁrst sight, a local beam search with k states might seem to be nothing more than
running k random restarts in parallel instead of in sequence. In fact, the two algorithms
are quite different. In a random-restart search, each search process runs independently of
the others. In a local beam search, useful information is passed among the parallel search
threads. In effect, the states that generate the best successors say to the others, “Come over
here, the grass is greener!” The algorithm quickly abandons unfruitful searches and moves
its resources to where the most progress is being made.
In its simplest form, local beam search can suffer from a lack of diversity among the
k states—they can quickly become concentrated in a small region of the state space, making
the search little more than an expensive version of hill climbing. A variant called stochastic
beam search, analogous to stochastic hill climbing, helps alleviate this problem. Instead
STOCHASTIC BEAM
SEARCH
of choosing the best k from the the pool of candidate successors, stochastic beam search
chooses k successors at random, with the probability of choosing a given successor being
an increasing function of its value. Stochastic beam search bears some resemblance to the
process of natural selection, whereby the “successors” (offspring) of a “state” (organism)
populate the next generation according to its “value” (ﬁtness).
4.1.4 Genetic algorithms
A genetic algorithm (or GA) is a variant of stochastic beam search in which successor statesGENETIC
ALGORITHM
are generated by combining two parent states rather than by modifying a single state. The
analogy to natural selection is the same as in stochastic beam search, except that now we are
dealing with sexual rather than asexual reproduction.
Section 4.1. Local Search Algorithms and Optimization Problems 127
(a)
Initial Population
(b)
Fitness Function
(c)
Selection
(d)
Crossover
(e)
Mutation
24
23
20
11
29%
31%
26%
14%
32752411
24748552
32752411
24415124
32748552
24752411
32752124
24415411
32252124
24752411
32748152
24415417
24748552
32752411
24415124
32543213
Figure 4.6 The genetic algorithm, illustrated for digit strings representing 8-queens states.
The initial population in (a) is ranked by the ﬁ tness function in (b), resulting in pairs for
mating in (c). They produce offspring in (d), which are subject to mutation in (e).
+=
Figure 4.7 The 8-queens states corresponding to the ﬁrst two parents in Figure 4.6(c) and
the ﬁrst offspring in Figure 4.6(d). The shaded columns are lost in the crossover step and the
unshaded columns are retained.
Like beam searches, GAs begin with a set of k randomly generated states, called the
population. Each state, or individual, is represented as a string over a ﬁnite alphabet—mostPOPULA TION
INDIVIDUAL commonly, a string of 0s and 1s. For example, an 8-queens state must specify the positions of
8 queens, each in a column of 8 squares, and so requires 8× log2 8=2 4 bits. Alternatively,
the state could be represented as 8 digits, each in the range from 1 to 8. (We demonstrate later
that the two encodings behave differently.) Figure 4.6(a) shows a population of four 8-digit
strings representing 8-queens states.
The production of the next generation of states is shown in Figure 4.6(b)–(e). In (b),
each state is rated by the objective function, or (in GA terminology) the ﬁtness function.A
FITNESS FUNCTION
ﬁtness function should return higher values for better states, so, for the 8-queens problem
we use the number of nonattacking pairs of queens, which has a value of 28 for a solution.
The values of the four states are 24, 23, 20, and 11. In this particular variant of the genetic
algorithm, the probability of being chosen for reproducing is directly proportional to the
ﬁtness score, and the percentages are shown next to the raw scores.
In (c), two pairs are selected at random for reproduction, in accordance with the prob-
128 Chapter 4. Beyond Classical Search
abilities in (b). Notice that one individual is selected twice and one not at all. 4 For each
pair to be mated, a crossover point is chosen randomly from the positions in the string. InCROSSOVER
Figure 4.6, the crossover points are after the third digit in the ﬁrst pair and after the ﬁfth digit
in the second pair.5
In (d), the offspring themselves are created by crossing over the parent strings at the
crossover point. For example, the ﬁrst child of the ﬁrst pair gets the ﬁrst three digits from the
ﬁrst parent and the remaining digits from the second parent, whereas the second child gets
the ﬁrst three digits from the second parent and the rest from the ﬁrst parent. The 8-queens
states involved in this reproduction step are shown in Figure 4.7. The example shows that
when two parent states are quite different, the crossover operation can produce a state that is
a long way from either parent state. It is often the case that the population is quite diverse
early on in the process, so crossover (like simulated annealing) frequently takes large steps in
the state space early in the search process and smaller steps later on when most individuals
are quite similar.
Finally, in (e), each location is subject to random mutation with a small independent
MUTA TION
probability. One digit was mutated in the ﬁrst, third, and fourth offspring. In the 8-queens
problem, this corresponds to choosing a queen at random and moving it to a random square
in its column. Figure 4.8 describes an algorithm that implements all these steps.
Like stochastic beam search, genetic algorithms combine an uphill tendency with ran-
dom exploration and exchange of information among parallel search threads. The primary
advantage, if any, of genetic algorithms comes from the crossover operation. Yet it can be
shown mathematically that, if the positions of the genetic code are permuted initially in a
random order, crossover conveys no advantage. Intuitively, the advantage comes from the
ability of crossover to combine large blocks of letters that have evolved independently to per-
form useful functions, thus raising the level of granularity at which the search operates. For
example, it could be that putting the ﬁrst three queens in positions 2, 4, and 6 (where they do
not attack each other) constitutes a useful block that can be combined with other blocks to
construct a solution.
The theory of genetic algorithms explains how this works using the idea of a schema,
SCHEMA
which is a substring in which some of the positions can be left unspeciﬁed. For example,
the schema 246***** describes all 8-queens states in which the ﬁrst three queens are in
positions 2, 4, and 6, respectively. Strings that match the schema (such as 24613578) are
called instances of the schema. It can be shown that if the average ﬁtness of the instances of
INSTANCE
a schema is above the mean, then the number of instances of the schema within the population
will grow over time. Clearly, this effect is unlikely to be signiﬁcant if adjacent bits are totally
unrelated to each other, because then there will be few contiguous blocks that provide a
consistent beneﬁt. Genetic algorithms work best when schemata correspond to meaningful
components of a solution. For example, if the string is a representation of an antenna, then the
schemata may represent components of the antenna, such as reﬂectors and deﬂectors. A good
4 There are many variants of this selection rule. The method of culling, in which all individuals below a given
threshold are discarded, can be shown to converge faster than the random version (Baum et al., 1995).
5 It is here that the encoding matters. If a 24-bit encoding is used instead of 8 digits, then the crossover point
has a 2/3 chance of being in the middle of a digit, which results in an essentially arbitrary mutation of that digit.
Section 4.2. Local Search in Continuous Spaces 129
function GENETIC -ALGORITHM (population,F ITNESS -FN) returns an individual
inputs: population, a set of individuals
FITNESS -FN, a function that measures the ﬁtness of an individual
repeat
new
 population←empty set
for i =1 to SIZE (population) do
x←RANDOM -SELECTION (population,F ITNESS -FN)
y←RANDOM -SELECTION (population,F ITNESS -FN)
child←REPRODUCE (x ,y)
if (small random probability) then child←MUTATE(child)
add child to new
 population
population←new
 population
until some individual is ﬁt enough, or enough time has elapsed
return the best individual in population, according to FITNESS -FN
function REPRODUCE (x ,y) returns an individual
inputs: x ,y, parent individuals
n←LENGTH (x ); c←random number from 1 to n
return APPEND (SUBSTRING (x ,1 ,c), SUBSTRING (y,c +1 ,n))
Figure 4.8 A genetic algorithm. The algorithm is the same as the one diagrammed in
Figure 4.6, with one variation: in this more popular version, each mating of two parents
produces only one offspring, not two.
component is likely to be good in a variety of different designs. This suggests that successful
use of genetic algorithms requires careful engineering of the representation.
In practice, genetic algorithms have had a widespread impact on optimization problems,
such as circuit layout and job-shop scheduling. At present, it is not clear whether the appeal
of genetic algorithms arises from their performance or from their æsthetically pleasing origins
in the theory of evolution. Much work remains to be done to identify the conditions under
which genetic algorithms perform well.
4.2 L OCAL SEARCH IN CONTINUOUS SPACES
In Chapter 2, we explained the distinction between discrete and continuous environments,
pointing out that most real-world environments are continuous. Yet none of the algorithms
we have described (except for ﬁrst-choice hill climbing and simulated annealing) can handle
continuous state and action spaces, because they have inﬁnite branching factors. This section
provides a very brief introduction to some local search techniques for ﬁnding optimal solu-
tions in continuous spaces. The literature on this topic is vast; many of the basic techniques
130 Chapter 4. Beyond Classical Search
EVOLUTION AND SEARCH
The theory of evolution was developed in Charles Darwin’s On the Origin of
Species by Means of Natural Selection (1859) and independently by Alfred Russel
Wallace (1858). The central idea is simple: variations occur in reproduction and
will be preserved in successive generations approximately in proportion to their
effect on reproductive ﬁtness.
Darwin’s theory was developed with no knowledge of how the traits of organ-
isms can be inherited and modiﬁed. The probabilistic laws governing these pro-
cesses were ﬁrst identiﬁed by Gregor Mendel (1866), a monk who experimented
with sweet peas. Much later, Watson and Crick (1953) identiﬁed the structure of the
DNA molecule and its alphabet, AGTC (adenine, guanine, thymine, cytosine). In
the standard model, variation occurs both by point mutations in the letter sequence
and by “crossover” (in which the DNA of an offspring is generated by combining
long sections of DNA from each parent).
The analogy to local search algorithms has already been described; the princi-
pal difference between stochastic beam search and evolution is the use ofsexual re-
production, wherein successors are generated from multiple organisms rather than
just one. The actual mechanisms of evolution are, however, far richer than most
genetic algorithms allow. For example, mutations can involve reversals, duplica-
tions, and movement of large chunks of DNA; some viruses borrow DNA from one
organism and insert it in another; and there are transposable genes that do nothing
but copy themselves many thousands of times within the genome. There are even
genes that poison cells from potential mates that do not carry the gene, thereby in-
creasing their own chances of replication. Most important is the fact that the genes
themselves encode the mechanisms whereby the genome is reproduced and trans-
lated into an organism. In genetic algorithms, those mechanisms are a separate
program that is not represented within the strings being manipulated.
Darwinian evolution may appear inefﬁcient, having generated blindly some
10
45 or so organisms without improving its search heuristics one iota. Fifty
years before Darwin, however, the otherwise great French naturalist Jean Lamarck
(1809) proposed a theory of evolution whereby traits acquired by adaptation dur-
ing an organism’s lifetime would be passed on to its offspring. Such a process
would be effective but does not seem to occur in nature. Much later, James Bald-
win (1896) proposed a superﬁcially similar theory: that behavior learned during an
organism’s lifetime could accelerate the rate of evolution. Unlike Lamarck’s, Bald-
win’s theory is entirely consistent with Darwinian evolution because it relies on se-
lection pressures operating on individuals that have found local optima among the
set of possible behaviors allowed by their genetic makeup. Computer simulations
conﬁrm that the “Baldwin effect” is real, once “ordinary” evolution has created
organisms whose internal performance measure correlates with actual ﬁtness.

Section 4.2. Local Search in Continuous Spaces 131
originated in the 17th century, after the development of calculus by Newton and Leibniz.6 We
ﬁnd uses for these techniques at several places in the book, including the chapters on learning,
vision, and robotics.
We begin with an example. Suppose we want to place three new airports anywhere
in Romania, such that the sum of squared distances from each city on the map (Figure 3.2)
to its nearest airport is minimized. The state space is then deﬁned by the coordinates of
the airports: (x
1,y1), (x2,y2),a n d (x3,y3).T h i s i s a six-dimensional space; we also say
that states are deﬁned by six variables. (In general, states are deﬁned by an n-dimensionalVARIABLE
vector of variables, x.) Moving around in this space corresponds to moving one or more of
the airports on the map. The objective function f(x1,y1,x2,y2,x3,y3) is relatively easy to
compute for any particular state once we compute the closest cities. Let Ci be the set of
cities whose closest airport (in the current state) is airporti. Then, in the neighborhood of the
current state,w h e r et h eCis remain constant, we have
f(x1,y1,x2,y2,x3,y3)=
3∑
i=1
∑
c∈Ci
(xi−xc)2 +( yi−yc)2 . (4.1)
This expression is correct locally, but not globally because the sets Ci are (discontinuous)
functions of the state.
One way to avoid continuous problems is simply todiscretize the neighborhood of eachDISCRETIZA TION
state. For example, we can move only one airport at a time in either the x or y direction by
a ﬁxed amount ±δ. With 6 variables, this gives 12 possible successors for each state. We
can then apply any of the local search algorithms described previously. We could also ap-
ply stochastic hill climbing and simulated annealing directly, without discretizing the space.
These algorithms choose successors randomly, which can be done by generating random vec-
tors of length δ.
Many methods attempt to use the gradient of the landscape to ﬁnd a maximum. The
GRADIENT
gradient of the objective function is a vector∇f that gives the magnitude and direction of the
steepest slope. For our problem, we have
∇f =
⎞ ∂f
∂x1
, ∂f
∂y1
, ∂f
∂x2
, ∂f
∂y2
, ∂f
∂x3
, ∂f
∂y3
⎠
.
In some cases, we can ﬁnd a maximum by solving the equation∇f =0 . (This could be done,
for example, if we were placing just one airport; the solution is the arithmetic mean of all the
cities’ coordinates.) In many cases, however, this equation cannot be solved in closed form.
For example, with three airports, the expression for the gradient depends on what cities are
closest to each airport in the current state. This means we can compute the gradient locally
(but not globally); for example,
∂f
∂x1
=2
∑
c∈C1
(xi−xc) . (4.2)
Given a locally correct expression for the gradient, we can perform steepest-ascent hill climb-
6 A basic knowledge of multivariate calculus and vector arithmetic is useful for reading this section.
132 Chapter 4. Beyond Classical Search
ing by updating the current state according to the formula
x←x + α∇f(x) ,
where α is a small constant often called the step size. In other cases, the objective functionSTEP SIZE
might not be available in a differentiable form at all—for example, the value of a particular set
of airport locations might be determined by running some large-scale economic simulation
package. In those cases, we can calculate a so-called empirical gradient by evaluating the
EMPIRICAL
GRADIENT
response to small increments and decrements in each coordinate. Empirical gradient search
is the same as steepest-ascent hill climbing in a discretized version of the state space.
Hidden beneath the phrase “ α is a small constant” lies a huge variety of methods for
adjusting α. The basic problem is that, if α is too small, too many steps are needed; if α
is too large, the search could overshoot the maximum. The technique of line search tries toLINE SEARCH
overcome this dilemma by extending the current gradient direction—usually by repeatedly
doubling α—until f starts to decrease again. The point at which this occurs becomes the new
current state. There are several schools of thought about how the new direction should be
chosen at this point.
For many problems, the most effective algorithm is the venerable Newton–Raphson
NEWTON–RAPHSON
method. This is a general technique for ﬁnding roots of functions—that is, solving equations
of the form g(x)=0 . It works by computing a new estimate for the root x according to
Newton’s formula
x←x−g(x)/g′(x) .
To ﬁnd a maximum or minimum of f, we need to ﬁnd x such that the gradient is zero (i.e.,
∇f(x)= 0). Thus, g(x) in Newton’s formula becomes∇f(x), and the update equation can
be written in matrix–vector form as
x←x−H−1
f (x)∇f(x) ,
where Hf (x) is the Hessian matrix of second derivatives, whose elements Hij are givenHESSIAN
by ∂2f/∂xi∂xj . For our airport example, we can see from Equation (4.2) that Hf (x) is
particularly simple: the off-diagonal elements are zero and the diagonal elements for airport
i are just twice the number of cities in C
i. A moment’s calculation shows that one step of
the update moves airport i directly to the centroid of Ci, which is the minimum of the local
expression for f from Equation (4.1).7 For high-dimensional problems, however, computing
the n2 entries of the Hessian and inverting it may be expensive, so many approximate versions
of the Newton–Raphson method have been developed.
Local search methods suffer from local maxima, ridges, and plateaux in continuous
state spaces just as much as in discrete spaces. Random restarts and simulated annealing can
be used and are often helpful. High-dimensional continuous spaces are, however, big places
in which it is easy to get lost.
A ﬁnal topic with which a passing acquaintance is useful is constrained optimization.
CONSTRAINED
OPTIMIZA TION
An optimization problem is constrained if solutions must satisfy some hard constraints on the
values of the variables. For example, in our airport-siting problem, we might constrain sites
7 In general, the Newton–Raphson update can be seen as ﬁtting a quadratic surface to f at x and then moving
directly to the minimum of that surface—which is also the minimum of f if f is quadratic.
Section 4.3. Searching with Nondeterministic Actions 133
to be inside Romania and on dry land (rather than in the middle of lakes). The difﬁculty of
constrained optimization problems depends on the nature of the constraints and the objective
function. The best-known category is that of linear programming problems, in which con-
LINEAR
PROGRAMMING
straints must be linear inequalities forming a convex set 8 and the objective function is alsoCONVEX SET
linear. The time complexity of linear programming is polynomial in the number of variables.
Linear programming is probably the most widely studied and broadly useful class of
optimization problems. It is a special case of the more general problem of convex opti-
mization, which allows the constraint region to be any convex region and the objective toCONVEX
OPTIMIZA TION
be any function that is convex within the constraint region. Under certain conditions, convex
optimization problems are also polynomially solvable and may be feasible in practice with
thousands of variables. Several important problems in machine learning and control theory
can be formulated as convex optimization problems (see Chapter 20).
4.3 S EARCHING WITH NONDETERMINISTIC ACTIONS
In Chapter 3, we assumed that the environment is fully observable and deterministic and that
the agent knows what the effects of each action are. Therefore, the agent can calculate exactly
which state results from any sequence of actions and always knows which state it is in. Its
percepts provide no new information after each action, although of course they tell the agent
the initial state.
When the environment is either partially observable or nondeterministic (or both), per-
cepts become useful. In a partially observable environment, every percept helps narrow down
the set of possible states the agent might be in, thus making it easier for the agent to achieve
its goals. When the environment is nondeterministic, percepts tell the agent which of the pos-
sible outcomes of its actions has actually occurred. In both cases, the future percepts cannot
be determined in advance and the agent’s future actions will depend on those future percepts.
So the solution to a problem is not a sequence but acontingency plan (also known as a strat-
CONTINGENCY PLAN
egy) that speciﬁes what to do depending on what percepts are received. In this section, weSTRA TEGY
examine the case of nondeterminism, deferring partial observability to Section 4.4.
4.3.1 The erratic vacuum world
As an example, we use the vacuum world, ﬁrst introduced in Chapter 2 and deﬁned as a
search problem in Section 3.2.1. Recall that the state space has eight states, as shown in
Figure 4.9. There are three actions— Left, Right,a n d Suck—and the goal is to clean up all
the dirt (states 7 and 8). If the environment is observable, deterministic, and completely
known, then the problem is trivially solvable by any of the algorithms in Chapter 3 and the
solution is an action sequence. For example, if the initial state is 1, then the action sequence
[Suck,Right,Suck] will reach a goal state, 8.
8 A set of points S is convex if the line joining any two points inS is also contained in S.A convex function is
one for which the space “above” it forms a convex set; by deﬁnition, convex functions have no local (as opposed
to global) minima.
134 Chapter 4. Beyond Classical Search
12
87
56
34
Figure 4.9 The eight possible states of the vacuum world; states 7 and 8 are goal states.
Now suppose that we introduce nondeterminism in the form of a powerful but erratic
vacuum cleaner. In the erratic vacuum world,t h eSuck action works as follows:ERRA TIC VACUUM
WORLD
•When applied to a dirty square the action cleans the square and sometimes cleans up
dirt in an adjacent square, too.
•When applied to a clean square the action sometimes deposits dirt on the carpet.9
To provide a precise formulation of this problem, we need to generalize the notion of atran-
sition model from Chapter 3. Instead of deﬁning the transition model by a R ESULT function
that returns a single state, we use a R ESULTS function that returns a set of possible outcome
states. For example, in the erratic vacuum world, the Suck action in state 1 leads to a state in
the set{5,7}—the dirt in the right-hand square may or may not be vacuumed up.
We also need to generalize the notion of a solution to the problem. For example, if we
start in state 1, there is no single sequence of actions that solves the problem. Instead, we
need a contingency plan such as the following:
[Suck, if State =5 then [Right, Suck] else [] ]. (4.3)
Thus, solutions for nondeterministic problems can contain nested if–then–else statements;
this means that they are trees rather than sequences. This allows the selection of actions
based on contingencies arising during execution. Many problems in the real, physical world
are contingency problems because exact prediction is impossible. For this reason, many
people keep their eyes open while walking around or driving.
9 We assume that most readers face similar problems and can sympathize with our agent. We apologize to
owners of modern, efﬁcient home appliances who cannot take advantage of this pedagogical device.
Section 4.3. Searching with Nondeterministic Actions 135
4.3.2 AND –OR search trees
The next question is how to ﬁnd contingent solutions to nondeterministic problems. As in
Chapter 3, we begin by constructing search trees, but here the trees have a different character.
In a deterministic environment, the only branching is introduced by the agent’s own choices
in each state. We call these nodes
OR nodes. In the vacuum world, for example, at an OROR NODE
node the agent chooses Left or Right or Suck . In a nondeterministic environment, branching
is also introduced by the environment’s choice of outcome for each action. We call these
nodes AND nodes. For example, the Suck action in state 1 leads to a state in the set {5,7},AND NODE
so the agent would need to ﬁnd a plan for state 5 and for state 7. These two kinds of nodes
alternate, leading to an AND –OR tree as illustrated in Figure 4.10.AND–OR TREE
A solution for an AND –OR search problem is a subtree that (1) has a goal node at every
leaf, (2) speciﬁes one action at each of its OR nodes, and (3) includes every outcome branch
at each of its AND nodes. The solution is shown in bold lines in the ﬁgure; it corresponds
to the plan given in Equation (4.3). (The plan uses if–then–else notation to handle the AND
branches, but when there are more than two branches at a node, it might be better to use acase
LeftSuck
RightSuck
RightSuck
6 
GOAL
8 
GOAL
7 
1 
2 5 
1 
LOOP
5 
LOOP
5 
LOOP
Left Suck
1 
LOOP GOAL
8 4 
Figure 4.10 The ﬁrst two levels of the search tree for the erratic vacuum world. State
nodes are OR nodes where some action must be chosen. At the AND nodes, shown as circles,
every outcome must be handled, as indicated by the arc linking the outgoing branches. The
solution found is shown in bold lines.

136 Chapter 4. Beyond Classical Search
function AND-OR-GRAPH -SEARCH (problem) returns a conditional plan, or failure
OR-SEARCH (problem.INITIAL -STATE,problem,[] )
function OR-SEARCH (state,problem,path) returns a conditional plan, or failure
if problem.GOAL -TEST (state) then return the empty plan
if state is on path then return failure
for each action in problem.ACTIONS (state) do
plan←AND-SEARCH (RESULTS (state,action),problem,[state| path])
if plan̸= failure then return [action| plan]
return failure
function AND-SEARCH (states,problem,path) returns a conditional plan, or failure
for each si in states do
plani←OR-SEARCH (si,problem,path)
if plani = failure then return failure
return [if s1 then plan1 else if s2 then plan2 else ... if sn− 1 then plann− 1 else plann]
Figure 4.11 An algorithm for searching AND –OR graphs generated by nondeterministic
environments. It returns a conditional plan that reaches a goal state in all circumstances. (The
notation [x| l] refers to the list formed by adding object x to the front of list l.)
construct.) Modifying the basic problem-solving agent shown in Figure 3.1 to execute con-
tingent solutions of this kind is straightforward. One may also consider a somewhat different
agent design, in which the agent can act before it has found a guaranteed plan and deals with
some contingencies only as they arise during execution. This type of interleaving of search
INTERLEAVING
and execution is also useful for exploration problems (see Section 4.5) and for game playing
(see Chapter 5).
Figure 4.11 gives a recursive, depth-ﬁrst algorithm for AND –OR graph search. One
key aspect of the algorithm is the way in which it deals with cycles, which often arise in
nondeterministic problems (e.g., if an action sometimes has no effect or if an unintended
effect can be corrected). If the current state is identical to a state on the path from the root,
then it returns with failure. This doesn’t mean that there is no solution from the current state;
it simply means that if there is a noncyclic solution, it must be reachable from the earlier
incarnation of the current state, so the new incarnation can be discarded. With this check, we
ensure that the algorithm terminates in every ﬁnite state space, because every path must reach
a goal, a dead end, or a repeated state. Notice that the algorithm does not check whether the
current state is a repetition of a state on someother path from the root, which is important for
efﬁciency. Exercise 4.5 investigates this issue.
AND –OR graphs can also be explored by breadth-ﬁrst or best-ﬁrst methods. The concept
of a heuristic function must be modiﬁed to estimate the cost of a contingent solution rather
than a sequence, but the notion of admissibility carries over and there is an analog of the A ∗
algorithm for ﬁnding optimal solutions. Pointers are given in the bibliographical notes at the
end of the chapter.
Section 4.3. Searching with Nondeterministic Actions 137
Suck Right
6 
1 
2 5 
Right
Figure 4.12 Part of the search graph for the slippery vacuum world, where we have shown
(some) cycles explicitly. All solutions for this problem are cyclic plans because there is no
w a yt om o v er e l i a b l y .
4.3.3 Try, try again
Consider the slippery vacuum world, which is identical to the ordinary (non-erratic) vac-
uum world except that movement actions sometimes fail, leaving the agent in the same loca-
tion. For example, moving Right in state 1 leads to the state set {1,2}. Figure 4.12 shows
part of the search graph; clearly, there are no longer any acyclic solutions from state 1, and
A
ND-OR-GRAPH -SEARCH would return with failure. There is, however, a cyclic solution,CYCLIC SOLUTION
w h i c hi st ok e e pt r y i n gRight until it works. We can express this solution by adding alabel toLABEL
denote some portion of the plan and using that label later instead of repeating the plan itself.
Thus, our cyclic solution is
[Suck,L1 : Right, if State =5 then L1 else Suck] .
(A better syntax for the looping part of this plan would be “ while State =5 do Right.”)
In general a cyclic plan may be considered a solution provided that every leaf is a goal
state and that a leaf is reachable from every point in the plan. The modiﬁcations needed
to A
ND-OR-GRAPH -SEARCH are covered in Exercise 4.6. The key realization is that a loop
in the state space back to a state L translates to a loop in the plan back to the point where the
subplan for state L is executed.
Given the deﬁnition of a cyclic solution, an agent executing such a solution will eventu-
ally reach the goalprovided that each outcome of a nondeterministic action eventually occurs.
Is this condition reasonable? It depends on the reason for the nondeterminism. If the action
rolls a die, then it’s reasonable to suppose that eventually a six will be rolled. If the action is
to insert a hotel card key into the door lock, but it doesn’t work the ﬁrst time, then perhaps it
will eventually work, or perhaps one has the wrong key (or the wrong room!). After seven or
138 Chapter 4. Beyond Classical Search
eight tries, most people will assume the problem is with the key and will go back to the front
desk to get a new one. One way to understand this decision is to say that the initial problem
formulation (observable, nondeterministic) is abandoned in favor of a different formulation
(partially observable, deterministic) where the failure is attributed to an unobservable prop-
erty of the key. We have more to say on this issue in Chapter 13.
4.4 S EARCHING WITH PARTIAL OBSERV ATIONS
We now turn to the problem of partial observability, where the agent’s percepts do not suf-
ﬁce to pin down the exact state. As noted at the beginning of the previous section, if the
agent is in one of several possible states, then an action may lead to one of several possible
outcomes—even if the environment is deterministic . The key concept required for solving
partially observable problems is the belief state, representing the agent’s current belief about
BELIEF STA TE
the possible physical states it might be in, given the sequence of actions and percepts up to
that point. We begin with the simplest scenario for studying belief states, which is when the
agent has no sensors at all; then we add in partial sensing as well as nondeterministic actions.
4.4.1 Searching with no observation
When the agent’s percepts provide no information at all , we have what is called a sensor-
less problem or sometimes a conformant problem. At ﬁrst, one might think the sensorlessSENSORLESS
CONFORMANT agent has no hope of solving a problem if it has no idea what state it’s in; in fact, sensorless
problems are quite often solvable. Moreover, sensorless agents can be surprisingly useful,
primarily because they don’t rely on sensors working properly. In manufacturing systems,
for example, many ingenious methods have been developed for orienting parts correctly from
an unknown initial position by using a sequence of actions with no sensing at all. The high
cost of sensing is another reason to avoid it: for example, doctors often prescribe a broad-
spectrum antibiotic rather than using the contingent plan of doing an expensive blood test,
then waiting for the results to come back, and then prescribing a more speciﬁc antibiotic and
perhaps hospitalization because the infection has progressed too far.
We can make a sensorless version of the vacuum world. Assume that the agent knows
the geography of its world, but doesn’t know its location or the distribution of dirt. In that
case, its initial state could be any element of the set{1,2,3,4,5,6,7, 8}. Now, consider what
happens if it tries the actionRight. This will cause it to be in one of the states{2,4,6,8}—the
agent now has more information! Furthermore, the action sequence [Right,Suck] will always
end up in one of the states{4,8}. Finally, the sequence [ Right,Suck,Left,Suck] is guaranteed
to reach the goal state 7 no matter what the start state. We say that the agent can coerce the
COERCION
w o r l di n t os t a t e7 .
To solve sensorless problems, we search in the space of belief states rather than physical
states.10 Notice that in belief-state space, the problem is fully observable because the agent
10 In a fully observable environment, each belief state contains one physical state. Thus, we can view the algo-
rithms in Chapter 3 as searching in a belief-state space of singleton belief states.
Section 4.4. Searching with Partial Observations 139
always knows its own belief state. Furthermore, the solution (if any) is always a sequence of
actions. This is because, as in the ordinary problems of Chapter 3, the percepts received after
each action are completely predictable—they’re always empty! So there are no contingencies
to plan for. This is true even if the environment is nondeterminstic.
It is instructive to see how the belief-state search problem is constructed. Suppose
the underlying physical problem P is deﬁned by A
CTIONS P ,R ESULT P ,G OAL -TEST P ,a n d
STEP -COST P . Then we can deﬁne the corresponding sensorless problem as follows:
•Belief states: The entire belief-state space contains every possible set of physical states.
If P has N states, then the sensorless problem has up to 2N states, although many may
be unreachable from the initial state.
•Initial state: Typically the set of all states in P , although in some cases the agent will
have more knowledge than this.
•Actions: This is slightly tricky. Suppose the agent is in belief state b={s1,s2},b u t
ACTIONS P (s1)̸= ACTIONS P (s2); then the agent is unsure of which actions are legal.
If we assume that illegal actions have no effect on the environment, then it is safe to
take the union of all the actions in any of the physical states in the current belief stateb:
A
CTIONS (b)=
⋃
s∈b
ACTIONS P (s) .
On the other hand, if an illegal action might be the end of the world, it is safer to allow
only the intersection, that is, the set of actions legal in all the states. For the vacuum
world, every state has the same legal actions, so both methods give the same result.
•Transition model: The agent doesn’t know which state in the belief state is the right
one; so as far as it knows, it might get to any of the states resulting from applying the
action to one of the physical states in the belief state. For deterministic actions, the set
of states that might be reached is
b
′ = RESULT (b,a)= {s′ : s′ = RESULT P (s,a) and s∈b} . (4.4)
With deterministic actions, b′ is never larger than b. With nondeterminism, we have
b′ = RESULT (b,a)= {s′ : s′ ∈RESULTS P (s,a) and s∈b}
=
⋃
s∈b
RESULTS P (s,a) ,
which may be larger than b, as shown in Figure 4.13. The process of generating
the new belief state after the action is called the prediction step; the notation b′ =PREDICTION
PREDICT P (b, a) will come in handy.
•Goal test: The agent wants a plan that is sure to work, which means that a belief state
satisﬁes the goal only if all the physical states in it satisfy G OAL -TEST P . The agent
may accidentally achieve the goal earlier, but it won’t know that it has done so.
•Path cost: This is also tricky. If the same action can have different costs in different
states, then the cost of taking an action in a given belief state could be one of several
values. (This gives rise to a new class of problems, which we explore in Exercise 4.9.)
For now we assume that the cost of an action is the same in all states and so can be
transferred directly from the underlying physical problem.
140 Chapter 4. Beyond Classical Search
2 
4 
1 
3 
2 
4 
1 
3 
1 
3 
(b)(a)
Figure 4.13 (a) Predicting the next belief state for the sensorless vacuum world with a
deterministic action, Right. (b) Prediction for the same belief state and action in the slippery
version of the sensorless vacuum world.
Figure 4.14 shows the reachable belief-state space for the deterministic, sensorless vacuum
world. There are only 12 reachable belief states out of 28 = 256possible belief states.
The preceding deﬁnitions enable the automatic construction of the belief-state problem
formulation from the deﬁnition of the underlying physical problem. Once this is done, we
can apply any of the search algorithms of Chapter 3. In fact, we can do a little bit more
than that. In “ordinary” graph search, newly generated states are tested to see if they are
identical to existing states. This works for belief states, too; for example, in Figure 4.14, the
action sequence [ Suck,Left,Suck] starting at the initial state reaches the same belief state as
[Right,Left,Suck], namely,{5,7}. Now, consider the belief state reached by [ Left], namely,
{1,3,5,7}. Obviously, this is not identical to {5,7}, but it is a superset. It is easy to prove
(Exercise 4.8) that if an action sequence is a solution for a belief stateb, it is also a solution for
any subset of b. Hence, we can discard a path reaching{1,3,5,7} if{5,7} has already been
generated. Conversely, if {1,3,5,7} has already been generated and found to be solvable,
then any subset,s u c ha s{5,7}, is guaranteed to be solvable. This extra level of pruning may
dramatically improve the efﬁciency of sensorless problem solving.
Even with this improvement, however, sensorless problem-solving as we have described
it is seldom feasible in practice. The difﬁculty is not so much the vastness of the belief-state
space—even though it is exponentially larger than the underlying physical state space; in
most cases the branching factor and solution length in the belief-state space and physical
state space are not so different. The real difﬁculty lies with the size of each belief state. For
example, the initial belief state for the 10× 10 vacuum world contains 100× 2
100 or around
1032 physical states—far too many if we use the atomic representation, which is an explicit
list of states.
One solution is to represent the belief state by some more compact description. In
English, we could say the agent knows “Nothing” in the initial state; after moving Left,w e
could say, “Not in the rightmost column,” and so on. Chapter 7 explains how to do this in a
formal representation scheme. Another approach is to avoid the standard search algorithms,
which treat belief states as black boxes just like any other problem state. Instead, we can look
Section 4.4. Searching with Partial Observations 141
L
R
S
L
R
S
LR
S
LR
S
L
R
S
LR
SL
R
S
113
57
24
68
23
45 6
78
45
78
53
7
64
8
4
8
5
7
6
8
87
3
7
Figure 4.14 The reachable portion of the belief-state space for the deterministic, sensor-
less vacuum world. Each shaded box corresponds to a single belief state. At any given point,
the agent is in a particular belief state but does not know which physical state it is in. The
initial belief state (complete ignorance) is t he top center box. Actions are represented by
labeled links. Self-loops are omitted for clarity.
inside the belief states and develop incremental belief-state search algorithms that build up
INCREMENT AL
BELIEF-STA TE
SEARCH
the solution one physical state at a time. For example, in the sensorless vacuum world, the
initial belief state is{1,2,3,4,5,6,7, 8}, and we have to ﬁnd an action sequence that works
in all 8 states. We can do this by ﬁrst ﬁnding a solution that works for state 1; then we check
if it works for state 2; if not, go back and ﬁnd a different solution for state 1, and so on. Just
as an
AND –OR search has to ﬁnd a solution for every branch at an AND node, this algorithm
has to ﬁnd a solution for every state in the belief state; the difference is that AND –OR search
can ﬁnd a different solution for each branch, whereas an incremental belief-state search has
to ﬁnd one solution that works for all the states.
The main advantage of the incremental approach is that it is typically able to detect
failure quickly—when a belief state is unsolvable, it is usually the case that a small subset of
the belief state, consisting of the ﬁrst few states examined, is also unsolvable. In some cases,
142 Chapter 4. Beyond Classical Search
this leads to a speedup proportional to the size of the belief states, which may themselves be
as large as the physical state space itself.
Even the most efﬁcient solution algorithm is not of much use when no solutions exist.
Many things just cannot be done without sensing. For example, the sensorless 8-puzzle is
impossible. On the other hand, a little bit of sensing can go a long way. For example, every
8-puzzle instance is solvable if just one square is visible—the solution involves moving each
tile in turn into the visible square and then keeping track of its location.
4.4.2 Searching with observations
For a general partially observable problem, we have to specify how the environment generates
percepts for the agent. For example, we might deﬁne the local-sensing vacuum world to be
one in which the agent has a position sensor and a local dirt sensor but has no sensor capable
of detecting dirt in other squares. The formal problem speciﬁcation includes a P
ERCEPT (s)
function that returns the percept received in a given state. (If sensing is nondeterministic,
then we use a P
ERCEPTS function that returns a set of possible percepts.) For example, in the
local-sensing vacuum world, the PERCEPT in state 1 is[A,Dirty]. Fully observable problems
are a special case in which P ERCEPT (s)= s for every state s, while sensorless problems are
a special case in which PERCEPT (s)= null .
When observations are partial, it will usually be the case that several states could have
produced any given percept. For example, the percept [A,Dirty] is produced by state 3 as
well as by state 1. Hence, given this as the initial percept, the initial belief state for the
local-sensing vacuum world will be {1,3}.T h e A
CTIONS ,S TEP -COST ,a n dG OAL -TEST
are constructed from the underlying physical problem just as for sensorless problems, but the
transition model is a bit more complicated. We can think of transitions from one belief state
to the next for a particular action as occurring in three stages, as shown in Figure 4.15:
•The prediction stage is the same as for sensorless problems: given the actiona in belief
state b, the predicted belief state is ˆb= PREDICT (b, a).11
•The observation prediction stage determines the set of percepts o that could be ob-
served in the predicted belief state:
POSSIBLE -PERCEPTS (ˆb)= {o : o= PERCEPT (s) and s∈ˆb} .
•The update stage determines, for each possible percept, the belief state that would
result from the percept. The new belief state bo is just the set of states in ˆb that could
have produced the percept:
bo = UPDATE(ˆb,o )= {s : o= PERCEPT (s) and s∈ˆb} .
Notice that each updated belief statebo can be no larger than the predicted belief stateˆb;
observations can only help reduce uncertainty compared to the sensorless case. More-
over, for deterministic sensing, the belief states for the different possible percepts will
be disjoint, forming a partition of the original predicted belief state.
11 Here, and throughout the book, the “hat” in ˆb means an estimated or predicted value for b.
Section 4.4. Searching with Partial Observations 143
2 
4 
4 
1 
2 
4 
1 
3 
2 
1 
3 3 
(b)
(a)
4 
2 
1 
3 
Right
[A,Dirty]
[B,Dirty]
[B,Clean]
Right [B,Dirty]
[B,Clean]
Figure 4.15 Two example of transitions in local-sensing vacuum worlds. (a) In the de-
terministic world, Right is applied in the initial belief state, resulting in a new belief state
with two possible physical states; for those states, the possible percepts are [B, Dirty] and
[B, Clean], leading to two belief states, each of which is a singleton. (b) In the slippery
world, Right is applied in the initial belief state, giving a new belief state with four physi-
cal states; for those states, the possible percepts are [A,Dirty], [B, Dirty],a n d[B, Clean],
leading to three belief states as shown.
Putting these three stages together, we obtain the possible belief states resulting from a given
action and the subsequent possible percepts:
RESULTS (b, a)= {bo : bo = UPDATE(PREDICT (b, a),o) and
o ∈POSSIBLE -PERCEPTS (PREDICT (b, a))} . (4.5)
Again, the nondeterminism in the partially observable problem comes from the inability
to predict exactly which percept will be received after acting; underlying nondeterminism in
the physical environment may contribute to this inability by enlarging the belief state at the
prediction stage, leading to more percepts at the observation stage.
4.4.3 Solving partially observable problems
The preceding section showed how to derive the R ESULTS function for a nondeterministic
belief-state problem from an underlying physical problem and the PERCEPT function. Given
144 Chapter 4. Beyond Classical Search
7 
5 
1 
3 
4 2 
Suck
[B,Dirty] [B,Clean]
Right
[A,Clean]
Figure 4.16 The ﬁrst level of the AND –OR search tree for a problem in the local-sensing
vacuum world; Suck is the ﬁrst step of the solution.
such a formulation, the AND –OR search algorithm of Figure 4.11 can be applied directly to
derive a solution. Figure 4.16 shows part of the search tree for the local-sensing vacuum
world, assuming an initial percept [A,Dirty]. The solution is the conditional plan
[Suck, Right, if Bstate ={6} then Suck else [] ].
Notice that, because we supplied a belief-state problem to the
AND –OR search algorithm, it
returned a conditional plan that tests the belief state rather than the actual state. This is as it
should be: in a partially observable environment the agent won’t be able to execute a solution
that requires testing the actual state.
As in the case of standard search algorithms applied to sensorless problems, the
AND –
OR search algorithm treats belief states as black boxes, just like any other states. One can
improve on this by checking for previously generated belief states that are subsets or supersets
of the current state, just as for sensorless problems. One can also derive incremental search
algorithms, analogous to those described for sensorless problems, that provide substantial
speedups over the black-box approach.
4.4.4 An agent for partially observable environments
The design of a problem-solving agent for partially observable environments is quite similar
to the simple problem-solving agent in Figure 3.1: the agent formulates a problem, calls a
search algorithm (such as A
ND-OR-GRAPH -SEARCH ) to solve it, and executes the solution.
There are two main differences. First, the solution to a problem will be a conditional plan
rather than a sequence; if the ﬁrst step is an if–then–else expression, the agent will need to
test the condition in the if-part and execute the then-part or the else-part accordingly. Second,
the agent will need to maintain its belief state as it performs actions and receives percepts.
This process resembles the prediction–observation–update process in Equation (4.5) but is
actually simpler because the percept is given by the environment rather than calculated by the
Section 4.4. Searching with Partial Observations 145
7 
5 
6 
2 1 
3 
6 
4 
8 
2 [B,Dirty]Right[A,Clean]
7 
5 
Suck
Figure 4.17 Two prediction–update cycles of belief-state maintenance in the kindergarten
vacuum world with local sensing.
agent. Given an initial belief state b, an action a, and a percept o, the new belief state is:
b′ = UPDATE(PREDICT (b, a),o) . (4.6)
Figure 4.17 shows the belief state being maintained in the kindergarten vacuum world with
local sensing, wherein any square may become dirty at any time unless the agent is actively
cleaning it at that moment.
12
In partially observable environments—which include the vast majority of real-world
environments—maintaining one’s belief state is a core function of any intelligent system.
This function goes under various names, including monitoring, ﬁltering and state estima-
MONITORING
FILTERING tion. Equation (4.6) is called a recursive state estimator because it computes the new belief
STA TE ESTIMA TION
RECURSIVE
state from the previous one rather than by examining the entire percept sequence. If the agent
is not to “fall behind,” the computation has to happen as fast as percepts are coming in. As
the environment becomes more complex, the exact update computation becomes infeasible
and the agent will have to compute an approximate belief state, perhaps focusing on the im-
plications of the percept for the aspects of the environment that are of current interest. Most
work on this problem has been done for stochastic, continuous-state environments with the
tools of probability theory, as explained in Chapter 15. Here we will show an example in a
discrete environment with detrministic sensors and nondeterministic actions.
The example concerns a robot with the task of localization: working out where it is,
LOCALIZA TION
given a map of the world and a sequence of percepts and actions. Our robot is placed in the
maze-like environment of Figure 4.18. The robot is equipped with four sonar sensors that
tell whether there is an obstacle—the outer wall or a black square in the ﬁgure—in each of
the four compass directions. We assume that the sensors give perfectly correct data, and that
the robot has a correct map of the enviornment. But unfortunately the robot’s navigational
system is broken, so when it executes aMove action, it moves randomly to one of the adjacent
squares. The robot’s task is to determine its current location.
Suppose the robot has just been switched on, so it does not know where it is. Thus its
initial belief state b consists of the set of all locations. The the robot receives the percept
12 The usual apologies to those who are unfamiliar with the effect of small children on the environment.
146 Chapter 4. Beyond Classical Search
(a) Possible locations of robot after E1 =N S W
(b) Possible locations of robot After E1 =N S W , E2 =N S
Figure 4.18 Possible positions of the robot,⊙, (a) after one observation E1 =NSW and
(b) after a second observationE2 = NS . When sensors are noiseless and the transition model
is accurate, there are no other possible locations for the robot consistent with this sequence
of two observations.
NSW, meaning there are obstacles to the north, west, and south, and does an update using the
equation bo = UPDATE(b), yielding the 4 locations shown in Figure 4.18(a). You can inspect
the maze to see that those are the only four locations that yield the percept NWS .
Next the robot executes a Move action, but the result is nondeterministic. The new be-
lief state, ba = PREDICT (bo,Move), contains all the locations that are one step away from the
locations in bo. When the second percept, NS, arrives, the robot does U PDATE(ba,NS) and
ﬁnds that the belief state has collapsed down to the single location shown in Figure 4.18(b).
That’s the only location that could be the result of
U
PDATE(PREDICT (UPDATE(b,NSW ),Move),NS) .
With nondetermnistic actions the P REDICT step grows the belief state, but the U PDATE step
shrinks it back down—as long as the percepts provide some useful identifying information.
Sometimes the percepts don’t help much for localization: If there were one or more long
east-west corridors, then a robot could receive a long sequence of NS percepts, but never
know where in the corridor(s) it was.
Section 4.5. Online Search Agents and Unknown Environments 147
4.5 O NLINE SEARCH AGENTS AND UNKNOWN ENVIRONMENTS
So far we have concentrated on agents that use ofﬂine search algorithms. They computeOFFLINE SEARCH
a complete solution before setting foot in the real world and then execute the solution. In
contrast, an online search13 agent interleaves computation and action: ﬁrst it takes an action,ONLINE SEARCH
then it observes the environment and computes the next action. Online search is a good idea
in dynamic or semidynamic domains—domains where there is a penalty for sitting around
and computing too long. Online search is also helpful in nondeterministic domains because
it allows the agent to focus its computational efforts on the contingencies that actually arise
rather than those that might happen but probably won’t. Of course, there is a tradeoff: the
more an agent plans ahead, the less often it will ﬁnd itself up the creek without a paddle.
Online search is a necessary idea for unknown environments, where the agent does not
know what states exist or what its actions do. In this state of ignorance, the agent faces an
exploration problem and must use its actions as experiments in order to learn enough to
EXPLORA TION
PROBLEM
make deliberation worthwhile.
The canonical example of online search is a robot that is placed in a new building and
must explore it to build a map that it can use for getting from A to B. Methods for escaping
from labyrinths—required knowledge for aspiring heroes of antiquity—are also examples of
online search algorithms. Spatial exploration is not the only form of exploration, however.
Consider a newborn baby: it has many possible actions but knows the outcomes of none of
them, and it has experienced only a few of the possible states that it can reach. The baby’s
gradual discovery of how the world works is, in part, an online search process.
4.5.1 Online search problems
An online search problem must be solved by an agent executing actions, rather than by pure
computation. We assume a deterministic and fully observable environment (Chapter 17 re-
laxes these assumptions), but we stipulate that the agent knows only the following:
•A
CTIONS (s), which returns a list of actions allowed in state s;
•The step-cost function c(s, a, s′)—note that this cannot be used until the agent knows
that s′ is the outcome; and
•GOAL -TEST (s).
Note in particular that the agent cannot determine R ESULT (s,a) except by actually being
in s and doing a. For example, in the maze problem shown in Figure 4.19, the agent does
not know that going Up from (1,1) leads to (1,2); nor, having done that, does it know that
going Down will take it back to (1,1). This degree of ignorance can be reduced in some
applications—for example, a robot explorer might know how its movement actions work and
be ignorant only of the locations of obstacles.
13 The term “online” is commonly used in computer science to refer to algorithms that must process input data
as they are received rather than waiting for the entire input data set to become available.
148 Chapter 4. Beyond Classical Search
G
S1
2
3
123
Figure 4.19 A simple maze problem. The agent starts at S and must reach G but knows
nothing of the environment.
S
G
S
G
A
A
S G
(a) (b)
Figure 4.20 (a) Two state spaces that might lead an online search agent into a dead end.
Any given agent will fail in at least one of these spaces. (b) A two-dimensional environment
that can cause an online search agent to follow an arbitrarily inefﬁcient route to the goal.
Whichever choice the agent makes, the adversary blocks that route with another long, thin
wall, so that the path followed is much longer than the best possible path.
Finally, the agent might have access to an admissible heuristic function h(s) that es-
timates the distance from the current state to a goal state. For example, in Figure 4.19, the
agent might know the location of the goal and be able to use the Manhattan-distance heuristic.
Typically, the agent’s objective is to reach a goal state while minimizing cost. (Another
possible objective is simply to explore the entire environment.) The cost is the total path cost
of the path that the agent actually travels. It is common to compare this cost with the path
cost of the path the agent would follow if it knew the search space in advance —that is, the
actual shortest path (or shortest complete exploration). In the language of online algorithms,
this is called the competitive ratio; we would like it to be as small as possible.
COMPETITIVE RA TIO
Section 4.5. Online Search Agents and Unknown Environments 149
Although this sounds like a reasonable request, it is easy to see that the best achievable
competitive ratio is inﬁnite in some cases. For example, if some actions are irreversible—IRREVERSIBLE
i.e., they lead to a state from which no action leads back to the previous state—the online
search might accidentally reach a dead-end state from which no goal state is reachable. Per-DEAD END
haps the term “accidentally” is unconvincing—after all, there might be an algorithm that
happens not to take the dead-end path as it explores. Our claim, to be more precise, is thatno
algorithm can avoid dead ends in all state spaces. Consider the two dead-end state spaces in
Figure 4.20(a). To an online search algorithm that has visited states S and A, the two state
spaces look identical, so it must make the same decision in both. Therefore, it will fail in
one of them. This is an example of an adversary argument—we can imagine an adversaryADVERSARY
ARGUMENT
constructing the state space while the agent explores it and putting the goals and dead ends
wherever it chooses.
Dead ends are a real difﬁculty for robot exploration—staircases, ramps, cliffs, one-way
streets, and all kinds of natural terrain present opportunities for irreversible actions. To make
progress, we simply assume that the state space issafely explorable—that is, some goal state
SAFEL Y EXPLORABLE
is reachable from every reachable state. State spaces with reversible actions, such as mazes
and 8-puzzles, can be viewed as undirected graphs and are clearly safely explorable.
Even in safely explorable environments, no bounded competitive ratio can be guaran-
teed if there are paths of unbounded cost. This is easy to show in environments with irre-
versible actions, but in fact it remains true for the reversible case as well, as Figure 4.20(b)
shows. For this reason, it is common to describe the performance of online search algorithms
in terms of the size of the entire state space rather than just the depth of the shallowest goal.
4.5.2 Online search agents
After each action, an online agent receives a percept telling it what state it has reached; from
this information, it can augment its map of the environment. The current map is used to
decide where to go next. This interleaving of planning and action means that online search
algorithms are quite different from the ofﬂine search algorithms we have seen previously. For
example, ofﬂine algorithms such as A
∗ can expand a node in one part of the space and then
immediately expand a node in another part of the space, because node expansion involves
simulated rather than real actions. An online algorithm, on the other hand, can discover
successors only for a node that it physically occupies. To avoid traveling all the way across
the tree to expand the next node, it seems better to expand nodes in a local order. Depth-ﬁrst
search has exactly this property because (except when backtracking) the next node expanded
is a child of the previous node expanded.
An online depth-ﬁrst search agent is shown in Figure 4.21. This agent stores its map
in a table, R
ESULT [s,a], that records the state resulting from executing action a in state s.
Whenever an action from the current state has not been explored, the agent tries that action.
The difﬁculty comes when the agent has tried all the actions in a state. In ofﬂine depth-ﬁrst
search, the state is simply dropped from the queue; in an online search, the agent has to
backtrack physically. In depth-ﬁrst search, this means going back to the state from which the
agent most recently entered the current state. To achieve that, the algorithm keeps a table that
150 Chapter 4. Beyond Classical Search
function ONLINE -DFS-A GENT (s′) returns an action
inputs: s′, a percept that identiﬁes the current state
persistent: result, a table indexed by state and action, initially empty
untried, a table that lists, for each state, the actions not yet tried
unbacktracked, a table that lists, for each state, the backtracks not yet tried
s, a, the previous state and action, initially null
if GOAL -TEST (s′) then return stop
if s′ i san e ws t a t e( n o ti nuntried) then untried[s′]←ACTIONS (s′)
if s is not null then
result[s,a]←s′
add s to the front of unbacktracked[s′]
if untried[s′]i se m p t ythen
if unbacktracked[s′]i se m p t ythen return stop
else a←an action b such that result[s′,b]=P OP(unbacktracked[s′])
else a←POP(untried[s′])
s←s′
return a
Figure 4.21 An online search agent that uses depth-ﬁrst exploration. The agent is appli-
cable only in state spaces in which every action can be “undone” by some other action.
lists, for each state, the predecessor states to which the agent has not yet backtracked. If the
agent has run out of states to which it can backtrack, then its search is complete.
We recommend that the reader trace through the progress of O NLINE -DFS-A GENT
when applied to the maze given in Figure 4.19. It is fairly easy to see that the agent will, in
the worst case, end up traversing every link in the state space exactly twice. For exploration,
this is optimal; for ﬁnding a goal, on the other hand, the agent’s competitive ratio could be
arbitrarily bad if it goes off on a long excursion when there is a goal right next to the initial
state. An online variant of iterative deepening solves this problem; for an environment that is
a uniform tree, the competitive ratio of such an agent is a small constant.
Because of its method of backtracking, O
NLINE -DFS-A GENT works only in state
spaces where the actions are reversible. There are slightly more complex algorithms that
work in general state spaces, but no such algorithm has a bounded competitive ratio.
4.5.3 Online local search
Like depth-ﬁrst search, hill-climbing search has the property of locality in its node expan-
sions. In fact, because it keeps just one current state in memory, hill-climbing search is
already an online search algorithm! Unfortunately, it is not very useful in its simplest form
because it leaves the agent sitting at local maxima with nowhere to go. Moreover, random
restarts cannot be used, because the agent cannot transport itself to a new state.
Instead of random restarts, one might consider using a random walk to explore theRANDOM WALK
environment. A random walk simply selects at random one of the available actions from the
Section 4.5. Online Search Agents and Unknown Environments 151
S G
Figure 4.22 An environment in which a random walk will take exponentially many steps
to ﬁnd the goal.
current state; preference can be given to actions that have not yet been tried. It is easy to
prove that a random walk will eventually ﬁnd a goal or complete its exploration, provided
that the space is ﬁnite.14 On the other hand, the process can be very slow. Figure 4.22 shows
an environment in which a random walk will take exponentially many steps to ﬁnd the goal
because, at each step, backward progress is twice as likely as forward progress. The example
is contrived, of course, but there are many real-world state spaces whose topology causes
these kinds of “traps” for random walks.
Augmenting hill climbing with memory rather than randomness turns out to be a more
effective approach. The basic idea is to store a “current best estimate” H(s) of the cost to
reach the goal from each state that has been visited. H(s) starts out being just the heuristic
estimate h(s) and is updated as the agent gains experience in the state space. Figure 4.23
shows a simple example in a one-dimensional state space. In (a), the agent seems to be
stuck in a ﬂat local minimum at the shaded state. Rather than staying where it is, the agent
should follow what seems to be the best path to the goal given the current cost estimates for
its neighbors. The estimated cost to reach the goal through a neighbor s
′ is the cost to get
to s′ plus the estimated cost to get to a goal from there—that is, c(s, a, s′)+ H(s′).I n t h e
example, there are two actions, with estimated costs1+9 and 1+2 , so it seems best to move
right. Now, it is clear that the cost estimate of 2 for the shaded state was overly optimistic.
Since the best move cost 1 and led to a state that is at least 2 steps from a goal, the shaded
state must be at least 3 steps from a goal, so its H should be updated accordingly, as shown
in Figure 4.23(b). Continuing this process, the agent will move back and forth twice more,
updating H each time and “ﬂattening out” the local minimum until it escapes to the right.
An agent implementing this scheme, which is called learning real-time A
∗ (LRTA∗), isLRT A*
shown in Figure 4.24. Like O NLINE -DFS-A GENT , it builds a map of the environment in
the result table. It updates the cost estimate for the state it has just left and then chooses the
“apparently best” move according to its current cost estimates. One important detail is that
actions that have not yet been tried in a states are always assumed to lead immediately to the
goal with the least possible cost, namelyh(s).T h i soptimism under uncertainty encouragesOPTIMISM UNDER
UNCERT AINTY
the agent to explore new, possibly promising paths.
An LRTA∗ agent is guaranteed to ﬁnd a goal in any ﬁnite, safely explorable environment.
Unlike A∗, however, it is not complete for inﬁnite state spaces—there are cases where it can be
led inﬁnitely astray. It can explore an environment ofn states inO(n2) steps in the worst case,
14 Random walks are complete on inﬁnite one-dimensional and two-dimensional grids. On a three-dimensional
grid, the probability that the walk ever returns to the starting point is only about 0.3405 (Hughes, 1995).
152 Chapter 4. Beyond Classical Search
1
2
1 11 1 11
1 1 11 1 11
1 1 11 1 11
2
2
3
4
4
4
3
3
3
1 1 11 1 11
3
1 1 11 1 11
5
3
5
5
4
(a)
(b)
(c)
(d)
(e)
8 9
8
9
89
8
9
89
44
34
Figure 4.23 Five iterations of LRTA∗ on a one-dimensional state space. Each state is
labeled with H(s), the current cost estimate to reach a goal, and each link is labeled with its
step cost. The shaded state marks the location of the agent, and the updated cost estimates at
each iteration are circled.
function LRTA*-A GENT (s′) returns an action
inputs: s′, a percept that identiﬁes the current state
persistent: result, a table, indexed by state and action, initially empty
H , a table of cost estimates indexed by state, initially empty
s, a, the previous state and action, initially null
if GOAL -TEST (s′) then return stop
if s′ i san e ws t a t e( n o ti nH ) then H [s′]←h(s′)
if s is not null
result[s,a]←s′
H [s]← min
b ∈ ACTIONS (s)
LRTA*-C OST (s,b,result[s,b],H )
a←an action b in ACTIONS (s′) that minimizes LRTA*-C OST (s′,b,result[s′,b],H )
s←s′
return a
function LRTA*-C OST (s,a,s′,H ) returns a cost estimate
if s′ is undeﬁned then return h(s)
else return c(s, a, s′)+ H[s′]
Figure 4.24 LRTA*-A GENT selects an action according to the values of neighboring
states, which are updated as the agent moves about the state space.

Section 4.6. Summary 153
but often does much better. The LRTA∗ agent is just one of a large family of online agents that
one can deﬁne by specifying the action selection rule and the update rule in different ways.
We discuss this family, developed originally for stochastic environments, in Chapter 21.
4.5.4 Learning in online search
The initial ignorance of online search agents provides several opportunities for learning. First,
the agents learn a “map” of the environment—more precisely, the outcome of each action in
each state—simply by recording each of their experiences. (Notice that the assumption of
deterministic environments means that one experience is enough for each action.) Second,
the local search agents acquire more accurate estimates of the cost of each state by using local
updating rules, as in LRTA
∗. In Chapter 21, we show that these updates eventually converge
to exact values for every state, provided that the agent explores the state space in the right
way. Once exact values are known, optimal decisions can be taken simply by moving to the
lowest-cost successor—that is, pure hill climbing is then an optimal strategy.
If you followed our suggestion to trace the behavior of O
NLINE -DFS-A GENT in the
environment of Figure 4.19, you will have noticed that the agent is not very bright. For
example, after it has seen that the Up action goes from (1,1) to (1,2), the agent still has no
idea that the Down action goes back to (1,1) or that the Up action also goes from (2,1) to
(2,2), from (2,2) to (2,3), and so on. In general, we would like the agent to learn that Up
increases the y-coordinate unless there is a wall in the way, that Down reduces it, and so on.
For this to happen, we need two things. First, we need a formal and explicitly manipulable
representation for these kinds of general rules; so far, we have hidden the information inside
the black box called the R
ESULT function. Part III is devoted to this issue. Second, we need
algorithms that can construct suitable general rules from the speciﬁc observations made by
the agent. These are covered in Chapter 18.
4.6 S UMMARY
This chapter has examined search algorithms for problems beyond the “classical” case of
ﬁnding the shortest path to a goal in an observable, deterministic, discrete environment.
•Local search methods such as hill climbing operate on complete-state formulations,
keeping only a small number of nodes in memory. Several stochastic algorithms have
been developed, including simulated annealing, which returns optimal solutions when
given an appropriate cooling schedule.
•Many local search methods apply also to problems in continuous spaces. Linear pro-
gramming and convex optimization problems obey certain restrictions on the shape
of the state space and the nature of the objective function, and admit polynomial-time
algorithms that are often extremely efﬁcient in practice.
•A genetic algorithm is a stochastic hill-climbing search in which a large population of
states is maintained. New states are generated by mutation and by crossover,w h i c h
combines pairs of states from the population.
154 Chapter 4. Beyond Classical Search
•In nondeterministic environments, agents can apply AND –OR search to generate con-
tingent plans that reach the goal regardless of which outcomes occur during execution.
•When the environment is partially observable, the belief state represents the set of
possible states that the agent might be in.
•Standard search algorithms can be applied directly to belief-state space to solvesensor-
less problems, and belief-state AND –OR search can solve general partially observable
problems. Incremental algorithms that construct solutions state-by-state within a belief
state are often more efﬁcient.
•Exploration problems arise when the agent has no idea about the states and actions of
its environment. For safely explorable environments, online search agents can build a
map and ﬁnd a goal if one exists. Updating heuristic estimates from experience provides
an effective method to escape from local minima.
BIBLIOGRAPHICAL AND HISTORICAL NOTES
Local search techniques have a long history in mathematics and computer science. Indeed,
the Newton–Raphson method (Newton, 1671; Raphson, 1690) can be seen as a very efﬁ-
cient local search method for continuous spaces in which gradient information is available.
Brent (1973) is a classic reference for optimization algorithms that do not require such in-
formation. Beam search, which we have presented as a local search algorithm, originated
as a bounded-width variant of dynamic programming for speech recognition in the H
ARPY
system (Lowerre, 1976). A related algorithm is analyzed in depth by Pearl (1984, Ch. 5).
The topic of local search was reinvigorated in the early 1990s by surprisingly good re-
sults for large constraint-satisfaction problems such as n-queens (Minton et al. , 1992) and
logical reasoning (Selman et al. , 1992) and by the incorporation of randomness, multiple
simultaneous searches, and other improvements. This renaissance of what Christos Papadim-
itriou has called “New Age” algorithms also sparked increased interest among theoretical
computer scientists (Koutsoupias and Papadimitriou, 1992; Aldous and Vazirani, 1994). In
the ﬁeld of operations research, a variant of hill climbing calledtabu search has gained popu-
TABU SEARCH
larity (Glover and Laguna, 1997). This algorithm maintains a tabu list ofk previously visited
states that cannot be revisited; as well as improving efﬁciency when searching graphs, this list
can allow the algorithm to escape from some local minima. Another useful improvement on
hill climbing is the S TAGE algorithm (Boyan and Moore, 1998). The idea is to use the local
maxima found by random-restart hill climbing to get an idea of the overall shape of the land-
scape. The algorithm ﬁts a smooth surface to the set of local maxima and then calculates the
global maximum of that surface analytically. This becomes the new restart point. The algo-
rithm has been shown to work in practice on hard problems. Gomeset al. (1998) showed that
the run times of systematic backtracking algorithms often have a heavy-tailed distribution,
HEAVY -T AILED
DISTRIBUTION
which means that the probability of a very long run time is more than would be predicted if
the run times were exponentially distributed. When the run time distribution is heavy-tailed,
random restarts ﬁnd a solution faster, on average, than a single run to completion.
Bibliographical and Historical Notes 155
Simulated annealing was ﬁrst described by Kirkpatrick et al. (1983), who borrowed
directly from the Metropolis algorithm (which is used to simulate complex systems in
physics (Metropolis et al., 1953) and was supposedly invented at a Los Alamos dinner party).
Simulated annealing is now a ﬁeld in itself, with hundreds of papers published every year.
Finding optimal solutions in continuous spaces is the subject matter of several ﬁelds,
including optimization theory, optimal control theory,a n dt h ecalculus of variations.T h e
basic techniques are explained well by Bishop (1995); Press et al. (2007) cover a wide range
of algorithms and provide working software.
As Andrew Moore points out, researchers have taken inspiration for search and opti-
mization algorithms from a wide variety of ﬁelds of study: metallurgy (simulated annealing),
biology (genetic algorithms), economics (market-based algorithms), entomology (ant colony
optimization), neurology (neural networks), animal behavior (reinforcement learning), moun-
taineering (hill climbing), and others.
Linear programming (LP) was ﬁrst studied systematically by the Russian mathemati-
cian Leonid Kantorovich (1939). It was one of the ﬁrst applications of computers; the sim-
plex algorithm (Dantzig, 1949) is still used despite worst-case exponential complexity. Kar-
markar (1984) developed the far more efﬁcient family of interior-point methods, which was
shown to have polynomial complexity for the more general class of convex optimization prob-
lems by Nesterov and Nemirovski (1994). Excellent introductions to convex optimization are
provided by Ben-Tal and Nemirovski (2001) and Boyd and Vandenberghe (2004).
Work by Sewall Wright (1931) on the concept of a ﬁtness landscape was an impor-
tant precursor to the development of genetic algorithms. In the 1950s, several statisticians,
including Box (1957) and Friedman (1959), used evolutionary techniques for optimization
problems, but it wasn’t until Rechenberg (1965) introduced evolution strategies to solve op-
EVOLUTION
STRA TEGY
timization problems for airfoils that the approach gained popularity. In the 1960s and 1970s,
John Holland (1975) championed genetic algorithms, both as a useful tool and as a method
to expand our understanding of adaptation, biological or otherwise (Holland, 1995). The ar-
tiﬁcial life movement (Langton, 1995) takes this idea one step further, viewing the products
ARTIFICIAL LIFE
of genetic algorithms as organisms rather than solutions to problems. Work in this ﬁeld by
Hinton and Nowlan (1987) and Ackley and Littman (1991) has done much to clarify the im-
plications of the Baldwin effect. For general background on evolution, we recommend Smith
and Szathm´ary (1999), Ridley (2004), and Carroll (2007).
Most comparisons of genetic algorithms to other approaches (especially stochastic hill
climbing) have found that the genetic algorithms are slower to converge (O’Reilly and Op-
pacher, 1994; Mitchell et al., 1996; Juels and Wattenberg, 1996; Baluja, 1997). Such ﬁndings
are not universally popular within the GA community, but recent attempts within that com-
munity to understand population-based search as an approximate form of Bayesian learning
(see Chapter 20) might help close the gap between the ﬁeld and its critics (Pelikan et al. ,
1999). The theory of quadratic dynamical systems may also explain the performance of
GAs (Rabani et al., 1998). See Lohn et al. (2001) for an example of GAs applied to antenna
design, and Renner and Ekart (2003) for an application to computer-aided design.
The ﬁeld of genetic programming is closely related to genetic algorithms. The princi-
GENETIC
PROGRAMMING
pal difference is that the representations that are mutated and combined are programs rather
156 Chapter 4. Beyond Classical Search
than bit strings. The programs are represented in the form of expression trees; the expressions
can be in a standard language such as Lisp or can be specially designed to represent circuits,
robot controllers, and so on. Crossover involves splicing together subtrees rather than sub-
strings. This form of mutation guarantees that the offspring are well-formed expressions,
which would not be the case if programs were manipulated as strings.
Interest in genetic programming was spurred by John Koza’s work (Koza, 1992, 1994),
but it goes back at least to early experiments with machine code by Friedberg (1958) and
with ﬁnite-state automata by Fogel et al. (1966). As with genetic algorithms, there is debate
about the effectiveness of the technique. Koza et al. (1999) describe experiments in the use
of genetic programming to design circuit devices.
The journals Evolutionary Computation and IEEE Transactions on Evolutionary Com-
putation cover genetic algorithms and genetic programming; articles are also found in Com-
plex Systems , Adaptive Behavior ,a n d Artiﬁcial Life . The main conference is the Genetic
and Evolutionary Computation Conference (GECCO). Good overview texts on genetic algo-
rithms are given by Mitchell (1996), Fogel (2000), and Langdon and Poli (2002), and by the
free online book by Poli et al. (2008).
The unpredictability and partial observability of real environments were recognized
early on in robotics projects that used planning techniques, including Shakey (Fikes et al. ,
1972) and F
REDDY (Michie, 1974). The problems received more attention after the publica-
tion of McDermott’s (1978a) inﬂuential article, Planning and Acting.
The ﬁrst work to make explicit use ofAND –OR trees seems to have been Slagle’s SAINT
program for symbolic integration, mentioned in Chapter 1. Amarel (1967) applied the idea
to propositional theorem proving, a topic discussed in Chapter 7, and introduced a search
algorithm similar to A
ND-OR-GRAPH -SEARCH . The algorithm was further developed and
formalized by Nilsson (1971), who also described AO ∗—which, as its name suggests, ﬁnds
optimal solutions given an admissible heuristic. AO∗ was analyzed and improved by Martelli
and Montanari (1973). AO ∗ is a top-down algorithm; a bottom-up generalization of A ∗ is
A∗LD, for A∗ Lightest Derivation (Felzenszwalb and McAllester, 2007). Interest in AND –OR
search has undergone a revival in recent years, with new algorithms for ﬁnding cyclic solu-
tions (Jimenez and Torras, 2000; Hansen and Zilberstein, 2001) and new techniques inspired
by dynamic programming (Bonet and Geffner, 2005).
The idea of transforming partially observable problems into belief-state problems orig-
inated with Astrom (1965) for the much more complex case of probabilistic uncertainty (see
Chapter 17). Erdmann and Mason (1988) studied the problem of robotic manipulation with-
out sensors, using a continuous form of belief-state search. They showed that it was possible
to orient a part on a table from an arbitrary initial position by a well-designed sequence of tilt-
ing actions. More practical methods, based on a series of precisely oriented diagonal barriers
across a conveyor belt, use the same algorithmic insights (Wiegley et al., 1996).
The belief-state approach was reinvented in the context of sensorless and partially ob-
servable search problems by Genesereth and Nourbakhsh (1993). Additional work was done
on sensorless problems in the logic-based planning community (Goldman and Boddy, 1996;
Smith and Weld, 1998). This work has emphasized concise representations for belief states,
as explained in Chapter 11. Bonet and Geffner (2000) introduced the ﬁrst effective heuristics
Exercises 157
for belief-state search; these were reﬁned by Bryce et al. (2006). The incremental approach
to belief-state search, in which solutions are constructed incrementally for subsets of states
within each belief state, was studied in the planning literature by Kurienet al. (2002); several
new incremental algorithms were introduced for nondeterministic, partially observable prob-
lems by Russell and Wolfe (2005). Additional references for planning in stochastic, partially
observable environments appear in Chapter 17.
Algorithms for exploring unknown state spaces have been of interest for many centuries.
Depth-ﬁrst search in a maze can be implemented by keeping one’s left hand on the wall; loops
can be avoided by marking each junction. Depth-ﬁrst search fails with irreversible actions;
the more general problem of exploring Eulerian graphs (i.e., graphs in which each node has
EULERIAN GRAPH
equal numbers of incoming and outgoing edges) was solved by an algorithm due to Hierholzer
(1873). The ﬁrst thorough algorithmic study of the exploration problem for arbitrary graphs
was carried out by Deng and Papadimitriou (1990), who developed a completely general
algorithm but showed that no bounded competitive ratio is possible for exploring a general
graph. Papadimitriou and Yannakakis (1991) examined the question of ﬁnding paths to a goal
in geometric path-planning environments (where all actions are reversible). They showed that
a small competitive ratio is achievable with square obstacles, but with general rectangular
obstacles no bounded ratio can be achieved. (See Figure 4.20.)
The LRTA
∗ algorithm was developed by Korf (1990) as part of an investigation into
real-time search for environments in which the agent must act after searching for only aREAL-TIME SEARCH
ﬁxed amount of time (a common situation in two-player games). LRTA ∗ is in fact a special
case of reinforcement learning algorithms for stochastic environments (Bartoet al., 1995). Its
policy of optimism under uncertainty—always head for the closest unvisited state—can result
in an exploration pattern that is less efﬁcient in the uninformed case than simple depth-ﬁrst
search (Koenig, 2000). Dasgupta et al. (1994) show that online iterative deepening search is
optimally efﬁcient for ﬁnding a goal in a uniform tree with no heuristic information. Sev-
eral informed variants on the LRTA
∗ theme have been developed with different methods for
searching and updating within the known portion of the graph (Pemberton and Korf, 1992).
As yet, there is no good understanding of how to ﬁnd goals with optimal efﬁciency when
using heuristic information.
EXERCISES
4.1 Give the name of the algorithm that results from each of the following special cases:
a. Local beam search with k =1 .
b. Local beam search with one initial state and no limit on the number of states retained.
c. Simulated annealing with T =0 at all times (and omitting the termination test).
d. Simulated annealing with T =∞at all times.
e. Genetic algorithm with population size N =1 .
158 Chapter 4. Beyond Classical Search
4.2 Exercise 3.16 considers the problem of building railway tracks under the assumption
that pieces ﬁt exactly with no slack. Now consider the real problem, in which pieces don’t
ﬁt exactly but allow for up to 10 degrees of rotation to either side of the “proper” alignment.
Explain how to formulate the problem so it could be solved by simulated annealing.
4.3 In this exercise, we explore the use of local search methods to solve TSPs of the type
deﬁned in Exercise 3.30.
a. Implement and test a hill-climbing method to solve TSPs. Compare the results with op-
timal solutions obtained from the A∗ algorithm with the MST heuristic (Exercise 3.30).
b. Repeat part (a) using a genetic algorithm instead of hill climbing. You may want to
consult Larra˜naga et al. (1999) for some suggestions for representations.
4.4 Generate a large number of 8-puzzle and 8-queens instances and solve them (where pos-
sible) by hill climbing (steepest-ascent and ﬁrst-choice variants), hill climbing with random
restart, and simulated annealing. Measure the search cost and percentage of solved problems
and graph these against the optimal solution cost. Comment on your results.
4.5 The A
ND-OR-GRAPH -SEARCH algorithm in Figure 4.11 checks for repeated states
only on the path from the root to the current state. Suppose that, in addition, the algorithm
were to store every visited state and check against that list. (See B
READTH -FIRST-SEARCH
in Figure 3.11 for an example.) Determine the information that should be stored and how the
algorithm should use that information when a repeated state is found. (Hint: You will need to
distinguish at least between states for which a successful subplan was constructed previously
and states for which no subplan could be found.) Explain how to use labels, as deﬁned in
Section 4.3.3, to avoid having multiple copies of subplans.
4.6 Explain precisely how to modify the AND-OR-GRAPH -SEARCH algorithm to generate
a cyclic plan if no acyclic plan exists. You will need to deal with three issues: labeling the plan
steps so that a cyclic plan can point back to an earlier part of the plan, modifying OR-SEARCH
so that it continues to look for acyclic plans after ﬁnding a cyclic plan, and augmenting the
plan representation to indicate whether a plan is cyclic. Show how your algorithm works on
(a) the slippery vacuum world, and (b) the slippery, erratic vacuum world. You might wish to
use a computer implementation to check your results.
4.7 In Section 4.4.1 we introduced belief states to solve sensorless search problems. A
sequence of actions solves a sensorless problem if it maps every physical state in the initial
belief state b to a goal state. Suppose the agent knows h
∗(s), the true optimal cost of solving
the physical state s in the fully observable problem, for every states in b. Find an admissible
heuristic h(b) for the sensorless problem in terms of these costs, and prove its admissibilty.
Comment on the accuracy of this heuristic on the sensorless vacuum problem of Figure 4.14.
How well does A
∗ perform?
4.8 This exercise explores subset–superset relations between belief states in sensorless or
partially observable environments.
a. Prove that if an action sequence is a solution for a belief state b, it is also a solution for
any subset of b. Can anything be said about supersets of b?
Exercises 159
b. Explain in detail how to modify graph search for sensorless problems to take advantage
of your answers in (a).
c. Explain in detail how to modify AND –OR search for partially observable problems,
beyond the modiﬁcations you describe in (b).
4.9 On page 139 it was assumed that a given action would have the same cost when ex-
ecuted in any physical state within a given belief state. (This leads to a belief-state search
problem with well-deﬁned step costs.) Now consider what happens when the assumption
does not hold. Does the notion of optimality still make sense in this context, or does it require
modiﬁcation? Consider also various possible deﬁnitions of the “cost” of executing an action
in a belief state; for example, we could use the minimum of the physical costs; or the maxi-
mum; or a cost interval with the lower bound being the minimum cost and the upper bound
being the maximum; or just keep the set of all possible costs for that action. For each of these,
explore whether A
∗ (with modiﬁcations if necessary) can return optimal solutions.
4.10 Consider the sensorless version of the erratic vacuum world. Draw the belief-state
space reachable from the initial belief state{1,2,3,4,5,6,7, 8}, and explain why the problem
is unsolvable.
4.11 We can turn the navigation problem in Exercise 3.7 into an environment as follows:
•The percept will be a list of the positions, relative to the agent , of the visible vertices.
The percept does not include the position of the robot! The robot must learn its own po-
sition from the map; for now, you can assume that each location has a different “view.”
•Each action will be a vector describing a straight-line path to follow. If the path is
unobstructed, the action succeeds; otherwise, the robot stops at the point where its
path ﬁrst intersects an obstacle. If the agent returns a zero motion vector and is at the
goal (which is ﬁxed and known), then the environment teleports the agent to a random
location (not inside an obstacle).
•The performance measure charges the agent 1 point for each unit of distance traversed
and awards 1000 points each time the goal is reached.
a. Implement this environment and a problem-solving agent for it. After each teleporta-
tion, the agent will need to formulate a new problem, which will involve discovering its
current location.
b. Document your agent’s performance (by having the agent generate suitable commentary
as it moves around) and report its performance over 100 episodes.
c. Modify the environment so that 30% of the time the agent ends up at an unintended
destination (chosen randomly from the other visible vertices if any; otherwise, no move
at all). This is a crude model of the motion errors of a real robot. Modify the agent
so that when such an error is detected, it ﬁnds out where it is and then constructs a
plan to get back to where it was and resume the old plan. Remember that sometimes
getting back to where it was might also fail! Show an example of the agent successfully
overcoming two successive motion errors and still reaching the goal.
160 Chapter 4. Beyond Classical Search
d. Now try two different recovery schemes after an error: (1) head for the closest vertex on
the original route; and (2) replan a route to the goal from the new location. Compare the
performance of the three recovery schemes. Would the inclusion of search costs affect
the comparison?
e. Now suppose that there are locations from which the view is identical. (For example,
suppose the world is a grid with square obstacles.) What kind of problem does the agent
now face? What do solutions look like?
4.12 Suppose that an agent is in a 3× 3 maze environment like the one shown in Fig-
ure 4.19. The agent knows that its initial location is (1,1), that the goal is at (3,3), and that the
actions Up, Down, Left, Right have their usual effects unless blocked by a wall. The agent
does not know where the internal walls are. In any given state, the agent perceives the set of
legal actions; it can also tell whether the state is one it has visited before.
a. Explain how this online search problem can be viewed as an ofﬂine search in belief-state
space, where the initial belief state includes all possible environment conﬁgurations.
How large is the initial belief state? How large is the space of belief states?
b. How many distinct percepts are possible in the initial state?
c. Describe the ﬁrst few branches of a contingency plan for this problem. How large
(roughly) is the complete plan?
Notice that this contingency plan is a solution forevery possible environment ﬁtting the given
description. Therefore, interleaving of search and execution is not strictly necessary even in
unknown environments.
4.13 In this exercise, we examine hill climbing in the context of robot navigation, using the
environment in Figure 3.31 as an example.
a. Repeat Exercise 4.11 using hill climbing. Does your agent ever get stuck in a local
minimum? Is it possible for it to get stuck with convex obstacles?
b. Construct a nonconvex polygonal environment in which the agent gets stuck.
c. Modify the hill-climbing algorithm so that, instead of doing a depth-1 search to decide
where to go next, it does a depth- k search. It should ﬁnd the best k-step path and do
one step along it, and then repeat the process.
d.I s t h e r e s o m ek for which the new algorithm is guaranteed to escape from local minima?
e. Explain how LRTA∗ enables the agent to escape from local minima in this case.
4.14 Like DFS, online DFS is incomplete for reversible state spaces with inﬁnite paths. For
example, suppose that states are points on the inﬁnite two-dimensional grid and actions are
unit vectors (1,0), (0,1), (−1,0), (0,−1), tried in that order. Show that online DFS starting
at (0,0) will not reach (1,−1). Suppose the agent can observe, in addition to its current
state, all successor states and the actions that would lead to them. Write an algorithm that
is complete even for bidirected state spaces with inﬁnite paths. What states does it visit in
reaching (1,−1)?


END_INSTRUCTION
