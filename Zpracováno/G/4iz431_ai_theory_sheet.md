# 4IZ431 AI Theory Sheet

## 1. Search Algorithms

### Breadth-First Search (BFS)
- **Definition:** An uninformed search strategy that expands the shallowest nodes in the search tree first. It uses a **FIFO (First-In-First-Out) Queue** to store the frontier.
- **Properties:**
    -   **Complete:** Yes (if branching factor $b$ is finite).
    -   **Optimal:** Yes (if path cost is a non-decreasing function of the depth of the node, e.g., all step costs are equal).
    -   **Time Complexity:** $O(b^d)$ where $d$ is the depth of the solution.
    -   **Space Complexity:** $O(b^d)$ (keeps all nodes in memory).

### Depth-First Search (DFS)
- **Definition:** An uninformed search strategy that expands the deepest node in the current frontier of the search tree. It uses a **LIFO (Last-In-First-Out) Stack** to store the frontier.
- **Properties:**
    -   **Complete:** No (fails in infinite spaces or loops without check).
    -   **Optimal:** No (can find a deeper, more expensive solution first).
    -   **Time Complexity:** $O(b^m)$ where $m$ is the maximum depth.
    -   **Space Complexity:** $O(b \cdot m)$ (linear space).

### Uniform Cost Search (UCS)
- **Definition:** An uninformed search strategy that expands the node with the lowest path cost $g(n)$ from the frontier. It uses a **Priority Queue** ordered by $g(n)$.
- **Properties:**
    -   **Complete:** Yes (if step costs $\ge \epsilon > 0$).
    -   **Optimal:** Yes.
    -   **Time/Space:** $O(b^{C^*/\epsilon})$ where $C^*$ is optimal cost.

### A* Search
- **Definition:** An informed best-first search that uses an evaluation function $f(n) = g(n) + h(n)$, where:
    -   $g(n)$: Cost from start to node $n$.
    -   $h(n)$: Estimated cost from node $n$ to goal (heuristic).
-   **Algorithm:** Expands the node with lowest $f(n)$.
-   **Optimality:** Guaranteed if the heuristic is Admissible (for Tree Search) or Consistent (for Graph Search).

### Heuristics: Admissible vs. Consistent
-   **Admissible Heuristic:**
    -   A heuristic $h(n)$ is admissible if it *never overestimates* the cost to reach the goal.
    -   **Formula:** $0 \le h(n) \le h^*(n)$, where $h^*(n)$ is the true optimal cost from $n$ to goal.
    -   **Benefit:** Ensures optimality in Tree Search.
-   **Consistent Heuristic (Monotonicity):**
    -   A heuristic is consistent if, for every node $n$ and every successor $n'$ generated by action $a$, the estimated cost of reaching the goal from $n$ is no greater than the step cost of getting to $n'$ plus the estimated cost from $n'$ to the goal.
    -   **Formula:** $h(n) \le c(n, a, n') + h(n')$
    -   **Benefit:** Ensures optimality in Graph Search (no need to re-open closed nodes). Consistency implies admissibility.

---

## 2. Game Theory / Multi-Agent

### Minimax Algorithm
-   **Definition:** A recursive algorithm for choosing the next move in a two-player, zero-sum game (MAX vs. MIN). MAX tries to maximize the utility value, while MIN tries to minimize it.
-   **Pseudocode Concept:**
    ```
    function MINIMAX-DECISION(state) returns an action
      return argmax_[a in ACTIONS(s)] MIN-VALUE(RESULT(state, a))

    function MAX-VALUE(state)
      if TERMINAL-TEST(state) return UTILITY(state)
      v = -infinity
      for each a in ACTIONS(state) do
        v = MAX(v, MIN-VALUE(RESULT(state, a)))
      return v

    function MIN-VALUE(state)
      if TERMINAL-TEST(state) return UTILITY(state)
      v = infinity
      for each a in ACTIONS(state) do
        v = MIN(v, MAX-VALUE(RESULT(state, a)))
      return v
    ```

### Alpha-Beta Pruning
-   **Definition:** An optimization technique for Minimax that prunes away branches in the game tree that cannot possibly influence the final decision.
-   **Parameters:**
    -   $\alpha$: The value of the best (highest-value) choice found so far at any choice point along the path for MAX.
    -   $\beta$: The value of the best (lowest-value) choice found so far at any choice point along the path for MIN.
-   **Pruning Condition:**
    -   Prune if $\alpha \ge \beta$.
-   **Benefits:** Does not affect the result. Can double the search depth reachable in the same amount of time ($O(b^{m/2})$ instead of $O(b^m)$ in best case).

### Expectimax
-   **Definition:** An adaptation of Minimax for games with chance/random elements (e.g., dice, stochastic ghosts in Pacman).
-   **Mechanism:**
    -   **MAX nodes:** Take the maximum value of successors (like Minimax).
    -   **CHANCE nodes:** Take the *weighted average* (expected value) of successors based on probability distribution.
    -   **Formula for Chance Node:** $V(s) = \sum_{s'} P(s'|s, a) V(s')$
-   **Note:** Expectimax does not prune efficiently (pruning is only possible if bounds on utility values are known).

---

## 3. Reinforcement Learning

### Q-Learning Update Formula
-   **Definition:** An off-policy TD (Temporal Difference) control algorithm that learns the value of the optimal policy independently of the agent's actions.
-   **Formula:**
    $$Q(s,a) \leftarrow (1-\alpha)Q(s,a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') \right]$$
    -   Or equivalently: $Q(s,a) \leftarrow Q(s,a) + \alpha [ \text{difference} ]$
    -   Where:
        -   $\alpha$: Learning rate ($0 < \alpha \le 1$).
        -   $\gamma$: Discount factor ($0 \le \gamma \le 1$).
        -   $r$: Reward received.
        -   $\max_{a'} Q(s', a')$: Estimate of optimal future value (greedy).

### Value Iteration vs. Policy Iteration
-   **Value Iteration (VI):**
    -   **Algorithm:** Iteratively updates utility values $V(s)$ for all states until convergence. The policy is derived from the final values.
    -   **Update Rule (Bellman Optimality Update):**
        $$V_{k+1}(s) \leftarrow \max_a \sum_{s'} T(s,a,s') [R(s,a,s') + \gamma V_k(s')]$$
-   **Policy Iteration (PI):**
    -   **Algorithm:** Alternates between two steps:
        1.  **Policy Evaluation:** Calculate $V^\pi(s)$ for the current policy $\pi$ (solving linear equations or iterative evaluation).
        2.  **Policy Improvement:** Update $\pi(s)$ to be greedy with respect to new $V^\pi(s)$.
    -   **Comparison:** PI often converges in fewer iterations than VI, but each iteration is more computationally expensive (solving systems of equations).

### Epsilon-greedy Exploration
-   **Definition:** A strategy to balance exploration and exploitation.
-   **Mechanism:**
    -   With probability $1 - \epsilon$: **Exploit** (choose the action with the highest current Q-value).
    -   With probability $\epsilon$: **Explore** (choose a random action uniformly).
-   **Purpose:** Ensures the agent continues to try new actions to find potentially better rewards (exploration) while maximizing reward based on current knowledge (exploitation).

---

## 4. Key Concepts from Papers

### BERT (from cv08 paper: Devlin et al.)
-   **Full Name:** Bidirectional Encoder Representations from Transformers.
-   **Core Innovation:** Designed to pre-train deep **bidirectional** representations from unlabeled text by jointly conditioning on both left and right context in all layers.
-   **Pre-training Tasks:**
    1.  **Masked Language Model (MLM):** Randomly masks 15% of input tokens and predicts them based on context (allows bidirectional training unlike left-to-right LMs).
    2.  **Next Sentence Prediction (NSP):** Predicts if sentence B actually follows sentence A (helps with sentence-pair tasks like QA).
-   **Usage:** Two steps: **Pre-training** (on large corpus) and **Fine-tuning** (on specific downstream tasks with minimal architectural changes).

### Deep RL (from cv10 paper: Mnih et al.)
-   **Contribution:** The **Deep Q-Network (DQN)** agent that learns control policies directly from high-dimensional sensory input (raw pixels) using reinforcement learning.
-   **Key Innovations:**
    1.  **Deep Convolutional Neural Network (CNN):** Approximate the Q-value function $Q(s, a; \theta)$. Input is raw pixels (84x84x4 after preprocessing).
    2.  **Experience Replay:** Stores transitions $(s, a, r, s')$ in a replay memory buffer and samples *random minibatches* for training. This breaks correlations between consecutive samples and smooths data distribution.
    3.  **Target Network:** Uses a separate network $\hat{Q}$ to generate target values, updated periodically. Improves stability.
-   **Result:** Achieved human-level performance on Atari 2600 games.

### Samuel's Checkers (from cv06 paper: A. L. Samuel)
-   **Overview:** Two machine-learning procedures applied to the game of checkers (1959).
-   **Rote Learning:**
    -   Saves board positions and their backed-up scores.
    -   Uses "effective ply" (looking up a saved board effectively extends the search depth).
    -   Required cataloging and "forgetting" (culling) to manage memory.
-   **Generalization Learning:**
    -   Learns a **scoring polynomial** (evaluation function) $V(s) = \sum w_i f_i(s)$.
    -   Adjusts weights $w_i$ based on the difference between the current evaluation and the backed-up evaluation from a look-ahead search ("delta").
    -   Alpha-Beta pruning (referred to as "looking ahead" with pruning conditions) was utilized to deepen the search.
-   **Outcome:** The program learned to play a better-than-average game, verifying that computers can learn to outperform their programmers.
